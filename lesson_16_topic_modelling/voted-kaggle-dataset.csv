Title,Subtitle,Owner,Votes,Versions,Tags,Data Type,Size,License,Views,Download,Kernels,Topics,URL,Description
Credit Card Fraud Detection,Anonymized credit card transactions labeled as fraudulent or genuine,Machine Learning Group - ULB,1241,"Version 2,2016-11-05|Version 1,2016-11-03","crime
finance",CSV,144 MB,ODbL,"442,136 views","53,128 downloads","1,782 kernels",26 topics,https://www.kaggle.com/mlg-ulb/creditcardfraud,"The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.
It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.
Given the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification.
The dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http://mlg.ulb.ac.be) of ULB (Université Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available on http://mlg.ulb.ac.be/BruFence and http://mlg.ulb.ac.be/ARTML
Please cite: Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015"
European Soccer Database,"25k+ matches, players & teams attributes for European Professional Football",Hugo Mathien,1046,"Version 10,2016-10-24|Version 9,2016-10-24|Version 8,2016-10-17|Version 7,2016-10-16|Version 6,2016-07-20|Version 5,2016-07-15|Version 4,2016-07-15|Version 3,2016-07-13|Version 2,2016-07-12|Version 1,2016-07-10","association football
europe",SQLite,299 MB,ODbL,"396,214 views","46,367 downloads","1,459 kernels",75 topics,https://www.kaggle.com/hugomathien/soccer,"The ultimate Soccer database for data analysis and machine learning
What you get:
+25,000 matches
+10,000 players
11 European Countries with their lead championship
Seasons 2008 to 2016
Players and Teams' attributes* sourced from EA Sports' FIFA video game series, including the weekly updates
Team line up with squad formation (X, Y coordinates)
Betting odds from up to 10 providers
Detailed match events (goal types, possession, corner, cross, fouls, cards etc...) for +10,000 matches
*16th Oct 2016: New table containing teams' attributes from FIFA !
Original Data Source:
You can easily find data about soccer matches but they are usually scattered across different websites. A thorough data collection and processing has been done to make your life easier. I must insist that you do not make any commercial use of the data. The data was sourced from:
http://football-data.mx-api.enetscores.com/ : scores, lineup, team formation and events
http://www.football-data.co.uk/ : betting odds. Click here to understand the column naming system for betting odds:
http://sofifa.com/ : players and teams attributes from EA Sports FIFA games. FIFA series and all FIFA assets property of EA Sports.
When you have a look at the database, you will notice foreign keys for players and matches are the same as the original data sources. I have called those foreign keys ""api_id"".
Improving the dataset:
You will notice that some players are missing from the lineup (NULL values). This is because I have not been able to source their attributes from FIFA. This will be fixed overtime as the crawling algorithm is being improved. The dataset will also be expanded to include international games, national cups, Champion's League and Europa League. Please ask me if you're after a specific tournament.
Please get in touch with me if you want to help improve this dataset.
CLICK HERE TO ACCESS THE PROJECT GITHUB
Important note for people interested in using the crawlers: since I first wrote the crawling scripts (in python), it appears sofifa.com has changed its design and with it comes new requirements for the scripts. The existing script to crawl players ('Player Spider') will not work until i've updated it.
Exploring the data:
Now that's the fun part, there is a lot you can do with this dataset. I will be adding visuals and insights to this overview page but please have a look at the kernels and give it a try yourself ! Here are some ideas for you:
The Holy Grail... ... is obviously to predict the outcome of the game. The bookies use 3 classes (Home Win, Draw, Away Win). They get it right about 53% of the time. This is also what I've achieved so far using my own SVM. Though it may sound high for such a random sport game, you've got to know that the home team wins about 46% of the time. So the base case (constantly predicting Home Win) has indeed 46% precision.
Probabilities vs Odds
When running a multi-class classifier like SVM you could also output a probability estimate and compare it to the betting odds. Have a look at your variance vs odds and see for what games you had very different predictions.
Explore and visualize features
With access to players and teams attributes, team formations and in-game events you should be able to produce some interesting insights into The Beautiful Game . Who knows, Guardiola himself may hire one of you some day!"
TMDB 5000 Movie Dataset,"Metadata on ~5,000 movies from TMDb",The Movie Database (TMDb),1024,"Version 2,2017-09-28",film,CSV,44 MB,Other,"446,255 views","62,002 downloads","1,394 kernels",46 topics,https://www.kaggle.com/tmdb/tmdb-movie-metadata,"Background
What can we say about the success of a movie before it is released? Are there certain companies (Pixar?) that have found a consistent formula? Given that major films costing over $100 million to produce can still flop, this question is more important than ever to the industry. Film aficionados might have different interests. Can we predict which films will be highly rated, whether or not they are a commercial success?
This is a great place to start digging in to those questions, with data on the plot, cast, crew, budget, and revenues of several thousand films.
Data Source Transfer Summary
We (Kaggle) have removed the original version of this dataset per a DMCA takedown request from IMDB. In order to minimize the impact, we're replacing it with a similar set of films and data fields from The Movie Database (TMDb) in accordance with their terms of use. The bad news is that kernels built on the old dataset will most likely no longer work.
The good news is that:
You can port your existing kernels over with a bit of editing. This kernel offers functions and examples for doing so. You can also find a general introduction to the new format here.
The new dataset contains full credits for both the cast and the crew, rather than just the first three actors.
Actor and actresses are now listed in the order they appear in the credits. It's unclear what ordering the original dataset used; for the movies I spot checked it didn't line up with either the credits order or IMDB's stars order.
The revenues appear to be more current. For example, IMDB's figures for Avatar seem to be from 2010 and understate the film's global revenues by over $2 billion.
Some of the movies that we weren't able to port over (a couple of hundred) were just bad entries. For example, this IMDB entry has basically no accurate information at all. It lists Star Wars Episode VII as a documentary.
Data Source Transfer Details
Several of the new columns contain json. You can save a bit of time by porting the load data functions from this kernel.
Even in simple fields like runtime may not be consistent across versions. For example, previous dataset shows the duration for Avatar's extended cut while TMDB shows the time for the original version.
There's now a separate file containing the full credits for both the cast and crew.
All fields are filled out by users so don't expect them to agree on keywords, genres, ratings, or the like.
Your existing kernels will continue to render normally until they are re-run.
If you are curious about how this dataset was prepared, the code to access TMDb's API is posted here.
New columns:
homepage
id
original_title
overview
popularity
production_companies
production_countries
release_date
spoken_languages
status
tagline
vote_average
Lost columns:
actor_1_facebook_likes
actor_2_facebook_likes
actor_3_facebook_likes
aspect_ratio
cast_total_facebook_likes
color
content_rating
director_facebook_likes
facenumber_in_poster
movie_facebook_likes
movie_imdb_link
num_critic_for_reviews
num_user_for_reviews
Open Questions About the Data
There are some things we haven't had a chance to confirm about the new dataset. If you have any insights, please let us know in the forums!
Are the budgets and revenues all in US dollars? Do they consistently show the global revenues?
This dataset hasn't yet gone through a data quality analysis. Can you find any obvious corrections? For example, in the IMDb version it was necessary to treat values of zero in the budget field as missing. Similar findings would be very helpful to your fellow Kagglers! (It's probably a good idea to keep treating zeros as missing, with the caveat that missing budgets much more likely to have been from small budget films in the first place).
Inspiration
Can you categorize the films by type, such as animated or not? We don't have explicit labels for this, but it should be possible to build them from the crew's job titles.
How sharp is the divide between major film studios and the independents? Do those two groups fall naturally out of a clustering analysis or is something more complicated going on?
Acknowledgements
This dataset was generated from The Movie Database API. This product uses the TMDb API but is not endorsed or certified by TMDb. Their API also provides access to data on many additional movies, actors and actresses, crew members, and TV shows. You can try it for yourself here."
Global Terrorism Database,"More than 170,000 terrorist attacks worldwide, 1970-2016",START Consortium,789,"Version 2,2017-07-19|Version 1,2016-12-08","crime
terrorism
international relations",CSV,144 MB,Other,"187,877 views","26,309 downloads",608 kernels,11 topics,https://www.kaggle.com/START-UMD/gtd,"Context
Information on more than 170,000 Terrorist Attacks
The Global Terrorism Database (GTD) is an open-source database including information on terrorist attacks around the world from 1970 through 2016 (with annual updates planned for the future). The GTD includes systematic data on domestic as well as international terrorist incidents that have occurred during this time period and now includes more than 170,000 cases. The database is maintained by researchers at the National Consortium for the Study of Terrorism and Responses to Terrorism (START), headquartered at the University of Maryland. More Information
Content
Geography: Worldwide
Time period: 1970-2016, except 1993 (2017 in progress, publication expected June 2018)
Unit of analysis: Attack
Variables: >100 variables on location, tactics, perpetrators, targets, and outcomes
Sources: Unclassified media articles (Note: Please interpret changes over time with caution. Global patterns are driven by diverse trends in particular regions, and data collection is influenced by fluctuations in access to media coverage over both time and place.)
Definition of terrorism:
""The threatened or actual use of illegal force and violence by a non-state actor to attain a political, economic, religious, or social goal through fear, coercion, or intimidation.""
See the GTD Codebook for important details on data collection methodology, definitions, and coding schema.
Acknowledgements
The Global Terrorism Database is funded through START, by the US Department of State (Contract Number: SAQMMA12M1292) and the US Department of Homeland Security Science and Technology Directorate’s Office of University Programs (Award Number 2012-ST-061-CS0001, CSTAB 3.1). The coding decisions and classifications contained in the database are determined independently by START researchers and should not be interpreted as necessarily representing the official views or policies of the United States Government.
GTD Team
Publications
The GTD has been leveraged extensively in scholarly publications, reports, and media articles. Putting Terrorism in Context: Lessons from the Global Terrorism Database, by GTD principal investigators LaFree, Dugan, and Miller investigates patterns of terrorism and provides perspective on the challenges of data collection and analysis. The GTD's data collection manager, Michael Jensen, discusses important Benefits and Drawbacks of Methodological Advancements in Data Collection and Coding.
Terms of Use
Use of the data signifies your agreement to the following terms and conditions.
Definitions: Within this section: ""GTD"" will refer to the Global Terrorism Database produced by the National Consortium for the Study of Terrorism and Responses to Terrorism. This includes the data and codebook, any auxiliary materials present, and the World Wide Web interface by which the data are presented. ""START"" will refer to the National Consortium for the Study of Terrorism and Responses to Terrorism, a United States Department of Homeland Security Center of Excellence based at the University of Maryland. ""USER"" denotes the individual or set of individuals who access the GTD, i.e. the data, codebook, any auxiliary materials, and the World Wide Web interface by which the data are presented. ""GTD representatives"" denotes any senior management staff of START, and any employee or representative of said organization whom senior management staff designate to represent START in dealings with the USER.
Usage Rights: Pursuant to this agreement, START grants the USER the non-exclusive, non-guaranteed right to search, browse, and view all contents of the GTD World Wide Web interface.
Authorship: All contents of the GTD were assembled by representatives of START and do not purport to reflect the official position or data collections of the Department of Homeland Security or any other agency of the United States government.
Acknowledgement: All information sourced from the GTD should be acknowledged by the USER and cited as follows: ""National Consortium for the Study of Terrorism and Responses to Terrorism (START). (2017). Global Terrorism Database [Data file]. Retrieved from https://www.kaggle.com/START-UMD/gtd""
Unauthorized Publication of the Data: No part of the GTD may be republished on any website or accessible for public download in any format without the express permission of a GTD staff member. In addition, no part of the GTD may be distributed for any commercial purpose, nor with the intent that the data be used in any commercial enterprise, without the express permission of a GTD staff member. START reserves the right to withhold this permission.
Penalties: Penalties for failure to comply with the terms of this agreement may result in loss of access to the GTD and the forfeiture of user privileges, in addition to any other appropriate legal remedies.
Limitation of Liability: Although every reasonable effort has been made to check sources and verify facts, START cannot guarantee that accounts reported in the open literature are complete and accurate. START shall not be held liable for any loss or damage caused by errors or omissions or resulting from any use, misuse, or alteration of GTD data by the USER. The USER should not infer any additional actions or results beyond what is presented in a GTD entry and specifically, the USER should not infer an individual associated with a particular incident was tried and convicted of terrorism or any other criminal offense. If new documentation about an event becomes available, an entry may be modified, as necessary and appropriate.
Termination of Rights: The GTD developers reserve the right to remove access to the GTD website from any particular IP address or set of IP addresses, or to remove the database entirely from public access, at their discretion. In such an event, all USER rights granted in this document are terminated.
Training
START has released the first in a series of training modules designed to equip GTD users with the knowledge and tools to best leverage the database. This training module provides a general overview of the GTD, including the data collection process, uses of the GTD, and patterns of global terrorism. Participants will learn basic data handling and how to generate summary statistics from the GTD using PivotTables in Microsoft Excel.
Questions?
Find answers to Frequently Asked Questions.
Contact the GTD staff at gtd@start.umd.edu."
Bitcoin Historical Data,"Bitcoin data at 1-min intervals from select exchanges, Jan 2012 to Jan 2018",Zielak,618,"Version 11,2018-01-11|Version 10,2017-11-17|Version 9,2017-10-24|Version 8,2017-10-22|Version 7,2017-10-22|Version 6,2017-06-12|Version 5,2017-06-08|Version 4,2017-06-06|Version 3,2017-06-03|Version 2,2017-06-02|Version 1,2017-06-02","history
finance",CSV,119 MB,CC4,"146,734 views","16,868 downloads",68 kernels,13 topics,https://www.kaggle.com/mczielinski/bitcoin-historical-data,"Context
Bitcoin is the longest running and most well known cryptocurrency, first released as open source in 2009 by the anonymous Satoshi Nakamoto. Bitcoin serves as a decentralized medium of digital exchange, with transactions verified and recorded in a public distributed ledger (the blockchain) without the need for a trusted record keeping authority or central intermediary. Transaction blocks contain a SHA-256 cryptographic hash of previous transaction blocks, and are thus ""chained"" together, serving as an immutable record of all transactions that have ever occurred. As with any currency/commodity on the market, bitcoin trading and financial instruments soon followed public adoption of bitcoin and continue to grow. Included here is historical bitcoin market data at 1-min intervals for select bitcoin exchanges where trading takes place. Happy (data) mining!
Content
coincheckJPY_1-min_data_2014-10-31_to_2018-01-08.csv
bitflyerJPY_1-min_data_2017-07-04_to_2018-01-08.csv
coinbaseUSD_1-min_data_2014-12-01_to_2018-01-08.csv
bitstampUSD_1-min_data_2012-01-01_to_2018-01-08.csv
CSV files for select bitcoin exchanges for the time period of Jan 2012 to Jan 2018, with minute to minute updates of OHLC (Open, High, Low, Close), Volume in BTC and indicated currency, and weighted bitcoin price. Timestamps are in Unix time. Timestamps without any trades or activity have their data fields populated with NaNs. If a timestamp is missing, or if there are jumps, this may be because the exchange (or its API) was down, the exchange (or its API) did not exist, or some other unforseen technical error in data reporting or gathering. All effort has been made to deduplicate entries and verify the contents are correct and complete to the best of my ability, but obviously trust at your own risk.
Acknowledgements and Inspiration
The various exchange APIs, for making it difficult or unintuitive enough to get OHLC and volume data at 1-min intervals that I set out on this data scraping project. Satoshi Nakamoto and the novel core concept of the blockchain, as well as its first execution via the bitcoin protocol. I'd also like to thank viewers like you! Can't wait to see what code or insights you all have to share.
I am a lowly Ph.D. student who did this for fun in my meager spare time. If you find this data interesting and you can spare a coffee to fuel my science, send it my way and I'd be immensely grateful!
1kmWmcQa8qN9ZrdGfdkw8EHKBgugKBRcF"
"Kaggle ML and Data Science Survey, 2017",A big picture view of the state of data science and machine learning.,Kaggle,574,"Version 4,2017-10-28|Version 3,2017-10-03|Version 2,2017-10-03|Version 1,2017-09-29","employment
sociology
artificial intelligence",CSV,28 MB,ODbL,"95,587 views","9,390 downloads",244 kernels,10 topics,https://www.kaggle.com/kaggle/kaggle-survey-2017,"Context
For the first time, Kaggle conducted an industry-wide survey to establish a comprehensive view of the state of data science and machine learning. The survey received over 16,000 responses and we learned a ton about who is working with data, what’s happening at the cutting edge of machine learning across industries, and how new data scientists can best break into the field.
To share some of the initial insights from the survey, we’ve worked with the folks from The Pudding to put together this interactive report. They’ve shared all of the kernels used in the report here.
Content
The data includes 5 files:
schema.csv: a CSV file with survey schema. This schema includes the questions that correspond to each column name in both the multipleChoiceResponses.csv and freeformResponses.csv.
multipleChoiceResponses.csv: Respondents' answers to multiple choice and ranking questions. These are non-randomized and thus a single row does correspond to all of a single user's answers. -freeformResponses.csv: Respondents' freeform answers to Kaggle's survey questions. These responses are randomized within a column, so that reading across a single row does not give a single user's answers.
conversionRates.csv: Currency conversion rates (to USD) as accessed from the R package ""quantmod"" on September 14, 2017
RespondentTypeREADME.txt: This is a schema for decoding the responses in the ""Asked"" column of the schema.csv file.
Kernel Awards in November
In the month of November, we’re awarding $1000 a week for code and analyses shared on this dataset via Kaggle Kernels. Read more about this month’s Kaggle Kernels Awards and help us advance the state of machine learning and data science by exploring this one of a kind dataset.
Methodology
This survey received 16,716 usable respondents from 171 countries and territories. If a country or territory received less than 50 respondents, we grouped them into a group named “Other” for anonymity.
We excluded respondents who were flagged by our survey system as “Spam” or who did not answer the question regarding their employment status (this question was the first required question, so not answering it indicates that the respondent did not proceed past the 5th question in our survey).
Most of our respondents were found primarily through Kaggle channels, like our email list, discussion forums and social media channels.
The survey was live from August 7th to August 25th. The median response time for those who participated in the survey was 16.4 minutes. We allowed respondents to complete the survey at any time during that window.
We received salary data by first asking respondents for their day-to-day currency, and then asking them to write in either their total compensation.
We’ve provided a csv with an exchange rate to USD for you to calculate the salary in US dollars on your own.
The question was optional
Not every question was shown to every respondent. In an attempt to ask relevant questions to each respondent, we generally asked work related questions to employed data scientists and learning related questions to students. There is a column in the schema.csv file called ""Asked"" that describes who saw each question. You can learn more about the different segments we used in the schema.csv file and RespondentTypeREADME.txt in the data tab.
To protect the respondents’ identity, the answers to multiple choice questions have been separated into a separate data file from the open-ended responses. We do not provide a key to match up the multiple choice and free form responses. Further, the free form responses have been randomized column-wise such that the responses that appear on the same row did not necessarily come from the same survey-taker."
Iris Species,Classify iris plants into three species in this classic dataset,UCI Machine Learning,512,"Version 2,2016-09-27|Version 1,2016-01-12",botany,SQLite,15 KB,CC0,"162,706 views","24,361 downloads","3,394 kernels",14 topics,https://www.kaggle.com/uciml/iris,"The Iris dataset was used in R.A. Fisher's classic 1936 paper, The Use of Multiple Measurements in Taxonomic Problems, and can also be found on the UCI Machine Learning Repository.
It includes three iris species with 50 samples each as well as some properties about each flower. One flower species is linearly separable from the other two, but the other two are not linearly separable from each other.
The columns in this dataset are:
Id
SepalLengthCm
SepalWidthCm
PetalLengthCm
PetalWidthCm
Species"
World Development Indicators,Explore country development indicators from around the world,World Bank,468,"Version 2,2017-05-02|Version 1,2016-01-28","economics
international relations",CSV,2 GB,Other,"134,038 views","20,364 downloads",389 kernels,5 topics,https://www.kaggle.com/worldbank/world-development-indicators,"The World Development Indicators from the World Bank contain over a thousand annual indicators of economic development from hundreds of countries around the world.
Here's a list of the available indicators along with a list of the available countries.
For example, this data includes the life expectancy at birth from many countries around the world:
The dataset hosted here is a slightly transformed verion of the raw files available here to facilitate analytics."
Daily News for Stock Market Prediction,Using 8 years daily news headlines to predict stock market movement,Aaron7sun,438,"Version 1,2016-08-25","news agencies
finance",CSV,14 MB,CC4,"95,770 views","10,820 downloads",293 kernels,8 topics,https://www.kaggle.com/aaron7sun/stocknews,"Actually, I prepare this dataset for students on my Deep Learning and NLP course.
But I am also very happy to see kagglers play around with it.
Have fun!
Description:
There are two channels of data provided in this dataset:
News data: I crawled historical news headlines from Reddit WorldNews Channel (/r/worldnews). They are ranked by reddit users' votes, and only the top 25 headlines are considered for a single date. (Range: 2008-06-08 to 2016-07-01)
Stock data: Dow Jones Industrial Average (DJIA) is used to ""prove the concept"". (Range: 2008-08-08 to 2016-07-01)
I provided three data files in .csv format:
RedditNews.csv: two columns The first column is the ""date"", and second column is the ""news headlines"". All news are ranked from top to bottom based on how hot they are. Hence, there are 25 lines for each date.
DJIA_table.csv: Downloaded directly from Yahoo Finance: check out the web page for more info.
Combined_News_DJIA.csv: To make things easier for my students, I provide this combined dataset with 27 columns. The first column is ""Date"", the second is ""Label"", and the following ones are news headlines ranging from ""Top1"" to ""Top25"".
=========================================
To my students:
I made this a binary classification task. Hence, there are only two labels:
""1"" when DJIA Adj Close value rose or stayed as the same;
""0"" when DJIA Adj Close value decreased.
For task evaluation, please use data from 2008-08-08 to 2014-12-31 as Training Set, and Test Set is then the following two years data (from 2015-01-02 to 2016-07-01). This is roughly a 80%/20% split.
And, of course, use AUC as the evaluation metric.
=========================================
+++++++++++++++++++++++++++++++++++++++++
To all kagglers:
Please upvote this dataset if you like this idea for market prediction.
If you think you coded an amazing trading algorithm,
friendly advice
do play safe with your own money :)
+++++++++++++++++++++++++++++++++++++++++
Feel free to contact me if there is any question~
And, remember me when you become a millionaire :P"
Pokemon with stats,721 Pokemon with stats and types,Alberto Barradas,428,"Version 2,2016-08-29|Version 1,2016-08-23","popular culture
games and toys
video games",CSV,43 KB,CC0,"133,256 views","16,610 downloads",706 kernels,13 topics,https://www.kaggle.com/abcsds/pokemon,"This data set includes 721 Pokemon, including their number, name, first and second type, and basic stats: HP, Attack, Defense, Special Attack, Special Defense, and Speed. It has been of great use when teaching statistics to kids. With certain types you can also give a geeky introduction to machine learning.
This are the raw attributes that are used for calculating how much damage an attack will do in the games. This dataset is about the pokemon games (NOT pokemon cards or Pokemon Go).
The data as described by Myles O'Neill is:
#: ID for each pokemon
Name: Name of each pokemon
Type 1: Each pokemon has a type, this determines weakness/resistance to attacks
Type 2: Some pokemon are dual type and have 2
Total: sum of all stats that come after this, a general guide to how strong a pokemon is
HP: hit points, or health, defines how much damage a pokemon can withstand before fainting
Attack: the base modifier for normal attacks (eg. Scratch, Punch)
Defense: the base damage resistance against normal attacks
SP Atk: special attack, the base modifier for special attacks (e.g. fire blast, bubble beam)
SP Def: the base damage resistance against special attacks
Speed: determines which pokemon attacks first each round
The data for this table has been acquired from several different sites, including:
pokemon.com
pokemondb
bulbapeida
One question has been answered with this database: The type of a pokemon cannot be inferred only by it's Attack and Deffence. It would be worthy to find which two variables can define the type of a pokemon, if any. Two variables can be plotted in a 2D space, and used as an example for machine learning. This could mean the creation of a visual example any geeky Machine Learning class would love."
Lending Club Loan Data,Analyze Lending Club's issued loans,Wendy Kan,406,"Version 1,2016-05-03",finance,SQLite,913 MB,Other,"151,714 views","26,758 downloads",521 kernels,25 topics,https://www.kaggle.com/wendykan/lending-club-loan-data,"These files contain complete loan data for all loans issued through the 2007-2015, including the current loan status (Current, Late, Fully Paid, etc.) and latest payment information. The file containing loan data through the ""present"" contains complete loan data for all loans issued through the previous completed calendar quarter. Additional features include credit scores, number of finance inquiries, address including zip codes, and state, and collections among others. The file is a matrix of about 890 thousand observations and 75 variables. A data dictionary is provided in a separate file. k"
Wine Reviews,"130k wine reviews with variety, location, winery, price, and description",zackthoutt,397,"Version 4,2017-11-28|Version 3,2017-11-25|Version 2,2017-06-22|Version 1,2017-06-22","critical theory
food and drink",CSV,51 MB,CC4,"65,098 views","10,070 downloads",87 kernels,12 topics,https://www.kaggle.com/zynicide/wine-reviews,"Context
After watching Somm (a documentary on master sommeliers) I wondered how I could create a predictive model to identify wines through blind tasting like a master sommelier would. The first step in this journey was gathering some data to train a model. I plan to use deep learning to predict the wine variety using words in the description/review. The model still won't be able to taste the wine, but theoretically it could identify the wine based on a description that a sommelier could give. If anyone has any ideas on how to accomplish this, please post them!
Content
The data consists of 10 fields:
Points: the number of points WineEnthusiast rated the wine on a scale of 1-100 (though they say they only post reviews for wines that score >=80)
Title: the title of the wine review, which often contains the vintage if you're interested in extracting that feature
Variety: the type of grapes used to make the wine (ie Pinot Noir)
Description: a few sentences from a sommelier describing the wine's taste, smell, look, feel, etc.
Country: the country that the wine is from
Province: the province or state that the wine is from
Region 1: the wine growing area in a province or state (ie Napa)
Region 2: sometimes there are more specific regions specified within a wine growing area (ie Rutherford inside the Napa Valley), but this value can sometimes be blank
Winery: the winery that made the wine
Designation: the vineyard within the winery where the grapes that made the wine are from
Price: the cost for a bottle of the wine
Taster Name: name of the person who tasted and reviewed the wine
Taster Twitter Handle: Twitter handle for the person who tasted and reviewed the wine
Acknowledgements
The data was scraped from WineEnthusiast during the week of June 15th, 2017. The code for the scraper can be found here if you have any more specific questions about data collection that I didn't address.
UPDATE 11/24/2017 After feedback from users of the dataset I scraped the reviews again on November 22nd, 2017. This time around I collected the title of each review, which you can parse the year out of, the tasters name, and the taster's Twitter handle. This should also fix the duplicate entry issue.
Inspiration
I think that this dataset offers some great opportunities for sentiment analysis and other text related predictive models. My overall goal is to create a model that can identify the variety, winery, and location of a wine based on a description. If anyone has any ideas, breakthroughs, or other interesting insights/models please post them."
Climate Change: Earth Surface Temperature Data,Exploring global temperatures since 1750,Berkeley Earth,392,"Version 2,2017-05-02|Version 1,2016-03-11","environment
climate",CSV,573 MB,CC4,"134,742 views","22,989 downloads",431 kernels,7 topics,https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data,"Some say climate change is the biggest threat of our age while others say it’s a myth based on dodgy science. We are turning some of the data over to you so you can form your own view.
Even more than with other data sets that Kaggle has featured, there’s a huge amount of data cleaning and preparation that goes into putting together a long-time study of climate trends. Early data was collected by technicians using mercury thermometers, where any variation in the visit time impacted measurements. In the 1940s, the construction of airports caused many weather stations to be moved. In the 1980s, there was a move to electronic thermometers that are said to have a cooling bias.
Given this complexity, there are a range of organizations that collate climate trends data. The three most cited land and ocean temperature data sets are NOAA’s MLOST, NASA’s GISTEMP and the UK’s HadCrut.
We have repackaged the data from a newer compilation put together by the Berkeley Earth, which is affiliated with Lawrence Berkeley National Laboratory. The Berkeley Earth Surface Temperature Study combines 1.6 billion temperature reports from 16 pre-existing archives. It is nicely packaged and allows for slicing into interesting subsets (for example by country). They publish the source data and the code for the transformations they applied. They also use methods that allow weather observations from shorter time series to be included, meaning fewer observations need to be thrown away.
In this dataset, we have include several files:
Global Land and Ocean-and-Land Temperatures (GlobalTemperatures.csv):
Date: starts in 1750 for average land temperature and 1850 for max and min land temperatures and global ocean and land temperatures
LandAverageTemperature: global average land temperature in celsius
LandAverageTemperatureUncertainty: the 95% confidence interval around the average
LandMaxTemperature: global average maximum land temperature in celsius
LandMaxTemperatureUncertainty: the 95% confidence interval around the maximum land temperature
LandMinTemperature: global average minimum land temperature in celsius
LandMinTemperatureUncertainty: the 95% confidence interval around the minimum land temperature
LandAndOceanAverageTemperature: global average land and ocean temperature in celsius
LandAndOceanAverageTemperatureUncertainty: the 95% confidence interval around the global average land and ocean temperature
Other files include:
Global Average Land Temperature by Country (GlobalLandTemperaturesByCountry.csv)
Global Average Land Temperature by State (GlobalLandTemperaturesByState.csv)
Global Land Temperatures By Major City (GlobalLandTemperaturesByMajorCity.csv)
Global Land Temperatures By City (GlobalLandTemperaturesByCity.csv)
The raw data comes from the Berkeley Earth data page."
Open Food Facts,Explore nutrition facts from foods around the world,Open Food Facts,361,"Version 5,2017-09-18|Version 4,2017-06-15|Version 3,2017-02-22|Version 2,2017-02-22|Version 1,2016-01-13","food and drink
nutrition",Other,963 MB,ODbL,"131,482 views","18,829 downloads",483 kernels,6 topics,https://www.kaggle.com/openfoodfacts/world-food-facts,"A food products database
Open Food Facts is a free, open, collbarative database of food products from around the world, with ingredients, allergens, nutrition facts and all the tidbits of information we can find on product labels.
Made by everyone
Open Food Facts is a non-profit association of volunteers. 5000+ contributors like you have added 100 000+ products from 150 countries using our Android, iPhone or Windows Phone app or their camera to scan barcodes and upload pictures of products and their labels.
For everyone
Data about food is of public interest and has to be open. The complete database is published as open data and can be reused by anyone and for any use. Check-out the cool reuses or make your own!
Dataset structure
The dataset contains a single table, FoodFacts, in CSV form in FoodFacts.csv and in SQLite form in database.sqlite.
The columns in Open Food Facts are as follows:
code (text)
url (text)
creator (text)
created_t (text)
created_datetime (text)
last_modified_t (text)
last_modified_datetime (text)
product_name (text)
generic_name (text)
quantity (text)
packaging (text)
packaging_tags (text)
brands (text)
brands_tags (text)
categories (text)
categories_tags (text)
categories_en (text)
origins (text)
origins_tags (text)
manufacturing_places (text)
manufacturing_places_tags (text)
labels (text)
labels_tags (text)
labels_en (text)
emb_codes (text)
emb_codes_tags (text)
first_packaging_code_geo (text)
cities (text)
cities_tags (text)
purchase_places (text)
stores (text)
countries (text)
countries_tags (text)
countries_en (text)
ingredients_text (text)
allergens (text)
allergens_en (text)
traces (text)
traces_tags (text)
traces_en (text)
serving_size (text)
no_nutriments (numeric)
additives_n (numeric)
additives (text)
additives_tags (text)
additives_en (text)
ingredients_from_palm_oil_n (numeric)
ingredients_from_palm_oil (numeric)
ingredients_from_palm_oil_tags (text)
ingredients_that_may_be_from_palm_oil_n (numeric)
ingredients_that_may_be_from_palm_oil (numeric)
ingredients_that_may_be_from_palm_oil_tags (text)
nutrition_grade_uk (numeric)
nutrition_grade_fr (text)
pnns_groups_1 (text)
pnns_groups_2 (text)
states (text)
states_tags (text)
states_en (text)
main_category (text)
main_category_en (text)
image_url (text)
image_small_url (text)
energy_100g (numeric)
energy_from_fat_100g (numeric)
fat_100g (numeric)
saturated_fat_100g (numeric)
butyric_acid_100g (numeric)
caproic_acid_100g (numeric)
caprylic_acid_100g (numeric)
capric_acid_100g (numeric)
lauric_acid_100g (numeric)
myristic_acid_100g (numeric)
palmitic_acid_100g (numeric)
stearic_acid_100g (numeric)
arachidic_acid_100g (numeric)
behenic_acid_100g (numeric)
lignoceric_acid_100g (numeric)
cerotic_acid_100g (numeric)
montanic_acid_100g (numeric)
melissic_acid_100g (numeric)
monounsaturated_fat_100g (numeric)
polyunsaturated_fat_100g (numeric)
omega_3_fat_100g (numeric)
alpha_linolenic_acid_100g (numeric)
eicosapentaenoic_acid_100g (numeric)
docosahexaenoic_acid_100g (numeric)
omega_6_fat_100g (numeric)
linoleic_acid_100g (numeric)
arachidonic_acid_100g (numeric)
gamma_linolenic_acid_100g (numeric)
dihomo_gamma_linolenic_acid_100g (numeric)
omega_9_fat_100g (numeric)
oleic_acid_100g (numeric)
elaidic_acid_100g (numeric)
gondoic_acid_100g (numeric)
mead_acid_100g (numeric)
erucic_acid_100g (numeric)
nervonic_acid_100g (numeric)
trans_fat_100g (numeric)
cholesterol_100g (numeric)
carbohydrates_100g (numeric)
sugars_100g (numeric)
sucrose_100g (numeric)
glucose_100g (numeric)
fructose_100g (numeric)
lactose_100g (numeric)
maltose_100g (numeric)
maltodextrins_100g (numeric)
starch_100g (numeric)
polyols_100g (numeric)
fiber_100g (numeric)
proteins_100g (numeric)
casein_100g (numeric)
serum_proteins_100g (numeric)
nucleotides_100g (numeric)
salt_100g (numeric)
sodium_100g (numeric)
alcohol_100g (numeric)
vitamin_a_100g (numeric)
beta_carotene_100g (numeric)
vitamin_d_100g (numeric)
vitamin_e_100g (numeric)
vitamin_k_100g (numeric)
vitamin_c_100g (numeric)
vitamin_b1_100g (numeric)
vitamin_b2_100g (numeric)
vitamin_pp_100g (numeric)
vitamin_b6_100g (numeric)
vitamin_b9_100g (numeric)
vitamin_b12_100g (numeric)
biotin_100g (numeric)
pantothenic_acid_100g (numeric)
silica_100g (numeric)
bicarbonate_100g (numeric)
potassium_100g (numeric)
chloride_100g (numeric)
calcium_100g (numeric)
phosphorus_100g (numeric)
iron_100g (numeric)
magnesium_100g (numeric)
zinc_100g (numeric)
copper_100g (numeric)
manganese_100g (numeric)
fluoride_100g (numeric)
selenium_100g (numeric)
chromium_100g (numeric)
molybdenum_100g (numeric)
iodine_100g (numeric)
caffeine_100g (numeric)
taurine_100g (numeric)
ph_100g (numeric)
fruits_vegetables_nuts_100g (numeric)
collagen_meat_protein_ratio_100g (numeric)
cocoa_100g (numeric)
chlorophyl_100g (numeric)
carbon_footprint_100g (numeric)
nutrition_score_fr_100g (numeric)
nutrition_score_uk_100g (numeric)"
Where it Pays to Attend College,"Salaries by college, region, and academic major",The Wall Street Journal,347,"Version 1,2017-04-29","news agencies
universities and colleges
employment
education",CSV,72 KB,Other,"50,598 views","9,473 downloads",43 kernels,3 topics,https://www.kaggle.com/wsj/college-salaries,"Salary Increase By Type of College
Party school? Liberal Arts college? State School? You already know your starting salary will be different depending on what type of school you attend. But, increased earning power shows less disparity. Ten years out, graduates of Ivy League schools earned 99% more than they did at graduation. Party school graduates saw an 85% increase. Engineering school graduates fared worst, earning 76% more 10 years out of school. See where your school ranks.
Salaries By Region
Attending college in the Midwest leads to the lowest salary both at graduation and at mid-career, according to the PayScale Inc. survey. Graduates of schools in the Northeast and California fared best.
Salary Increase By Major
Your parents might have worried when you chose Philosophy or International Relations as a major. But a year-long survey of 1.2 million people with only a bachelor's degree by PayScale Inc. shows that graduates in these subjects earned 103.5% and 97.8% more, respectively, about 10 years post-commencement. Majors that didn't show as much salary growth include Nursing and Information Technology.
All data was obtained from the Wall Street Journal based on data from Payscale, Inc:
Salaries for Colleges by Type
Salaries for Colleges by Region
Degrees that Pay you Back"
Mushroom Classification,Safe to eat or deadly poison?,UCI Machine Learning,341,"Version 1,2016-12-02",food and drink,CSV,365 KB,CC0,"103,535 views","14,777 downloads",449 kernels,4 topics,https://www.kaggle.com/uciml/mushroom-classification,"Context
Although this dataset was originally contributed to the UCI Machine Learning repository nearly 30 years ago, mushroom hunting (otherwise known as ""shrooming"") is enjoying new peaks in popularity. Learn which features spell certain death and which are most palatable in this dataset of mushroom characteristics. And how certain can your model be?
Content
This dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family Mushroom drawn from The Audubon Society Field Guide to North American Mushrooms (1981). Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom; no rule like ""leaflets three, let it be'' for Poisonous Oak and Ivy.
Time period: Donated to UCI ML 27 April 1987
Inspiration
What types of machine learning models perform best on this dataset?
Which features are most indicative of a poisonous mushroom?
Acknowledgements
This dataset was originally donated to the UCI Machine Learning repository. You can learn more about past research using the data here.
Start a new kernel"
Breast Cancer Wisconsin (Diagnostic) Data Set,Predict whether the cancer is benign or malignant,UCI Machine Learning,302,"Version 2,2016-09-25|Version 1,2016-09-20",healthcare,CSV,122 KB,CC4,"127,057 views","17,488 downloads",573 kernels,11 topics,https://www.kaggle.com/uciml/breast-cancer-wisconsin-data,"Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. n the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: ""Robust Linear Programming Discrimination of Two Linearly Inseparable Sets"", Optimization Methods and Software 1, 1992, 23-34].
This database is also available through the UW CS ftp server: ftp ftp.cs.wisc.edu cd math-prog/cpo-dataset/machine-learn/WDBC/
Also can be found on UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29
Attribute Information:
1) ID number 2) Diagnosis (M = malignant, B = benign) 3-32)
Ten real-valued features are computed for each cell nucleus:
a) radius (mean of distances from center to points on the perimeter) b) texture (standard deviation of gray-scale values) c) perimeter d) area e) smoothness (local variation in radius lengths) f) compactness (perimeter^2 / area - 1.0) g) concavity (severity of concave portions of the contour) h) concave points (number of concave portions of the contour) i) symmetry j) fractal dimension (""coastline approximation"" - 1)
The mean, standard error and ""worst"" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.
All feature values are recoded with four significant digits.
Missing attribute values: none
Class distribution: 357 benign, 212 malignant"
Amazon Fine Food Reviews,"Analyze ~500,000 food reviews from Amazon",Stanford Network Analysis Project,281,"Version 2,2017-05-02|Version 1,2016-01-09","food and drink
linguistics
internet",SQLite,642 MB,CC0,"119,362 views","16,079 downloads",327 kernels,9 topics,https://www.kaggle.com/snap/amazon-fine-food-reviews,"Context
This dataset consists of reviews of fine foods from amazon. The data span a period of more than 10 years, including all ~500,000 reviews up to October 2012. Reviews include product and user information, ratings, and a plain text review. It also includes reviews from all other Amazon categories.
Contents
Reviews.csv: Pulled from the corresponding SQLite table named Reviews in database.sqlite
database.sqlite: Contains the table 'Reviews'

Data includes:
- Reviews from Oct 1999 - Oct 2012
- 568,454 reviews
- 256,059 users
- 74,258 products
- 260 users with > 50 reviews
Acknowledgements
See this SQLite query for a quick sample of the dataset.
If you publish articles based on this dataset, please cite the following paper:
J. McAuley and J. Leskovec. From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews. WWW, 2013."
World Happiness Report,"Happiness scored according to economic production, social support, etc.",Sustainable Development Solutions Network,272,"Version 2,2017-06-15|Version 1,2017-02-28","emotion
social sciences
economics",CSV,62 KB,CC0,"90,642 views","19,316 downloads",234 kernels,6 topics,https://www.kaggle.com/unsdsn/world-happiness,"Context
The World Happiness Report is a landmark survey of the state of global happiness. The first report was published in 2012, the second in 2013, the third in 2015, and the fourth in the 2016 Update. The World Happiness 2017, which ranks 155 countries by their happiness levels, was released at the United Nations at an event celebrating International Day of Happiness on March 20th. The report continues to gain global recognition as governments, organizations and civil society increasingly use happiness indicators to inform their policy-making decisions. Leading experts across fields – economics, psychology, survey analysis, national statistics, health, public policy and more – describe how measurements of well-being can be used effectively to assess the progress of nations. The reports review the state of happiness in the world today and show how the new science of happiness explains personal and national variations in happiness.
Content
The happiness scores and rankings use data from the Gallup World Poll. The scores are based on answers to the main life evaluation question asked in the poll. This question, known as the Cantril ladder, asks respondents to think of a ladder with the best possible life for them being a 10 and the worst possible life being a 0 and to rate their own current lives on that scale. The scores are from nationally representative samples for the years 2013-2016 and use the Gallup weights to make the estimates representative. The columns following the happiness score estimate the extent to which each of six factors – economic production, social support, life expectancy, freedom, absence of corruption, and generosity – contribute to making life evaluations higher in each country than they are in Dystopia, a hypothetical country that has values equal to the world’s lowest national averages for each of the six factors. They have no impact on the total score reported for each country, but they do explain why some countries rank higher than others.
Inspiration
What countries or regions rank the highest in overall happiness and each of the six factors contributing to happiness? How did country ranks or scores change between the 2015 and 2016 as well as the 2016 and 2017 reports? Did any country experience a significant increase or decrease in happiness?
What is Dystopia?
Dystopia is an imaginary country that has the world’s least-happy people. The purpose in establishing Dystopia is to have a benchmark against which all countries can be favorably compared (no country performs more poorly than Dystopia) in terms of each of the six key variables, thus allowing each sub-bar to be of positive width. The lowest scores observed for the six key variables, therefore, characterize Dystopia. Since life would be very unpleasant in a country with the world’s lowest incomes, lowest life expectancy, lowest generosity, most corruption, least freedom and least social support, it is referred to as “Dystopia,” in contrast to Utopia.
What are the residuals?
The residuals, or unexplained components, differ for each country, reflecting the extent to which the six variables either over- or under-explain average 2014-2016 life evaluations. These residuals have an average value of approximately zero over the whole set of countries. Figure 2.2 shows the average residual for each country when the equation in Table 2.1 is applied to average 2014- 2016 data for the six variables in that country. We combine these residuals with the estimate for life evaluations in Dystopia so that the combined bar will always have positive values. As can be seen in Figure 2.2, although some life evaluation residuals are quite large, occasionally exceeding one point on the scale from 0 to 10, they are always much smaller than the calculated value in Dystopia, where the average life is rated at 1.85 on the 0 to 10 scale.
What do the columns succeeding the Happiness Score(like Family, Generosity, etc.) describe?
The following columns: GDP per Capita, Family, Life Expectancy, Freedom, Generosity, Trust Government Corruption describe the extent to which these factors contribute in evaluating the happiness in each country. The Dystopia Residual metric actually is the Dystopia Happiness Score(1.85) + the Residual value or the unexplained value for each country as stated in the previous answer.
If you add all these factors up, you get the happiness score so it might be un-reliable to model them to predict Happiness Scores.
Start a new kernel"
Fashion MNIST,"An MNIST-like dataset of 70,000 28x28 labeled fashion images",Zalando Research,268,"Version 4,2017-12-07|Version 3,2017-08-31|Version 2,2017-08-30|Version 1,2017-08-29","clothing
image data
multiclass classification
object identification",Other,69 MB,Other,"53,821 views","6,059 downloads",119 kernels,6 topics,https://www.kaggle.com/zalando-research/fashionmnist,"Context
Fashion-MNIST is a dataset of Zalando's article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Zalando intends Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.
The original MNIST dataset contains a lot of handwritten digits. Members of the AI/ML/Data Science community love this dataset and use it as a benchmark to validate their algorithms. In fact, MNIST is often the first dataset researchers try. ""If it doesn't work on MNIST, it won't work at all"", they said. ""Well, if it does work on MNIST, it may still fail on others.""
Zalando seeks to replace the original MNIST dataset
Content
Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255. The training and test data sets have 785 columns. The first column consists of the class labels (see above), and represents the article of clothing. The rest of the columns contain the pixel-values of the associated image.
To locate a pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27. The pixel is located on row i and column j of a 28 x 28 matrix.
For example, pixel31 indicates the pixel that is in the fourth column from the left, and the second row from the top, as in the ascii-diagram below.

Labels
Each training and test example is assigned to one of the following labels:
0 T-shirt/top
1 Trouser
2 Pullover
3 Dress
4 Coat
5 Sandal
6 Shirt
7 Sneaker
8 Bag
9 Ankle boot

TL;DR
Each row is a separate image
Column 1 is the class label.
Remaining columns are pixel numbers (784 total).
Each value is the darkness of the pixel (1 to 255)
Acknowledgements
Original dataset was downloaded from https://github.com/zalandoresearch/fashion-mnist
Dataset was converted to CSV with this script: https://pjreddie.com/projects/mnist-in-csv/
License
The MIT License (MIT) Copyright © [2017] Zalando SE, https://tech.zalando.com
Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:
The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
2016 US Election,Explore data related to the 2016 US Election,Ben Hamner,261,"Version 8,2016-07-01|Version 7,2016-07-01|Version 6,2016-07-01|Version 5,2016-03-26|Version 4,2016-02-29|Version 3,2016-02-29|Version 2,2016-02-26|Version 1,2016-02-19",politics,CSV,48 MB,CC4,"140,095 views","17,273 downloads",260 kernels,17 topics,https://www.kaggle.com/benhamner/2016-us-election,"This contains data relevant for the 2016 US Presidential Election, including up-to-date primary results.
Exploration Ideas
What candidates within the Republican party have results that are the most anti-correlated?
Which Republican candidate is Hillary Clinton most correlated with based on county voting patterns? What about Bernie Sanders?
What insights can you discover by mapping this data?
Do you have answers or other exploration ideas? Add your ideas to this forum post and share your insights through Kaggle Scripts!
Do you think that we should augment this dataset with more data sources? Submit a pull request to this repo, or let us know here!
Data Description
The 2016 US Election dataset contains several main files and folders at the moment. You may download the entire archive via the ""Download Data"" link at the top of the page, or interact with the data in Kaggle Scripts through the ../input directory.
Original Data Sources
Primary Results from CNN
New Hampshire County-Level Results
County Shapefiles
County QuickFacts"
Game of Thrones,Explore deaths and battles from this fantasy world,Myles O'Neill,258,"Version 1,2016-05-20","literature
social groups
war",CSV,257 KB,CC0,"113,897 views","13,525 downloads",206 kernels,5 topics,https://www.kaggle.com/mylesoneill/game-of-thrones,"Overview
Game of Thrones is a hit fantasy tv show based on the equally famous book series ""A Song of Fire and Ice"" by George RR Martin. The show is well known for its vastly complicated political landscape, large number of characters, and its frequent character deaths.
Data Sources
This dataset combines three sources of data, all of which are based on information from the book series.
Firstly, there is battles.csv which contains Chris Albon's ""The War of the Five Kings"" Dataset, which can be found here: https://github.com/chrisalbon/war_of_the_five_kings_dataset . Its a great collection of all of the battles in the series.
Secondly we have character-deaths.csv from Erin Pierce and Ben Kahle. This dataset was created as a part of their Bayesian Survival Analysis which can be found here: http://allendowney.blogspot.com/2015/03/bayesian-survival-analysis-for-game-of.html
Finally we have a more comprehensive character dataset with character-predictions.csv. This comes from the team at A Song of Ice and Data who scraped it from http://awoiaf.westeros.org/ . It also includes their predictions on which character will die, the methodology of which can be found here: https://got.show/machine-learning-algorithm-predicts-death-game-of-thrones
What insights about the complicated political landscape of this fantasy world can you find in this data?
Of course, it goes without saying that this dataset contains spoilers ;)"
Speed Dating Experiment,What attributes influence the selection of a romantic partner?,Anna Montoya,256,"Version 1,2016-05-09","love
sociology",Other,5 MB,Other,"119,295 views","24,348 downloads",240 kernels,7 topics,https://www.kaggle.com/annavictoria/speed-dating-experiment,"What influences love at first sight? (Or, at least, love in the first four minutes?) This dataset was compiled by Columbia Business School professors Ray Fisman and Sheena Iyengar for their paper Gender Differences in Mate Selection: Evidence From a Speed Dating Experiment.
Data was gathered from participants in experimental speed dating events from 2002-2004. During the events, the attendees would have a four minute ""first date"" with every other participant of the opposite sex. At the end of their four minutes, participants were asked if they would like to see their date again. They were also asked to rate their date on six attributes: Attractiveness, Sincerity, Intelligence, Fun, Ambition, and Shared Interests.
The dataset also includes questionnaire data gathered from participants at different points in the process. These fields include: demographics, dating habits, self-perception across key attributes, beliefs on what others find valuable in a mate, and lifestyle information. See the Speed Dating Data Key document below for details.
For more analysis from Iyengar and Fisman, read Racial Preferences in Dating.
Data Exploration Ideas
What are the least desirable attributes in a male partner? Does this differ for female partners?
How important do people think attractiveness is in potential mate selection vs. its real impact?
Are shared interests more important than a shared racial background?
Can people accurately predict their own perceived value in the dating market?
In terms of getting a second date, is it better to be someone's first speed date of the night or their last?"
Football Events,"More than 900,000 events from 9,074 football games across Europe",Alin Secareanu,256,"Version 1,2017-01-25",association football,CSV,174 MB,Other,"49,559 views","9,488 downloads",89 kernels,5 topics,https://www.kaggle.com/secareanualin/football-events,"Context
Most publicly available football (soccer) statistics are limited to aggregated data such as Goals, Shots, Fouls, Cards. When assessing performance or building predictive models, this simple aggregation, without any context, can be misleading. For example, a team that produced 10 shots on target from long range has a lower chance of scoring than a club that produced the same amount of shots from inside the box. However, metrics derived from this simple count of shots will similarly asses the two teams.
A football game generates much more events and it is very important and interesting to take into account the context in which those events were generated. This dataset should keep sports analytics enthusiasts awake for long hours as the number of questions that can be asked is huge.
Content
This dataset is a result of a very tiresome effort of webscraping and integrating different data sources. The central element is the text commentary. All the events were derived by reverse engineering the text commentary, using regex. Using this, I was able to derive 11 types of events, as well as the main player and secondary player involved in those events and many other statistics. In case I've missed extracting some useful information, you are gladly invited to do so and share your findings. The dataset provides a granular view of 9,074 games, totaling 941,009 events from the biggest 5 European football (soccer) leagues: England, Spain, Germany, Italy, France from 2011/2012 season to 2016/2017 season as of 25.01.2017. There are games that have been played during these seasons for which I could not collect detailed data. Overall, over 90% of the played games during these seasons have event data.
The dataset is organized in 3 files:
events.csv contains event data about each game. Text commentary was scraped from: bbc.com, espn.com and onefootball.com
ginf.csv - contains metadata and market odds about each game. odds were collected from oddsportal.com
dictionary.txt contains a dictionary with the textual description of each categorical variable coded with integers
Past Research
I have used this data to:
create predictive models for football games in order to bet on football outcomes.
make visualizations about upcoming games
build expected goals models and compare players
Inspiration
There are tons of interesting questions a sports enthusiast can answer with this dataset. For example:
What is the value of a shot? Or what is the probability of a shot being a goal given it's location, shooter, league, assist method, gamestate, number of players on the pitch, time - known as expected goals (xG) models
When are teams more likely to score?
Which teams are the best or sloppiest at holding the lead?
Which teams or players make the best use of set pieces?
In which leagues is the referee more likely to give a card?
How do players compare when they shoot with their week foot versus strong foot? Or which players are ambidextrous?
Identify different styles of plays (shooting from long range vs shooting from the box, crossing the ball vs passing the ball, use of headers)
Which teams have a bias for attacking on a particular flank?
And many many more..."
Zillow Economics Data,Turning on the lights in housing research.,Zillow,253,"Version 6,2018-01-25|Version 5,2018-01-13|Version 4,2017-12-08|Version 3,2017-12-08|Version 2,2017-11-14|Version 1,2017-10-25","housing
business
demographics
economics",CSV,511 MB,Other,"39,924 views","4,014 downloads",27 kernels,5 topics,https://www.kaggle.com/zillow/zecon,"Context
Zillow's Economic Research Team collects, cleans and publishes housing and economic data from a variety of public and proprietary sources. Public property record data filed with local municipalities -- including deeds, property facts, parcel information and transactional histories -- forms the backbone of our data products, and is fleshed out with proprietary data derived from property listings and user behavior on Zillow.
The large majority of Zillow's aggregated housing market and economic data is made available for free download at zillow.com/data.
Content
Variable Availability:
Zillow Home Value Index (ZHVI): A smoothed seasonally adjusted measure of the median estimated home value across a given region and housing type. A dollar denominated alternative to repeat-sales indices. Find a more detailed methodology here: http://www.zillow.com/research/zhvi-methodology-6032/
Zillow Rent Index (ZRI): A smoothed seasonally adjusted measure of the median estimated market rate rent across a given region and housing type. A dollar denominated alternative to repeat-rent indices. Find a more detailed methodology here: http://www.zillow.com/research/zillow-rent-index-methodology-2393/
For-Sale Listing/Inventory Metrics: Zillow provides many variables capturing current and historical for-sale listings availability, generally from 2012 to current. These variables include median list prices and inventory counts, both by various property types. Variables capturing for-sale market competitiveness including share of listings with a price cut, median price cut size, age of inventory, and the days a listing spend on Zillow before the sale is final.
Home Sales Metrics: Zillow provides data on sold homes including median sale price by various housing types, sale counts (methodology here: http://www.zillow.com/research/home-sales-methodology-7733/), and a normalized view of sale volume referred to as turnover. The prevalence of foreclosures is also provided as ratio of the housing stock and the share of all sales in which the home was previously foreclosed upon.
For-Rent Listing Metrics: Zillow provides median rents prices and median rent price per square foot by property type and bedroom count.
Housing type definitions:
All Homes: Zillow defines all homes as single-family, condominium and co-operative homes with a county record. Unless specified, all series cover this segment of the housing stock.
Condo/Co-op: Condominium and co-operative homes.
Multifamily 5+ units: Units in buildings with 5 or more housing units, that are not a condominiums or co-ops.
Duplex/Triplex: Housing units in buildings with 2 or 3 housing units.
Tiers: By metro, we determine price tier cutoffs that divide the all homes housing stock into thirds using the full distribution of estimated home values. We then estimate real estate metrics within the property sets, Bottom, Middle, and Top, defined by these cutoffs. When reported at the national level, all Bottom Tier homes defined at the metro level are pooled together to form the national bottom tier. The same holds for Middle and Top Tier homes.
Regional Availability:
Zillow metrics are reported for common US geographies including Nation, State, Metro (2013 Census Defined CBSAs), County, City, ZIP code, and Neighborhood.
We provide a crosswalk between colloquial Zillow region names and federally defined region names and linking variables such as County FIPS codes and CBSA codes. Cities and Neighborhoods do not match standard jurisdictional boundaries. Zillow city boundaries reflect mailing address conventions and so are often visually similar to collections of ZIP codes. Zillow neighborhood boundaries can be found here.
Suppression Rules: To ensure reliability of reported values the Zillow Economic Research team applies suppression rules triggered by low sample sizes and excessive volatility. These rules are customized to the metric and region type and explain most missingness found in the provided datasets.
Additional Data Products
The following data products and more are available for free download exclusively at Zillow.com/Data:
Zillow Home Value Forecast
Zillow Rent Forecast
Negative Equity (the share of mortgaged properties worth less than mortgage balance)
Zillow Home Price Expectations Survey
Zillow Housing Aspirations Report
Zillow Rising Sea Levels Research
Cash Buyers Time Series
Buy vs. Rent Breakeven Horizon
Mortgage Affordability, Rental Affordability, Price-to-Income Ratio
Conventional 30-year Fixed Mortgage Rate, Weekly Time Series
Jumbo 30-year Fixed Mortgage Rates, Weekly Time Series
Acknowledgements
The mission of the Zillow Economic Research Team is to be the most open, authoritative source for timely and accurate housing data and unbiased insight. We aim to empower consumers, industry professionals, policy makers and researchers looking to better understand the housing market.
To see more of our mission in action, we invite you to learn more about us and to check out our collection of research briefs, stories, data tools and past presentations at https://www.zillow.com/research/
Inspiration
Zillow, and the Zillow Economic Research Team, firmly believe that not only do data want to be free, data are going to be free. Instead of simply publishing raw data, we believe in the power of pushing data up the ladder from raw data bits, to actionable information and finally to unique insight. We aim to answer questions of all kinds, even questions our users may not have known they had before coming to us. When done right, we firmly believe this process of turning data into insight can be transformational in people's lives.
Please join us on this journey, and we're excited to see what insights you can discover hidden amongst our data!"
"House Sales in King County, USA",Predict house price using regression,harlfoxem,249,"Version 1,2016-08-25","home
finance",CSV,2 MB,CC0,"100,966 views","16,286 downloads",406 kernels,11 topics,https://www.kaggle.com/harlfoxem/housesalesprediction,"This dataset contains house sale prices for King County, which includes Seattle. It includes homes sold between May 2014 and May 2015.
It's a great dataset for evaluating simple regression models."
The Movies Dataset,"Metadata on over 45,000 movies. 26 million ratings from over 270,000 users.",Rounak Banik,246,"Version 7,2017-11-10|Version 6,2017-11-10|Version 5,2017-10-25|Version 4,2017-10-25|Version 3,2017-10-25|Version 2,2017-10-25|Version 1,2017-10-25","popular culture
film",CSV,900 MB,CC0,"51,102 views","7,834 downloads",22 kernels,4 topics,https://www.kaggle.com/rounakbanik/the-movies-dataset,"Context
These files contain metadata for all 45,000 movies listed in the Full MovieLens Dataset. The dataset consists of movies released on or before July 2017. Data points include cast, crew, plot keywords, budget, revenue, posters, release dates, languages, production companies, countries, TMDB vote counts and vote averages.
This dataset also has files containing 26 million ratings from 270,000 users for all 45,000 movies. Ratings are on a scale of 1-5 and have been obtained from the official GroupLens website.
Content
This dataset consists of the following files:
movies_metadata.csv: The main Movies Metadata file. Contains information on 45,000 movies featured in the Full MovieLens dataset. Features include posters, backdrops, budget, revenue, release dates, languages, production countries and companies.
keywords.csv: Contains the movie plot keywords for our MovieLens movies. Available in the form of a stringified JSON Object.
credits.csv: Consists of Cast and Crew Information for all our movies. Available in the form of a stringified JSON Object.
links.csv: The file that contains the TMDB and IMDB IDs of all the movies featured in the Full MovieLens dataset.
links_small.csv: Contains the TMDB and IMDB IDs of a small subset of 9,000 movies of the Full Dataset.
ratings_small.csv: The subset of 100,000 ratings from 700 users on 9,000 movies.
The Full MovieLens Dataset consisting of 26 million ratings and 750,000 tag applications from 270,000 users on all the 45,000 movies in this dataset can be accessed here
Acknowledgements
This dataset is an ensemble of data collected from TMDB and GroupLens. The Movie Details, Credits and Keywords have been collected from the TMDB Open API. This product uses the TMDb API but is not endorsed or certified by TMDb. Their API also provides access to data on many additional movies, actors and actresses, crew members, and TV shows. You can try it for yourself here.
The Movie Links and Ratings have been obtained from the Official GroupLens website. The files are a part of the dataset available here
Inspiration
This dataset was assembled as part of my second Capstone Project for Springboard's Data Science Career Track. I wanted to perform an extensive EDA on Movie Data to narrate the history and the story of Cinema and use this metadata in combination with MovieLens ratings to build various types of Recommender Systems.
Both my notebooks are available as kernels with this dataset: The Story of Film and Movie Recommender Systems
Some of the things you can do with this dataset: Predicting movie revenue and/or movie success based on a certain metric. What movies tend to get higher vote counts and vote averages on TMDB? Building Content Based and Collaborative Filtering Based Recommendation Engines."
New York Stock Exchange,S&P 500 companies historical prices with fundamental data,Dominik Gawlik,236,"Version 3,2017-02-22|Version 2,2017-02-20|Version 1,2017-02-19",finance,CSV,101 MB,CC0,"67,291 views","11,236 downloads",209 kernels,7 topics,https://www.kaggle.com/dgawlik/nyse,"Context
This dataset is a playground for fundamental and technical analysis. It is said that 30% of traffic on stocks is already generated by machines, can trading be fully automated? If not, there is still a lot to learn from historical data.
Content
Dataset consists of following files:
prices.csv: raw, as-is daily prices. Most of data spans from 2010 to the end 2016, for companies new on stock market date range is shorter. There have been approx. 140 stock splits in that time, this set doesn't account for that.
prices-split-adjusted.csv: same as prices, but there have been added adjustments for splits.
securities.csv: general description of each company with division on sectors
fundamentals.csv: metrics extracted from annual SEC 10K fillings (2012-2016), should be enough to derive most of popular fundamental indicators.
Acknowledgements
Prices were fetched from Yahoo Finance, fundamentals are from Nasdaq Financials, extended by some fields from EDGAR SEC databases.
Mining
Here is couple of things one could try out with this data:
Technical
One day ahead prediction: Rolling Linear Regression, ARIMA, Neural Networks, LSTM
Momentum/Mean-Reversion Strategies
Security clustering, portfolio construction/hedging
Fundamental
Which company has biggest chance of being bankrupt? Which one is undervalued (how prices behaved afterwards), what is Return on Investment?"
Cryptocurrency Historical Prices,"Prices of top cryptocurrencies including Bitcoin, Ethereum, Ripple, Bitcoin cash",SRK,235,"Version 13,2018-02-21|Version 12,2017-11-08|Version 11,2017-10-04|Version 10,2017-09-19|Version 9,2017-09-19|Version 8,2017-09-06|Version 7,2017-08-30|Version 6,2017-08-29|Version 5,2017-08-28|Version 4,2017-08-27|Version 3,2017-08-27|Version 2,2017-08-16|Version 1,2017-08-09","history
finance",CSV,2 MB,CC0,"87,120 views","11,620 downloads",30 kernels,10 topics,https://www.kaggle.com/sudalairajkumar/cryptocurrencypricehistory,"Context
In the last few days, I have been hearing a lot of buzz around cryptocurrencies. Things like Block chain, Bitcoin, Bitcoin cash, Ethereum, Ripple etc are constantly coming in the news articles I read. So I wanted to understand more about it and this post helped me get started. Once the basics are done, the DS guy sleeping inside me (always lazy.!) woke up and started raising questions like
How many such cryptocurrencies are there and what are their prices and valuations?
Why is there a sudden surge in the interest in recent days? Is it due to the increase in the price in the last few days? etc.
For getting answers to all these questions (and if possible to predict the future prices ;)), I started getting the data from coinmarketcap about the cryptocurrencies.
Update : Bitcoin dataset
So what next.? Now that we have the price data, I wanted to dig a little more about the factors affecting the price of coins. I started of with Bitcoin and there are quite a few parameters which affect the price of Bitcoin. Thanks to Blockchain Info, I was able to get quite a few parameters on once in two day basis.
This will help understand the other factors related to Bitcoin price and also help one make future predictions in a better way than just using the historical price.
Update2: Ethereum Dataset:
This dataset has features related to Ethereum. This is very similar to the bitcoin dataset and is available on a daily basis. Data is taken from Etherscan and the credits go to them for allowing us to use.
Content
This dataset has the historical price information of some of the top cryptocurrencies by market capitalization. The currencies included are
Bitcoin
Ethereum
Ripple
Bitcoin cash
Bitconnect
Dash
Ethereum Classic
Iota
Litecoin
Monero
Nem
Neo
Numeraire
Stratis
Waves
In case if you are interested in the prices of some other currencies, please post in comments section and I will try to add them in the next version. I am planning to revise it once in a week.
Dataset has one csv file for each currency. Price history is available on a daily basis from April 28, 2013. The columns in the csv file are
Date : date of observation
Open : Opening price on the given day
High : Highest price on the given day
Low : Lowest price on the given day
Close : Closing price on the given day
Volume : Volume of transactions on the given day
Market Cap : Market capitalization in USD
Bitcoin Dataset (bitcoin_dataset.csv) :
This dataset has the following features.
Date : Date of observation
btc_market_price : Average USD market price across major bitcoin exchanges.
btc_total_bitcoins : The total number of bitcoins that have already been mined.
btc_market_cap : The total USD value of bitcoin supply in circulation.
btc_trade_volume : The total USD value of trading volume on major bitcoin exchanges.
btc_blocks_size : The total size of all block headers and transactions.
btc_avg_block_size : The average block size in MB.
btc_n_orphaned_blocks : The total number of blocks mined but ultimately not attached to the main Bitcoin blockchain.
btc_n_transactions_per_block : The average number of transactions per block.
btc_median_confirmation_time : The median time for a transaction to be accepted into a mined block.
btc_hash_rate : The estimated number of tera hashes per second the Bitcoin network is performing.
btc_difficulty : A relative measure of how difficult it is to find a new block.
btc_miners_revenue : Total value of coinbase block rewards and transaction fees paid to miners.
btc_transaction_fees : The total value of all transaction fees paid to miners.
btc_cost_per_transaction_percent : miners revenue as percentage of the transaction volume.
btc_cost_per_transaction : miners revenue divided by the number of transactions.
btc_n_unique_addresses : The total number of unique addresses used on the Bitcoin blockchain.
btc_n_transactions : The number of daily confirmed Bitcoin transactions.
btc_n_transactions_total : Total number of transactions.
btc_n_transactions_excluding_popular : The total number of Bitcoin transactions, excluding the 100 most popular addresses.
btc_n_transactions_excluding_chains_longer_than_100 : The total number of Bitcoin transactions per day excluding long transaction chains.
btc_output_volume : The total value of all transaction outputs per day.
btc_estimated_transaction_volume : The total estimated value of transactions on the Bitcoin blockchain.
btc_estimated_transaction_volume_usd : The estimated transaction value in USD value.
Ethereum Dataset (ethereum_dataset.csv):
This dataset has the following features
Date(UTC) : Date of transaction
UnixTimeStamp : unix timestamp
eth_etherprice : price of ethereum
eth_tx : number of transactions per day
eth_address : Cumulative address growth
eth_supply : Number of ethers in supply
eth_marketcap : Market cap in USD
eth_hashrate : hash rate in GH/s
eth_difficulty : Difficulty level in TH
eth_blocks : number of blocks per day
eth_uncles : number of uncles per day
eth_blocksize : average block size in bytes
eth_blocktime : average block time in seconds
eth_gasprice : Average gas price in Wei
eth_gaslimit : Gas limit per day
eth_gasused : total gas used per day
eth_ethersupply : new ether supply per day
eth_chaindatasize : chain data size in bytes
eth_ens_register : Ethereal Name Service (ENS) registrations per day
Acknowledgements
This data is taken from coinmarketcap and it is free to use the data.
Bitcoin dataset is obtained from Blockchain Info.
Ethereum dataset is obtained from Etherscan.
Cover Image : Photo by Thomas Malama on Unsplash
Inspiration
Some of the questions which could be inferred from this dataset are:
How did the historical prices / market capitalizations of various currencies change over time?
Predicting the future price of the currencies
Which currencies are more volatile and which ones are more stable?
How does the price fluctuations of currencies correlate with each other?
Seasonal trend in the price fluctuations
Bitcoin / Ethereum dataset could be used to look at the following:
Factors affecting the bitcoin / ether price.
Directional prediction of bitcoin / ether price. (refer this paper for more inspiration)
Actual bitcoin price prediction."
TED Talks,"Data about TED Talks on the TED.com website until September 21st, 2017",Rounak Banik,233,"Version 3,2017-09-26|Version 2,2017-09-10|Version 1,2017-09-09","society
linguistics",CSV,34 MB,CC4,"54,830 views","6,135 downloads",28 kernels,4 topics,https://www.kaggle.com/rounakbanik/ted-talks,"Context
These datasets contain information about all audio-video recordings of TED Talks uploaded to the official TED.com website until September 21st, 2017. The TED main dataset contains information about all talks including number of views, number of comments, descriptions, speakers and titles. The TED transcripts dataset contains the transcripts for all talks available on TED.com.
Content (for the CSV files)
TED Main Dataset
name: The official name of the TED Talk. Includes the title and the speaker.
title: The title of the talk
description: A blurb of what the talk is about.
main_speaker: The first named speaker of the talk.
speaker_occupation: The occupation of the main speaker.
num_speaker: The number of speakers in the talk.
duration: The duration of the talk in seconds.
event: The TED/TEDx event where the talk took place.
film_date: The Unix timestamp of the filming.
published_date: The Unix timestamp for the publication of the talk on TED.com
comments: The number of first level comments made on the talk.
tags: The themes associated with the talk.
languages: The number of languages in which the talk is available.
ratings: A stringified dictionary of the various ratings given to the talk (inspiring, fascinating, jaw dropping, etc.)
related_talks: A list of dictionaries of recommended talks to watch next.
url: The URL of the talk.
views: The number of views on the talk.
TED Transcripts Dataset
url: The URL of the talk
transcript: The official English transcript of the talk.
Acknowledgements
The data has been scraped from the official TED Website and is available under the Creative Commons License.
Inspiration
I've always been fascinated by TED Talks and the immense diversity of content that it provides for free. I was also thoroughly inspired by a TED Talk that visually explored TED Talks stats and I was motivated to do the same thing, albeit on a much less grander scale.
Some of the questions that can be answered with this dataset: 1. How is each TED Talk related to every other TED Talk? 2. Which are the most viewed and most favorited Talks of all time? Are they mostly the same? What does this tell us? 3. What kind of topics attract the maximum discussion and debate (in the form of comments)? 4. Which months are most popular among TED and TEDx chapters? 5. Which themes are most popular amongst TEDsters?"
Health Insurance Marketplace,Explore health and dental plans data in the US Health Insurance Marketplace,US Department of Health and Human Services,232,"Version 2,2017-05-02|Version 1,2016-01-20","healthcare
economics",CSV,11 GB,CC0,"79,817 views","10,565 downloads",203 kernels,12 topics,https://www.kaggle.com/hhs/health-insurance-marketplace,"The Health Insurance Marketplace Public Use Files contain data on health and dental plans offered to individuals and small businesses through the US Health Insurance Marketplace.
Exploration Ideas
To help get you started, here are some data exploration ideas:
How do plan rates and benefits vary across states?
How do plan benefits relate to plan rates?
How do plan rates vary by age?
How do plans vary across insurance network providers?
See this forum thread for more ideas, and post there if you want to add your own ideas or answer some of the open questions!
Data Description
This data was originally prepared and released by the Centers for Medicare & Medicaid Services (CMS). Please read the CMS Disclaimer-User Agreement before using this data.
Here, we've processed the data to facilitate analytics. This processed version has three components:
1. Original versions of the data
The original versions of the 2014, 2015, 2016 data are available in the ""raw"" directory of the download and ""../input/raw"" on Kaggle Scripts. Search for ""dictionaries"" on this page to find the data dictionaries describing the individual raw files.
2. Combined CSV files that contain
In the top level directory of the download (""../input"" on Kaggle Scripts), there are six CSV files that contain the combined at across all years:
BenefitsCostSharing.csv
BusinessRules.csv
Network.csv
PlanAttributes.csv
Rate.csv
ServiceArea.csv
Additionally, there are two CSV files that facilitate joining data across years:
Crosswalk2015.csv - joining 2014 and 2015 data
Crosswalk2016.csv - joining 2015 and 2016 data
3. SQLite database
The ""database.sqlite"" file contains tables corresponding to each of the processed CSV files.
The code to create the processed version of this data is available on GitHub."
(MBTI) Myers-Briggs Personality Type Dataset,Includes a large number of people's MBTI type and content written by them,Mitchell J,225,"Version 1,2017-09-22","personality
demographics
linguistics
+ 2 more...",CSV,60 MB,CC0,"42,504 views","4,267 downloads",19 kernels,2 topics,https://www.kaggle.com/datasnaek/mbti-type,"Context
The Myers Briggs Type Indicator (or MBTI for short) is a personality type system that divides everyone into 16 distinct personality types across 4 axis:
Introversion (I) – Extroversion (E)
Intuition (N) – Sensing (S)
Thinking (T) – Feeling (F)
Judging (J) – Perceiving (P)
(More can be learned about what these mean here)
So for example, someone who prefers introversion, intuition, thinking and perceiving would be labelled an INTP in the MBTI system, and there are lots of personality based components that would model or describe this person’s preferences or behaviour based on the label.
It is one of, if not the, the most popular personality test in the world. It is used in businesses, online, for fun, for research and lots more. A simple google search reveals all of the different ways the test has been used over time. It’s safe to say that this test is still very relevant in the world in terms of its use.
From scientific or psychological perspective it is based on the work done on cognitive functions by Carl Jung i.e. Jungian Typology. This was a model of 8 distinct functions, thought processes or ways of thinking that were suggested to be present in the mind. Later this work was transformed into several different personality systems to make it more accessible, the most popular of which is of course the MBTI.
Recently, its use/validity has come into question because of unreliability in experiments surrounding it, among other reasons. But it is still clung to as being a very useful tool in a lot of areas, and the purpose of this dataset is to help see if any patterns can be detected in specific types and their style of writing, which overall explores the validity of the test in analysing, predicting or categorising behaviour.
Content
This dataset contains over 8600 rows of data, on each row is a person’s:
Type (This persons 4 letter MBTI code/type)
A section of each of the last 50 things they have posted (Each entry separated by ""|||"" (3 pipe characters))
Acknowledgements
This data was collected through the PersonalityCafe forum, as it provides a large selection of people and their MBTI personality type, as well as what they have written.
Inspiration
Some basic uses could include:
Use machine learning to evaluate the MBTIs validity and ability to predict language styles and behaviour online.
Production of a machine learning algorithm that can attempt to determine a person’s personality type based on some text they have written."
World University Rankings,Investigate the best universities in the world,Myles O'Neill,224,"Version 3,2016-09-28|Version 2,2016-03-12|Version 1,2016-03-09",universities and colleges,CSV,11 MB,Other,"105,477 views","20,198 downloads",282 kernels,5 topics,https://www.kaggle.com/mylesoneill/world-university-rankings,"Of all the universities in the world, which are the best?
Ranking universities is a difficult, political, and controversial practice. There are hundreds of different national and international university ranking systems, many of which disagree with each other. This dataset contains three global university rankings from very different places.
University Ranking Data
The Times Higher Education World University Ranking is widely regarded as one of the most influential and widely observed university measures. Founded in the United Kingdom in 2010, it has been criticized for its commercialization and for undermining non-English-instructing institutions.
The Academic Ranking of World Universities, also known as the Shanghai Ranking, is an equally influential ranking. It was founded in China in 2003 and has been criticized for focusing on raw research power and for undermining humanities and quality of instruction.
The Center for World University Rankings, is a less well know listing that comes from Saudi Arabia, it was founded in 2012.
How do these rankings compare to each other?
Are the various criticisms levied against these rankings fair or not?
How does your alma mater fare against the world?
Supplementary Data
To further extend your analyses, we've also included two sets of supplementary data.
The first of these is a set of data on educational attainment around the world. It comes from The World Data Bank and comprises information from the UNESCO Institute for Statistics and the Barro-Lee Dataset. How does national educational attainment relate to the quality of each nation's universities?
The second supplementary dataset contains information about public and private direct expenditure on education across nations. This data comes from the National Center for Education Statistics. It represents expenditure as a percentage of gross domestic product. Does spending more on education lead to better international university rankings?"
Mental Health in Tech Survey,Survey on Mental Health in the Tech Workplace in 2014,"Open Sourcing Mental Illness, LTD",224,"Version 3,2016-11-04|Version 2,2016-11-04|Version 1,2016-11-03","mental health
healthcare
human genetics
+ 2 more...",CSV,297 KB,CC4,"62,030 views","8,712 downloads",108 kernels,5 topics,https://www.kaggle.com/osmi/mental-health-in-tech-survey,"Dataset Information
This dataset is from a 2014 survey that measures attitudes towards mental health and frequency of mental health disorders in the tech workplace. You are also encouraged to analyze data from the ongoing 2016 survey found here.
Content
This dataset contains the following data:
Timestamp
Age
Gender
Country
state: If you live in the United States, which state or territory do you live in?
self_employed: Are you self-employed?
family_history: Do you have a family history of mental illness?
treatment: Have you sought treatment for a mental health condition?
work_interfere: If you have a mental health condition, do you feel that it interferes with your work?
no_employees: How many employees does your company or organization have?
remote_work: Do you work remotely (outside of an office) at least 50% of the time?
tech_company: Is your employer primarily a tech company/organization?
benefits: Does your employer provide mental health benefits?
care_options: Do you know the options for mental health care your employer provides?
wellness_program: Has your employer ever discussed mental health as part of an employee wellness program?
seek_help: Does your employer provide resources to learn more about mental health issues and how to seek help?
anonymity: Is your anonymity protected if you choose to take advantage of mental health or substance abuse treatment resources?
leave: How easy is it for you to take medical leave for a mental health condition?
mental_health_consequence: Do you think that discussing a mental health issue with your employer would have negative consequences?
phys_health_consequence: Do you think that discussing a physical health issue with your employer would have negative consequences?
coworkers: Would you be willing to discuss a mental health issue with your coworkers?
supervisor: Would you be willing to discuss a mental health issue with your direct supervisor(s)?
mental_health_interview: Would you bring up a mental health issue with a potential employer in an interview?
phys_health_interview: Would you bring up a physical health issue with a potential employer in an interview?
mental_vs_physical: Do you feel that your employer takes mental health as seriously as physical health?
obs_consequence: Have you heard of or observed negative consequences for coworkers with mental health conditions in your workplace?
comments: Any additional notes or comments
Inspiration
Some questions worth exploring:
How does the frequency of mental health illness and attitudes towards mental health vary by geographic location?
What are the strongest predictors of mental health illness or certain attitudes towards mental health in the workplace?
Acknowledgements
The original dataset is from Open Sourcing Mental Illness and can be downloaded here."
Huge Stock Market Dataset,Historical daily prices and volumes of all U.S. stocks and ETFs,Boris Marjanovic,221,"Version 3,2017-11-16|Version 2,2017-11-14|Version 1,2017-11-13","business
finance
economics
artificial intelligence",Other,245 MB,CC0,"22,421 views","3,593 downloads",10 kernels,6 topics,https://www.kaggle.com/borismarjanovic/price-volume-data-for-all-us-stocks-etfs,"Context
High-quality financial data is expensive to acquire and is therefore rarely shared for free. Here I provide the full historical daily price and volume data for all U.S.-based stocks and ETFs trading on the NYSE, NASDAQ, and NYSE MKT. It's one of the best datasets of its kind you can obtain.
Content
The data (last updated 11/10/2017) is presented in CSV format as follows: Date, Open, High, Low, Close, Volume, OpenInt. Note that prices have been adjusted for dividends and splits.
Acknowledgements
This dataset belongs to me. I’m sharing it here for free. You may do with it as you wish.
Inspiration
Many have tried, but most have failed, to predict the stock market's ups and downs. Can you do any better?"
Medical Appointment No Shows,Why do 30% of patients miss their scheduled appointments?,JoniHoppen,218,"Version 5,2017-08-21|Version 4,2017-08-19|Version 3,2017-02-14|Version 2,2017-02-14|Version 1,2017-02-08","brazil
healthcare
public health",CSV,10 MB,CC4,"78,477 views","9,988 downloads",219 kernels,35 topics,https://www.kaggle.com/joniarroba/noshowappointments,"Context
A person makes a doctor appointment, receives all the instructions and no-show. Who to blame? If this is help, don´t forget to upvote :) Greatings!
Content
300k medical appointments and its 15 variables (characteristics) of each. The most important one if the patient show-up or no-show the appointment. Variable names are self-explanatory, if you have doubts, just let me know!
scholarship variable means this concept = https://en.wikipedia.org/wiki/Bolsa_Fam%C3%ADlia
Data Dictionary
PatientId - Identification of a patient AppointmentID - Identification of each appointment Gender = Male or Female . Female is the greater proportion, woman takes way more care of they health in comparison to man. DataMarcacaoConsulta = The day of the actuall appointment, when they have to visit the doctor. DataAgendamento = The day someone called or registered the appointment, this is before appointment of course. Age = How old is the patient. Neighbourhood = Where the appointment takes place. Scholarship = Ture of False . Observation, this is a broad topic, consider reading this article https://en.wikipedia.org/wiki/Bolsa_Fam%C3%ADlia Hipertension = True or False Diabetes = True or False Alcoholism = True or False Handcap = True or False SMS_received = 1 or more messages sent to the patient. No-show = True or False.
Inspiration
What if that possible to predict someone to no-show an appointment?"
Twitter US Airline Sentiment,Analyze how travelers in February 2015 expressed their feelings on Twitter,Crowdflower,212,"Version 2,2016-10-06|Version 1,2016-01-07","linguistics
twitter
internet
aviation",SQLite,8 MB,CC4,"92,154 views","12,779 downloads",233 kernels,8 topics,https://www.kaggle.com/crowdflower/twitter-airline-sentiment,"This data originally came from Crowdflower's Data for Everyone library.
As the original source says,
A sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as ""late flight"" or ""rude service"").
The data we're providing on Kaggle is a slightly reformatted version of the original source. It includes both a CSV file and SQLite database. The code that does these transformations is available on GitHub
For example, it contains whether the sentiment of the tweets in this set was positive, neutral, or negative for six US airlines:"
Gender Recognition by Voice,Identify a voice as male or female,Kory Becker,210,"Version 1,2016-08-26","gender
linguistics",CSV,1 MB,CC4,"83,213 views","8,458 downloads",393 kernels,6 topics,https://www.kaggle.com/primaryobjects/voicegender,"Voice Gender
Gender Recognition by Voice and Speech Analysis
This database was created to identify a voice as male or female, based upon acoustic properties of the voice and speech. The dataset consists of 3,168 recorded voice samples, collected from male and female speakers. The voice samples are pre-processed by acoustic analysis in R using the seewave and tuneR packages, with an analyzed frequency range of 0hz-280hz (human vocal range).
The Dataset
The following acoustic properties of each voice are measured and included within the CSV:
meanfreq: mean frequency (in kHz)
sd: standard deviation of frequency
median: median frequency (in kHz)
Q25: first quantile (in kHz)
Q75: third quantile (in kHz)
IQR: interquantile range (in kHz)
skew: skewness (see note in specprop description)
kurt: kurtosis (see note in specprop description)
sp.ent: spectral entropy
sfm: spectral flatness
mode: mode frequency
centroid: frequency centroid (see specprop)
peakf: peak frequency (frequency with highest energy)
meanfun: average of fundamental frequency measured across acoustic signal
minfun: minimum fundamental frequency measured across acoustic signal
maxfun: maximum fundamental frequency measured across acoustic signal
meandom: average of dominant frequency measured across acoustic signal
mindom: minimum of dominant frequency measured across acoustic signal
maxdom: maximum of dominant frequency measured across acoustic signal
dfrange: range of dominant frequency measured across acoustic signal
modindx: modulation index. Calculated as the accumulated absolute difference between adjacent measurements of fundamental frequencies divided by the frequency range
label: male or female
Accuracy
Baseline (always predict male)
50% / 50%
Logistic Regression
97% / 98%
CART
96% / 97%
Random Forest
100% / 98%
SVM
100% / 99%
XGBoost
100% / 99%
Research Questions
An original analysis of the data-set can be found in the following article:
Identifying the Gender of a Voice using Machine Learning
The best model achieves 99% accuracy on the test set. According to a CART model, it appears that looking at the mean fundamental frequency might be enough to accurately classify a voice. However, some male voices use a higher frequency, even though their resonance differs from female voices, and may be incorrectly classified as female. To the human ear, there is apparently more than simple frequency, that determines a voice's gender.
Questions
What other features differ between male and female voices?
Can we find a difference in resonance between male and female voices?
Can we identify falsetto from regular voices? (separate data-set likely needed for this)
Are there other interesting features in the data?
CART Diagram
Mean fundamental frequency appears to be an indicator of voice gender, with a threshold of 140hz separating male from female classifications.
References
The Harvard-Haskins Database of Regularly-Timed Speech
Telecommunications & Signal Processing Laboratory (TSP) Speech Database at McGill University, Home
VoxForge Speech Corpus, Home
Festvox CMU_ARCTIC Speech Database at Carnegie Mellon University"
Every Cryptocurrency Daily Market Price,"Daily crypto markets open, close, low, high data for every token ever",jvent,210,"Version 12,2018-02-22|Version 11,2018-02-07|Version 10,2018-01-28|Version 9,2018-01-11|Version 8,2018-01-03|Version 7,2017-12-19|Version 6,2017-11-07|Version 5,2017-10-22|Version 4,2017-09-30|Version 3,2017-09-18|Version 2,2017-09-10|Version 1,2017-08-16","business
finance
internet",CSV,16 MB,Other,"31,723 views","3,904 downloads",17 kernels,20 topics,https://www.kaggle.com/jessevent/all-crypto-currencies,"Cryptocurrency Market Data
Historical Cryptocurrency Prices For ALL Tokens!
Summary
> Observations: 649,051
> Variables: 13  
> Crypto Tokens: 1,382  
> Start Date: 28/04/2017  
> End Date: 03/01/2018  
Description
All historic open, high, low, close, trading volume and market cap info for all cryptocurrencies.
I've had to go over the code with a fine tooth comb to get it compatible with CRAN so there have been significant enhancements to how some of the field conversions have been undertaken and the data being cleaned. This should eliminate a few issues around number formatting or unexpected handling of scientific notations.
Data Structure
Observations: 649,051    
Variables: 13    
$ slug        <chr> ""bitcoin"", ""bitcoin"", ""bitcoin"", ""bitcoin""...        
$ symbol      <chr> ""BTC"", ""BTC"", ""BTC"", ""BTC"", ""BTC"", ""BTC"", ...    
$ name        <chr> ""Bitcoin"", ""Bitcoin"", ""Bitcoin"", ""Bitcoin""...    
$ date        <date> 2013-04-28, 2013-04-29, 2013-04-30, 2013-...    
$ ranknow     <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...    
$ open        <dbl> 135.30, 134.44, 144.00, 139.00, 116.38, 10...    
$ high        <dbl> 135.98, 147.49, 146.93, 139.89, 125.60, 10...    
$ low         <dbl> 132.10, 134.00, 134.05, 107.72, 92.28, 79...    
$ close       <dbl> 134.21, 144.54, 139.00, 116.99, 105.21, 97...    
$ volume      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    
$ market      <dbl> 1500520000, 1491160000, 1597780000, 154282...    
$ close_ratio <dbl> 0.5438, 0.7813, 0.3843, 0.2882, 0.3881, 0...    
$ spread      <dbl> 3.88, 13.49, 12.88, 32.17, 33.32, 29.03, 2...    
Built With :heart: R
I've ported my original kernel to generate this data, across into a R package which is awaiting being published on CRAN. Run the below to go scrape all the historical tables of all the different cryptocurrencies listed on CoinMarketCap and turn it into a data frame.
You can install it via the github link below, or:
devtools::install_github(""jessevent/crypto"")  
library(crypto)  
will_i_get_rich <- getCoins()  
Authors
Jesse Vent - Package Author - jessevent
Acknowledgments
Github - View my github repository for the full package.
CoinSpot - Invest $AUD into Crypto today!
CoinMarketCap - Providing amazing data @CoinMarketCap
If this helps you become rich please consider making a donation!
    ERC-20: 0x375923Bf82F0b728d23A5704261a6e16341fd860
    XRP: rK59semLsuJZEWftxBFhWuNE6uhznjz2bK
    LTC: LWpiZMd2cEyqCdrZrs9TjsouTLWbFFxwCj"
Student Alcohol Consumption,"Social, gender and study data from secondary school students",UCI Machine Learning,207,"Version 2,2016-10-19|Version 1,2016-10-19","food and drink
public health",CSV,108 KB,CC0,"108,583 views","18,426 downloads",259 kernels,6 topics,https://www.kaggle.com/uciml/student-alcohol-consumption,"Context:
The data were obtained in a survey of students math and portuguese language courses in secondary school. It contains a lot of interesting social, gender and study information about students. You can use it for some EDA or try to predict students final grade.
Content:
Attributes for both student-mat.csv (Math course) and student-por.csv (Portuguese language course) datasets:
school - student's school (binary: 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira)
sex - student's sex (binary: 'F' - female or 'M' - male)
age - student's age (numeric: from 15 to 22)
address - student's home address type (binary: 'U' - urban or 'R' - rural)
famsize - family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3)
Pstatus - parent's cohabitation status (binary: 'T' - living together or 'A' - apart)
Medu - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)
Fedu - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)
Mjob - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')
Fjob - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')
reason - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other')
guardian - student's guardian (nominal: 'mother', 'father' or 'other')
traveltime - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)
studytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)
failures - number of past class failures (numeric: n if 1<=n<3, else 4)
schoolsup - extra educational support (binary: yes or no)
famsup - family educational support (binary: yes or no)
paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)
activities - extra-curricular activities (binary: yes or no)
nursery - attended nursery school (binary: yes or no)
higher - wants to take higher education (binary: yes or no)
internet - Internet access at home (binary: yes or no)
romantic - with a romantic relationship (binary: yes or no)
famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)
freetime - free time after school (numeric: from 1 - very low to 5 - very high)
goout - going out with friends (numeric: from 1 - very low to 5 - very high)
Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)
Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)
health - current health status (numeric: from 1 - very bad to 5 - very good)
absences - number of school absences (numeric: from 0 to 93)
These grades are related with the course subject, Math or Portuguese:
G1 - first period grade (numeric: from 0 to 20)
G2 - second period grade (numeric: from 0 to 20)
G3 - final grade (numeric: from 0 to 20, output target)
Additional note: there are several (382) students that belong to both datasets . These students can be identified by searching for identical attributes that characterize each student, as shown in the annexed R file.
Source Information
P. Cortez and A. Silva. Using Data Mining to Predict Secondary School Student Performance. In A. Brito and J. Teixeira Eds., Proceedings of 5th FUture BUsiness TEChnology Conference (FUBUTEC 2008) pp. 5-12, Porto, Portugal, April, 2008, EUROSIS, ISBN 978-9077381-39-7.
Fabio Pagnotta, Hossain Mohammad Amran. Email:fabio.pagnotta@studenti.unicam.it, mohammadamra.hossain '@' studenti.unicam.it University Of Camerino
https://archive.ics.uci.edu/ml/datasets/STUDENT+ALCOHOL+CONSUMPTION"
H-1B Visa Petitions 2011-2016,3 million petitions for H-1B visas,Sharan Naribole,205,"Version 2,2017-02-28|Version 1,2017-02-28","law
international relations",CSV,469 MB,CC4,"71,190 views","8,575 downloads",150 kernels,12 topics,https://www.kaggle.com/nsharan/h-1b-visa,"Context
H-1B visas are a category of employment-based, non-immigrant visas for temporary foreign workers in the United States. For a foreign national to apply for H1-B visa, a US employer must offer them a job and submit a petition for a H-1B visa to the US immigration department. This is also the most common visa status applied for and held by international students once they complete college or higher education and begin working in a full-time position.
The following articles contain more information about the H-1B visa process:
What is H1B LCA ? Why file it ? Salary, Processing times – DOL
H1B Application Process: Step by Step Guide
Content
This dataset contains five year's worth of H-1B petition data, with approximately 3 million records overall. The columns in the dataset include case status, employer name, worksite coordinates, job title, prevailing wage, occupation code, and year filed.
For more information on individual columns, refer to the column metadata. A detailed description of the underlying raw dataset is available in an official data dictionary.
Acknowledgements
The Office of Foreign Labor Certification (OFLC) generates program data, including data about H1-B visas. The disclosure data updated annually and is available online.
The raw data available is messy and not immediately suitable analysis. A set of data transformations were performed making the data more accessible for quick exploration. To learn more, refer to this blog post and to the complimentary R Notebook.
Inspiration
Is the number of petitions with Data Engineer job title increasing over time?
Which part of the US has the most Hardware Engineer jobs?
Which industry has the most number of Data Scientist positions?
Which employers file the most petitions each year?"
Young People Survey,"Explore the preferences, interests, habits, opinions, and fears of young people",Miroslav Sabo,204,"Version 2,2016-12-06|Version 1,2016-08-22","social groups
psychometrics
demographics
+ 2 more...",CSV,448 KB,CC0,"67,506 views","17,873 downloads",126 kernels,3 topics,https://www.kaggle.com/miroslavsabo/young-people-survey,"Introduction
In 2013, students of the Statistics class at FSEV UK were asked to invite their friends to participate in this survey.
The data file (responses.csv) consists of 1010 rows and 150 columns (139 integer and 11 categorical).
For convenience, the original variable names were shortened in the data file. See the columns.csv file if you want to match the data with the original names.
The data contain missing values.
The survey was presented to participants in both electronic and written form.
The original questionnaire was in Slovak language and was later translated into English.
All participants were of Slovakian nationality, aged between 15-30.
The variables can be split into the following groups:
Music preferences (19 items)
Movie preferences (12 items)
Hobbies & interests (32 items)
Phobias (10 items)
Health habits (3 items)
Personality traits, views on life, & opinions (57 items)
Spending habits (7 items)
Demographics (10 items)
Research questions
Many different techniques can be used to answer many questions, e.g.
Clustering: Given the music preferences, do people make up any clusters of similar behavior?
Hypothesis testing: Do women fear certain phenomena significantly more than men? Do the left handed people have different interests than right handed?
Predictive modeling: Can we predict spending habits of a person from his/her interests and movie or music preferences?
Dimension reduction: Can we describe a large number of human interests by a smaller number of latent concepts?
Correlation analysis: Are there any connections between music and movie preferences?
Visualization: How to effectively visualize a lot of variables in order to gain some meaningful insights from the data?
(Multivariate) Outlier detection: Small number of participants often cheats and randomly answers the questions. Can you identify them? Hint: Local outlier factor may help.
Missing values analysis: Are there any patterns in missing responses? What is the optimal way of imputing the values in surveys?
Recommendations: If some of user's interests are known, can we predict the other? Or, if we know what a person listen, can we predict which kind of movies he/she might like?
Past research
(in slovak) Sleziak, P. - Sabo, M.: Gender differences in the prevalence of specific phobias. Forum Statisticum Slovacum. 2014, Vol. 10, No. 6. [Differences (gender + whether people lived in village/town) in the prevalence of phobias.]
Sabo, Miroslav. Multivariate Statistical Methods with Applications. Diss. Slovak University of Technology in Bratislava, 2014. [Clustering of variables (music preferences, movie preferences, phobias) + Clustering of people w.r.t. their interests.]
Questionnaire
MUSIC PREFERENCES
I enjoy listening to music.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I prefer.: Slow paced music 1-2-3-4-5 Fast paced music (integer)
Dance, Disco, Funk: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)
Folk music: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)
Country: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)
Classical: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)
Musicals: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)
Pop: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)
Rock: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)
Metal, Hard rock: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)
Punk: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)
Hip hop, Rap: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)
Reggae, Ska: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)
Swing, Jazz: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)
Rock n Roll: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)
Alternative music: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)
Latin: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)
Techno, Trance: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)
Opera: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)
MOVIE PREFERENCES
I really enjoy watching movies.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
Horror movies: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)
Thriller movies: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)
Comedies: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)
Romantic movies: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)
Sci-fi movies: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)
War movies: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)
Tales: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)
Cartoons: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)
Documentaries: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)
Western movies: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)
Action movies: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)
HOBBIES & INTERESTS
History: Not interested 1-2-3-4-5 Very interested (integer)
Psychology: Not interested 1-2-3-4-5 Very interested (integer)
Politics: Not interested 1-2-3-4-5 Very interested (integer)
Mathematics: Not interested 1-2-3-4-5 Very interested (integer)
Physics: Not interested 1-2-3-4-5 Very interested (integer)
Internet: Not interested 1-2-3-4-5 Very interested (integer)
PC Software, Hardware: Not interested 1-2-3-4-5 Very interested (integer)
Economy, Management: Not interested 1-2-3-4-5 Very interested (integer)
Biology: Not interested 1-2-3-4-5 Very interested (integer)
Chemistry: Not interested 1-2-3-4-5 Very interested (integer)
Poetry reading: Not interested 1-2-3-4-5 Very interested (integer)
Geography: Not interested 1-2-3-4-5 Very interested (integer)
Foreign languages: Not interested 1-2-3-4-5 Very interested (integer)
Medicine: Not interested 1-2-3-4-5 Very interested (integer)
Law: Not interested 1-2-3-4-5 Very interested (integer)
Cars: Not interested 1-2-3-4-5 Very interested (integer)
Art: Not interested 1-2-3-4-5 Very interested (integer)
Religion: Not interested 1-2-3-4-5 Very interested (integer)
Outdoor activities: Not interested 1-2-3-4-5 Very interested (integer)
Dancing: Not interested 1-2-3-4-5 Very interested (integer)
Playing musical instruments: Not interested 1-2-3-4-5 Very interested (integer)
Poetry writing: Not interested 1-2-3-4-5 Very interested (integer)
Sport and leisure activities: Not interested 1-2-3-4-5 Very interested (integer)
Sport at competitive level: Not interested 1-2-3-4-5 Very interested (integer)
Gardening: Not interested 1-2-3-4-5 Very interested (integer)
Celebrity lifestyle: Not interested 1-2-3-4-5 Very interested (integer)
Shopping: Not interested 1-2-3-4-5 Very interested (integer)
Science and technology: Not interested 1-2-3-4-5 Very interested (integer)
Theatre: Not interested 1-2-3-4-5 Very interested (integer)
Socializing: Not interested 1-2-3-4-5 Very interested (integer)
Adrenaline sports: Not interested 1-2-3-4-5 Very interested (integer)
Pets: Not interested 1-2-3-4-5 Very interested (integer)
PHOBIAS
Flying: Not afraid at all 1-2-3-4-5 Very afraid of (integer)
Thunder, lightning: Not afraid at all 1-2-3-4-5 Very afraid of (integer)
Darkness: Not afraid at all 1-2-3-4-5 Very afraid of (integer)
Heights: Not afraid at all 1-2-3-4-5 Very afraid of (integer)
Spiders: Not afraid at all 1-2-3-4-5 Very afraid of (integer)
Snakes: Not afraid at all 1-2-3-4-5 Very afraid of (integer)
Rats, mice: Not afraid at all 1-2-3-4-5 Very afraid of (integer)
Ageing: Not afraid at all 1-2-3-4-5 Very afraid of (integer)
Dangerous dogs: Not afraid at all 1-2-3-4-5 Very afraid of (integer)
Public speaking: Not afraid at all 1-2-3-4-5 Very afraid of (integer)
HEALTH HABITS
Smoking habits: Never smoked - Tried smoking - Former smoker - Current smoker (categorical)
Drinking: Never - Social drinker - Drink a lot (categorical)
I live a very healthy lifestyle.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
PERSONALITY TRAITS, VIEWS ON LIFE & OPINIONS
I take notice of what goes on around me.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I try to do tasks as soon as possible and not leave them until last minute.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I always make a list so I don't forget anything.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I often study or work even in my spare time.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I look at things from all different angles before I go ahead.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I believe that bad people will suffer one day and good people will be rewarded.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I am reliable at work and always complete all tasks given to me.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I always keep my promises.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I can fall for someone very quickly and then completely lose interest.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I would rather have lots of friends than lots of money.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I always try to be the funniest one.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I can be two faced sometimes.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I damaged things in the past when angry.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I take my time to make decisions.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I always try to vote in elections.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I often think about and regret the decisions I make.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I can tell if people listen to me or not when I talk to them.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I am a hypochondriac.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I am emphatetic person.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I eat because I have to. I don't enjoy food and eat as fast as I can.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I try to give as much as I can to other people at Christmas.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I don't like seeing animals suffering.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I look after things I have borrowed from others.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I feel lonely in life.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I used to cheat at school.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I worry about my health.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I wish I could change the past because of the things I have done.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I believe in God.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I always have good dreams.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I always give to charity.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I have lots of friends.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
Timekeeping.: I am often early. - I am always on time. - I am often running late. (categorical)
Do you lie to others?: Never. - Only to avoid hurting someone. - Sometimes. - Everytime it suits me. (categorical)
I am very patient.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I can quickly adapt to a new environment.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
My moods change quickly.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I am well mannered and I look after my appearance.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I enjoy meeting new people.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I always let other people know about my achievements.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I think carefully before answering any important letters.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I enjoy childrens' company.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I am not afraid to give my opinion if I feel strongly about something.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I can get angry very easily.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I always make sure I connect with the right people.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I have to be well prepared before public speaking.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I will find a fault in myself if people don't like me.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I cry when I feel down or things don't go the right way.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I am 100% happy with my life.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I am always full of life and energy.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I prefer big dangerous dogs to smaller, calmer dogs.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I believe all my personality traits are positive.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
If I find something the doesn't belong to me I will hand it in.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I find it very difficult to get up in the morning.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I have many different hobbies and interests.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I always listen to my parents' advice.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I enjoy taking part in surveys.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
How much time do you spend online?: No time at all - Less than an hour a day - Few hours a day - Most of the day (categorical)
SPENDING HABITS
I save all the money I can.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I enjoy going to large shopping centres.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I prefer branded clothing to non branded.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I spend a lot of money on partying and socializing.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I spend a lot of money on my appearance.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I spend a lot of money on gadgets.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
I will hapilly pay more money for good, quality or healthy food.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)
DEMOGRAPHICS
Age: (integer)
Height: (integer)
Weight: (integer)
How many siblings do you have?: (integer)
Gender: Female - Male (categorical)
I am: Left handed - Right handed (categorical)
Highest education achieved: Currently a Primary school pupil - Primary school - Secondary school - College/Bachelor degree (categorical)
I am the only child: No - Yes (categorical)
I spent most of my childhood in a: City - village (categorical)
I lived most of my childhood in a: house/bungalow - block of flats (categorical)"
Trending YouTube Video Statistics,Daily statistics for trending YouTube videos,Mitchell J,199,"Version 40,2018-02-23|Version 39,2018-02-19|Version 38,2018-02-15|Version 37,2018-02-12|Version 36,2018-02-08|Version 35,2018-02-02|Version 34,2018-01-26|Version 33,2018-01-22|Version 32,2018-01-16|Version 31,2018-01-09|Version 30,2018-01-06|Version 29,2018-01-04|Version 28,2018-01-02|Version 27,2017-12-25|Version 26,2017-12-23|Version 25,2017-12-19|Version 24,2017-12-13|Version 23,2017-12-11|Version 22,2017-12-08|Version 21,2017-12-07|Version 20,2017-12-04|Version 19,2017-12-02|Version 18,2017-12-01|Version 17,2017-11-29|Version 16,2017-11-29|Version 15,2017-11-28|Version 14,2017-11-27|Version 13,2017-11-24|Version 12,2017-11-24|Version 11,2017-11-23|Version 10,2017-11-22|Version 9,2017-11-21|Version 8,2017-11-20|Version 7,2017-11-18|Version 6,2017-11-18|Version 5,2017-11-17|Version 4,2017-11-16|Version 3,2017-11-15|Version 2,2017-11-14|Version 1,2017-11-14","languages
popular culture
statistics
+ 2 more...",CSV,54 MB,CC0,"33,195 views","5,856 downloads",14 kernels,7 topics,https://www.kaggle.com/datasnaek/youtube-new,"Context
YouTube (the world-famous video sharing website) maintains a list of the top trending videos on the platform. According to Variety magazine, “To determine the year’s top-trending videos, YouTube uses a combination of factors including measuring users interactions (number of views, shares, comments and likes). Note that they’re not the most-viewed videos overall for the calendar year”. Top performers on the YouTube trending list are music videos (such as the famously virile “Gangam Style”), celebrity and/or reality TV performances, and the random dude-with-a-camera viral videos that YouTube is well-known for.
This dataset is a daily record of the top trending YouTube videos.
Note that this dataset is a structurally improved version of this dataset.
Content
This dataset includes several months (and counting) of data on daily trending YouTube videos. Data is included for the US, GB, DE, CA, and FR regions (USA, Great Britain, Germany, Canada, and France, respectively), with up to 200 listed trending videos per day.
Each region’s data is in a separate file. Data includes the video title, channel title, publish time, tags, views, likes and dislikes, description, and comment count.
The data also includes a category_id field, which varies between regions. To retrieve the categories for a specific video, find it in the associated JSON. One such file is included for each of the five regions in the dataset.
For more information on specific columns in the dataset refer to the column metadata.
Acknowledgements
This dataset was collected using the YouTube API.
Inspiration
Possible uses for this dataset could include:
Sentiment analysis in a variety of forms
Categorising YouTube videos based on their comments and statistics.
Training ML algorithms like RNNs to generate their own YouTube comments.
Analysing what factors affect how popular a YouTube video will be.
Statistical analysis over time .
For further inspiration, see the kernels on this dataset!"
Video Game Sales,"Analyze sales data from more than 16,500 games.",GregorySmith,198,"Version 2,2016-10-26|Version 1,2016-10-26",video games,CSV,1 MB,Other,"86,011 views","13,910 downloads",283 kernels,11 topics,https://www.kaggle.com/gregorut/videogamesales,"This dataset contains a list of video games with sales greater than 100,000 copies. It was generated by a scrape of vgchartz.com.
Fields include
Rank - Ranking of overall sales
Name - The games name
Platform - Platform of the games release (i.e. PC,PS4, etc.)
Year - Year of the game's release
Genre - Genre of the game
Publisher - Publisher of the game
NA_Sales - Sales in North America (in millions)
EU_Sales - Sales in Europe (in millions)
JP_Sales - Sales in Japan (in millions)
Other_Sales - Sales in the rest of the world (in millions)
Global_Sales - Total worldwide sales.
The script to scrape the data is available at https://github.com/GregorUT/vgchartzScrape. It is based on BeautifulSoup using Python. There are 16,598 records. 2 records were dropped due to incomplete information."
20 Years of Games,18000+ rows of review data from ign.com,Eric Grinstein,186,"Version 2,2016-09-28|Version 1,2016-09-28","games and toys
video games",CSV,2 MB,Other,"50,320 views","7,818 downloads",160 kernels,4 topics,https://www.kaggle.com/egrinstein/20-years-of-games,"This dataset is the result of a crawl I did on http://ign.com/games/reviews .
It contains 18625 lines with the fields like the release date, it's platform and IGN's score. All the lines are fully filled.
In 20 years, the gaming industry has grown and sophisticated. By exploring this dataset, one is able to find trends about the industry, compare consoles against eachother, search through the most popular genres and more.
The dataset can also be a great place for beginners to start using Python modules such as Pandas and Seaborn.
You can find the crawl I used for the retrieval here"
Hillary Clinton's Emails,Uncover the political landscape in Hillary Clinton's emails,Kaggle,185,"Version 2,2016-10-06|Version 1,2015-09-11","politics
telecommunications",CSV,51 MB,CC0,"115,287 views","11,464 downloads",347 kernels,12 topics,https://www.kaggle.com/kaggle/hillary-clinton-emails,"Throughout 2015, Hillary Clinton has been embroiled in controversy over the use of personal email accounts on non-government servers during her time as the United States Secretary of State. Some political experts and opponents maintain that Clinton's use of personal email accounts to conduct Secretary of State affairs is in violation of protocols and federal laws that ensure appropriate recordkeeping of government activity. Hillary's campaign has provided their own four sentence summary of her email use here.
There have been a number of Freedom of Information lawsuits filed over the State Department's failure to fully release the emails sent and received on Clinton's private accounts. On Monday, August 31, the State Department released nearly 7,000 pages of Clinton's heavily redacted emails (its biggest release of emails to date).
The documents were released by the State Department as PDFs. We've cleaned and normalized the released documents and are hosting them for public analysis. Kaggle's choice to host this dataset is not meant to express any particular political affiliation or intent.
Here's the code that creates this data release."
IBM HR Analytics Employee Attrition & Performance,Predict attrition of your valuable employees,pavansubhash,183,"Version 1,2017-03-31","employment
business",CSV,223 KB,ODbL,"67,957 views","9,905 downloads",235 kernels,10 topics,https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset,"Uncover the factors that lead to employee attrition and explore important questions such as ‘show me a breakdown of distance from home by job role and attrition’ or ‘compare average monthly income by education and attrition’. This is a fictional data set created by IBM data scientists.
Education 1 'Below College' 2 'College' 3 'Bachelor' 4 'Master' 5 'Doctor'
EnvironmentSatisfaction 1 'Low' 2 'Medium' 3 'High' 4 'Very High'
JobInvolvement
1 'Low' 2 'Medium' 3 'High' 4 'Very High'
JobSatisfaction 1 'Low' 2 'Medium' 3 'High' 4 'Very High'
PerformanceRating
1 'Low' 2 'Good' 3 'Excellent' 4 'Outstanding'
RelationshipSatisfaction
1 'Low' 2 'Medium' 3 'High' 4 'Very High'
WorkLifeBalance 1 'Bad' 2 'Good' 3 'Better' 4 'Best'"
Chocolate Bar Ratings,"Expert ratings of over 1,700 chocolate bars",Rachael Tatman,180,"Version 1,2017-08-12","critical theory
food and drink",CSV,125 KB,CC0,"35,945 views","7,128 downloads",89 kernels,2 topics,https://www.kaggle.com/rtatman/chocolate-bar-ratings,"Context
Chocolate is one of the most popular candies in the world. Each year, residents of the United States collectively eat more than 2.8 billions pounds. However, not all chocolate bars are created equal! This dataset contains expert ratings of over 1,700 individual chocolate bars, along with information on their regional origin, percentage of cocoa, the variety of chocolate bean used and where the beans were grown.
Flavors of Cacao Rating System:
5= Elite (Transcending beyond the ordinary limits)
4= Premium (Superior flavor development, character and style)
3= Satisfactory(3.0) to praiseworthy(3.75) (well made with special qualities)
2= Disappointing (Passable but contains at least one significant flaw)
1= Unpleasant (mostly unpalatable)
Each chocolate is evaluated from a combination of both objective qualities and subjective interpretation. A rating here only represents an experience with one bar from one batch. Batch numbers, vintages and review dates are included in the database when known.
The database is narrowly focused on plain dark chocolate with an aim of appreciating the flavors of the cacao when made into chocolate. The ratings do not reflect health benefits, social missions, or organic status.
Flavor is the most important component of the Flavors of Cacao ratings. Diversity, balance, intensity and purity of flavors are all considered. It is possible for a straight forward single note chocolate to rate as high as a complex flavor profile that changes throughout. Genetics, terroir, post harvest techniques, processing and storage can all be discussed when considering the flavor component.
Texture has a great impact on the overall experience and it is also possible for texture related issues to impact flavor. It is a good way to evaluate the makers vision, attention to detail and level of proficiency.
Aftermelt is the experience after the chocolate has melted. Higher quality chocolate will linger and be long lasting and enjoyable. Since the aftermelt is the last impression you get from the chocolate, it receives equal importance in the overall rating.
Overall Opinion is really where the ratings reflect a subjective opinion. Ideally it is my evaluation of whether or not the components above worked together and an opinion on the flavor development, character and style. It is also here where each chocolate can usually be summarized by the most prominent impressions that you would remember about each chocolate.
Acknowledgements
These ratings were compiled by Brady Brelinski, Founding Member of the Manhattan Chocolate Society. For up-to-date information, as well as additional content (including interviews with craft chocolate makers), please see his website: Flavors of Cacao
Inspiration
Where are the best cocoa beans grown?
Which countries produce the highest-rated bars?
What’s the relationship between cocoa solids percentage and rating?"
Uber Pickups in New York City,Trip data for over 20 million Uber (and other for-hire vehicle) trips in NYC,FiveThirtyEight,180,"Version 2,2016-11-14|Version 1,2016-11-13",road transport,CSV,835 MB,CC0,"67,549 views","8,765 downloads",208 kernels,3 topics,https://www.kaggle.com/fivethirtyeight/uber-pickups-in-new-york-city,"Uber TLC FOIL Response
This directory contains data on over 4.5 million Uber pickups in New York City from April to September 2014, and 14.3 million more Uber pickups from January to June 2015. Trip-level data on 10 other for-hire vehicle (FHV) companies, as well as aggregated data for 329 FHV companies, is also included. All the files are as they were received on August 3, Sept. 15 and Sept. 22, 2015.
FiveThirtyEight obtained the data from the NYC Taxi & Limousine Commission (TLC) by submitting a Freedom of Information Law request on July 20, 2015. The TLC has sent us the data in batches as it continues to review trip data Uber and other HFV companies have submitted to it. The TLC's correspondence with FiveThirtyEight is included in the files TLC_letter.pdf, TLC_letter2.pdf and TLC_letter3.pdf. TLC records requests can be made here.
This data was used for four FiveThirtyEight stories: Uber Is Serving New York’s Outer Boroughs More Than Taxis Are, Public Transit Should Be Uber’s New Best Friend, Uber Is Taking Millions Of Manhattan Rides Away From Taxis, and Is Uber Making NYC Rush-Hour Traffic Worse?.
The Data
The dataset contains, roughly, four groups of files:
Uber trip data from 2014 (April - September), separated by month, with detailed location information
Uber trip data from 2015 (January - June), with less fine-grained location information
non-Uber FHV (For-Hire Vehicle) trips. The trip information varies by company, but can include day of trip, time of trip, pickup location, driver's for-hire license number, and vehicle's for-hire license number.
aggregate ride and vehicle statistics for all FHV companies (and, occasionally, for taxi companies)
Uber trip data from 2014
There are six files of raw data on Uber pickups in New York City from April to September 2014. The files are separated by month and each has the following columns:
Date/Time : The date and time of the Uber pickup
Lat : The latitude of the Uber pickup
Lon : The longitude of the Uber pickup
Base : The TLC base company code affiliated with the Uber pickup
These files are named:
uber-raw-data-apr14.csv
uber-raw-data-aug14.csv
uber-raw-data-jul14.csv
uber-raw-data-jun14.csv
uber-raw-data-may14.csv
uber-raw-data-sep14.csv
Uber trip data from 2015
Also included is the file uber-raw-data-janjune-15.csv This file has the following columns:
Dispatching_base_num : The TLC base company code of the base that dispatched the Uber
Pickup_date : The date and time of the Uber pickup
Affiliated_base_num : The TLC base company code affiliated with the Uber pickup
locationID : The pickup location ID affiliated with the Uber pickup
The Base codes are for the following Uber bases:
B02512 : Unter B02598 : Hinter B02617 : Weiter B02682 : Schmecken B02764 : Danach-NY B02765 : Grun B02835 : Dreist B02836 : Drinnen
For coarse-grained location information from these pickups, the file taxi-zone-lookup.csv shows the taxi Zone (essentially, neighborhood) and Borough for each locationID.
Non-Uber FLV trips
The dataset also contains 10 files of raw data on pickups from 10 for-hire vehicle (FHV) companies. The trip information varies by company, but can include day of trip, time of trip, pickup location, driver's for-hire license number, and vehicle's for-hire license number.
These files are named:
American_B01362.csv
Diplo_B01196.csv
Highclass_B01717.csv
Skyline_B00111.csv
Carmel_B00256.csv
Federal_02216.csv
Lyft_B02510.csv
Dial7_B00887.csv
Firstclass_B01536.csv
Prestige_B01338.csv
Aggregate Statistics
There is also a file other-FHV-data-jan-aug-2015.csv containing daily pickup data for 329 FHV companies from January 2015 through August 2015.
The file Uber-Jan-Feb-FOIL.csv contains aggregated daily Uber trip statistics in January and February 2015."
Getting Real about Fake News,Text & metadata from fake & biased news sources around the web,Megan Risdal,179,"Version 1,2016-11-26","news agencies
languages
politics",CSV,54 MB,CC0,"83,505 views","6,220 downloads",60 kernels,5 topics,https://www.kaggle.com/mrisdal/fake-news,"The latest hot topic in the news is fake news and many are wondering what data scientists can do to detect it and stymie its viral spread. This dataset is only a first step in understanding and tackling this problem. It contains text and metadata scraped from 244 websites tagged as ""bullshit"" by the BS Detector Chrome Extension by Daniel Sieradski.
Warning: I did not modify the list of news sources from the BS Detector so as not to introduce my (useless) layer of bias; I'm not an authority on fake news. There may be sources whose inclusion you disagree with. It's up to you to decide how to work with the data and how you might contribute to ""improving it"". The labels of ""bs"" and ""junksci"", etc. do not constitute capital ""t"" Truth. If there are other sources you would like to include, start a discussion. If there are sources you believe should not be included, start a discussion or write a kernel analyzing the data. Or take the data and do something else productive with it. Kaggle's choice to host this dataset is not meant to express any particular political affiliation or intent.
Contents
The dataset contains text and metadata from 244 websites and represents 12,999 posts in total from the past 30 days. The data was pulled using the webhose.io API; because it's coming from their crawler, not all websites identified by the BS Detector are present in this dataset. Each website was labeled according to the BS Detector as documented here. Data sources that were missing a label were simply assigned a label of ""bs"". There are (ostensibly) no genuine, reliable, or trustworthy news sources represented in this dataset (so far), so don't trust anything you read.
Fake news in the news
For inspiration, I've included some (presumably non-fake) recent stories covering fake news in the news. This is a sensitive, nuanced topic and if there are other resources you'd like to see included here, please leave a suggestion. From defining fake, biased, and misleading news in the first place to deciding how to take action (a blacklist is not a good answer), there's a lot of information to consider beyond what can be neatly arranged in a CSV file.
How Fake News Spreads (NYT)
We Tracked Down A Fake-News Creator In The Suburbs. Here's What We Learned (NPR)
Does Facebook Generate Over Half of its Revenue from Fake News? (Forbes)
Fake News is Not the Only Problem (Points - Medium)
Washington Post Disgracefully Promotes a McCarthyite Blacklist From a New, Hidden, and Very Shady Group (The Intercept)
Improvements
If you have suggestions for improvements or would like to contribute, please let me know. The most obvious extensions are to include data from ""real"" news sites and to address the bias in the current list. I'd be happy to include any contributions in future versions of the dataset.
Acknowledgements
Thanks to Anthony for pointing me to Daniel Sieradski's BS Detector. Thank you to Daniel Nouri for encouraging me to add a disclaimer to the dataset's page."
SF Salaries,Explore San Francisco city employee salary data,Kaggle,172,"Version 2,2016-10-06|Version 1,2015-12-22",income,SQLite,33 MB,CC0,"95,375 views","14,929 downloads",266 kernels,7 topics,https://www.kaggle.com/kaggle/sf-salaries,"One way to understand how a city government works is by looking at who it employs and how its employees are compensated. This data contains the names, job title, and compensation for San Francisco city employees on an annual basis from 2011 to 2014.
Exploration Ideas
To help get you started, here are some data exploration ideas:
How have salaries changed over time between different groups of people?
How are base pay, overtime pay, and benefits allocated between different groups?
Is there any evidence of pay discrimination based on gender in this dataset?
How is budget allocated based on different groups and responsibilities?
Have other ideas you're curious for someone else to explore? Post them in this forum thread.
Data Description
sf-salaries-release-*.zip (downloadable via the ""Download Data"" link in the header above) contains a CSV table and a SQLite database (with the same data as the CSV file). Here's the code that creates this data release.
The original source for this data is here. We've taken the raw files here and combined/normalized them into a single CSV file as well as a SQLite database with an equivalently-defined table."
UFO Sightings,Reports of unidentified flying object reports in the last century,National UFO Reporting Center (NUFORC),170,"Version 1,2016-11-17",space,CSV,28 MB,Other,"48,914 views","7,234 downloads",151 kernels,2 topics,https://www.kaggle.com/NUFORC/ufo-sightings,"Context
This dataset contains over 80,000 reports of UFO sightings over the last century.
Content
There are two versions of this dataset: scrubbed and complete. The complete data includes entries where the location of the sighting was not found or blank (0.8146%) or have an erroneous or blank time (8.0237%). Since the reports date back to the 20th century, some older data might be obscured. Data contains city, state, time, description, and duration of each sighting.
Inspiration
What areas of the country are most likely to have UFO sightings?
Are there any trends in UFO sightings over time? Do they tend to be clustered or seasonal?
Do clusters of UFO sightings correlate with landmarks, such as airports or government research centers?
What are the most common UFO descriptions?
Acknowledgement
This dataset was scraped, geolocated, and time standardized from NUFORC data by Sigmond Axel here."
Death in the United States,Learn more about the leading causes of death from 2005-2015,Centers for Disease Control and Prevention,168,"Version 2,2017-08-04|Version 1,2016-03-10","death
demographics",{}JSON,4 GB,CC0,"60,962 views","10,077 downloads",188 kernels,7 topics,https://www.kaggle.com/cdc/mortality,"Every year the CDC releases the country’s most detailed report on death in the United States under the National Vital Statistics Systems. This mortality dataset is a record of every death in the country for 2005 through 2015, including detailed information about causes of death and the demographic background of the deceased.
It's been said that ""statistics are human beings with the tears wiped off."" This is especially true with this dataset. Each death record represents somebody's loved one, often connected with a lifetime of memories and sometimes tragically too short.
Putting the sensitive nature of the topic aside, analyzing mortality data is essential to understanding the complex circumstances of death across the country. The US Government uses this data to determine life expectancy and understand how death in the U.S. differs from the rest of the world. Whether you’re looking for macro trends or analyzing unique circumstances, we challenge you to use this dataset to find your own answers to one of life’s great mysteries.
Overview
This dataset is a collection of CSV files each containing one year's worth of data and paired JSON files containing the code mappings, plus an ICD 10 code set. The CSVs were reformatted from their original fixed-width file formats using information extracted from the CDC's PDF manuals using this script. Please note that this process may have introduced errors as the text extracted from the pdf is not a perfect match. If you have any questions or find errors in the preparation process, please leave a note in the forums. We hope to publish additional years of data using this method soon.
A more detailed overview of the data can be found here. You'll find that the fields are consistent within this time window, but some of data codes change every few years. For example, the 113_cause_recode entry 069 only covers ICD codes (I10,I12) in 2005, but by 2015 it covers (I10,I12,I15). When I post data from years prior to 2005, expect some of the fields themselves to change as well.
All data comes from the CDC’s National Vital Statistics Systems, with the exception of the Icd10Code, which are sourced from the World Health Organization.
Project ideas
The CDC's mortality data was the basis of a widely publicized paper, by Anne Case and Nobel prize winner Angus Deaton, arguing that middle-aged whites are dying at elevated rates. One of the criticisms against the paper is that it failed to properly account for the exact ages within the broad bins available through the CDC's WONDER tool. What do these results look like with exact/not-binned age data?
Similarly, how sensitive are the mortality trends being discussed in the news to the choice of bin-widths?
As noted above, the data preparation process could have introduced errors. Can you find any discrepancies compared to the aggregate metrics on WONDER? If so, please let me know in the forums!
WONDER is cited in numerous economics, sociology, and public health research papers. Can you find any papers whose conclusions would be altered if they used the exact data available here rather than binned data from Wonder?
Differences from the first version of the dataset
This version of the dataset was prepared in a completely different many. This has allowed us to provide a much larger volume of data and ensure that codes are available for every field.
We've replaced the batch of sql files with a single JSON per year. Kaggle's platform currently offer's better support for JSON files, and this keeps the number of files manageable.
A tutorial kernel providing a quick introduction to the new format is available here.
Lastly, I apologize if the transition has interrupted anyone's work! If need be, you can still download v1."
Indian Premier League,Data for all the IPL seasons,Manas,167,"Version 5,2017-11-23|Version 4,2016-11-06|Version 3,2016-11-06|Version 2,2016-11-06|Version 1,2016-11-06","cricket
india",CSV,1 MB,CC4,"51,036 views","9,011 downloads",274 kernels,4 topics,https://www.kaggle.com/manasgarg/ipl,"This is the ball by ball data of all the IPL cricket matches till season 9.
Source: http://cricsheet.org/ (data is available on this website in the YAML format. This is converted to CSV format by the contributors)
The dataset contains 2 files: deliveries.csv and matches.csv. matches.csv contains details related to the match such as location, contesting teams, umpires, results, etc. deliveries.csv is the ball-by-ball data of all the IPL matches including data of the batting team, batsman, bowler, non-striker, runs scored, etc.
Research scope: Predicting the winner of the next season of IPL based on past data, Visualizations, Perspectives, etc."
Monty Python Flying Circus,"Remember, buy Whizzo butter and this dead crab.",Allan,163,"Version 1,2017-09-03","humor
mass media",SQLite,4 MB,ODbL,"11,214 views","1,270 downloads",4 kernels,,https://www.kaggle.com/allank/monty-python-flying-circus,"Context
45 episodes across 4 seasons of Monty Python's Flying Circus - all of the scripts broken down into reusable bits.
Content
The data attempts to create a structure around the Flying Circus scripts by breaking actions down into Dialogue (someone is speaking) and Direction (instructions for the actors). Along with each action I have tried to allocate the episode number, episode name, recording date, air date, segment name, name of character and name of actor playing the character.
Acknowledgements
The scripts are hosted in HTML at http://www.ibras.dk/montypython/justthewords.htm I have loaded all of the code that I wrote to scrape and process the data (warning: very messy) at https://github.com/allank/monty-python
Inspiration
I scraped the data because I was looking at data sources for doing RNN to generate text based on an existing corpus. While the amount of data available in the Flying Circus scripts is probably not sufficient, there might be some interesting things to do with the data. For example, some Markov chain generated dialogue lines:
Remember, buy Whizzo butter and this dead crab. Yeah, er, I, I personally think this is getting too silly. I don't like the sound of two bricks being bashed together."
Heartbeat Sounds,Classifying heartbeat anomalies from stethoscope audio,Ed King,161,"Version 1,2016-11-27","healthcare
human genetics
sound technology",CSV,152 MB,CC0,"36,595 views","3,376 downloads",32 kernels,,https://www.kaggle.com/kinguistics/heartbeat-sounds,"Try your hand at automatically separating normal heartbeats from abnormal heartbeats and heart murmur with this machine learning challenge by Peter Bentley et al
The Data
Here's a brief overview of the format of this dataset as uploaded to Kaggle. For a more detailed description, look at the Description section below.
The dataset is split into two sources, A and B: A was collected from the general public via an iPhone app, and B was collected from a clinical trial in hospitals using a digital stethoscope.
The goal of the task is to first (1) identify the locations of heart sounds from the audio, and (2) to classify the heart sounds into one of several categories (normal v. various non-normal heartbeat sounds).
The CSV files provided are: set_a.csv
set_b.csv set_a_timing.csv
The fields for set_a and set_b are as follows:
dataset: a or b
fname: the audio file
label: either ""normal"", blank (for unlabelled data), or one of various categories of abnormal heartbeats
sublabel: in set_b, some recordings are categorized as noisy, meaning they contain non-heart background noise; this field holds information on whether something is e.g. ""noisynormal"" or ""noisymurmur""
The file set_a_timing.csv contains gold-standard timing information for the ""normal"" recordings from Set A. This file contains the following fields:
fname: the audio file
cycle: anywhere from 1 to 19; the heartbeat cycle that the time observation refers to
sound: either S1 or S2; see below for what these mean
location: the time location of this sound, in audio samples
Description
The task, as described by the original authors
Task Overview
Data has been gathered from two sources: (A) from the general public via the iStethoscope Pro iPhone app, provided in Dataset A, and (B) from a clinic trial in hospitals using the digital stethoscope DigiScope, provided in Dataset B.
CHALLENGE 1 - Heart Sound Segmentation
The first challenge is to produce a method that can locate S1(lub) and S2(dub) sounds within audio data, segmenting the Normal audio files in both datasets. To enable your machine learning method to learn we provide the exact location of S1 and S2 sounds for some of the audio files. You need to use them to identify and locate the S1 and S2 sounds of all the heartbeats in the unlabelled group. The locations of sounds are measured in audio samples for better precision. Your method must use the same unit.
CHALLENGE 2 - Heart Sound Classification
The task is to produce a method that can classify real heart audio (also known as “beat classification”) into one of four categories for Dataset A:
Normal
Murmur
Extra Heart Sound
Artifact
and three classes for Dataset B:
Normal
Murmur
Extrasystole
You may tackle either or both of these challenges. If you can solve the first challenge, the second will be considerably easier! The winner of each challenge will be the method best able to segment and/or classify two sets of unlabelled data into the correct categories after training on both datasets provided below.
[Obviously no longer applicable -- ed.]: The creator of the winning method will receive a WiFi 32Gb iPad as the prize, awarded at a workshop at AISTATS 2012.
The audio files are of varying lengths, between 1 second and 30 seconds (some have been clipped to reduce excessive noise and provide the salient fragment of the sound). Most information in heart sounds is contained in the low frequency components, with noise in the higher frequencies. It is common to apply a low-pass filter at 195 Hz. Fast Fourier transforms are also likely to provide useful information about volume and frequency over time. More domain-specific knowledge about the difference between the categories of sounds is provided below.
Normal Category
In the Normal category there are normal, healthy heart sounds. These may contain noise in the final second of the recording as the device is removed from the body. They may contain a variety of background noises (from traffic to radios). They may also contain occasional random noise corresponding to breathing, or brushing the microphone against clothing or skin. A normal heart sound has a clear “lub dub, lub dub” pattern, with the time from “lub” to “dub” shorter than the time from “dub” to the next “lub” (when the heart rate is less than 140 beats per minute). Note the temporal description of “lub” and “dub” locations over time in the following illustration:
…lub……….dub……………. lub……….dub……………. lub……….dub……………. lub……….dub…
In medicine we call the lub sound ""S1"" and the dub sound ""S2"". Most normal heart rates at rest will be between about 60 and 100 beats (‘lub dub’s) per minute. However, note that since the data may have been collected from children or adults in calm or excited states, the heart rates in the data may vary from 40 to 140 beats or higher per minute. Dataset B also contains noisy_normal data - normal data which includes a substantial amount of background noise or distortion. You may choose to use this or ignore it, however the test set will include some equally noisy examples.
Murmur Category
Heart murmurs sound as though there is a “whooshing, roaring, rumbling, or turbulent fluid” noise in one of two temporal locations: (1) between “lub” and “dub”, or (2) between “dub” and “lub”. They can be a symptom of many heart disorders, some serious. There will still be a “lub” and a “dub”. One of the things that confuses non-medically trained people is that murmurs happen between lub and dub or between dub and lub; not on lub and not on dub. Below, you can find an asterisk* at the locations a murmur may be.
…lub..*...dub……………. lub..*..dub ……………. lub..*..dub ……………. lub..*..dub … or …lub……….dub…*….lub………. dub…*….lub ………. dub…**….lub ……….dub…
Dataset B also contains noisy_murmur data - murmur data which includes a substantial amount of background noise or distortion. You may choose to use this or ignore it, however the test set will include some equally noisy examples
Extra Heart Sound Category (Dataset A)
Extra heart sounds can be identified because there is an additional sound, e.g. a “lub-lub dub” or a “lub dub-dub”. An extra heart sound may not be a sign of disease. However, in some situations it is an important sign of disease, which if detected early could help a person. The extra heart sound is important to be able to detect as it cannot be detected by ultrasound very well. Below, note the temporal description of the extra heart sounds:
…lub.lub……….dub………..………. lub. lub……….dub…………….lub.lub……..…….dub……. or …lub………. dub.dub………………….lub.……….dub.dub………………….lub……..…….dub. dub……
Artifact Category (Dataset A)
In the Artifact category there are a wide range of different sounds, including feedback squeals and echoes, speech, music and noise. There are usually no discernable heart sounds, and thus little or no temporal periodicity at frequencies below 195 Hz. This category is the most different from the others. It is important to be able to distinguish this category from the other three categories, so that someone gathering the data can be instructed to try again.
Extrasystole Category (Dataset B)
Extrasystole sounds may appear occasionally and can be identified because there is a heart sound that is out of rhythm involving extra or skipped heartbeats, e.g. a “lub-lub dub” or a “lub dub-dub”. (This is not the same as an extra heart sound as the event is not regularly occuring.) An extrasystole may not be a sign of disease. It can happen normally in an adult and can be very common in children. However, in some situations extrasystoles can be caused by heart diseases. If these diseases are detected earlier, then treatment is likely to be more effective. Below, note the temporal description of the extra heart sounds: …........lub……….dub………..………. lub. ………..……….dub…………….lub.lub……..…….dub……. or …lub………. dub......………………….lub.…………………dub.dub………………….lub……..…….dub.……
Acknowledgments
Please use the following citation if the data is used:
@misc{pascal-chsc-2011, author = ""Bentley, P. and Nordehn, G. and Coimbra, M. and Mannor, S."", title = ""The {PASCAL} {C}lassifying {H}eart {S}ounds {C}hallenge 2011 {(CHSC2011)} {R}esults"", howpublished = ""http://www.peterjbentley.com/heartchallenge/index.html""}"
A 6-figure prize by soccer prediction (Live Feed),"Clean, yet rich dataset of 7300 soccer matches & LIVE FEED",Mohammad Ghahramani,161,"Version 10,2017-09-19|Version 9,2017-09-18|Version 8,2017-09-18|Version 7,2017-09-07|Version 6,2017-09-01|Version 5,2017-09-01|Version 4,2017-09-01|Version 3,2017-08-26|Version 2,2017-08-26|Version 1,2017-08-26","association football
world
sports",CSV,17 MB,CC0,"9,497 views","2,374 downloads",4 kernels,12 topics,https://www.kaggle.com/analystmasters/earn-your-6-figure-prize,"Live Feed
Please comment on the Discussion above ""Live Feed"" and I will share details with you in a message. Hope we won't exceed server limitations.
Context
I have been recording available different types of data on soccer matches since 2012, live 24/7. The whole database contains more than 350,000 soccer matches held all around the world from over 27,000 teams of more than 180 countries. An all-in-one package including servers, algorithms and its database are now under the ""Analyst Masters"" research platform. The app is also free for everyone to get its predictions on Android Play Store . How could it become useful for a data scientist?
Did you know that,
more than 1000 soccer matches are played in a week?
the average profit of the stock market from its beginning to now has been less than 10% a year? but you can earn at least 10% on a single match in 2 hours and get your profit in cash
It is one of the very rare datasets that you do not need to prove to other companies your method is the most accurate one and get the prize :) . On the other hand you do not have to classify every data point to be rewarded. Just tune or focus to correctly classify only 1% of matches and there you go! Let me give you a simple hint how easily it can become a classification problem rather than a time series prediction:
Example 1: Who wins based on the number of wins in a head 2 head history?
Q) Consider two teams Midtjylland and Randers from Denmark. They have played against each other for very long time. Midtjyland has won Randers over 8 times in the past 10 matches in a 4 year time span. Forget any other complicated algorithm and simply predict who wins this match?
A) That is easy! However, I am also gathering a lot more information than just their history. You can check their head-to-head history and the odds you could get for predicting this match is ""1.73"" check here.
Example 2: Number of Goals based on their history?
Q) Consider two teams ""San Martin S.J."" and ""Rosario Central"" from Argentina. Their odds for wining ""Team 1 (Home)"", ""Draw"" and ""Team 2 (away)"" is [3.16, 3.2, 2.25] respectively. They rank 22 and 13 in their league. They have recently won 45%,35% of their matches in their past 14 matches. Their average head to head goals in their last 7 matches were 1.3 full time (F) and 0.3 until half-time (HT). How many goals do you think they score in their match? (Note that a safe side of number of goals in soccer betting is Over 0.5 goals in HT, Under 1.5 goals in HT, Over 1.5 goals in FT and Under 3.5 goals in FT). Which one do you choose?
A) For sure under 1.5 goals in HT (you get 35%) and under 3.5 goals in FT (you get 30%) . Bingo you get 65% in a single match in 2 hours
Example 3: Based on the money placed for betting on teams who wins the match?
Q) ""Memmingen"" and ""Munich 1860"" are well known in Germany. One of our reliable sources of data is the ratio of money placed on betting from 10 hours before the match until it starts. Assume that the ratio of bets on ""Munich 1860"" to ""Memmingen"" are recorded every hour as below, which team do you think will win?
[bets in $ on Munich 1860]/[bets in $ on Memmingen] : {1.01, 1.02, 1.04, 1.1, 1.2, 1.4, 1.58, 2.3, 2.6, 2.8}
A) in 10 hours the amount of money placed on wining Munich 1860 Vs Memmingen increased from 1.01 to 2.8, who is the winner? Easy again, Munich 1860 that gives you 160% as stated here.
Try the dataset and inspect every strategy you may come up with, as I gave you three reliable examples above. Just perform well enough to predict 15 matches correctly in a row, start with $1000 and you are a millionaire. If you can't be that accurate use the Kelly Criterion to divide your whole money into smaller stakes.
Let me do the math for you, if you can only get 90% accuracy on 1% of data points (10 out of 1000 matches a week) and your average profit on each match is only 20%. You earn (9*20% = 180%) and lose 100% for your error in 10 predictions. Your net profit would be 80% in a week or approximately 12% in a day. if you risk only 33% of your whole money on each match then the daily net profit becomes 4%. I guess you can easily calculate how fast you can progress @ 4% daily accumulative profit.
For sure one needs a live data feed to predict the outcome before the match. If everything goes well and enough users are interested I will open the live feed of data for you in a shared folder of Dropbox saved in CSV.
Content
Here is what the dataset contains for 'n' matches:
names6.csv
: team names as the order of ""home-away"" separated using ""/"" ; size : (n x 1)
results6.csv*
: Scores recorded during the match, every 2 rows show scores for one match ; size : (2n x 14)
fresults6.csv*
: Final scores after full-time ; size : (n x 2)
odds6.csv
: odds in the order of: Home-Draw-Away ; size : (n x 3)
dollars6.csv*
: Ratio of the money spent on teams at 15 minutes intervals ; size : (n x 76)
ranks6.csv
: their ranks in the league at the day of the match Irrespectively ; size : (n x 2)
winrate6.csv
: their winrate In the last (maximum 14) matches in 2017 Irrespectively ; size : (n x 2)
country6.csv
: their country as some countries are difficult to analyze e.g. Belarussia ; size : (n x 1)
wins6.csv
: number of wins in their last (maximum 6) head to head matches ; size : (n x 1)
FT_HT6.csv
: average of total goals in their last (maximum 6) head to head matches FT and HT ; size : (n x 2)
*. recorded every 15 minutes
Try to predict the match as examples above using the given data in the zip file. I will upload the respective data from Mid of August to Mid September later on.
Acknowledgements
I developed various data scrappers and classifiers running on multiple servers worldwide and never published a paper due to their sensitivity. You may refer to this database by mentioning the ""Analyst Masters"" research package.
Inspiration
10 years ago I invented the world's first home-size cooking robot in my father's basement but in the end after cooking for us for 2 years it ended up in nothing. So, you as a data scientist can earn money using this live data stream for yourself if you can perform accurately without outperforming others in the competition just get an acceptable accuracy and you are good to go :)
For more information on the overall platform and its live, pre-match and in-play analysis read at www.analystmasters.com or download the app for FREE to get easy predictions at 5% profit per week. More details on how the app operates is available at https://youtu.be/fqlu0YEyqc0"
Yelp Dataset,"A trove of reviews, businesses, users, tips, and check-in data!","Yelp, Inc.",160,"Version 6,2018-02-07|Version 5,2018-02-07|Version 4,2018-01-23|Version 3,2018-01-23|Version 2,2018-01-18|Version 1,2018-01-18",food and drink,CSV,3 GB,Other,"21,641 views","3,347 downloads",29 kernels,6 topics,https://www.kaggle.com/yelp-dataset/yelp-dataset,"Context
This dataset is a subset of Yelp's businesses, reviews, and user data. It was originally put together for the Yelp Dataset Challenge which is a chance for students to conduct research or analysis on Yelp's data and share their discoveries. In the dataset you'll find information about businesses across 11 metropolitan areas in four countries.
Content
This dataset contains seven CSV files. The original JSON files can be found in yelp_academic_dataset.zip.

You may find this documentation helpful:
https://www.yelp.com/dataset/documentation/json
In total, there are :
5,200,000 user reviews
Information on 174,000 businesses
The data spans 11 metropolitan areas
Acknowledgements
The dataset was converted from JSON to CSV format and we thank the team of the Yelp dataset challenge for creating this dataset.
By downloading this dataset, you agree to the Yelp Dataset Terms of Use.
Inspiration
Natural Language Processing & Sentiment Analysis
What's in a review? Is it positive or negative? Yelp's reviews contain a lot of metadata that can be mined and used to infer meaning, business attributes, and sentiment.
Graph Mining
We recently launched our Local Graph but can you take the graph further? How do user's relationships define their usage patterns? Where are the trend setters eating before it becomes popular?"
Used cars database,"Over 370,000 used cars scraped from Ebay Kleinanzeigen",Orges Leka,156,"Version 3,2016-11-28|Version 2,2016-11-28|Version 1,2016-11-20",automobiles,CSV,65 MB,CC0,"82,725 views","11,526 downloads",209 kernels,7 topics,https://www.kaggle.com/orgesleka/used-cars-database,"If you want to download and experiment with the Scrapy script, you can do so from forum data science This page is a forum for data scientist I started, in hope , that you will participate and maybe even improve the scrapy script.
Over 370000 used cars scraped with Scrapy from Ebay-Kleinanzeigen. The content of the data is in german, so one has to translate it first if one can not speak german. Those fields are included: autos.csv:
dateCrawled : when this ad was first crawled, all field-values are taken from this date
name : ""name"" of the car
seller : private or dealer
offerType
price : the price on the ad to sell the car
abtest
vehicleType
yearOfRegistration : at which year the car was first registered
gearbox
powerPS : power of the car in PS
model
kilometer : how many kilometers the car has driven
monthOfRegistration : at which month the car was first registered
fuelType
brand
notRepairedDamage : if the car has a damage which is not repaired yet
dateCreated : the date for which the ad at ebay was created
nrOfPictures : number of pictures in the ad (unfortunately this field contains everywhere a 0 and is thus useless (bug in crawler!) )
postalCode
lastSeenOnline : when the crawler saw this ad last online
The fields lastSeen and dateCreated could be used to estimate how long a car will be at least online before it is sold.
brought to you by Orges Leka
Regression on average Price per Year based on this dataset
Table of value loss of an average used car per year
The second file is produced in MySQL from the first one through the query:
select 
 count(*) as count, 
 kilometer, 
 yearOfRegistration, 
20*round(powerPS/20) as powerPS, 
min(price) as minprice, 
max(price) as maxPrice, 
avg(price) as avgPreis, 
sqrt(variance(price)) as sdPreis from items where 
     yearOfRegistration > 1990 and yearOfRegistration < 2016 
    and price > 100 and price < 100000 
    and powerPS < 600 and powerPS > 0 
 group by yearOfRegistration, round(powerPS/20),kilometer 
having count > 10 
into outfile '/tmp/cnt_km_year_powerPS_minPrice_maxPrice_avgPrice_sdPrice.csv' 
fields terminated by ',' lines terminated by '\n';
Happy Coding!"
Video Game Sales with Ratings,Video game sales from Vgchartz and corresponding ratings from Metacritic,Rush Kirubi,154,"Version 2,2016-12-30|Version 1,2016-12-22",video games,CSV,2 MB,Other,"61,430 views","10,594 downloads",184 kernels,8 topics,https://www.kaggle.com/rush4ratio/video-game-sales-with-ratings,"Context
Motivated by Gregory Smith's web scrape of VGChartz Video Games Sales, this data set simply extends the number of variables with another web scrape from Metacritic. Unfortunately, there are missing observations as Metacritic only covers a subset of the platforms. Also, a game may not have all the observations of the additional variables discussed below. Complete cases are ~ 6,900
Content
Alongside the fields: Name, Platform, Year_of_Release, Genre, Publisher, NA_Sales, EU_Sales, JP_Sales, Other_Sales, Global_Sales, we have:-
Critic_score - Aggregate score compiled by Metacritic staff
Critic_count - The number of critics used in coming up with the Critic_score
User_score - Score by Metacritic's subscribers
User_count - Number of users who gave the user_score
Developer - Party responsible for creating the game
Rating - The ESRB ratings
Acknowledgements
This repository, https://github.com/wtamu-cisresearch/scraper, after a few adjustments worked extremely well!
Inspiration
It would be interesting to see any machine learning techniques or continued data visualizations applied on this data set."
US Baby Names,Explore naming trends from babies born in the US,Kaggle,153,"Version 2,2017-11-22|Version 1,2015-12-19",children,Other,173 MB,CC0,"93,894 views","10,822 downloads",417 kernels,10 topics,https://www.kaggle.com/kaggle/us-baby-names,"US Social Security applications are a great way to track trends in how babies born in the US are named.
Data.gov releases two datasets that are helplful for this: one at the national level and another at the state level. Note that only names with at least 5 babies born in the same year (/ state) are included in this dataset for privacy.
I've taken the raw files here and combined/normalized them into two CSV files (one for each dataset) as well as a SQLite database with two equivalently-defined tables. The code that did these transformations is available here.
New to data exploration in R? Take the free, interactive DataCamp course, ""Data Exploration With Kaggle Scripts,"" to learn the basics of visualizing data with ggplot. You'll also create your first Kaggle Scripts along the way."
"Homicide Reports, 1980-2014",Can you develop an algorithm to detect serial killer activity?,Murder Accountability Project,153,"Version 1,2017-02-10",crime,CSV,107 MB,CC4,"55,970 views","9,390 downloads",323 kernels,11 topics,https://www.kaggle.com/murderaccountability/homicide-reports,"Content
The Murder Accountability Project is the most complete database of homicides in the United States currently available. This dataset includes murders from the FBI's Supplementary Homicide Report from 1976 to the present and Freedom of Information Act data on more than 22,000 homicides that were not reported to the Justice Department. This dataset includes the age, race, sex, ethnicity of victims and perpetrators, in addition to the relationship between the victim and perpetrator and weapon used.
Acknowledgements
The data was compiled and made available by the Murder Accountability Project, founded by Thomas Hargrove."
Dota 2 Matches,Explore player behavior and predict match outcomes.,devin,139,"Version 3,2016-11-06|Version 2,2016-10-25|Version 1,2016-10-24",video games,CSV,1 GB,CC4,"35,491 views","3,684 downloads",132 kernels,2 topics,https://www.kaggle.com/devinanzelmo/dota-2-matches,"Overview
This dataset contains 50000 ranked ladder matches from the Dota 2 data dump created by Opendota. It was inspired by the Dota 2 Matches data published here by Joe Ramir. This is an update and improved version of that dataset. I have kept the same image and a similar title.
Dota 2 is a popular MOBA available as free to play, and can take up thousands of hours of your life. The number of games in this dataset are played about every hour. If you like the data there are an additional 2-3 million matches easily available for download.
The aim of this dataset is to enable the exploration of player behavior, skill estimation, or anything you find interesting. The intent is to create an accessible, and easy to use resource, which can be expanded and modified if needed. As such I am open to a wide variety of suggestions as to what additions or changes to make.
Help getting started
If there is some aspect of this data you would like to explore but seems difficult to get figure out how to work with please feel free to request some starter code in one of the following two Kernels discussion section. I usually check kaggle every day or so. If you post a request about the current data I will try to get something working.
Python https://www.kaggle.com/devinanzelmo/d/devinanzelmo/dota-2-matches/misc-howtos-dota-requests-welcome/
R https://www.kaggle.com/devinanzelmo/d/devinanzelmo/dota-2-matches/howtos-request-welcome/
Whats Currently Available
See https://github.com/odota/core/wiki/JSON-Data-Dump for documentaion on data. I have found a few undocumented areas in the data, including the objectives information. player_slot can be used to combine most of the data, and it is available in most of the tables. Additionally all tables include match_id, and some have account_id to make it easier to look at an individual players matches. match_id, and account_id have been reencoded to save a little space. I can upload tables to allow conversion if needed.
matches: contains top level information about each match. see https://wiki.teamfortress.com/wiki/WebAPI/GetMatchDetails#Tower_Status%22tower_status_dire%22:%202047) for interpreting tower and barracks status. Cluster can link matches to geographic region.
players: Individual players are identified by account_id but there is an option to play anonymously and roughly one third of the account_id are not available. Anonymous users have the value of 0 for account_id. Contains totals for kills, deaths, denies, etc. Player action counts are available, and are indicated by variable names beginning with unit_order_. Counts for reasons for acquiring or losing gold, and gaining experience, have prefixes gold_, and xp_.
player_time: Contains last hits, experience, and gold sampled at one minute interval for all players in all matches. The column names indicate the player_slot. For instance xp_t_1 indicates that this column has experience sums for the player in slot one.
teamfights: Start and stop time of teamfights, as well as last death time. Teamfights appear to be all battles with three or more deaths. As such this does not include all battles for the entire match.
teamfights_players : Additional information provided for each player in each teamfight. player_slot can be used to link this back to players.csv
objectives: Gives information on all the objectives completed, by which player and at what time.
chat: All chat for the 50k matches. There is plenty of profanity, and good natured trolling.
test_labels: match_id and radiant_win(as integer 1 or 0)
test_player: full player and match table with hero_id, player_slot, match_id, and account_id
Nov 5th Update
Added several additional tables. None of the previously uploaded data was altered. I plan to add several Kernels in the next week going over how to use the data, and performing some EDA. Many improvements to the player rating method I used are possible for those interested in MMR.
player_ratings contains match counts, win counts, and TrueSkill rating, calculated on 900k matches which occurred prior to other uploaded data. trueskill ratings have two components, mu, which can be interpreted as the skill, with higher value being better, and sigma which is the uncertainty of the rating.
match_outcomes data for ~900k matches used to calculate player ratings. Use this to improve on the ratings I uploaded.
purchase_log item purchase times
ability_upgrade ability upgrade times and levels
cluster_region allows the mapping cluster found in match.csv to geographic region.
patch_dates release dates for various patches, use start_time from match.csv to determine which patch a match was played in.
ability_ids use with ability_upgrades.csv to get the names of upgraded abilities
item_ids use with purchase_log.csv to get the names of purchased items
Kernel showing how player skill was computed: Contains several resources on trueskill rating system.
Past Research
There seem to be some efforts to establish indicators for skillfull play based on specific parts of gameplay. Opendota has many statistics, and some analysis for specific benchmarks at different times in the game. Dotabuff has a lot of information I have not explored it deeply. This is an area to gather more information.
Some possible directions of investigation
Insight from domain experts would also be useful to help clarify what problems are interesting to work on. Some initial task ideas
Predict match outcomes based on aggregates for individual players using only account_id as prior information
Add hero id to this and see if there is a differences in performance
Estimate player skill based on a sample of in game play(this might need an external mmr source or different definition skill)
Create improved indicators of skillful play based game actions to help players target areas for improvement
All of these areas have been worked on, but I am not aware of the most up to date research on dota2 gameplay.
I plan on setting up several different predictive tasks in the upcoming weeks. A test set of an additional 50 to 100 thousand matches with just hero_id, and account_id included along with outcome of the match.
The current dataset seems pretty small for modeling individual players. I would prefer to have a wide range of features instead of a larger dataset for the moment.
Dataset idea for anyone interested in creating their own Dota 2 dataset. It would be useful to have a few full matches available to work on. They would need to be extracted from the .dem replay file to something easily parsed by R and Python as available in kernels. Given the size of a full match data only a few matches would be needed. There are files available from opendota' s website(check for replays). Looking at fine grained match details would potentially allow for the creation of better high level parsed data. I think it would be a lot of work just to get a handle on working with full match data so a sample would be good to have.
Acknowledgements
Orginal kaggle dataset on dota2 matches by Joe Ramir I also borrowed the image and some of the content for these acknowledgements from the above, thanks!.
image source
Data download source created by yasp
Description of original dataset creation: https://github.com/yasp-dota/yasp/issues/924
yasp's license
""License: CC BY-SA 4.0""
""Terms: We ask that you attribute yasp.co if you create or publish anything related to our data. Also, please seed for as long as possible.""
Yasp is now known as opendota here are links to their website and github page
https://www.opendota.com/ the data is used to for this site and its a easy way to get familier with it
https://github.com/odota/core check here for info especially this wiki page which gives details on the schema."
Airplane Crashes Since 1908,"Full history of airplane crashes throughout the world, from 1908-present",Sauro Grandi,139,"Version 4,2016-09-10|Version 3,2016-05-21|Version 2,2016-05-21|Version 1,2016-05-21",aviation,CSV,2 MB,ODbL,"63,487 views","15,228 downloads",415 kernels,5 topics,https://www.kaggle.com/saurograndi/airplane-crashes-since-1908,"Analysis of the public dataset: ""Airplane Crashes and Fatalities Since 1908"" (Full history of airplane crashes throughout the world, from 1908-present) hosted by Open Data by Socrata available at:
https://opendata.socrata.com/Government/Airplane-Crashes-and-Fatalities-Since-1908/q2te-8cvq
Questions
Yearly how many planes crashed? how many people were on board? how many survived? how many died?
Highest number of crashes by operator and Type of aircrafts.
‘Summary’ field has the details about the crashes. Find the reasons of the crash and categorize them in different clusters i.e Fire, shot down, weather (for the ‘Blanks’ in the data category can be UNKNOWN) you are open to make clusters of your choice but they should not exceed 7.
Find the number of crashed aircrafts and number of deaths against each category from above step.
Find any interesting trends/behaviors that you encounter when you analyze the dataset.
My solution
The following bar charts display the answers requested by point 1. of the assignment, in particular:
the planes crashed per year
people aboard per year during crashes
people dead per year during crashes
people survived per year during crashes
The following answers regard point 2 of the assignment
Highest number of crashes by operator: Aeroflot with 179 crashes
By Type of aircraft: Douglas DC-3 with 334 crashes
I have identified 7 clusters using k-means clustering technique on a matrix obtained by a text corpus created by using Text Analysis (plain text, remove punctuation, to lower, etc.) The following table summarize for each cluster the number of crashes and death.
Cluster 1: 258 crashes, 6368 deaths
Cluster 2: 500 crashes, 9408 deaths
Cluster 3: 211 crashes, 3513 deaths
Cluster 4: 1014 crashes, 14790 deaths
Cluster 5: 2749 crashes, 58826 deaths
Cluster 6: 195 crashes, 4439 deaths
Cluster 7: 341 crashes, 8135 deaths
The following picture shows clusters using the first 2 principal components:
For each clusters I will summarize the most used words and I will try to identify the causes of the crash
Cluster 1 (258) aircraft, crashed, plane, shortly, taking. No many information about this cluster can be deducted using Text Analysis
Cluster 2 (500) aircraft, airport, altitude, crashed, crew, due, engine, failed, failure, fire, flight, landing, lost, pilot, plane, runway, takeoff, taking. Engine failure on the runway after landing or takeoff
Cluster 3 (211): aircraft, crashed, fog Crash caused by fog
Cluster 4 (1014): aircraft, airport, attempting, cargo, crashed, fire, land, landing, miles, pilot, plane, route, runway, struck, takeoff Struck a cargo during landing or takeoff
Cluster 5 (2749): accident, aircraft, airport, altitude, approach, attempting, cargo, conditions, control, crashed, crew, due, engine, failed, failure, feet, fire, flight, flying, fog, ground, killed, land, landing, lost, low, miles, mountain, pilot. plane, poor, route, runway, short, shortly, struck, takeoff, taking, weather
Struck a cargo due to engine failure or bad weather conditions mainly fog
Cluster 6 (195): aircraft, crashed, engine, failure, fire, flight, left, pilot, plane, runway
Engine failure on the runway
Cluster 7 (341): accident, aircraft, altitude, cargo, control, crashed, crew, due, engine, failure, flight, landing, loss, lost, pilot, plane, takeoff
Engine failure during landing or takeoff
Better solutions are welcome. Thanks."
US Mass Shootings,Last 50 Years (1966-2017),Zeeshan-ul-hassan Usmani,139,"Version 4,2017-11-06|Version 3,2017-11-05|Version 3,2017-11-05|Version 2,2017-10-11|Version 1,2017-10-03","united states
crime
violence
terrorism",CSV,679 KB,ODbL,"34,332 views","4,893 downloads",40 kernels,18 topics,https://www.kaggle.com/zusmani/us-mass-shootings-last-50-years,"Context
Mass Shootings in the United States of America (1966-2017) The US has witnessed 398 mass shootings in last 50 years that resulted in 1,996 deaths and 2,488 injured. The latest and the worst mass shooting of October 2, 2017 killed 58 and injured 515 so far. The number of people injured in this attack is more than the number of people injured in all mass shootings of 2015 and 2016 combined. The average number of mass shootings per year is 7 for the last 50 years that would claim 39 lives and 48 injured per year.
Content
Geography: United States of America
Time period: 1966-2017
Unit of analysis: Mass Shooting Attack
Dataset: The dataset contains detailed information of 398 mass shootings in the United States of America that killed 1996 and injured 2488 people.
Variables: The dataset contains Serial No, Title, Location, Date, Summary, Fatalities, Injured, Total Victims, Mental Health Issue, Race, Gender, and Lat-Long information.
Acknowledgements
I’ve consulted several public datasets and web pages to compile this data. Some of the major data sources include Wikipedia, Mother Jones, Stanford, USA Today and other web sources.
Inspiration
With a broken heart, I like to call the attention of my fellow Kagglers to use Machine Learning and Data Sciences to help me explore these ideas:
• How many people got killed and injured per year?
• Visualize mass shootings on the U.S map
• Is there any correlation between shooter and his/her race, gender
• Any correlation with calendar dates? Do we have more deadly days, weeks or months on average
• What cities and states are more prone to such attacks
• Can you find and combine any other external datasets to enrich the analysis, for example, gun ownership by state
• Any other pattern you see that can help in prediction, crowd safety or in-depth analysis of the event
• How many shooters have some kind of mental health problem? Can we compare that shooter with general population with same condition
Mass Shootings Dataset Ver 3
This is the new Version of Mass Shootings Dataset. I've added eight new variables:
Incident Area (where the incident took place),
Open/Close Location (Inside a building or open space)
Target (possible target audience or company),
Cause (Terrorism, Hate Crime, Fun (for no obvious reason etc.)
Policeman Killed (how many on duty officers got killed)
Age (age of the shooter)
Employed (Y/N)
Employed at (Employer Name)
Age, Employed and Employed at (3 variables) contain shooter details
Mass Shootings Dataset Ver 4
Quite a few missing values have been added
Mass Shootings Dataset Ver 5
Three more recent mass shootings have been added including the Texas Church shooting of November 5, 2017
I hope it will help create more visualization and extract patterns.
Keep Coding!"
Can You Predict Product Backorders?,Based on historical data predict backorder risk for products,tiredgeek,138,"Version 4,2017-04-27|Version 3,2017-04-27|Version 2,2017-04-27|Version 1,2017-03-20","business
management",CSV,134 MB,CC4,"45,608 views","6,034 downloads",109 kernels,6 topics,https://www.kaggle.com/tiredgeek/predict-bo-trial,"Context
Part backorders is a common supply chain problem. Working to identify parts at risk of backorder before the event occurs so the business has time to react.
Content
Training data file contains the historical data for the 8 weeks prior to the week we are trying to predict. The data was taken as weekly snapshots at the start of each week. Columns are defined as follows:
sku - Random ID for the product
national_inv - Current inventory level for the part
lead_time - Transit time for product (if available)
in_transit_qty - Amount of product in transit from source
forecast_3_month - Forecast sales for the next 3 months
forecast_6_month - Forecast sales for the next 6 months
forecast_9_month - Forecast sales for the next 9 months
sales_1_month - Sales quantity for the prior 1 month time period
sales_3_month - Sales quantity for the prior 3 month time period
sales_6_month - Sales quantity for the prior 6 month time period
sales_9_month - Sales quantity for the prior 9 month time period
min_bank - Minimum recommend amount to stock
potential_issue - Source issue for part identified
pieces_past_due - Parts overdue from source
perf_6_month_avg - Source performance for prior 6 month period
perf_12_month_avg - Source performance for prior 12 month period
local_bo_qty - Amount of stock orders overdue
deck_risk - Part risk flag
oe_constraint - Part risk flag
ppap_risk - Part risk flag
stop_auto_buy - Part risk flag
rev_stop - Part risk flag
went_on_backorder - Product actually went on backorder. This is the target value."
SMS Spam Collection Dataset,Collection of SMS messages tagged as spam or legitimate,UCI Machine Learning,138,"Version 1,2016-12-03","languages
linguistics
human-computer interaction",CSV,492 KB,Other,"50,596 views","7,255 downloads",158 kernels,0 topics,https://www.kaggle.com/uciml/sms-spam-collection-dataset,"Context
The SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam.
Content
The files contain one message per line. Each line is composed by two columns: v1 contains the label (ham or spam) and v2 contains the raw text.
Acknowledgements
The original dataset can be found here. The creators would like to note that in case you find the dataset useful, please make a reference to previous paper and the web page: http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/ in your papers, research, etc.
Inspiration
Can you use this dataset to build a prediction model that will accurately classify which texts are spam?"
U.S. Pollution Data,Pollution in the U.S. since 2000,BrendaSo,137,"Version 1,2016-11-05","environment
pollution",CSV,382 MB,ODbL,"56,876 views","7,252 downloads",79 kernels,5 topics,https://www.kaggle.com/sogun3/uspollution,"Context
This dataset deals with pollution in the U.S. Pollution in the U.S. has been well documented by the U.S. EPA but it is a pain to download all the data and arrange them in a format that interests data scientists. Hence I gathered four major pollutants (Nitrogen Dioxide, Sulphur Dioxide, Carbon Monoxide and Ozone) for every day from 2000 - 2016 and place them neatly in a CSV file.
Content
There is a total of 28 fields. The four pollutants (NO2, O3, SO2 and O3) each has 5 specific columns. Observations totaled to over 1.4 million. This kernel provides a good introduction to this dataset!
For observations on specific columns visit the Column Metadata on the Data tab.
Acknowledgements
All the data is scraped from the database of U.S. EPA : https://aqsdr1.epa.gov/aqsweb/aqstmp/airdata/download_files.html
Inspiration
I did a related project with some of my friends in college, and decided to open source our dataset so that data scientists don't need to re-scrape the U.S. EPA site for historical pollution data."
2015 Flight Delays and Cancellations,Which airline should you fly on to avoid significant delays?,Department of Transportation,136,"Version 1,2017-02-10",aviation,CSV,565 MB,CC0,"64,292 views","12,508 downloads",80 kernels,10 topics,https://www.kaggle.com/usdot/flight-delays,"Context
The U.S. Department of Transportation's (DOT) Bureau of Transportation Statistics tracks the on-time performance of domestic flights operated by large air carriers. Summary information on the number of on-time, delayed, canceled, and diverted flights is published in DOT's monthly Air Travel Consumer Report and in this dataset of 2015 flight delays and cancellations.
Acknowledgements
The flight delay and cancellation data was collected and published by the DOT's Bureau of Transportation Statistics."
FIFA 18 Complete Player Dataset,"17k+ players, 70+ attributes extracted from the latest edition of FIFA",Aman Shrivastava,135,"Version 5,2017-10-30|Version 4,2017-10-30|Version 3,2017-09-28|Version 2,2017-09-26|Version 1,2017-09-16","popular culture
video games
association football",CSV,15 MB,CC4,"45,156 views","5,277 downloads",33 kernels,13 topics,https://www.kaggle.com/thec03u5/fifa-18-demo-player-dataset,"The Dataset you can play with.
Context
Dataset for people who love data science and have grown up playing FIFA.
Content
Every player featuring in FIFA 18
70+ attributes
Player and Flag Images
Playing Position Data
Attributes based on actual data of the latest EA's FIFA 18 game
Attributes include on all player style statistics like Dribbling, Aggression, GK Skills etc.
Player personal data like Nationality, Photo, Club, Age, Wage, Salary etc.
Upcoming Update will Include :
Team (National and Club) Data
Player Images in Zip folder
Betting Odds
The dataset contains all the statistics and playing attributes of all the players in the Full version of FIFA 18.
Data Source
The data is scraped from the website https://sofifa.com by extracting the Player personal data and Player Ids and then the playing and style statistics.
Github Project
Possible Explorations
Make your dream team
Analyse which Club or National Team has the best-rated players
Assess the strength of a team at a particular position
Analyse the team with the best dribbling speed
Co-relate between Age and Overall rating
Co-relate between Age and Nationality
Co-relate between Age and Potential
Could prove of immense value to Fantasy Premier League enthusiasts.
These are just basic examples, sky is the limit.
Acknowledgements
The data has been crawled from the https://sofifa.com website.
Inspiration
Several insights and correlations between player value, wage, age, and performance can be derived from the dataset. Furthermore, how do the players in this dataset compare against themselves in last year's dataset?
Contributing
Changes and Improvement suggestions are welcome. Feel free to comment new additions that you think are useful or drop a PR on the github project."
Meta Kaggle,"Kaggle's public data on competitions, users, submission scores, and kernels",Kaggle,134,"Version 7,2016-07-21|Version 6,2016-07-06|Version 5,2016-06-23|Version 4,2016-06-23|Version 3,2016-05-04|Version 2,2016-04-20|Version 1,2015-09-09","statistics
telecommunications",CSV,2 GB,CC4,"45,477 views","3,441 downloads",414 kernels,23 topics,https://www.kaggle.com/kaggle/meta-kaggle,"We aren't saying this dataset is the Rosetta Stone of machine learning competitions, but we do think there is a lot to learn from (and a lot of fun to be had by) releasing some of our most interesting tables on Kaggle community and competition activity.
Strategizing to become a Master? Wondering who, where, and what goes in to a winning team? Deciding between evaluation metrics for your next data science project? We hope the scripts published here will enrich and entertain Kagglers, spark some lively conversations, and act as a resource for the larger machine learning community.
This data (available through Kaggle Scripts as CSV files and a SQLite database) contains the tables listed below.
Note that this data is not a complete dump: rows, columns, and tables have been filtered out, and it is a small subset of the data that we can release publicly. Over time, we'll add more of the tables that we can release publicly to it."
Complete FIFA 2017 Player dataset (Global),"15k+ players, 50+ Attributes per player from the latest EA Sports Fifa 17",Soumitra Agarwal,133,"Version 5,2017-04-13|Version 4,2017-04-12|Version 3,2017-04-12|Version 2,2017-04-02|Version 1,2017-04-01","video games
association football",CSV,9 MB,ODbL,"39,473 views","5,801 downloads",91 kernels,5 topics,https://www.kaggle.com/artimous/complete-fifa-2017-player-dataset-global,"The dataset for people who double on Fifa and Data Science
Content
17,000+ players
50+ attributes per player ranging from ball skills aggression etc.
Player's attributes sourced from EA Sports' FIFA video game series, including the weekly updates
Players from all around the globe
URLs to their homepage
Club logos
Player images male and female
National and club team data
Weekly Updates would include :
Real life data (Match events etc.)
The fifa generated player dataset
Betting odds
Growth
Data Source
Data was scraped from https://www.fifaindex.com/ first by getting player profile url set (as stored in PlayerNames.csv) and then scraping the individual pages for their attributes
Improvements
You may have noticed that for a lot of players, their national details are absent (Team and kit number) even though the nationality is listed. This may be attributed to the missing data on fifa sites.
GITHUB PROJECT
There is much more than just 50 attributes by which fifa decides what happens to players over time, how they perform under pressure, how they grow etc. This data obviously would be well hidden by the organisation and thus would be tough to find
Important note for people interested in using the scraping: The site is not uniform and thus the scraping script requires considering a lot of corner cases (i.e. interchanged position of different attributes). Also the script contains proxy preferences which may be removed if not required.
Exploring the data
For starters you can become a scout:
Create attribute dependent or overall best teams
Create the fastest/slowest teams
See which areas of the world provide which attributes (like Africa : Stamina, Pace)
See which players are the best at each position
See which outfield players can play a better role at some other position
See which youngsters have attributes which can be developed
And that is just the beginning. This is the playground.. literally!
Data description
The file FullData.csv contains attributes describing the in game play style and also some of the real statistics such as Nationality etc.
The file PlayerNames.csv contains URLs for different players from their profiles on fifaindex.com. Append the URLs after the base url fifaindex.com.
The compressed file Pictures.zip contains pictures for top 1000 players in Fifa 17.
The compressed file Pictures_f.zip contains pictures for top 139 female players in Fifa 17.
The compressed file ClubPictures.zip contains pictures for emblems of some major clubs in Fifa 17.
Inspiration
I am a huge FIFA fanatic. While playing career mode I realised that I picked great young players early on every single time and since a lot of digital learning relies on how our brain works, I thought scouting great qualities in players would be something that can be worked on. Since then I started working on scraping the website and here is the data. I hope we can build something on it.
With access to players attributes you can become the best scout in the world. Go for it!"
Netflix Prize data,Dataset from Netflix's competition to improve their reccommendation algorithm,Netflix,131,"Version 1,2017-07-20","film
artificial intelligence",Other,2 GB,Other,"40,645 views","3,967 downloads",4 kernels,2 topics,https://www.kaggle.com/netflix-inc/netflix-prize-data,"Context
Netflix held the Netflix Prize open competition for the best algorithm to predict user ratings for films. The grand prize was $1,000,000 and was won by BellKor's Pragmatic Chaos team. This is the dataset that was used in that competition.
Content
This comes directly from the README:
TRAINING DATASET FILE DESCRIPTION
The file ""training_set.tar"" is a tar of a directory containing 17770 files, one per movie. The first line of each file contains the movie id followed by a colon. Each subsequent line in the file corresponds to a rating from a customer and its date in the following format:
CustomerID,Rating,Date
MovieIDs range from 1 to 17770 sequentially.
CustomerIDs range from 1 to 2649429, with gaps. There are 480189 users.
Ratings are on a five star (integral) scale from 1 to 5.
Dates have the format YYYY-MM-DD.
MOVIES FILE DESCRIPTION
Movie information in ""movie_titles.txt"" is in the following format:
MovieID,YearOfRelease,Title
MovieID do not correspond to actual Netflix movie ids or IMDB movie ids.
YearOfRelease can range from 1890 to 2005 and may correspond to the release of corresponding DVD, not necessarily its theaterical release.
Title is the Netflix movie title and may not correspond to titles used on other sites. Titles are in English.
QUALIFYING AND PREDICTION DATASET FILE DESCRIPTION
The qualifying dataset for the Netflix Prize is contained in the text file ""qualifying.txt"". It consists of lines indicating a movie id, followed by a colon, and then customer ids and rating dates, one per line for that movie id. The movie and customer ids are contained in the training set. Of course the ratings are withheld. There are no empty lines in the file.
MovieID1:
CustomerID11,Date11
CustomerID12,Date12
...
MovieID2:
CustomerID21,Date21
CustomerID22,Date22
For the Netflix Prize, your program must predict the all ratings the customers gave the movies in the qualifying dataset based on the information in the training dataset.
The format of your submitted prediction file follows the movie and customer id, date order of the qualifying dataset. However, your predicted rating takes the place of the corresponding customer id (and date), one per line.
For example, if the qualifying dataset looked like:
111:
3245,2005-12-19
5666,2005-12-23
6789,2005-03-14
225:
1234,2005-05-26
3456,2005-11-07
then a prediction file should look something like:
111:
3.0
3.4
4.0
225:
1.0
2.0
which predicts that customer 3245 would have rated movie 111 3.0 stars on the 19th of Decemeber, 2005, that customer 5666 would have rated it slightly higher at 3.4 stars on the 23rd of Decemeber, 2005, etc.
You must make predictions for all customers for all movies in the qualifying dataset.
THE PROBE DATASET FILE DESCRIPTION
To allow you to test your system before you submit a prediction set based on the qualifying dataset, we have provided a probe dataset in the file ""probe.txt"". This text file contains lines indicating a movie id, followed by a colon, and then customer ids, one per line for that movie id.
MovieID1:
CustomerID11
CustomerID12
...
MovieID2:
CustomerID21
CustomerID22
Like the qualifying dataset, the movie and customer id pairs are contained in the training set. However, unlike the qualifying dataset, the ratings (and dates) for each pair are contained in the training dataset.
If you wish, you may calculate the RMSE of your predictions against those ratings and compare your RMSE against the Cinematch RMSE on the same data. See http://www.netflixprize.com/faq#probe for that value.
Acknowledgements
The training data came in 17,000+ files. In the interest of keeping files together and file sizes as low as possible, I combined them into four text files: combined_data_(1,2,3,4).txt
The contest was originally hosted at http://netflixprize.com/index.html
The dataset was downloaded from https://archive.org/download/nf_prize_dataset.tar
Inspiration
This is a fun dataset to work with. You can read about the winning algorithm by BellKor's Pragmatic Chaos here"
The Enron Email Dataset,"500,000+ emails from 150 employees of the Enron Corporation",William Cukierski,128,"Version 2,2016-06-17|Version 1,2016-06-16","crime
linguistics",CSV,1 GB,Other,"62,396 views","6,596 downloads",144 kernels,2 topics,https://www.kaggle.com/wcukierski/enron-email-dataset,"The Enron email dataset contains approximately 500,000 emails generated by employees of the Enron Corporation. It was obtained by the Federal Energy Regulatory Commission during its investigation of Enron's collapse.
This is the May 7, 2015 Version of dataset, as published at https://www.cs.cmu.edu/~./enron/"
NBA shot logs,"Moneyball data, for basketball.",DanB,127,"Version 1,2016-08-18","basketball
sports",CSV,16 MB,Other,"45,505 views","5,458 downloads",148 kernels,18 topics,https://www.kaggle.com/dansbecker/nba-shot-logs,"Data on shots taken during the 2014-2015 season, who took the shot, where on the floor was the shot taken from, who was the nearest defender, how far away was the nearest defender, time on the shot clock, and much more. The column titles are generally self-explanatory.
Useful for evaluating who the best shooter is, who the best defender is, the hot-hand hypothesis, etc.
Scraped from NBA's REST API."
Synthetic Financial Datasets For Fraud Detection,Synthetic datasets generated by the PaySim mobile money simulator,TESTIMON @ NTNU,127,"Version 2,2017-04-03|Version 1,2017-03-31","crime
finance",CSV,471 MB,CC4,"45,657 views","5,222 downloads",67 kernels,10 topics,https://www.kaggle.com/ntnu-testimon/paysim1,"Context
There is a lack of public available datasets on financial services and specially in the emerging mobile money transactions domain. Financial datasets are important to many researchers and in particular to us performing research in the domain of fraud detection. Part of the problem is the intrinsically private nature of financial transactions, that leads to no publicly available datasets.
We present a synthetic dataset generated using the simulator called PaySim as an approach to such a problem. PaySim uses aggregated data from the private dataset to generate a synthetic dataset that resembles the normal operation of transactions and injects malicious behaviour to later evaluate the performance of fraud detection methods.
Content
PaySim simulates mobile money transactions based on a sample of real transactions extracted from one month of financial logs from a mobile money service implemented in an African country. The original logs were provided by a multinational company, who is the provider of the mobile financial service which is currently running in more than 14 countries all around the world.
This synthetic dataset is scaled down 1/4 of the original dataset and it is created just for Kaggle.
Headers
This is a sample of 1 row with headers explanation:
1,PAYMENT,1060.31,C429214117,1089.0,28.69,M1591654462,0.0,0.0,0,0
step - maps a unit of time in the real world. In this case 1 step is 1 hour of time. Total steps 744 (30 days simulation).
type - CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER.
amount - amount of the transaction in local currency.
nameOrig - customer who started the transaction
oldbalanceOrg - initial balance before the transaction
newbalanceOrig - new balance after the transaction
nameDest - customer who is the recipient of the transaction
oldbalanceDest - initial balance recipient before the transaction. Note that there is not information for customers that start with M (Merchants).
newbalanceDest - new balance recipient after the transaction. Note that there is not information for customers that start with M (Merchants).
isFraud - This is the transactions made by the fraudulent agents inside the simulation. In this specific dataset the fraudulent behavior of the agents aims to profit by taking control or customers accounts and try to empty the funds by transferring to another account and then cashing out of the system.
isFlaggedFraud - The business model aims to control massive transfers from one account to another and flags illegal attempts. An illegal attempt in this dataset is an attempt to transfer more than 200.000 in a single transaction.
Past Research
There are 5 similar files that contain the run of 5 different scenarios. These files are better explained at my PhD thesis chapter 7 (PhD Thesis Available here http://urn.kb.se/resolve?urn=urn:nbn:se:bth-12932).
We ran PaySim several times using random seeds for 744 steps, representing each hour of one month of real time, which matches the original logs. Each run took around 45 minutes on an i7 intel processor with 16GB of RAM. The final result of a run contains approximately 24 million of financial records divided into the 5 types of categories: CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER.
Acknowledgements
This work is part of the research project ”Scalable resource-efficient systems for big data analytics” funded by the Knowledge Foundation (grant: 20140032) in Sweden.
Please refer to this dataset using the following citations:
PaySim first paper of the simulator:
E. A. Lopez-Rojas , A. Elmir, and S. Axelsson. ""PaySim: A financial mobile money simulator for fraud detection"". In: The 28th European Modeling and Simulation Symposium-EMSS, Larnaca, Cyprus. 2016"
YouTube Faces With Facial Keypoints,Videos of Celebrity Faces with Facial Keypoints for each Image Frame,Selfish Gene,126,"Version 22,2017-11-14|Version 21,2017-11-13|Version 20,2017-11-13|Version 19,2017-11-12|Version 18,2017-11-12|Version 17,2017-11-11|Version 16,2017-11-10|Version 15,2017-11-10|Version 14,2017-10-27|Version 13,2017-10-27|Version 12,2017-10-27|Version 11,2017-10-27|Version 10,2017-10-23|Version 9,2017-10-23|Version 8,2017-10-23|Version 7,2017-10-15|Version 6,2017-10-15|Version 5,2017-10-15|Version 4,2017-10-14|Version 3,2017-10-13|Version 2,2017-10-13|Version 1,2017-10-13","popular culture
celebrity
humans
internet",Other,10 GB,CC0,"16,902 views","1,133 downloads",3 kernels,,https://www.kaggle.com/selfishgene/youtube-faces-with-facial-keypoints,"YouTube Faces Dataset with Facial Keypoints
This dataset is a processed version of the YouTube Faces Dataset, that basically contained short videos of celebrities that are publicly available and were downloaded from YouTube. There are multiple videos of each celebrity (up to 6 videos per celebrity). I've cropped the original videos around the faces, plus kept only consecutive frames of up to 240 frames for each original video. This is done also for reasons of disk space, but mainly to make the dataset easier to use.
Additionally, for this kaggle version of the dataset I've extracted facial keypoints for each frame of each video using this amazing 2D and 3D Face alignment library that was recently published. please check out this video demonstrating the library. It's performance is really amazing, and I feel I'm quite qualified to say that after manually curating many thousands of individual frames and their corresponding keypoints. I removed all videos with extremely bad keypoints labeling. The end result of my curation process is approximately 2800 videos. Right now only 1293 of those videos are uploaded due to dataset size limitations (10GB), but since overall this totals into 155,560 single image frames, I think this is more than enough to do a lot of interesting kernels as well as potentially very interesting research.
Context
Kaggle datasets platform and its integration with kernels is really amazing, but it's yet to have a videos dataset (at least that I'm aware of). Videos are special in the fact that they contain rich spatial patterns (in this case images of human faces) and rich temporal patterns (in this case how the faces move in time).
I was also inspired by the Face Images with Marked Landmark Points dataset uploaded by DrGuillermo and decided to create and share a dataset that would be similar but would also add something extra.
Acknowledgements
If you use The YouTube Faces Dataset, or refer to its results, please cite the following paper:
Lior Wolf, Tal Hassner and Itay Maoz
Face Recognition in Unconstrained Videos with Matched Background Similarity.
IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2011. (pdf)
if you use the 2D or 3D keypoints, or refer to its results, please cite the following paper:
Adrian Bulat and Georgios Tzimiropoulos.
How far are we from solving the 2D & 3D Face Alignment problem?
(and a dataset of 230,000 3D facial landmarks), arxiv, 2017. (pdf)
Also, I would like to thank Gil Levi for pointing out YouTube Faces to me a few years back.
Inspiration
The YouTube Faces Dataset was originally intended to be used for face recognition across videos, i.e. given two videos, are those videos of the same person or not?
I think it can be used to serve many additional goals, especially when combined with the keypoints information. For example, can we build a face movement model and predict what facial expression will come next?
This dataset can also be used to test transfer learning between other face datasets (like Face Images with Marked Landmark Points that I mentioned earlier), or even other types of faces like cat or dog faces (like here or here). Also, using the pre-trained Keras models might be useful (example kernel).
Have Fun!"
Default of Credit Card Clients Dataset,Default Payments of Credit Card Clients in Taiwan from 2005,UCI Machine Learning,124,"Version 1,2016-11-03",finance,CSV,3 MB,CC0,"74,601 views","7,420 downloads",135 kernels,5 topics,https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset,"Dataset Information
This dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005.
Content
There are 25 variables:
ID: ID of each client
LIMIT_BAL: Amount of given credit in NT dollars (includes individual and family/supplementary credit
SEX: Gender (1=male, 2=female)
EDUCATION: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)
MARRIAGE: Marital status (1=married, 2=single, 3=others)
AGE: Age in years
PAY_0: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, ... 8=payment delay for eight months, 9=payment delay for nine months and above)
PAY_2: Repayment status in August, 2005 (scale same as above)
PAY_3: Repayment status in July, 2005 (scale same as above)
PAY_4: Repayment status in June, 2005 (scale same as above)
PAY_5: Repayment status in May, 2005 (scale same as above)
PAY_6: Repayment status in April, 2005 (scale same as above)
BILL_AMT1: Amount of bill statement in September, 2005 (NT dollar)
BILL_AMT2: Amount of bill statement in August, 2005 (NT dollar)
BILL_AMT3: Amount of bill statement in July, 2005 (NT dollar)
BILL_AMT4: Amount of bill statement in June, 2005 (NT dollar)
BILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)
BILL_AMT6: Amount of bill statement in April, 2005 (NT dollar)
PAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)
PAY_AMT2: Amount of previous payment in August, 2005 (NT dollar)
PAY_AMT3: Amount of previous payment in July, 2005 (NT dollar)
PAY_AMT4: Amount of previous payment in June, 2005 (NT dollar)
PAY_AMT5: Amount of previous payment in May, 2005 (NT dollar)
PAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)
default.payment.next.month: Default payment (1=yes, 0=no)
Inspiration
Some ideas for exploration:
How does the probability of default payment vary by categories of different demographic variables?
Which variables are the strongest predictors of default payment?
Acknowledgements
Any publications based on this dataset should acknowledge the following:
Lichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.
The original dataset can be found here at the UCI Machine Learning Repository."
Emergency - 911 Calls,"Montgomery County, PA",Mike Chirico,121,"Version 48,2017-12-30|Version 47,2017-12-30|Version 46,2017-11-04|Version 45,2017-09-21|Version 44,2017-08-19|Version 43,2017-05-29|Version 42,2017-04-14|Version 41,2017-04-10|Version 40,2017-03-16|Version 39,2017-03-09|Version 38,2017-03-09|Version 37,2017-02-25|Version 36,2017-02-18|Version 35,2017-02-10|Version 34,2017-02-09|Version 33,2017-02-09|Version 32,2017-01-28|Version 31,2017-01-18|Version 30,2017-01-09|Version 29,2017-01-01|Version 28,2016-12-30|Version 27,2016-12-24|Version 26,2016-12-18|Version 25,2016-12-18|Version 24,2016-12-18|Version 23,2016-12-14|Version 22,2016-11-30|Version 21,2016-11-20|Version 20,2016-11-13|Version 19,2016-11-05|Version 18,2016-11-03|Version 17,2016-10-30|Version 16,2016-10-29|Version 15,2016-10-26|Version 14,2016-10-17|Version 13,2016-10-13|Version 12,2016-10-08|Version 11,2016-10-07|Version 10,2016-09-30|Version 9,2016-09-25|Version 8,2016-09-21|Version 7,2016-09-17|Version 6,2016-09-09|Version 5,2016-09-07|Version 4,2016-08-31|Version 3,2016-08-24|Version 2,2016-08-18|Version 1,2016-08-18","crime
law",CSV,11 MB,ODbL,"58,170 views","6,687 downloads",309 kernels,3 topics,https://www.kaggle.com/mchirico/montcoalert,"Emergency (911) Calls: Fire, Traffic, EMS for Montgomery County, PA
You can get a quick introduction to this Dataset with this kernel: Dataset Walk-through
Acknowledgements: Data provided by montcoalert.org"
Pima Indians Diabetes Database,Predict the onset of diabetes based on diagnostic measures,UCI Machine Learning,120,"Version 1,2016-10-07",healthcare,CSV,23 KB,CC0,"65,473 views","9,433 downloads",368 kernels,7 topics,https://www.kaggle.com/uciml/pima-indians-diabetes-database,"This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective is to predict based on diagnostic measurements whether a patient has diabetes.
Dataset information
Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.
Relevant papers
Smith, J.W., Everhart, J.E., Dickson, W.C., Knowler, W.C., & Johannes, R.S. (1988). Using the ADAP learning algorithm to forecast the onset of diabetes mellitus. In Proceedings of the Symposium on Computer Applications and Medical Care (pp. 261--265). IEEE Computer Society Press."
Exoplanet Hunting in Deep Space,Kepler labelled time series data,WΔ,116,"Version 3,2017-04-12|Version 2,2017-04-04|Version 1,2017-04-03","astronomy
space",CSV,278 MB,CC0,"25,599 views","2,231 downloads",30 kernels,6 topics,https://www.kaggle.com/keplersmachines/kepler-labelled-time-series-data,"The Search for New Earths
GitHub
The data describe the change in flux (light intensity) of several thousand stars. Each star has a binary label of 2 or 1. 2 indicated that that the star is confirmed to have at least one exoplanet in orbit; some observations are in fact multi-planet systems.
As you can imagine, planets themselves do not emit light, but the stars that they orbit do. If said star is watched over several months or years, there may be a regular 'dimming' of the flux (the light intensity). This is evidence that there may be an orbiting body around the star; such a star could be considered to be a 'candidate' system. Further study of our candidate system, for example by a satellite that captures light at a different wavelength, could solidify the belief that the candidate can in fact be 'confirmed'.
In the above diagram, a star is orbited by a blue planet. At t = 1, the starlight intensity drops because it is partially obscured by the planet, given our position. The starlight rises back to its original value at t = 2. The graph in each box shows the measured flux (light intensity) at each time interval.
Description
Trainset:
5087 rows or observations.
3198 columns or features.
Column 1 is the label vector. Columns 2 - 3198 are the flux values over time.
37 confirmed exoplanet-stars and 5050 non-exoplanet-stars.
Testset:
570 rows or observations.
3198 columns or features.
Column 1 is the label vector. Columns 2 - 3198 are the flux values over time.
5 confirmed exoplanet-stars and 565 non-exoplanet-stars.
Acknowledgements
The data presented here are cleaned and are derived from observations made by the NASA Kepler space telescope. The Mission is ongoing - for instance data from Campaign 12 was released on 8th March 2017. Over 99% of this dataset originates from Campaign 3. To boost the number of exoplanet-stars in the dataset, confirmed exoplanets from other campaigns were also included.
To be clear, all observations from Campaign 3 are included. And in addition to this, confirmed exoplanet-stars from other campaigns are also included.
The datasets were prepared late-summer 2016.
Campaign 3 was used because 'it was felt' that this Campaign is unlikely to contain any undiscovered (i.e. wrongly labelled) exoplanets.
NASA open-sources the original Kepler Mission data and it is hosted at the Mikulski Archive. After being beamed down to Earth, NASA applies de-noising algorithms to remove artefacts generated by the telescope. The data - in the .fits format - is stored online. And with the help of a seasoned astrophysicist, anyone with an internet connection can embark on a search to find and retrieve the datafiles from the Archive.
The cover image is copyright © 2011 by Dan Lessmann"
May 2015 Reddit Comments,Get personal with a dataset of comments from May 2015,Reddit,115,"Version 1,2015-08-05","linguistics
reddit
internet",Other,0 B,Other,"46,111 views","4,626 downloads",608 kernels,11 topics,https://www.kaggle.com/reddit/reddit-comments-may-2015,"Recently Reddit released an enormous dataset containing all ~1.7 billion of their publicly available comments. The full dataset is an unwieldy 1+ terabyte uncompressed, so we've decided to host a small portion of the comments here for Kagglers to explore. (You don't even need to leave your browser!)
You can find all the comments from May 2015 on scripts for your natural language processing pleasure. What had redditors laughing, bickering, and NSFW-ing this spring?
Who knows? Top visualizations may just end up on Reddit.
Data Description
The database has one table, May2015, with the following fields:
created_utc
ups
subreddit_id
link_id
name
score_hidden
author_flair_css_class
author_flair_text
subreddit
id
removal_reason
gilded
downs
archived
author
score
retrieved_on
body
distinguished
edited
controversiality
parent_id"
Bitcoin Blockchain,Complete live historical Bitcoin blockchain data,Google BigQuery,115,"Version 1,2018-02-01","finance
money
internet
bigquery",BigQuery,821 GB,CC0,"21,005 views",0 downloads,405 kernels,4 topics,https://www.kaggle.com/bigquery/bitcoin-blockchain,"Context
Blockchain technology, first implemented by Satoshi Nakamoto in 2009 as a core component of Bitcoin, is a distributed, public ledger recording transactions. Its usage allows secure peer-to-peer communication by linking blocks containing hash pointers to a previous block, a timestamp, and transaction data. Bitcoin is a decentralized digital currency (cryptocurrency) which leverages the Blockchain to store transactions in a distributed manner in order to mitigate against flaws in the financial industry.
Nearly ten years after its inception, Bitcoin and other cryptocurrencies experienced an explosion in popular awareness. The value of Bitcoin, on the other hand, has experienced more volatility. Meanwhile, as use cases of Bitcoin and Blockchain grow, mature, and expand, hype and controversy have swirled.
Content
In this dataset, you will have access to information about blockchain blocks and transactions. All historical data are in the bigquery-public-data:bitcoin_blockchain dataset. It’s updated it every 10 minutes. The data can be joined with historical prices in kernels. See available similar datasets here: https://www.kaggle.com/datasets?search=bitcoin.
Querying BigQuery tables
You can use the BigQuery Python client library to query tables in this dataset in Kernels. Note that methods available in Kernels are limited to querying data. Tables are at bigquery-public-data.bitcoin_blockchain.[TABLENAME]. Fork this kernel to get started.
Method & Acknowledgements
Allen Day (Twitter | Medium), Google Cloud Developer Advocate & Colin Bookman, Google Cloud Customer Engineer retrieve data from the Bitcoin network using a custom client available on GitHub that they built with the bitcoinj Java library. Historical data from the origin block to 2018-01-31 were loaded in bulk to two BigQuery tables, blocks_raw and transactions. These tables contain fresh data, as they are now appended when new blocks are broadcast to the Bitcoin network. For additional information visit the Google Cloud Big Data and Machine Learning Blog post ""Bitcoin in BigQuery: Blockchain analytics on public data"".
Photo by Andre Francois on Unsplash.
Inspiration
How many bitcoins are sent each day?
How many addresses receive bitcoin each day?
Compare transaction volume to historical prices by joining with other available data sources"
"Stack Overflow Developer Survey, 2017","A look into the lives of over 64,000 Stack Overflow developers",Stack Overflow,113,"Version 2,2017-06-15|Version 1,2017-06-15","information technology
internet",CSV,89 MB,ODbL,"23,607 views","2,603 downloads",121 kernels,,https://www.kaggle.com/stackoverflow/so-survey-2017,"Every year, Stack Overflow conducts a massive survey of people on the site, covering all sorts of information like programming languages, salary, code style and various other information. This year, they amassed more than 64,000 responses fielded from 213 countries.
Data
The data is made up of two files:
1. survey_results_public.csv - CSV file with main survey results, one respondent per row and one column per answer
2. survey_results_schema.csv - CSV file with survey schema, i.e., the questions that correspond to each column name m
Acknowledgements
Data is directly taken from StackOverflow and licensed under the ODbL license."
US Dept of Education: College Scorecard,Raise the curtain on the true cost of higher education,Kaggle,112,"Version 3,2017-11-10|Version 2,2017-05-02|Version 1,2015-09-25","education
finance",CSV,4 GB,CC0,"56,642 views","9,242 downloads",191 kernels,8 topics,https://www.kaggle.com/kaggle/college-scorecard,"It's no secret that US university students often graduate with debt repayment obligations that far outstrip their employment and income prospects. While it's understood that students from elite colleges tend to earn more than graduates from less prestigious universities, the finer relationships between future income and university attendance are quite murky. In an effort to make educational investments less speculative, the US Department of Education has matched information from the student financial aid system with federal tax returns to create the College Scorecard dataset.
Kaggle is hosting the College Scorecard dataset in order to facilitate shared learning and collaboration. Insights from this dataset can help make the returns on higher education more transparent and, in turn, more fair.
Data Description
Here's a script showing an exploratory overview of some of the data.
college-scorecard-release-*.zip contains a compressed version of the same data available through Kaggle Scripts.
It consists of three components:
All the raw data files released in version 1.40 of the college scorecard data
Scorecard.csv, a single CSV file with all the years data combined. In it, we've converted categorical variables represented by integer keys in the original data to their labels and added a Year column
database.sqlite, a SQLite database containing a single Scorecard table that contains the same information as Scorecard.csv
New to data exploration in R? Take the free, interactive DataCamp course, ""Data Exploration With Kaggle Scripts,"" to learn the basics of visualizing data with ggplot. You'll also create your first Kaggle Scripts along the way."
Anime Recommendations Database,"Recommendation data from 76,000 users at myanimelist.net",CooperUnion,111,"Version 1,2016-12-21","popular culture
film",CSV,107 MB,CC0,"28,899 views","3,440 downloads",96 kernels,12 topics,https://www.kaggle.com/CooperUnion/anime-recommendations-database,"Context
This data set contains information on user preference data from 73,516 users on 12,294 anime. Each user is able to add anime to their completed list and give it a rating and this data set is a compilation of those ratings.
Content
Anime.csv
anime_id - myanimelist.net's unique id identifying an anime.
name - full name of anime.
genre - comma separated list of genres for this anime.
type - movie, TV, OVA, etc.
episodes - how many episodes in this show. (1 if movie).
rating - average rating out of 10 for this anime.
members - number of community members that are in this anime's ""group"".
Rating.csv
user_id - non identifiable randomly generated user id.
anime_id - the anime that this user has rated.
rating - rating out of 10 this user has assigned (-1 if the user watched it but didn't assign a rating).
Acknowledgements
Thanks to myanimelist.net API for providing anime data and user ratings.
Inspiration
Building a better anime recommendation system based only on user viewing history."
Board Games Dataset,What makes a game a good game?,Gabriele Baldassarre,110,"Version 5,2017-06-25|Version 4,2017-06-25|Version 3,2017-06-25|Version 2,2017-05-04|Version 1,2017-05-04",board games,SQLite,140 MB,Other,"19,357 views","2,575 downloads",20 kernels,,https://www.kaggle.com/gabrio/board-games-dataset,"About
A dataset containing the attributes and the ratings for around 94,000 among board games and expansions as get from BoardGameGeek.
Resources
The same data and the scripts used to crawl it from BoardGameGeek are available in the form of R package on Github."
EEG brain wave for confusion,For variable selection and causal inference. Challenging for classification,Haohan Wang,110,"Version 3,2016-12-01|Version 2,2016-08-28|Version 1,2016-08-19","healthcare
neuroscience",CSV,115 MB,CC0,"29,814 views","2,454 downloads",81 kernels,6 topics,https://www.kaggle.com/wanghaohan/eeg-brain-wave-for-confusion,"Description
We collected EEG signal data from 10 college students while they watched MOOC video clips. We extracted online education videos that are assumed not to be confusing for college students, such as videos of the introduction of basic algebra or geometry. We also prepare videos that are expected to confuse a typical college student if a student is not familiar with the video topics like Quantum Mechanics, and Stem Cell Research. We prepared 20 videos, 10 in each category. Each video was about 2 minutes long. We chopped the two-minute clip in the middle of a topic to make the videos more confusing. The students wore a single-channel wireless MindSet that measured activity over the frontal lobe. The MindSet measures the voltage between an electrode resting on the forehead and two electrodes (one ground and one reference) each in contact with an ear. After each session, the student rated his/her confusion level on a scale of 1-7, where one corresponded to the least confusing and seven corresponded to the most confusing. These labels if further normalized into labels of whether the students are confused or not. This label is offered as self-labelled confusion in addition to our predefined label of confusion.
Data information:
-----data.csv
Column 1: Subject ID
Column 2: Video ID
Column 3: Attention (Proprietary measure of mental focus)
Column 4: Mediation (Proprietary measure of calmness)
Column 5: Raw (Raw EEG signal)
Column 6: Delta (1-3 Hz of power spectrum)
Column 7: Theta (4-7 Hz of power spectrum)
Column 8: Alpha 1 (Lower 8-11 Hz of power spectrum)
Column 9: Alpha 2 (Higher 8-11 Hz of power spectrum)
Column 10: Beta 1 (Lower 12-29 Hz of power spectrum)
Column 11: Beta 2 (Higher 12-29 Hz of power spectrum)
Column 12: Gamma 1 (Lower 30-100 Hz of power spectrum)
Column 13: Gamma 2 (Higher 30-100 Hz of power spectrum)
Column 14: predefined label (whether the subject is expected to be confused)
Column 15: user-defined label (whether the subject is actually confused)
-----subject demographic
Column 1: Subject ID
Column 2: Age
Column 3: Ethnicity (Categorized according to https://en.wikipedia.org/wiki/List_of_contemporary_ethnic_groups)
Column 4: Gender
-----video data
Each video lasts roughly two-minute long, we remove the first 30 seconds and last 30 seconds, only collect the EEG data during the middle 1 minute.
Format
These data are collected from ten students, each watching ten videos.
Therefore, it can be seen as only 100 data points for these 12000+ rows. If you look at this way, then each data point consists of 120+ rows, which is sampled every 0.5 seconds (so each data point is a one minute video). Signals with higher frequency are reported as the mean value during each 0.5 second.
Reference:
Wang, H., Li, Y., Hu, X., Yang, Y., Meng, Z., & Chang, K. M. (2013, June). Using EEG to Improve Massive Open Online Courses Feedback Interaction. In AIED Workshops. [PDF]
Data Collection
The data is collected from a software that we implemented ourselves. Check HaohanWang/Bioimaging for the source code.
Inspiration
This dataset is an extremely challenging data set to perform binary classification. 65% of prediction accuracy is quite decent according to our experience.
It is an interesting data set to carry out the variable selection (causal inference) task that may help further research. Past research has indicated that Theta signal is correlated with confusion level.
It is also an interesting data set for confounding factors correction model because we offer two labels (subject id and video id) that could profoundly confound the results.
Warning
The data for subject 3 might be corrupted.
Other Resources
Promotion Video
Source Code of Data Collection Software
Contact
Haohan Wang"
The History of Baseball,A complete history of major league baseball stats from 1871 to 2015,SeanLahman,109,"Version 2,2016-09-08|Version 1,2016-03-09","baseball
history",CSV,66 MB,CC3,"27,811 views","4,751 downloads",129 kernels,2 topics,https://www.kaggle.com/seanlahman/the-history-of-baseball,"Baffled why your team traded for that 34-year-old pitcher? Convinced you can create a new and improved version of WAR? Wondering what made the 1907 Cubs great and if can they do it again?
The History of Baseball is a reformatted version of the famous Lahman’s Baseball Database. It contains Major League Baseball’s complete batting and pitching statistics from 1871 to 2015, plus fielding statistics, standings, team stats, park stats, player demographics, managerial records, awards, post-season data, and more.
Scripts, Kaggle’s free, in-browser analytics tool, makes it easy to share detailed sabermetrics, predict the next hall of fame inductee, illustrate how speed scores runs, or publish a definitive analysis on why the Los Angeles Dodgers will never win another World Series.
We have more ideas for analysis than games in a season, but here are a few we’d really love to see:
Is there a most error-prone position?
When do players at different positions peak?
Are the best performers selected for all-star game?
How many walks does it take for a starting pitcher to get pulled?
Do players with a high ground into double play (GIDP) have a lower batting average?
Which players are the most likely to choke during the post-season?
Why should or shouldn’t the National League adopt the designated hitter rule?
See the full SQLite schema."
2016 Parties in New York,225k noise complaints to the police about ongoing parties in the city,Evgenii Vasilev,108,"Version 4,2017-08-23|Version 3,2017-08-23|Version 2,2017-07-29|Version 1,2017-07-28","parties
cities",CSV,53 MB,CC0,"12,035 views","1,834 downloads",19 kernels,0 topics,https://www.kaggle.com/somesnm/partynyc,"Context
This dataset contains all noise complaints calls that were received by the city police with complaint type ""Loud music/Party"" in 2016. The data contains the time of the call, time of the police response, coordinates and part of the city.
This data should help match taxi rides from ""New York City Taxi Trip Duration"" competition to the night rides of partygoers.
Content
The New York city hotline receives non-urgent community concerns, which are made public by the city through NYC Open Data portal. The full dataset contains a variety of complaints ranging from illegal parking to customer complaints. This dataset focuses on Noise complaints that were collected in 2016 and indicate ongoing party in a given neighborhood.
parties_in_nyc.csv:
Columns:
Created Date - time of the call
Closed Date - time when ticket was closed by police
Location Type - type of the location
Incident Zip - zip code of the location
City - name of the city (almost the same as the Borough field)
Borough - administrative division of the city
Latitude - latitude of the location
Longitude - longitude of the location

test_parties and train_parties:
Columns:
id - id of the ride
num_complaints - number of noise complaints about ongoing parties within ~500 meters and within 2 hours of pickup place and time
Acknowledgements
https://opendata.cityofnewyork.us/ - NYC Open Data portal contains many other interesting datasets Photo by Yvette de Wit on Unsplash
Inspiration
After a fun night out in the city majority of people are too exhausted to travel by public transport, so they catch a cab to their home. I hope this data will help the community to find the patterns in the data that will lead to better solutions."
Breast Cancer Proteomes,Dividing breast cancer patients into separate sub-classes,kajot,107,"Version 3,2016-07-03|Version 2,2016-07-03|Version 1,2016-07-03",healthcare,CSV,12 MB,Other,"38,640 views","4,430 downloads",76 kernels,7 topics,https://www.kaggle.com/piotrgrabo/breastcancerproteomes,"Context: This data set contains published iTRAQ proteome profiling of 77 breast cancer samples generated by the Clinical Proteomic Tumor Analysis Consortium (NCI/NIH). It contains expression values for ~12.000 proteins for each sample, with missing values present when a given protein could not be quantified in a given sample.
Content:
File: 77_cancer_proteomes_CPTAC_itraq.csv
RefSeq_accession_number: RefSeq protein ID (each protein has a unique ID in a RefSeq database)
gene_symbol: a symbol unique to each gene (every protein is encoded by some gene)
gene_name: a full name of that gene Remaining columns: log2 iTRAQ ratios for each sample (protein expression data, most important), three last columns are from healthy individuals
File: clinical_data_breast_cancer.csv
First column ""Complete TCGA ID"" is used to match the sample IDs in the main cancer proteomes file (see example script). All other columns have self-explanatory names, contain data about the cancer classification of a given sample using different methods. 'PAM50 mRNA' classification is being used in the example script.
File: PAM50_proteins.csv
Contains the list of genes and proteins used by the PAM50 classification system. The column RefSeqProteinID contains the protein IDs that can be matched with the IDs in the main protein expression data set.
Past Research: The original study: http://www.nature.com/nature/journal/v534/n7605/full/nature18003.html (paywall warning)
In brief: the data were used to assess how the mutations in the DNA are affecting the protein expression landscape in breast cancer. Genes in our DNA are first transcribed into RNA molecules which then are translated into proteins. Changing the information content of DNA has impact on the behavior of the proteome, which is the main functional unit of cells, taking care of cell division, DNA repair, enzymatic reactions and signaling etc. They performed K-means clustering on the protein data to divide the breast cancer patients into sub-types, each having unique protein expression signature. They found that the best clustering was achieved using 3 clusters (original PAM50 gene set yields four different subtypes using RNA data).
Inspiration:
This is an interesting study and I myself wanted to use this breast cancer proteome data set for other types of analyses using machine learning that I am performing as a part of my PhD. However, I though that the Kaggle community (or at least that part with biomedical interests) would enjoy playing with it. I added a simple K-means clustering example for that data with some comments, the same approach as used in the original paper. One thing is that there is a panel of genes, the PAM50 which is used to classify breast cancers into subtypes. This panel was originally based on the RNA expression data which is (in my opinion) not as robust as the measurement of mRNA's final product, the protein. Perhaps using this data set, someone could find a different set of proteins (they all have unique NP_/XP_ identifiers) that would divide the data set even more robustly? Perhaps into a higher numbers of clusters with very distinct protein expression signatures?
Example K-means analysis script: http://pastebin.com/A0Wj41DP"
55000+ Song Lyrics,Lyrics for 55000+ songs in English from LyricsFreak,Sergey Kuznetsov,107,"Version 1,2017-01-05","languages
music
linguistics",CSV,69 MB,CC0,"22,801 views","2,901 downloads",55 kernels,0 topics,https://www.kaggle.com/mousehead/songlyrics,"Context
These are the lyrics for 57650 songs. They can be used for Natural Language Processing purposes, such as clustering of the words with similar meanings or predicting artist by the song. The dataset can be expanded with some more features for more advanced research like sentiment analysis. The data is not modified, only slightly cleaned, which gives a lot of freedom to devise your own applications.
Mining
I have mined this dataset as a corpus for my NLP studies. However, before performing any transformation to bag-of-words or bag-of-N-grams, I decided to share the data. The data has been acquired from LyricsFreak through scraping. Then I did some very basic work on removing inconvenient data: non-English lyrics, extremely short and extremely long lyrics, lyrics with non-ASCII symbols. However, there's still work to be done in terms of data preparation.
Content
The dataset contains 4 columns:
Artist
Song Name
Link to a webpage with the song (for reference). This is to be concatenated with http://www.lyricsfreak.com to form a real URL.
Lyrics of the song, unmodified.
Acknowledgements
I would like to acknowledge LyricsFreak, which is the direct source of the data."
Kickstarter projects,"More than 300,000 kickstarter projects",Kemical,107,"Version 7,2018-02-08|Version 6,2018-01-26|Version 5,2018-01-03|Version 4,2018-01-02|Version 3,2018-01-02|Version 2,2017-11-27|Version 1,2017-11-06","finance
crowdfunding",CSV,37 MB,CC4,"13,492 views","2,525 downloads",10 kernels,5 topics,https://www.kaggle.com/kemical/kickstarter-projects,"Context
I'm a crowdfunding enthusiast and i'm watching kickstarter since its early days. Right now I just collect data and the only app i've made is this twitter bot which tweet any project reaching some milestone: @bloomwatcher . I have a lot of other ideas, but sadly not enough time to develop them... But I hope you can!
Content
You'll find most useful data for project analysis. Columns are self explanatory except:
usd_pledged: conversion in US dollars of the pledged column (conversion done by kickstarter).
usd pledge real: conversion in US dollars of the pledged column (conversion from Fixer.io API).
usd goal real: conversion in US dollars of the goal column (conversion from Fixer.io API).
Acknowledgements
Data are collected from Kickstarter Platform
usd conversion (usd_pledged_real and usd_goal_real columns) were generated from convert ks pledges to usd script done by tonyplaysguitar
Inspiration
I hope to see great projects, and why not a model to predict if a project will be successful before it is released? :)"
Students' Academic Performance Dataset,xAPI-Educational Mining Dataset,IbrahimAljarah,107,"Version 6,2016-11-27|Version 5,2016-11-26|Version 4,2016-11-26|Version 3,2016-11-26|Version 2,2016-11-24|Version 1,2016-11-24",education,CSV,37 KB,CC4,"74,553 views","8,845 downloads",159 kernels,5 topics,https://www.kaggle.com/aljarah/xAPI-Edu-Data,"Students' Academic Performance Dataset (xAPI-Edu-Data)
Data Set Characteristics: Multivariate
Number of Instances: 480
Area: E-learning, Education, Predictive models, Educational Data Mining
Attribute Characteristics: Integer/Categorical
Number of Attributes: 16
Date: 2016-11-8
Associated Tasks: Classification
Missing Values? No
File formats: xAPI-Edu-Data.csv
Source:
Elaf Abu Amrieh, Thair Hamtini, and Ibrahim Aljarah, The University of Jordan, Amman, Jordan, http://www.Ibrahimaljarah.com www.ju.edu.jo
Dataset Information:
This is an educational data set which is collected from learning management system (LMS) called Kalboard 360. Kalboard 360 is a multi-agent LMS, which has been designed to facilitate learning through the use of leading-edge technology. Such system provides users with a synchronous access to educational resources from any device with Internet connection.
The data is collected using a learner activity tracker tool, which called experience API (xAPI). The xAPI is a component of the training and learning architecture (TLA) that enables to monitor learning progress and learner’s actions like reading an article or watching a training video. The experience API helps the learning activity providers to determine the learner, activity and objects that describe a learning experience. The dataset consists of 480 student records and 16 features. The features are classified into three major categories: (1) Demographic features such as gender and nationality. (2) Academic background features such as educational stage, grade Level and section. (3) Behavioral features such as raised hand on class, opening resources, answering survey by parents, and school satisfaction.
The dataset consists of 305 males and 175 females. The students come from different origins such as 179 students are from Kuwait, 172 students are from Jordan, 28 students from Palestine, 22 students are from Iraq, 17 students from Lebanon, 12 students from Tunis, 11 students from Saudi Arabia, 9 students from Egypt, 7 students from Syria, 6 students from USA, Iran and Libya, 4 students from Morocco and one student from Venezuela.
The dataset is collected through two educational semesters: 245 student records are collected during the first semester and 235 student records are collected during the second semester.
The data set includes also the school attendance feature such as the students are classified into two categories based on their absence days: 191 students exceed 7 absence days and 289 students their absence days under 7.
This dataset includes also a new category of features; this feature is parent parturition in the educational process. Parent participation feature have two sub features: Parent Answering Survey and Parent School Satisfaction. There are 270 of the parents answered survey and 210 are not, 292 of the parents are satisfied from the school and 188 are not.
(See the related papers for more details).
Attributes
1 Gender - student's gender (nominal: 'Male' or 'Female’)
2 Nationality- student's nationality (nominal:’ Kuwait’,’ Lebanon’,’ Egypt’,’ SaudiArabia’,’ USA’,’ Jordan’,’ Venezuela’,’ Iran’,’ Tunis’,’ Morocco’,’ Syria’,’ Palestine’,’ Iraq’,’ Lybia’)
3 Place of birth- student's Place of birth (nominal:’ Kuwait’,’ Lebanon’,’ Egypt’,’ SaudiArabia’,’ USA’,’ Jordan’,’ Venezuela’,’ Iran’,’ Tunis’,’ Morocco’,’ Syria’,’ Palestine’,’ Iraq’,’ Lybia’)
4 Educational Stages- educational level student belongs (nominal: ‘lowerlevel’,’MiddleSchool’,’HighSchool’)
5 Grade Levels- grade student belongs (nominal: ‘G-01’, ‘G-02’, ‘G-03’, ‘G-04’, ‘G-05’, ‘G-06’, ‘G-07’, ‘G-08’, ‘G-09’, ‘G-10’, ‘G-11’, ‘G-12 ‘)
6 Section ID- classroom student belongs (nominal:’A’,’B’,’C’)
7 Topic- course topic (nominal:’ English’,’ Spanish’, ‘French’,’ Arabic’,’ IT’,’ Math’,’ Chemistry’, ‘Biology’, ‘Science’,’ History’,’ Quran’,’ Geology’)
8 Semester- school year semester (nominal:’ First’,’ Second’)
9 Parent responsible for student (nominal:’mom’,’father’)
10 Raised hand- how many times the student raises his/her hand on classroom (numeric:0-100)
11- Visited resources- how many times the student visits a course content(numeric:0-100)
12 Viewing announcements-how many times the student checks the new announcements(numeric:0-100)
13 Discussion groups- how many times the student participate on discussion groups (numeric:0-100)
14 Parent Answering Survey- parent answered the surveys which are provided from school or not (nominal:’Yes’,’No’)
15 Parent School Satisfaction- the Degree of parent satisfaction from school(nominal:’Yes’,’No’)
16 Student Absence Days-the number of absence days for each student (nominal: above-7, under-7)
The students are classified into three numerical intervals based on their total grade/mark:
Low-Level: interval includes values from 0 to 69,
Middle-Level: interval includes values from 70 to 89,
High-Level: interval includes values from 90-100.
Relevant Papers:
Amrieh, E. A., Hamtini, T., & Aljarah, I. (2016). Mining Educational Data to Predict Student’s academic Performance using Ensemble Methods. International Journal of Database Theory and Application, 9(8), 119-136.
Amrieh, E. A., Hamtini, T., & Aljarah, I. (2015, November). Preprocessing and analyzing educational data set using X-API for improving student's performance. In Applied Electrical Engineering and Computing Technologies (AEECT), 2015 IEEE Jordan Conference on (pp. 1-5). IEEE.
Citation Request:
Please include these citations if you plan to use this dataset:
Amrieh, E. A., Hamtini, T., & Aljarah, I. (2016). Mining Educational Data to Predict Student’s academic Performance using Ensemble Methods. International Journal of Database Theory and Application, 9(8), 119-136.
Amrieh, E. A., Hamtini, T., & Aljarah, I. (2015, November). Preprocessing and analyzing educational data set using X-API for improving student's performance. In Applied Electrical Engineering and Computing Technologies (AEECT), 2015 IEEE Jordan Conference on (pp. 1-5). IEEE."
S&P 500 stock data,Historical stock data for all current S&P 500 companies,Cam Nugent,105,"Version 4,2018-02-10|Version 3,2018-02-10|Version 2,2017-08-13|Version 1,2017-08-11",finance,Other,19 MB,CC0,"28,224 views","3,663 downloads",13 kernels,4 topics,https://www.kaggle.com/camnugent/sandp500,"Context
Stock market data can be interesting to analyze and as a further incentive, strong predictive models can have large financial payoff. The amount of financial data on the web is seemingly endless. A large and well structured dataset on a wide array of companies can be hard to come by. Here I provide a dataset with historical stock prices (last 5 years) for all companies currently found on the S&P 500 index.
The script I used to acquire all of these .csv files can be found in this GitHub repository In the future if you wish for a more up to date dataset, this can be used to acquire new versions of the .csv files.
Feb 2018 note: I have just updated the dataset to include data up to Feb 2018. I have also accounted for changes in the stocks on the S&P 500 index (RIP whole foods etc. etc.).
Content
The data is presented in a couple of formats to suit different individual's needs or computational limitations. I have included files containing 5 years of stock data (in the all_stocks_5yr.csv and corresponding folder).
The folder individual_stocks_5yr contains files of data for individual stocks, labelled by their stock ticker name. The all_stocks_5yr.csv contains the same data, presented in a merged .csv file. Depending on the intended use (graphing, modelling etc.) the user may prefer one of these given formats.
All the files have the following columns: Date - in format: yy-mm-dd
Open - price of the stock at market open (this is NYSE data so all in USD)
High - Highest price reached in the day
Low Close - Lowest price reached in the day
Volume - Number of shares traded
Name - the stock's ticker name
Acknowledgements
Due to volatility in google finance, for the newest version I have switched over to acquiring the data from The Investor's Exchange api, the simple script I use to do this is found here. Special thanks to Kaggle, Github, pandas_datareader and The Market.
Inspiration
This dataset lends itself to a some very interesting visualizations. One can look at simple things like how prices change over time, graph an compare multiple stocks at once, or generate and graph new metrics from the data provided. From these data informative stock stats such as volatility and moving averages can be easily calculated. The million dollar question is: can you develop a model that can beat the market and allow you to make statistically informed trades!"
Cervical Cancer Risk Classification,prediction of cancer indicators; Please download; run kernel & upvote,Gokagglers,103,"Version 6,2017-08-31|Version 5,2017-08-30|Version 4,2017-08-30|Version 3,2017-08-24|Version 2,2017-08-24|Version 1,2017-08-18","healthcare
human genetics",CSV,100 KB,Other,"21,513 views","4,466 downloads",23 kernels,18 topics,https://www.kaggle.com/loveall/cervical-cancer-risk-classification,"Cervical Cancer Risk Factors for Biopsy: This Dataset is Obtained from UCI Repository and kindly acknowledged!
This file contains a List of Risk Factors for Cervical Cancer leading to a Biopsy Examination!
About 11,000 new cases of invasive cervical cancer are diagnosed each year in the U.S. However, the number of new cervical cancer cases has been declining steadily over the past decades. Although it is the most preventable type of cancer, each year cervical cancer kills about 4,000 women in the U.S. and about 300,000 women worldwide. In the United States, cervical cancer mortality rates plunged by 74% from 1955 - 1992 thanks to increased screening and early detection with the Pap test. AGE Fifty percent of cervical cancer diagnoses occur in women ages 35 - 54, and about 20% occur in women over 65 years of age. The median age of diagnosis is 48 years. About 15% of women develop cervical cancer between the ages of 20 - 30. Cervical cancer is extremely rare in women younger than age 20. However, many young women become infected with multiple types of human papilloma virus, which then can increase their risk of getting cervical cancer in the future. Young women with early abnormal changes who do not have regular examinations are at high risk for localized cancer by the time they are age 40, and for invasive cancer by age 50. SOCIOECONOMIC AND ETHNIC FACTORS Although the rate of cervical cancer has declined among both Caucasian and African-American women over the past decades, it remains much more prevalent in African-Americans -- whose death rates are twice as high as Caucasian women. Hispanic American women have more than twice the risk of invasive cervical cancer as Caucasian women, also due to a lower rate of screening. These differences, however, are almost certainly due to social and economic differences. Numerous studies report that high poverty levels are linked with low screening rates. In addition, lack of health insurance, limited transportation, and language difficulties hinder a poor woman’s access to screening services. HIGH SEXUAL ACTIVITY Human papilloma virus (HPV) is the main risk factor for cervical cancer. In adults, the most important risk factor for HPV is sexual activity with an infected person. Women most at risk for cervical cancer are those with a history of multiple sexual partners, sexual intercourse at age 17 years or younger, or both. A woman who has never been sexually active has a very low risk for developing cervical cancer. Sexual activity with multiple partners increases the likelihood of many other sexually transmitted infections (chlamydia, gonorrhea, syphilis).Studies have found an association between chlamydia and cervical cancer risk, including the possibility that chlamydia may prolong HPV infection. FAMILY HISTORY Women have a higher risk of cervical cancer if they have a first-degree relative (mother, sister) who has had cervical cancer. USE OF ORAL CONTRACEPTIVES Studies have reported a strong association between cervical cancer and long-term use of oral contraception (OC). Women who take birth control pills for more than 5 - 10 years appear to have a much higher risk HPV infection (up to four times higher) than those who do not use OCs. (Women taking OCs for fewer than 5 years do not have a significantly higher risk.) The reasons for this risk from OC use are not entirely clear. Women who use OCs may be less likely to use a diaphragm, condoms, or other methods that offer some protection against sexual transmitted diseases, including HPV. Some research also suggests that the hormones in OCs might help the virus enter the genetic material of cervical cells. HAVING MANY CHILDREN Studies indicate that having many children increases the risk for developing cervical cancer, particularly in women infected with HPV. SMOKING Smoking is associated with a higher risk for precancerous changes (dysplasia) in the cervix and for progression to invasive cervical cancer, especially for women infected with HPV. IMMUNOSUPPRESSION Women with weak immune systems, (such as those with HIV / AIDS), are more susceptible to acquiring HPV. Immunocompromised patients are also at higher risk for having cervical precancer develop rapidly into invasive cancer. DIETHYLSTILBESTROL (DES) From 1938 - 1971, diethylstilbestrol (DES), an estrogen-related drug, was widely prescribed to pregnant women to help prevent miscarriages. The daughters of these women face a higher risk for cervical cancer. DES is no longer prsecribed."
US Stocks Fundamentals (XBRL),"Fundamental data for 12,129 companies based on XBRL",usfundamentals,102,"Version 3,2016-09-07|Version 2,2016-09-04|Version 1,2016-08-27",finance,CSV,145 MB,CC0,"23,982 views","3,659 downloads",74 kernels,3 topics,https://www.kaggle.com/usfundamentals/us-stocks-fundamentals,"This dataset contains US stocks fundamental data, such as income statement, balance sheet and cash flows.
12,129 companies
8,526 unique indicators
~20 indicators comparable across most companies
Five years of data, yearly
The data is provided by http://usfundamentals.com."
Nutrition Facts for McDonald's Menu,"Calories, fat, and sugar for every cheeseburger, fries, and milkshake on menu",McDonald's,102,"Version 1,2017-03-03","food and drink
health",CSV,29 KB,Other,"53,015 views","8,441 downloads",188 kernels,2 topics,https://www.kaggle.com/mcdonalds/nutrition-facts,"Context
Ray Kroc wanted to build a restaurant system that would be famous for providing food of consistently high quality and uniform methods of preparation. He wanted to serve burgers, buns, fries and beverages that tasted just the same in Alaska as they did in Alabama. To achieve this, he chose a unique path: persuading both franchisees and suppliers to buy into his vision, working not for McDonald’s but for themselves, together with McDonald’s. Many of McDonald’s most famous menu items – like the Big Mac, Filet-O-Fish, and Egg McMuffin – were created by franchisees.
Content
This dataset provides a nutrition analysis of every menu item on the US McDonald's menu, including breakfast, beef burgers, chicken and fish sandwiches, fries, salads, soda, coffee and tea, milkshakes, and desserts.
Acknowledgements
The menu items and nutrition facts were scraped from the McDonald's website.
Inspiration
How many calories does the average McDonald's value meal contain? How much do beverages, like soda or coffee, contribute to the overall caloric intake? Does ordered grilled chicken instead of crispy increase a sandwich's nutritional value? What about ordering egg whites instead of whole eggs? What is the least number of items could you order from the menu to meet one day's nutritional requirements?
Start a new kernel"
Philadelphia Crime Data,"Ten Years of Crime Data, by OpenDataPhilly",Mike Chirico,101,"Version 19,2017-03-24|Version 18,2017-03-15|Version 17,2017-02-26|Version 16,2017-02-09|Version 15,2017-01-28|Version 14,2017-01-03|Version 13,2016-12-24|Version 12,2016-12-12|Version 11,2016-12-01|Version 10,2016-11-29|Version 9,2016-11-20|Version 8,2016-11-13|Version 7,2016-11-07|Version 6,2016-10-26|Version 5,2016-10-13|Version 4,2016-09-26|Version 3,2016-09-15|Version 2,2016-09-10|Version 1,2016-08-19",crime,CSV,296 MB,Other,"42,461 views","5,158 downloads",201 kernels,9 topics,https://www.kaggle.com/mchirico/philadelphiacrimedata,"Crime Data for Philadelphia
To get started quickly, take a look at Philly Data Crime Walk-through.
Data was provided by OpenDataPhilly"
Human Activity Recognition with Smartphones,Recordings of 30 study participants performing activities of daily living,UCI Machine Learning,101,"Version 1,2016-10-07","sociology
human-computer interaction
telecommunications",CSV,64 MB,CC0,"46,442 views","4,398 downloads",109 kernels,6 topics,https://www.kaggle.com/uciml/human-activity-recognition-with-smartphones,"The Human Activity Recognition database was built from the recordings of 30 study participants performing activities of daily living (ADL) while carrying a waist-mounted smartphone with embedded inertial sensors. The objective is to classify activities into one of the six activities performed.
Description of experiment
The experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually. The obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers was selected for generating the training data and 30% the test data.
The sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. From each window, a vector of features was obtained by calculating variables from the time and frequency domain.
Attribute information
For each record in the dataset the following is provided:
Triaxial acceleration from the accelerometer (total acceleration) and the estimated body acceleration.
Triaxial Angular velocity from the gyroscope.
A 561-feature vector with time and frequency domain variables.
Its activity label.
An identifier of the subject who carried out the experiment.
Relevant papers
Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. Human Activity Recognition on Smartphones using a Multiclass Hardware-Friendly Support Vector Machine. International Workshop of Ambient Assisted Living (IWAAL 2012). Vitoria-Gasteiz, Spain. Dec 2012
Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, Jorge L. Reyes-Ortiz. Energy Efficient Smartphone-Based Activity Recognition using Fixed-Point Arithmetic. Journal of Universal Computer Science. Special Issue in Ambient Assisted Living: Home Care. Volume 19, Issue 9. May 2013
Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. Human Activity Recognition on Smartphones using a Multiclass Hardware-Friendly Support Vector Machine. 4th International Workshop of Ambient Assited Living, IWAAL 2012, Vitoria-Gasteiz, Spain, December 3-5, 2012. Proceedings. Lecture Notes in Computer Science 2012, pp 216-223.
Jorge Luis Reyes-Ortiz, Alessandro Ghio, Xavier Parra-Llanas, Davide Anguita, Joan Cabestany, Andreu Català. Human Activity and Motion Disorder Recognition: Towards Smarter Interactive Cognitive Environments. 21st European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN 2013. Bruges, Belgium 24-26 April 2013.
Citation
Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. A Public Domain Dataset for Human Activity Recognition Using Smartphones. 21st European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN 2013. Bruges, Belgium 24-26 April 2013."
Ethereum Historical Data,All Ethereum data from the start to Dec 2017,LiamLarsen,100,"Version 7,2018-02-02|Version 6,2017-12-16|Version 5,2017-12-13|Version 4,2017-06-14|Version 3,2017-05-06|Version 2,2017-04-16|Version 1,2017-04-14","history
finance",CSV,437 KB,Other,"22,270 views","2,188 downloads",12 kernels,0 topics,https://www.kaggle.com/kingburrito666/ethereum-historical-data,"Context
The Ethereum blockchain gives a revolutionary way of decentralized applications and provides its own cryptocurrency. Ethereum is a decentralized platform that runs smart contracts: applications that run exactly as programmed without any possibility of downtime, censorship, fraud or third party interference. These apps run on a custom built blockchain, an enormously powerful shared global infrastructure that can move value around and represent the ownership of property. This enables developers to create markets, store registries of debts or promises, move funds in accordance with instructions given long in the past (like a will or a futures contract) and many other things that have not been invented yet, all without a middle man or counterparty risk.
Content
What you may see in the CSVs are just numbers, but there is more to this. Numbers make machine learning easy. I've labeled each column, the first in all of them is the day; it may look weird but it makes sense if you look closely.
Note:
TIMESTAMP FORMAT
How to convert timestamp in python:
import datetime as dt
# The (would-be) timestamp value is below
timestamp = 1339521878.04 
# Technechly you would iterate through and change them all if you were graphing
timeValue = dt.datetime.fromtimestamp(timestamp)
#Year, month, day, hour, minute, second
print(timeValue.strftime('%Y-%m-%d %H:%M:%S'))
Acknowledgements
MR. Vitalik Buterin. co-founder of Ethereum and as a co-founder of Bitcoin Magazine.
Hit a brother up
0x767e8b211f70c5b8b4caa38c2efe05bf8eac0da7
Will be updating every month with new Ethereum history!"
E-Commerce Data,Actual transactions from UK retailer,Carrie,99,"Version 1,2017-08-17",,CSV,43 MB,Other,"25,892 views","4,091 downloads",20 kernels,0 topics,https://www.kaggle.com/carrie1/ecommerce-data,"Context
Typically e-commerce datasets are proprietary and consequently hard to find among publicly available data. However, The UCI Machine Learning Repository has made this dataset containing actual transactions from 2010 and 2011. The dataset is maintained on their site, where it can be found by the title ""Online Retail"".
Content
""This is a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail.The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.""
Acknowledgements
Per the UCI Machine Learning Repository, this data was made available by Dr Daqing Chen, Director: Public Analytics group. chend '@' lsbu.ac.uk, School of Engineering, London South Bank University, London SE1 0AA, UK.
Image from stocksnap.io.
Inspiration
Analyses for this dataset could include time series, clustering, classification and more."
FiveThirtyEight,80 Datasets published by FiveThirtyEight,FiveThirtyEight,96,"Version 7,2017-01-30|Version 6,2017-01-30|Version 5,2017-01-30|Version 4,2017-01-30|Version 3,2017-01-30|Version 2,2017-01-30|Version 1,2017-01-30",news agencies,Other,14 MB,Other,"28,075 views","3,097 downloads",85 kernels,,https://www.kaggle.com/fivethirtyeight/fivethirtyeight,"Introduction
Data and code behind the stories and interactives at FiveThirtyEight. There are 80 Datasets here, included in one place. This allows you to make comparisons and utilize multiple Datasets.
airline-safety
alcohol-consumption
avengers
bad-drivers
bechdel
biopics
births
bob-ross
buster-posey-mvp
classic-rock
college-majors
comic-characters
comma-survey-data
congress-age
cousin-marriage
... For the complete list, see the zip files listed here
Python
If you're planning to use Python, you might want to take a look at How to read datasets, which helps navigate the zipped Datasets. Again, there are multiple, 80 Datasets, in this Dataset.
R
If you're planning to use R, it's included in the kernels. No need to unzip files. For example, if you wanted to load ""bechdel"", you could use the following commands.
library(fivethirtyeight)
data(package = ""fivethirtyeight"")
head(bechdel) 
Reference R Quick Start, for an example.
References
The following is FiveThirtyEight's public repository on Github.
https://github.com/fivethirtyeight/data
The CRAN package is maintained at the following link:
https://github.com/rudeboybert/fivethirtyeight
License
Copyright (c) 2014 ESPN Internet Ventures
Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:
The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
GitHub Repos,Code and comments from 2.8 million repos,Github,93,"Version 1,2017-12-06","programming languages
programming
software engineering
bigquery",BigQuery,3 TB,Other,"12,297 views",0 downloads,348 kernels,3 topics,https://www.kaggle.com/github/github-repos,"GitHub is how people build software and is home to the largest community of open source developers in the world, with over 12 million people contributing to 31 million projects on GitHub since 2008.
This 3TB+ dataset comprises the largest released source of GitHub activity to date. It contains a full snapshot of the content of more than 2.8 million open source GitHub repositories including more than 145 million unique commits, over 2 billion different file paths, and the contents of the latest revision for 163 million files, all of which are searchable with regular expressions.
Querying BigQuery tables
You can use the BigQuery Python client library to query tables in this dataset in Kernels. Note that methods available in Kernels are limited to querying data. Tables are at bigquery-public-data.github_repos.[TABLENAME]. Fork this kernel to get started to learn how to safely manage analyzing large BigQuery datasets.
Acknowledgements
This dataset was made available per GitHub's terms of service.
Inspiration
This is the perfect dataset for fighting language wars.
Can you identify any signals that predict which packages or languages will become popular, in advance of their mass adoption?"
The Simpsons by the Data,"27 seasons of ""Simpsons did it.""",William Cukierski,91,"Version 1,2016-09-29",popular culture,CSV,34 MB,Other,"38,229 views","4,172 downloads",69 kernels,9 topics,https://www.kaggle.com/wcukierski/the-simpsons-by-the-data,"This dataset contains the characters, locations, episode details, and script lines for approximately 600 Simpsons episodes, dating back to 1989.
Inspiration and credit for gathering the data goes to Todd Schneider:
http://toddwschneider.com/posts/the-simpsons-by-the-data/
https://github.com/toddwschneider/flim-springfield"
2016 US Presidential Debates,Full transcripts of the face-off between Clinton & Trump,Megan Risdal,88,"Version 4,2016-10-24|Version 3,2016-10-10|Version 2,2016-10-05|Version 1,2016-09-28","politics
political science",CSV,366 KB,CC0,"29,298 views","3,042 downloads",169 kernels,4 topics,https://www.kaggle.com/mrisdal/2016-us-presidential-debates,"Context
In November, the United States will elect a new president. Before then, three presidential debates will take place between Hillary Clinton and Donald Trump as well as one vice presidential debate between Tim Kaine and Mike Pence. While you can watch the debates live, why not also read deeply into the candidates' responses with text analytics?
You can now answer any questions you have about the platforms of our presidential hopefuls or their speaking skills.
Which candidate is the most given to loquaciousness?
How many times does Clinton get interrupted?
Who gets the most audience applause?
When is positive sentiment at its highest during the candidates' word play?
Content & Acknowledgements
For consistency, full transcripts of the debates were all taken from The Washington Post who made annotated transcripts available following each debate:
First debate taking place September 26th, 2016 was obtained from The Washington Post.
The vice presidential debate from October 4th, 2016 was similarly obtained here.
The ""town hall"" presidential debate on October 9th is found here.
The final presidential debate taking place on October 19th is found here.
Please make any dataset suggestions or requests on the forum. Word cloud from Debate Visualization by Jane Yu."
Lower Back Pain Symptoms Dataset,Collection of physical spine data,sammy123,87,"Version 1,2016-08-19",healthcare,CSV,42 KB,Other,"26,864 views","3,212 downloads",87 kernels,7 topics,https://www.kaggle.com/sammy123/lower-back-pain-symptoms-dataset,"310 Observations, 13 Attributes (12 Numeric Predictors, 1 Binary Class Attribute - No Demographics)
Lower back pain can be caused by a variety of problems with any parts of the complex, interconnected network of spinal muscles, nerves, bones, discs or tendons in the lumbar spine. Typical sources of low back pain include:
The large nerve roots in the low back that go to the legs may be irritated
The smaller nerves that supply the low back may be irritated
The large paired lower back muscles (erector spinae) may be strained
The bones, ligaments or joints may be damaged
An intervertebral disc may be degenerating
An irritation or problem with any of these structures can cause lower back pain and/or pain that radiates or is referred to other parts of the body. Many lower back problems also cause back muscle spasms, which don't sound like much but can cause severe pain and disability.
While lower back pain is extremely common, the symptoms and severity of lower back pain vary greatly. A simple lower back muscle strain might be excruciating enough to necessitate an emergency room visit, while a degenerating disc might cause only mild, intermittent discomfort.
This data set is about to identify a person is abnormal or normal using collected physical spine details/data."
NBA Players stats since 1950,"3000+ Players over 60+ Seasons, and 50+ features per player",DrGuillermo,86,"Version 1,2017-06-06",basketball,CSV,5 MB,Other,"20,492 views","3,474 downloads",33 kernels,4 topics,https://www.kaggle.com/drgilermo/nba-players-stats,"Content
The data-set contains aggregate individual statistics for 67 NBA seasons. from basic box-score attributes such as points, assists, rebounds etc., to more advanced money-ball like features such as Value Over Replacement.
Acknowledgements
The data was scraped from Basketball-reference Take a look in their glossary for a detailed column description Glossary"
Pokemon- Weedle's Cave,Welcome to Weedle's cave,T7 - Pokemon Challenge,86,"Version 1,2017-09-21","popular culture
video games",CSV,682 KB,CC0,"15,187 views","6,606 downloads",38 kernels,4 topics,https://www.kaggle.com/terminus7/pokemon-challenge,"Welcome to Weedle's cave. Will you be able to predict the outcome of future matches?
To do it you will have the pokemon characteristics and the results of previous combats.
Three files are available. The first one contains the pokemon characteristics (the first column being the id of the pokemon). The second one contains information about previous combats. The first two columns contain the ids of the combatants and the third one the id of the winner. Important: The pokemon in the first columns attacks first.
The goal is to develop a Machine Learning model able to predict the result of future pokemon combats.
If you have any questions, please email: t7pokemonchallenge@intelygenz.com
DISCLAIMER
In Intelygenz we are against animal abuse. No animal real or imaginary should be forced to fight against other. Freedom for the pokemons !"
Craft Beers Dataset,2K+ craft canned beers from the US and 500+ breweries in the United States.,Jean-NicholasHould,85,"Version 1,2017-01-18",food and drink,CSV,178 KB,ODbL,"34,226 views","4,236 downloads",90 kernels,6 topics,https://www.kaggle.com/nickhould/craft-cans,"This dataset contains a list of 2410 US craft beers and 510 US breweries. The beers and breweries are linked together with an ""id"". This data was collected in January 2017 on CraftCans.com. The dataset is an a tidy format and values have been cleaned up for your enjoyment.
If you are interested in learning more about how this dataset was acquired, I wrote an extensive blogpost about it (http://www.jeannicholashould.com/python-web-scraping-tutorial-for-craft-beers.html).
Enjoy!"
Starbucks Locations Worldwide,"Name, ownership type, and location of every Starbucks store in operation",Starbucks,85,"Version 1,2017-02-14",food and drink,CSV,4 MB,Other,"35,348 views","5,208 downloads",103 kernels,7 topics,https://www.kaggle.com/starbucks/store-locations,"Context
Starbucks started as a roaster and retailer of whole bean and ground coffee, tea and spices with a single store in Seattle’s Pike Place Market in 1971. The company now operates more than 24,000 retail stores in 70 countries.
Content
This dataset includes a record for every Starbucks or subsidiary store location currently in operation as of February 2017.
Acknowledgements
This data was scraped from the Starbucks store locator webpage by Github user chrismeller.
Inspiration
What city or country has the highest number of Starbucks stores per capita? What two Starbucks locations are the closest in proximity to one another? What location on Earth is farthest from a Starbucks? How has Starbucks expanded overseas?"
The Holy Quran,Understanding God - Sacred Meanings,Zeeshan-ul-hassan Usmani,83,"Version 3,2017-11-20|Version 2,2017-11-10|Version 1,2017-11-08","languages
faith and traditions
islam",CSV,16 MB,CC0,"10,214 views",498 downloads,8 kernels,7 topics,https://www.kaggle.com/zusmani/the-holy-quran,"Context
The Holy Quran is the central text for 1.5 billion Muslims around the world. It literally means ""The Recitation."" It is undoubtedly the finest work in Arabic literature and revealed by Allah (God) to His Messenger Prophet Muhammed (Peace Be Upon Him) through angel Gabriel. It was revealed verbally from December 22, 609 (AD) to 632 AD (when Prophet Muhammed (Peace Be Upon Him) died)
The book is divided into 30 parts, 114 Chapters and 6,000+ verses.
There has been a lot of questions and comments on the text of this holy book given the contemporary Geo-political situation of the world, wars in the Middle East and Afghanistan and the ongoing terrorism.
I have put this dataset together to call my fellow data scientists to run their NLP algorithms and Kernels to find and explore the sacred text by them selves.
Content
The data contains complete Holy Quran in following 21 languages (so data scientists from different parts of the world can work with it). The original text was revealed in Arabic. Other 20 files are the translations of the original text.
Arabic (Original Book by God)
English (Transalation by Yusuf Ali)
Persian (Makarim Sheerazi)
Urdu (Jalandhari)
Turkish (Y. N. Ozturk)
Portuguese (El. Hayek)
Dutch (Keyzer)
Norwegian (Einar Berg)
Italian (Piccardo)
French (Hamidullah)
German (Zaidan)
Swedish (Rashad Kalifa)
Indonesia (Bhasha Indoenisan)
Bangla
Chinese/Madarin
Japanese
Malay
Malayalam
Russian
Tamil
Uzbek
Inspiration
Here are some ideas to explore:
Can we make a word cloud for each chapter
Can we make a word cloud of the whole book and find out the frequency of each word
Can we describe or annotate subjects in each chapter and verse
Can we find how many times The Quran has mentioned Humans, Women, Humility, Heaven or Hell
Can we compare the text with other famous books and see the correlation
Can we compare the text with laws in multiple countries to see the resemblance
Any other ideas you can think of
I am looking forward to see your work and ideas and will keep adding more ideas to explore
Welcome on board to learn the finest text on earth with Data Sciences and Machine Learning!
Updates
Complete Verse-By-Verse Dataset has been shared. A good contribution by Zohaib Ali - https://www.kaggle.com/zohaib1111 (Nov 20, 2017)"
Deep-NLP,natural language processing,samdeeplearning,83,"Version 1,2017-03-01","languages
linguistics",CSV,663 KB,Other,"25,369 views","1,667 downloads",49 kernels,2 topics,https://www.kaggle.com/samdeeplearning/deepnlp,"What's In The Deep-NLP Dataset?
Sheet_1.csv contains 80 user responses, in the response_text column, to a therapy chatbot. Bot said: 'Describe a time when you have acted as a resource for someone else'.  User responded. If a response is 'not flagged', the user can continue talking to the bot. If it is 'flagged', the user is referred to help.
Sheet_2.csv contains 125 resumes, in the resume_text column. Resumes were queried from Indeed.com with keyword 'data scientist', location 'Vermont'. If a resume is 'not flagged', the applicant can submit a modified resume version at a later date. If it is 'flagged', the applicant is invited to interview.
What Do I Do With This?
Classify new resumes/responses as flagged or not flagged.
There are two sets of data here - resumes and responses. Split the data into a train set and a test set to test the accuracy of your classifier. Bonus points for using the same classifier for both problems.
Good luck.
Acknowledgements
Thank you to Parsa Ghaffari (Aylien), without whom these visuals (cover photo is in Parsa Ghaffari's excellent LinkedIn article on English, Spanish and German postive v. negative sentiment analysis) would not exist.
There Is A 'deep natural language processing' Kernel. I will update it. I Hope You Find It Useful.
You can use any of the code in that kernel anywhere, on or off Kaggle. Ping me at @_samputnam for questions."
Japan Trade Statistics,Japan's international trade by country and type of good,TadashiNagao,81,"Version 41,2018-02-04|Version 40,2017-12-30|Version 39,2017-12-02|Version 38,2017-10-31|Version 37,2017-09-29|Version 36,2017-09-24|Version 35,2017-08-19|Version 34,2017-08-19|Version 33,2017-08-11|Version 32,2017-07-29|Version 31,2017-07-22|Version 30,2017-07-15|Version 29,2017-07-05|Version 28,2017-06-11|Version 27,2017-06-01|Version 26,2017-04-30|Version 25,2017-04-20|Version 24,2017-03-26|Version 23,2017-03-26|Version 22,2017-02-26|Version 21,2017-02-05|Version 20,2017-01-05|Version 19,2016-10-30|Version 18,2016-10-09|Version 17,2016-10-09|Version 16,2016-10-02|Version 15,2016-09-10|Version 14,2016-09-07|Version 13,2016-09-04|Version 12,2016-09-04|Version 11,2016-09-02|Version 10,2016-09-02|Version 9,2016-09-02|Version 8,2016-08-31|Version 7,2016-08-31|Version 6,2016-08-31|Version 5,2016-08-02|Version 4,2016-08-01|Version 3,2016-07-31|Version 2,2016-06-29|Version 1,2016-06-24","business
finance",CSV,241 MB,CC4,"46,192 views","4,354 downloads",166 kernels,16 topics,https://www.kaggle.com/zanjibar/japan-trade-statistics,"Because of memory limitations,data format change csv -> db (sqlite format)
This dataset includes yearly and monthly versions of Japan's international trading data (segmented by country , the type of good and local custom ).
Japan trade statistics is searchable here"
Hacker News,All posts from Y Combinator's social news website from 2006 to late 2017,Hacker News,81,"Version 1,2017-12-05","journalism
information technology
internet
bigquery",BigQuery,14 GB,CC0,"14,579 views",0 downloads,723 kernels,,https://www.kaggle.com/hacker-news/hacker-news,"Context
This dataset contains all stories and comments from Hacker News from its launch in 2006. Each story contains a story id, the author that made the post, when it was written, and the number of points the story received. Hacker News is a social news website focusing on computer science and entrepreneurship. It is run by Paul Graham's investment fund and startup incubator, Y Combinator. In general, content that can be submitted is defined as ""anything that gratifies one's intellectual curiosity"".
Content
Each story contains a story ID, the author that made the post, when it was written, and the number of points the story received.
Please note that the text field includes profanity. All texts are the author’s own, do not necessarily reflect the positions of Kaggle or Hacker News, and are presented without endorsement.
Querying BigQuery tables
You can use the BigQuery Python client library to query tables in this dataset in Kernels. Note that methods available in Kernels are limited to querying data. Tables are at bigquery-public-data.hacker_news.[TABLENAME]. Fork this kernel to get started.
Acknowledgements
This dataset was kindly made publicly available by Hacker News under the MIT license.
Inspiration
Recent studies have found that many forums tend to be dominated by a very small fraction of users. Is this true of Hacker News?
Hacker News has received complaints that the site is biased towards Y Combinator startups. Do the data support this?
Is the amount of coverage by Hacker News predictive of a startup’s success?"
Zika Virus Epidemic,Analyze the ongoing spread of this infectious disease,Centers for Disease Control and Prevention,79,"Version 1,2016-07-16","brazil
healthcare
epidemiology",CSV,11 MB,CC0,"36,494 views","4,487 downloads",113 kernels,3 topics,https://www.kaggle.com/cdc/zika-virus-epidemic,"An outbreak of the Zika virus, an infection transmitted mostly by the Aedes species mosquito (Ae. aegypti and Ae. albopictus), has been sweeping across the Americas and the Pacific since mid-2015. Although first isolated in 1947 in Uganda, a lack of previous research has challenged the scientific community to quickly understand its devastating effects as the epidemic continues to spread.
All Countries & Territories with Active Zika Virus Transmission
The data
This dataset shares publicly available data related to the ongoing Zika epidemic. It is being provided as a resource to the scientific community engaged in the public health response. The data provided here is not official and should be considered provisional and non-exhaustive. The data in reports may change over time, reflecting delays in reporting or changes in classifications. And while accurate representation of the reported data is the objective in the machine readable files shared here, that accuracy is not guaranteed. Before using any of these data, it is advisable to review the original reports and sources, which are provided whenever possible along with further information on the CDC Zika epidemic GitHub repo.
The dataset includes the following fields:
report_date - The report date is the date that the report was published. The date should be specified in standard ISO format (YYYY-MM-DD).
location - A location is specified for each observation following the specific names specified in the country place name database. This may be any place with a 'location_type' as listed below, e.g. city, state, country, etc. It should be specified at up to three hierarchical levels in the following format: [country]-[state/province]-[county/municipality/city], always beginning with the country name. If the data is for a particular city, e.g. Salvador, it should be specified: Brazil-Bahia-Salvador.
location_type - A location code is included indicating: city, district, municipality, county, state, province, or country. If there is need for an additional 'location_type', open an Issue to create a new 'location_type'.
data_field - The data field is a short description of what data is represented in the row and is related to a specific definition defined by the report from which it comes.
data_field_code - This code is defined in the country data guide. It includes a two letter country code (ISO-3166 alpha-2, list), followed by a 4-digit number corresponding to a specific report type and data type.
time_period - Optional. If the data pertains to a specific period of time, for example an epidemiological week, that number should be indicated here and the type of time period in the 'time_period_type', otherwise it should be NA.
time_period_type - Required only if 'time_period' is specified. Types will also be specified in the country data guide. Otherwise should be NA.
value - The observation indicated for the specific 'report_date', 'location', 'data_field' and when appropriate, 'time_period'.
unit - The unit of measurement for the 'data_field'. This should conform to the 'data_field' unit options as described in the country-specific data guide.
If you find the data useful, please support data sharing by referencing this dataset and the original data source. If you're interested in contributing to the Zika project from GitHub, you can read more here. The source for the Zika virus structure is available here."
Melbourne Housing Market,Melbourne housing clearance data from Jan 2016,Tony Pino,78,"Version 19,2018-02-13|Version 18,2018-01-11|Version 17,2017-11-27|Version 16,2017-11-23|Version 15,2017-09-26|Version 14,2017-08-18|Version 13,2017-08-18|Version 12,2017-08-16|Version 11,2017-07-05|Version 10,2017-07-04|Version 9,2017-05-26|Version 8,2017-05-22|Version 7,2017-04-10|Version 6,2017-04-10|Version 5,2017-03-22|Version 4,2017-03-07|Version 3,2017-02-15|Version 2,2016-12-13|Version 1,2016-11-23","housing
demographics",CSV,929 KB,CC4,"25,357 views","3,534 downloads",77 kernels,8 topics,https://www.kaggle.com/anthonypino/melbourne-housing-market,"Update 28/11/2017 - Last few weeks clearance levels starting to decrease (I may just be seeing a pattern I want to see.. maybe I'm just evil). Anyway, can any of you magicians make any sense of it?
Melbourne is currently experiencing a housing bubble (some experts say it may burst soon). Maybe someone can find a trend or give a prediction? Which suburbs are the best to buy in? Which ones are value for money? Where's the expensive side of town? And more importantly where should I buy a 2 bedroom unit?
Content & Acknowledgements
This data was scraped from publicly available results posted every week from Domain.com.au, I've cleaned it as best I can, now it's up to you to make data analysis magic. The dataset includes Address, Type of Real estate, Suburb, Method of Selling, Rooms, Price, Real Estate Agent, Date of Sale and distance from C.B.D.
....Now with extra data including including property size, land size and council area, you may need to change your code!
Some Key Details
Suburb: Suburb
Address: Address
Rooms: Number of rooms
Price: Price in dollars
Method: S - property sold; SP - property sold prior; PI - property passed in; PN - sold prior not disclosed; SN - sold not disclosed; NB - no bid; VB - vendor bid; W - withdrawn prior to auction; SA - sold after auction; SS - sold after auction price not disclosed. N/A - price or highest bid not available.
Type: br - bedroom(s); h - house,cottage,villa, semi,terrace; u - unit, duplex; t - townhouse; dev site - development site; o res - other residential.
SellerG: Real Estate Agent
Date: Date sold
Distance: Distance from CBD
Regionname: General Region (West, North West, North, North east ...etc)
Propertycount: Number of properties that exist in the suburb.
Bedroom2 : Scraped # of Bedrooms (from different source)
Bathroom: Number of Bathrooms
Car: Number of carspots
Landsize: Land Size
BuildingArea: Building Size
YearBuilt: Year the house was built
CouncilArea: Governing council for the area
Lattitude: Self explanitory
Longtitude: Self explanitory"
Crimes in Chicago,"An extensive dataset of crimes in Chicago (2001-2017), by City of Chicago",Currie32,78,"Version 1,2017-01-28",crime,CSV,2 GB,Other,"42,154 views","6,867 downloads",95 kernels,7 topics,https://www.kaggle.com/currie32/crimes-in-chicago,"Context
This dataset reflects reported incidents of crime (with the exception of murders where data exists for each victim) that occurred in the City of Chicago from 2001 to present, minus the most recent seven days. Data is extracted from the Chicago Police Department's CLEAR (Citizen Law Enforcement Analysis and Reporting) system. In order to protect the privacy of crime victims, addresses are shown at the block level only and specific locations are not identified. Should you have questions about this dataset, you may contact the Research & Development Division of the Chicago Police Department at 312.745.6071 or RDAnalysis@chicagopolice.org. Disclaimer: These crimes may be based upon preliminary information supplied to the Police Department by the reporting parties that have not been verified. The preliminary crime classifications may be changed at a later date based upon additional investigation and there is always the possibility of mechanical or human error. Therefore, the Chicago Police Department does not guarantee (either expressed or implied) the accuracy, completeness, timeliness, or correct sequencing of the information and the information should not be used for comparison purposes over time. The Chicago Police Department will not be responsible for any error or omission, or for the use of, or the results obtained from the use of this information. All data visualizations on maps should be considered approximate and attempts to derive specific addresses are strictly prohibited. The Chicago Police Department is not responsible for the content of any off-site pages that are referenced by or that reference this web page other than an official City of Chicago or Chicago Police Department web page. The user specifically acknowledges that the Chicago Police Department is not responsible for any defamatory, offensive, misleading, or illegal conduct of other users, links, or third parties and that the risk of injury from the foregoing rests entirely with the user. The unauthorized use of the words ""Chicago Police Department,"" ""Chicago Police,"" or any colorable imitation of these words or the unauthorized use of the Chicago Police Department logo is unlawful. This web page does not, in any way, authorize such use. Data are updated daily. The dataset contains more than 6,000,000 records/rows of data and cannot be viewed in full in Microsoft Excel. To access a list of Chicago Police Department - Illinois Uniform Crime Reporting (IUCR) codes, go to http://data.cityofchicago.org/Public-Safety/Chicago-Police-Department-Illinois-Uniform-Crime-R/c7ck-438e
Content
ID - Unique identifier for the record.
Case Number - The Chicago Police Department RD Number (Records Division Number), which is unique to the incident.
Date - Date when the incident occurred. this is sometimes a best estimate.
Block - The partially redacted address where the incident occurred, placing it on the same block as the actual address.
IUCR - The Illinois Unifrom Crime Reporting code. This is directly linked to the Primary Type and Description. See the list of IUCR codes at https://data.cityofchicago.org/d/c7ck-438e.
Primary Type - The primary description of the IUCR code.
Description - The secondary description of the IUCR code, a subcategory of the primary description.
Location Description - Description of the location where the incident occurred.
Arrest - Indicates whether an arrest was made.
Domestic - Indicates whether the incident was domestic-related as defined by the Illinois Domestic Violence Act.
Beat - Indicates the beat where the incident occurred. A beat is the smallest police geographic area – each beat has a dedicated police beat car. Three to five beats make up a police sector, and three sectors make up a police district. The Chicago Police Department has 22 police districts. See the beats at https://data.cityofchicago.org/d/aerh-rz74.
District - Indicates the police district where the incident occurred. See the districts at https://data.cityofchicago.org/d/fthy-xz3r.
Ward - The ward (City Council district) where the incident occurred. See the wards at https://data.cityofchicago.org/d/sp34-6z76.
Community Area - Indicates the community area where the incident occurred. Chicago has 77 community areas. See the community areas at https://data.cityofchicago.org/d/cauq-8yn6.
FBI Code - Indicates the crime classification as outlined in the FBI's National Incident-Based Reporting System (NIBRS). See the Chicago Police Department listing of these classifications at http://gis.chicagopolice.org/clearmap_crime_sums/crime_types.html.
X Coordinate - The x coordinate of the location where the incident occurred in State Plane Illinois East NAD 1983 projection. This location is shifted from the actual location for partial redaction but falls on the same block.
Y Coordinate - The y coordinate of the location where the incident occurred in State Plane Illinois East NAD 1983 projection. This location is shifted from the actual location for partial redaction but falls on the same block.
Year - Year the incident occurred.
Updated On - Date and time the record was last updated.
Latitude - The latitude of the location where the incident occurred. This location is shifted from the actual location for partial redaction but falls on the same block.
Longitude - The longitude of the location where the incident occurred. This location is shifted from the actual location for partial redaction but falls on the same block.
Location - The location where the incident occurred in a format that allows for creation of maps and other geographic operations on this data portal. This location is shifted from the actual location for partial redaction but falls on the same block.
Acknowledgements
I really want to say thank you to the City of Chicago and the Chicago Police Department for making this comprehensive data set available to everyone!
Inspiration
How has crime changed over the years? Is it possible to predict where or when a crime will be committed? Which areas of the city have evolved over this time span?"
Predict'em All,Predict where Pokemon appear in PokemonGo based on historical data,SemionKorchevskiy,78,"Version 3,2016-10-12|Version 2,2016-10-04|Version 1,2016-10-01","games and toys
video games",Other,763 MB,Other,"32,888 views","3,876 downloads",99 kernels,6 topics,https://www.kaggle.com/semioniy/predictemall,"Overview
PokemonGo is a mobile augmented reality game developed by Niantic inc. for iOS, Android, and Apple Watch devices. It was initially released in selected countries in July 2016. In the game, players use a mobile device's GPS capability to locate, capture, battle, and train virtual creatures, called Pokémon, who appear on the screen as if they were in the same real-world location as the player.
Dataset
Dataset consists of roughly 293,000 pokemon sightings (historical appearances of Pokemon), having coordinates, time, weather, population density, distance to pokestops/ gyms etc. as features. The target is to train a machine learning algorithm so that it can predict where pokemon appear in future. So, can you predict'em all?)
Feature description
pokemonId - the identifier of a pokemon, should be deleted to not affect predictions. (numeric; ranges between 1 and 151)
latitude, longitude - coordinates of a sighting (numeric)
appearedLocalTime - exact time of a sighting in format yyyy-mm-dd'T'hh-mm-ss.ms'Z' (nominal)
cellId 90-5850m - geographic position projected on a S2 Cell, with cell sizes ranging from 90 to 5850m (numeric)
appearedTimeOfDay - time of the day of a sighting (night, evening, afternoon, morning)
appearedHour/appearedMinute - local hour/minute of a sighting (numeric)
appearedDayOfWeek - week day of a sighting (Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday)
appearedDay/appearedMonth/appearedYear - day/month/year of a sighting (numeric)
terrainType - terrain where pokemon appeared described with help of GLCF Modis Land Cover (numeric)
closeToWater - did pokemon appear close (100m or less) to water (Boolean, same source as above)
city - the city of a sighting (nominal)
continent (not always parsed right) - the continent of a sighting (nominal)
weather - weather type during a sighting (Foggy Clear, PartlyCloudy, MostlyCloudy, Overcast, Rain, BreezyandOvercast, LightRain, Drizzle, BreezyandPartlyCloudy, HeavyRain, BreezyandMostlyCloudy, Breezy, Windy, WindyandFoggy, Humid, Dry, WindyandPartlyCloudy, DryandMostlyCloudy, DryandPartlyCloudy, DrizzleandBreezy, LightRainandBreezy, HumidandPartlyCloudy, HumidandOvercast, RainandWindy) // Source for all weather features
temperature - temperature in celsius at the location of a sighting (numeric)
windSpeed - speed of the wind in km/h at the location of a sighting (numeric)
windBearing - wind direction (numeric)
pressure - atmospheric pressure in bar at the location of a sighting (numeric)
weatherIcon - a compact representation of the weather at the location of a sighting (fog, clear-night, partly-cloudy-night, partly-cloudy-day, cloudy, clear-day, rain, wind)
sunriseMinutesMidnight-sunsetMinutesBefore - time of appearance relatively to sunrise/sunset Source
population density - what is the population density per square km of a sighting (numeric, Source)
urban-rural - how urban is location where pokemon appeared (Boolean, built on Population density, <200 for rural, >=200 and <400 for midUrban, >=400 and <800 for subUrban, >800 for urban)
gymDistanceKm, pokestopDistanceKm - how far is the nearest gym/pokestop in km from a sighting? (numeric, extracted from this dataset)
gymIn100m-pokestopIn5000m - is there a gym/pokestop in 100/200/etc meters? (Boolean)
cooc 1-cooc 151 - co-occurrence with any other pokemon (pokemon ids range between 1 and 151) within 100m distance and within the last 24 hours (Boolean)
class - says which pokemonId it is, to be predicted.
Data dump
All pokemon sightings (in JSON file, without features) can be found in Discussion ""Datadump"""
Ocean Ship Logbooks (1750-1850),Explore changing climatology with data from early shipping logs,CWILOC,78,"Version 2,2017-11-16|Version 1,2015-08-19","environment
climate
shipping",CSV,19 MB,CC0,"23,833 views","2,814 downloads",91 kernels,11 topics,https://www.kaggle.com/cwiloc/climate-data-from-ocean-ships,"In the mid-eighteenth to nineteenth centuries, navigating the open ocean was an imprecise and often dangerous feat. In order to calculate their daily progress and avoid running/sailing into the unknown, a ship's crew kept a detailed logbook with data on winds, waves, and any remarkable weather.
Handwritten in archived logbooks, these rich datasets were nearly impossible to study until the European Union funded their digitization in 2001. You can visit the EU project website for detailed information on the countries and ships included.
We're hosting the full 1750-1850 dataset on Kaggle to promote the exploration of this unique and invaluable climatology resource.
Data Description
This data comes from the Climatological Database for the World's Oceans 1750-1850 (CLIWOC), version 1.5 data release.
The primary data file is CLIWOC15.csv. The columns in this table are described on this page (scroll down to the table that starts with ""Field abbreviation""). It includes 280,280 observational records of ship locations weather data, and other associated information.
The ancillary data files are described on the above site."
1.88 Million US Wildfires,24 years of geo-referenced wildfire records,Rachael Tatman,77,"Version 1,2017-09-14","climate
firefighting",SQLite,759 MB,CC0,"13,794 views","1,590 downloads",9 kernels,,https://www.kaggle.com/rtatman/188-million-us-wildfires,"Context:
This data publication contains a spatial database of wildfires that occurred in the United States from 1992 to 2015. It is the third update of a publication originally generated to support the national Fire Program Analysis (FPA) system. The wildfire records were acquired from the reporting systems of federal, state, and local fire organizations. The following core data elements were required for records to be included in this data publication: discovery date, final fire size, and a point location at least as precise as Public Land Survey System (PLSS) section (1-square mile grid). The data were transformed to conform, when possible, to the data standards of the National Wildfire Coordinating Group (NWCG). Basic error-checking was performed and redundant records were identified and removed, to the degree possible. The resulting product, referred to as the Fire Program Analysis fire-occurrence database (FPA FOD), includes 1.88 million geo-referenced wildfire records, representing a total of 140 million acres burned during the 24-year period.
Content:
This dataset is an SQLite database that contains the following information:
Fires: Table including wildfire data for the period of 1992-2015 compiled from US federal, state, and local reporting systems.
FOD_ID = Global unique identifier.
FPA_ID = Unique identifier that contains information necessary to track back to the original record in the source dataset.
SOURCE_SYSTEM_TYPE = Type of source database or system that the record was drawn from (federal, nonfederal, or interagency).
SOURCE_SYSTEM = Name of or other identifier for source database or system that the record was drawn from. See Table 1 in Short (2014), or \Supplements\FPA_FOD_source_list.pdf, for a list of sources and their identifier.
NWCG_REPORTING_AGENCY = Active National Wildlife Coordinating Group (NWCG) Unit Identifier for the agency preparing the fire report (BIA = Bureau of Indian Affairs, BLM = Bureau of Land Management, BOR = Bureau of Reclamation, DOD = Department of Defense, DOE = Department of Energy, FS = Forest Service, FWS = Fish and Wildlife Service, IA = Interagency Organization, NPS = National Park Service, ST/C&L = State, County, or Local Organization, and TRIBE = Tribal Organization).
NWCG_REPORTING_UNIT_ID = Active NWCG Unit Identifier for the unit preparing the fire report.
NWCG_REPORTING_UNIT_NAME = Active NWCG Unit Name for the unit preparing the fire report.
SOURCE_REPORTING_UNIT = Code for the agency unit preparing the fire report, based on code/name in the source dataset.
SOURCE_REPORTING_UNIT_NAME = Name of reporting agency unit preparing the fire report, based on code/name in the source dataset.
LOCAL_FIRE_REPORT_ID = Number or code that uniquely identifies an incident report for a particular reporting unit and a particular calendar year.
LOCAL_INCIDENT_ID = Number or code that uniquely identifies an incident for a particular local fire management organization within a particular calendar year.
FIRE_CODE = Code used within the interagency wildland fire community to track and compile cost information for emergency fire suppression (https://www.firecode.gov/).
FIRE_NAME = Name of the incident, from the fire report (primary) or ICS-209 report (secondary).
ICS_209_INCIDENT_NUMBER = Incident (event) identifier, from the ICS-209 report.
ICS_209_NAME = Name of the incident, from the ICS-209 report.
MTBS_ID = Incident identifier, from the MTBS perimeter dataset.
MTBS_FIRE_NAME = Name of the incident, from the MTBS perimeter dataset.
COMPLEX_NAME = Name of the complex under which the fire was ultimately managed, when discernible.
FIRE_YEAR = Calendar year in which the fire was discovered or confirmed to exist.
DISCOVERY_DATE = Date on which the fire was discovered or confirmed to exist.
DISCOVERY_DOY = Day of year on which the fire was discovered or confirmed to exist.
DISCOVERY_TIME = Time of day that the fire was discovered or confirmed to exist.
STAT_CAUSE_CODE = Code for the (statistical) cause of the fire.
STAT_CAUSE_DESCR = Description of the (statistical) cause of the fire.
CONT_DATE = Date on which the fire was declared contained or otherwise controlled (mm/dd/yyyy where mm=month, dd=day, and yyyy=year).
CONT_DOY = Day of year on which the fire was declared contained or otherwise controlled.
CONT_TIME = Time of day that the fire was declared contained or otherwise controlled (hhmm where hh=hour, mm=minutes).
FIRE_SIZE = Estimate of acres within the final perimeter of the fire.
FIRE_SIZE_CLASS = Code for fire size based on the number of acres within the final fire perimeter expenditures (A=greater than 0 but less than or equal to 0.25 acres, B=0.26-9.9 acres, C=10.0-99.9 acres, D=100-299 acres, E=300 to 999 acres, F=1000 to 4999 acres, and G=5000+ acres).
LATITUDE = Latitude (NAD83) for point location of the fire (decimal degrees).
LONGITUDE = Longitude (NAD83) for point location of the fire (decimal degrees).
OWNER_CODE = Code for primary owner or entity responsible for managing the land at the point of origin of the fire at the time of the incident.
OWNER_DESCR = Name of primary owner or entity responsible for managing the land at the point of origin of the fire at the time of the incident.
STATE = Two-letter alphabetic code for the state in which the fire burned (or originated), based on the nominal designation in the fire report.
COUNTY = County, or equivalent, in which the fire burned (or originated), based on nominal designation in the fire report.
FIPS_CODE = Three-digit code from the Federal Information Process Standards (FIPS) publication 6-4 for representation of counties and equivalent entities.
FIPS_NAME = County name from the FIPS publication 6-4 for representation of counties and equivalent entities.
NWCG_UnitIDActive_20170109: Look-up table containing all NWCG identifiers for agency units that were active (i.e., valid) as of 9 January 2017, when the list was downloaded from https://www.nifc.blm.gov/unit_id/Publish.html and used as the source of values available to populate the following fields in the Fires table: NWCG_REPORTING_AGENCY, NWCG_REPORTING_UNIT_ID, and NWCG_REPORTING_UNIT_NAME.
UnitId = NWCG Unit ID.
GeographicArea = Two-letter code for the geographic area in which the unit is located (NA=National, IN=International, AK=Alaska, CA=California, EA=Eastern Area, GB=Great Basin, NR=Northern Rockies, NW=Northwest, RM=Rocky Mountain, SA=Southern Area, and SW=Southwest).
Gacc = Seven or eight-letter code for the Geographic Area Coordination Center in which the unit is located or primarily affiliated with (CAMBCIFC=Canadian Interagency Forest Fire Centre, USAKCC=Alaska Interagency Coordination Center, USCAONCC=Northern California Area Coordination Center, USCAOSCC=Southern California Coordination Center, USCORMCC=Rocky Mountain Area Coordination Center, USGASAC=Southern Area Coordination Center, USIDNIC=National Interagency Coordination Center, USMTNRC=Northern Rockies Coordination Center, USNMSWC=Southwest Area Coordination Center, USORNWC=Northwest Area Coordination Center, USUTGBC=Western Great Basin Coordination Center, USWIEACC=Eastern Area Coordination Center).
WildlandRole = Role of the unit within the wildland fire community.
UnitType = Type of unit (e.g., federal, state, local).
Department = Department (or state/territory) to which the unit belongs (AK=Alaska, AL=Alabama, AR=Arkansas, AZ=Arizona, CA=California, CO=Colorado, CT=Connecticut, DE=Delaware, DHS=Department of Homeland Security, DOC= Department of Commerce, DOD=Department of Defense, DOE=Department of Energy, DOI= Department of Interior, DOL=Department of Labor, FL=Florida, GA=Georgia, IA=Iowa, IA/GC=Non-Departmental Agencies, ID=Idaho, IL=Illinois, IN=Indiana, KS=Kansas, KY=Kentucky, LA=Louisiana, MA=Massachusetts, MD=Maryland, ME=Maine, MI=Michigan, MN=Minnesota, MO=Missouri, MS=Mississippi, MT=Montana, NC=North Carolina, NE=Nebraska, NG=Non-Government, NH=New Hampshire, NJ=New Jersey, NM=New Mexico, NV=Nevada, NY=New York, OH=Ohio, OK=Oklahoma, OR=Oregon, PA=Pennsylvania, PR=Puerto Rico, RI=Rhode Island, SC=South Carolina, SD=South Dakota, ST/L=State or Local Government, TN=Tennessee, Tribe=Tribe, TX=Texas, USDA=Department of Agriculture, UT=Utah, VA=Virginia, VI=U. S. Virgin Islands, VT=Vermont, WA=Washington, WI=Wisconsin, WV=West Virginia, WY=Wyoming).
Agency = Agency or bureau to which the unit belongs (AG=Air Guard, ANC=Alaska Native Corporation, BIA=Bureau of Indian Affairs, BLM=Bureau of Land Management, BOEM=Bureau of Ocean Energy Management, BOR=Bureau of Reclamation, BSEE=Bureau of Safety and Environmental Enforcement, C&L=County & Local, CDF=California Department of Forestry & Fire Protection, DC=Department of Corrections, DFE=Division of Forest Environment, DFF=Division of Forestry Fire & State Lands, DFL=Division of Forests and Land, DFR=Division of Forest Resources, DL=Department of Lands, DNR=Department of Natural Resources, DNRC=Department of Natural Resources and Conservation, DNRF=Department of Natural Resources Forest Service, DOA=Department of Agriculture, DOC=Department of Conservation, DOE=Department of Energy, DOF=Department of Forestry, DVF=Division of Forestry, DWF=Division of Wildland Fire, EPA=Environmental Protection Agency, FC=Forestry Commission, FEMA=Federal Emergency Management Agency, FFC=Bureau of Forest Fire Control, FFP=Forest Fire Protection, FFS=Forest Fire Service, FR=Forest Rangers, FS=Forest Service, FWS=Fish & Wildlife Service, HQ=Headquarters, JC=Job Corps, NBC=National Business Center, NG=National Guard, NNSA=National Nuclear Security Administration, NPS=National Park Service, NWS=National Weather Service, OES=Office of Emergency Services, PRI=Private, SF=State Forestry, SFS=State Forest Service, SP=State Parks, TNC=The Nature Conservancy, USA=United States Army, USACE=United States Army Corps of Engineers, USAF=United States Air Force, USGS=United States Geological Survey, USN=United States Navy).
Parent = Agency subgroup to which the unit belongs (A concatenation of State and Unit from this report - https://www.nifc.blm.gov/unit_id/publish/UnitIdReport.rtf).
Country = Country in which the unit is located (e.g. US = United States).
State = Two-letter code for the state in which the unit is located (or primarily affiliated).
Code = Unit code (follows state code to create UnitId).
Name = Unit name.
Acknowledgements:
These data were collected using funding from the U.S. Government and can be used without additional permissions or fees. If you use these data in a publication, presentation, or other research product please use the following citation:
Short, Karen C. 2017. Spatial wildfire occurrence data for the United States, 1992-2015 [FPA_FOD_20170508]. 4th Edition. Fort Collins, CO: Forest Service Research Data Archive. https://doi.org/10.2737/RDS-2013-0009.4
Inspiration:
Have wildfires become more or less frequent over time?
What counties are the most and least fire-prone?
Given the size, location and date, can you predict the cause of a fire wildfire?"
Trending YouTube Video Statistics and Comments,"Daily statistics (views, likes, category, comments+) for trending YouTube videos",Mitchell J,77,"Version 24,2017-10-26|Version 23,2017-10-22|Version 22,2017-10-20|Version 21,2017-10-17|Version 20,2017-10-16|Version 19,2017-10-12|Version 18,2017-10-10|Version 17,2017-10-07|Version 16,2017-10-05|Version 15,2017-10-03|Version 14,2017-10-02|Version 13,2017-09-28|Version 12,2017-09-27|Version 11,2017-09-25|Version 10,2017-09-24|Version 9,2017-09-23|Version 8,2017-09-22|Version 7,2017-09-21|Version 6,2017-09-21|Version 5,2017-09-20|Version 4,2017-09-18|Version 3,2017-09-16|Version 2,2017-09-16|Version 1,2017-09-16","languages
popular culture
statistics
+ 2 more...",CSV,149 MB,CC0,"26,908 views","3,111 downloads",10 kernels,,https://www.kaggle.com/datasnaek/youtube,"UPDATED: https://www.kaggle.com/datasnaek/youtube-new
Plan
Data collected from the (up to) 200 listed trending YouTube videos every day in the US and the UK.
Description
The dataset includes data gathered from videos on YouTube that are contained within the trending category each day.
There are two kinds of data files, one includes comments and one includes video statistics. They are linked by the unique video_id field.
The headers in the video file are:
video_id (Common id field to both comment and video csv files)
title
channel_title
category_id (Can be looked up using the included JSON files, but varies per region so use the appropriate JSON file for the CSV file's country)
tags (Separated by | character, [none] is displayed if there are no tags)
views
likes
dislikes
thumbnail_link
date (Formatted like so: [day].[month])
The headers in the comments file are:
video_id (Common id field to both comment and video csv files)
comment_text
likes
replies
Extra info: The YouTube API is not effective at formatting comments by relevance, although it claims to do so. As a result, the most relevant comments do not align with the top comments at all, they aren't even sorted by likes or replies.
Inspiration
Possible uses for this dataset could include:
Sentiment analysis in a variety of forms
Categorising YouTube videos based on their comments and statistics.
Training ML algorithms to generate their own YouTube comments.
Analysing what factors affect how popular a YouTube video will be.
Although there are likely many more possibilities, including analysis of changes over time etc."
2013 American Community Survey,Find insights in the 2013 American Community Survey,US Census Bureau,77,"Version 2,2017-05-02|Version 1,2015-07-18","social groups
demographics
sociology",CSV,4 GB,CC0,"39,307 views","3,187 downloads",662 kernels,23 topics,https://www.kaggle.com/census/2013-american-community-survey,"The American Community Survey is an ongoing survey from the US Census Bureau. In this survey, approximately 3.5 million households per year are asked detailed questions about who they are and how they live. Many topics are covered, including ancestry, education, work, transportation, internet use, and residency.
The responses reveal a fascinating, granular snapshot into the lives of many Americans.
We''re publishing this data on scripts to make it easy for you to explore this rich dataset, share your work, and collaborate with other data scientists. No data download or local environment needed! We''ve also added shapefiles to simplify publishing maps.
What surprising insights can you find in this data? We look forward to seeing and sharing what you discover on scripts!
Data Description
Here''s a data dictionary.
There are two types of survey data provided, housing and population.
For the housing data, each row is a housing unit, and the characteristics are properties like rented vs. owned, age of home, etc.
For the population data, each row is a person and the characteristics are properties like age, gender, whether they work, method/length of commute, etc.
Each data set is divided in two pieces, ""a"" and ""b"" (where ""a"" contains states 1 to 25 and ""b"" contains states 26 to 50).
Both data sets have weights associated with them. Weights are included to account for the fact that individuals are not sampled with equal probably (people who have a greater chance of being sampled have a lower weight to reflect this).
Weight variable for the housing data: WGTP
Weight variable for the population data: PWGTP
In Kaggle Scripts, these files can be accessed at:
../input/pums/ss13husa.csv (housing, a)
../input/pums/ss13husb.csv (housing, b)
../input/pums/ss13pusa.csv (population, a)
../input/pums/ss13pusb.csv (population, b)
You can download the data from the census website:
housing
population
In scripts, they are accessed at:
../input/shapefiles/pums/tl_2013_[state]_puma10.[extension].
The shapefiles can also be downloaded here.
DataCamp and Kaggle have teamed up to bring you the basics of Data Exploration With Kaggle Scripts. Take the free, interactive course here and start building your data science portfolio."
How ISIS Uses Twitter,Analyze how ISIS fanboys have been using Twitter since 2015 Paris Attacks,Fifth Tribe,77,"Version 4,2016-05-18|Version 3,2016-05-16|Version 2,2016-05-14|Version 1,2016-05-14","crime
twitter
internet",CSV,6 MB,CC0,"52,509 views","4,804 downloads",99 kernels,12 topics,https://www.kaggle.com/fifthtribe/how-isis-uses-twitter,"We scraped over 17,000 tweets from 100+ pro-ISIS fanboys from all over the world since the November 2015 Paris Attacks. We are working with content producers and influencers to develop effective counter-messaging measures against violent extremists at home and abroad. In order to maximize our impact, we need assistance in quickly analyzing message frames.
The dataset includes the following:
Name
Username
Description
Location
Number of followers at the time the tweet was downloaded
Number of statuses by the user when the tweet was downloaded
Date and timestamp of the tweet
The tweet itself
Based on this data, here are some useful ways of deriving insights and analysis:
Social Network Cluster Analysis: Who are the major players in the pro-ISIS twitter network? Ideally, we would like this visualized via a cluster network with the biggest influencers scaled larger than smaller influencers.
Keyword Analysis: Which keywords derived from the name, username, description, location, and tweets were the most commonly used by ISIS fanboys? Examples include: ""baqiyah"", ""dabiq"", ""wilayat"", ""amaq""
Data Categorization of Links: Which websites are pro-ISIS fanboys linking to? Categories include: Mainstream Media, Altermedia, Jihadist Websites, Image Upload, Video Upload,
Sentiment Analysis: Which clergy do pro-ISIS fanboys quote the most and which ones do they hate the most? Search the tweets for names of prominent clergy and classify the tweet as positive, negative, or neutral and if negative, include the reasons why. Examples of clergy they like the most: ""Anwar Awlaki"", ""Ahmad Jibril"", ""Ibn Taymiyyah"", ""Abdul Wahhab"". Examples of clergy that they hate the most: ""Hamza Yusuf"", ""Suhaib Webb"", ""Yaser Qadhi"", ""Nouman Ali Khan"", ""Yaqoubi"".
Timeline View: Visualize all the tweets over a timeline and identify peak moments
Further Reading: ""ISIS Has a Twitter Strategy and It is Terrifying [Infographic]""
About Fifth Tribe
Fifth Tribe is a digital agency based out of DC that serves businesses, non-profits, and government agencies. We provide our clients with product development, branding, web/mobile development, and digital marketing services. Our client list includes Oxfam, Ernst and Young, Kaiser Permanente, Aetna Innovation Health, the U.S. Air Force, and the U.S. Peace Corps. Along with Goldman Sachs International and IBM, we serve on the Private Sector Committee of the Board of the Global Community Engagement and Resilience Fund (GCERF), the first global effort to support local, community-level initiatives aimed at strengthening resilience against violent extremism. In December 2014, we won the anti-ISIS ""Hedaya Hack"" organized by Affinis Labs and hosted at the ""Global Countering Violent Extremism (CVE) Expo "" in Abu Dhabi. Since then, we've been actively involved in working with the open-source community and community content producers in developing counter-messaging campaigns and tools."
Celebrity Deaths,All wikipedia-listed celebrity deaths from 2006,HugoDarwood,76,"Version 4,2017-01-14|Version 3,2016-12-31|Version 2,2016-12-29|Version 1,2016-12-29","biography
celebrity
death",CSV,2 MB,CC4,"24,667 views","2,487 downloads",136 kernels,8 topics,https://www.kaggle.com/hugodarwood/celebrity-deaths,"Context
I created this dataset to investigate the claim that 2016 had an unnaturally large number of celebrity deaths.
Content
Points listed by Name, Age, Cause of death and Reason for fame
Acknowledgements
Lifted from: https://en.wikipedia.org/wiki/Deaths_in_2016 for all years"
Hospital Charges for Inpatients,How inpatient hospital charges can differ among different providers in the US,Pranay Aryal,75,"Version 2,2016-09-19|Version 1,2016-09-19","healthcare
finance",CSV,26 MB,Other,"22,926 views","2,887 downloads",71 kernels,6 topics,https://www.kaggle.com/speedoheck/inpatient-hospital-charges,"Variation of hospital charges in the various hospitals in the US for the top 100 diagnoses.
The dataset is owned by the US government. It is freely available on data.gov The dataset keeps getting updated periodically here
This dataset will show you how price for the same diagnosis and the same treatment and in the same city can vary differently across different providers. It might help you or your loved one find a better hospital for your treatment. You can also analyze to detect fraud among providers."
Kickstarter Project Statistics,4000 live projects plus 4000 most backed projects,Cathie So,75,"Version 1,2016-11-01",finance,CSV,3 MB,CC4,"30,850 views","3,817 downloads",104 kernels,2 topics,https://www.kaggle.com/socathie/kickstarter-project-statistics,"Crowdfunding has become one of the main sources of initial capital for small businesses and start-up companies that are looking to launch their first products. Websites like Kickstarter and Indiegogo provide a platform for millions of creators to present their innovative ideas to the public. This is a win-win situation where creators could accumulate initial fund while the public get access to cutting-edge prototypical products that are not available in the market yet.
At any given point, Indiegogo has around 10,000 live campaigns while Kickstarter has 6,000. It has become increasingly difficult for projects to stand out of the crowd. Of course, advertisements via various channels are by far the most important factor to a successful campaign. However, for creators with a smaller budget, this leaves them wonder,
""How do we increase the probability of success of our campaign starting from the very moment we create our project on these websites?""
Data Sources
All of my raw data are scraped from Kickstarter.com.
First 4000 live projects that are currently campaigning on Kickstarter (live.csv)
Last updated: 2016-10-29 5pm PDT
amt.pledged: amount pledged (float)
blurb: project blurb (string)
by: project creator (string)
country: abbreviated country code (string of length 2)
currency: currency type of amt.pledged (string of length 3)
end.time: campaign end time (string ""YYYY-MM-DDThh:mm:ss-TZD"")
location: mostly city (string)
pecentage.funded: unit % (int)
state: mostly US states (string of length 2) and others (string)
title: project title (string)
type: type of location (string: County/Island/LocalAdmin/Suburb/Town/Zip)
url: project url after domain (string)
Top 4000 most backed projects ever on Kickstarter (most_backed.csv)
Last updated: 2016-10-30 10pm PDT
amt.pledged
blurb
by
category: project category (string)
currency
goal: original pledge goal (float)
location
num.backers: total number of backers (int)
num.backers.tier: number of backers corresponds to the pledge amount in pledge.tier (int[len(pledge.tier)])
pledge.tier: pledge tiers in USD (float[])
title
url
See more at http://datapolymath.paperplane.io/"
Airlines Delay,Airline on-time statistics and delay causes,Giovanni Gonzalez,75,"Version 2,2016-11-12|Version 1,2016-11-12",aviation,Other,239 MB,Other,"41,946 views","6,056 downloads",53 kernels,4 topics,https://www.kaggle.com/giovamata/airlinedelaycauses,"The U.S. Department of Transportation's (DOT) Bureau of Transportation Statistics (BTS) tracks the on-time performance of domestic flights operated by large air carriers. Summary information on the number of on-time, delayed, canceled and diverted flights appears in DOT's monthly Air Travel Consumer Report, published about 30 days after the month's end, as well as in summary tables posted on this website. BTS began collecting details on the causes of flight delays in June 2003. Summary statistics and raw data are made available to the public at the time the Air Travel Consumer Report is released.
This version of the dataset was compiled from the Statistical Computing Statistical Graphics 2009 Data Expo and is also available here."
League of Legends,"Competitive matches, 2015 to 2018",Chuck Ephron,75,"Version 7,2018-01-30|Version 6,2018-01-09|Version 5,2017-05-25|Version 4,2017-04-27|Version 3,2017-04-06|Version 2,2017-04-06|Version 1,2017-04-04",video games,CSV,30 MB,CC0,"21,150 views","2,875 downloads",46 kernels,6 topics,https://www.kaggle.com/chuckephron/leagueoflegends,"League of Legends competitive matches between 2015-2017. The matches include the NALCS, EULCS, LCK, LMS, and CBLoL leagues as well as the World Championship and Mid-Season Invitational tournaments."
A Million News Headlines,News headlines published over a period of 14 years.,Rohk,75,"Version 6,2018-01-02|Version 5,2017-10-11|Version 4,2017-08-31|Version 3,2017-08-18|Version 2,2017-07-30|Version 1,2017-07-23","news agencies
historiography
linguistics
sociology",CSV,19 MB,CC4,"14,723 views","1,281 downloads",23 kernels,4 topics,https://www.kaggle.com/therohk/million-headlines,"Context
This contains data of news headlines published over a period of 15 years. From the reputable Australian news source ABC (Australian Broadcasting Corp.)
Site: http://www.abc.net.au/
Prepared by Rohit Kulkarni
Content
Format: CSV Rows: 1,103,665
Column 1: publish_date (yyyyMMdd format)
Column 2: headline_text (ascii, lowercase)
Start Date: 2003-02-19 End Date: 2017-12-31
Acknowledgements
Special thanks to the java jsoup library.
This dataset is free to use with citation:
Rohit Kulkarni (2017), A Million News Headlines [CSV Data file], doi:10.7910/DVN/SYBGZL, Retrieved from: [this url]
Inspiration
I look at this news dataset as a summarised historical record of noteworthy events in the globe from early-2003 to end-2017 with a more granular focus on Australia.
This includes the entire corpus of articles published by the ABC website in the given time range. With a volume of 200 articles per day and a good focus on international news, we can be fairly certain that every event of significance has been captured here.
Digging into the keywords, one can see all the important episodes shaping the last decade and how they evolved over time. Ex: financial crisis, iraq war, multiple US elections, ecological disasters, terrorism, famous people, Australian crimes etc.
Similar Work
Your kernals can be reused with minimal changes across all these datasets
3M Clickbait Headlines for 6 years: Examine the Examiner
1.3M Global Headlines from 20K sources over 1 week: Global News Week
2.6M News Headlines from India from 2001-2017: Headlines of India"
1 million Sudoku games,1 million numpy array pairs of Sudoku games and solutions,Kyubyong Park,74,"Version 3,2016-12-29|Version 2,2016-12-28|Version 1,2016-12-27",games and toys,CSV,156 MB,CC0,"28,707 views","1,584 downloads",20 kernels,3 topics,https://www.kaggle.com/bryanpark/sudoku,"Context
Sudoku is a popular number puzzle that requires you to fill blanks in a 9X9 grid with digits so that each column, each row, and each of the nine 3×3 subgrids contains all of the digits from 1 to 9. Sudoku-solving has gained much attention from various fields. As a deep learning researcher, I was inclined to investigate the possibilities of neural networks solving Sudoku. This dataset was prepared for that.
Content
There are dozens of source codes to generate Sudoku games available. I picked one of them, and ran the code. It took approximately 6 hours to generate 1 million games ( + solutions).
A Sudoku puzzle is represented as a 9x9 Python numpy array. The blanks were replaced with 0's. You can easily load and explore the data by running this.
import numpy as np
quizzes = np.load('sudoku_quizzes.npy') # shape = (1000000, 9, 9)
solutions = np.load('sudoku_solutions.npy') # shape = (1000000, 9, 9)
for quiz, solution in zip(quizzes[:10], solutions[:10]):
    print(quiz)
    print(solution)
** Updates for Version 3. **
I converted NumPy arrays to csv so they are easily accessible, irrespective of language. In each line, a Sudoku quiz and its corresponding solution are separated by a comma. You can restore the csv file content to Numpy arrays if needed as follows:
import numpy as np
quizzes = np.zeros((1000000, 81), np.int32)
solutions = np.zeros((1000000, 81), np.int32)
for i, line in enumerate(open('sudoku.csv', 'r').read().splitlines()[1:]):
    quiz, solution = line.split("","")
    for j, q_s in enumerate(zip(quiz, solution)):
        q, s = q_s
        quizzes[i, j] = q
        solutions[i, j] = s
quizzes = quizzes.reshape((-1, 9, 9))
solutions = solutions.reshape((-1, 9, 9))
Acknowledgements
I'm grateful to Arel Cordero, who wrote and shared this great Sudoku generation code. https://www.ocf.berkeley.edu/~arel/sudoku/main.html.
Inspiration
Check https://github.com/Kyubyong/sudoku to see if CNNs can crack Sudoku puzzles.
Also, reinforcement learning can be a promising alternative to this task.
Feel free to challenge Sudoku puzzles."
U.S. Opiate Prescriptions/Overdoses,Can you save lives through predictive modeling?,"Alan ""AJ"" Pryor",74,"Version 2,2016-10-24|Version 1,2016-10-23",healthcare,CSV,14 MB,CC0,"22,101 views","3,366 downloads",41 kernels,4 topics,https://www.kaggle.com/apryor6/us-opiate-prescriptions,"U.S. Opiate Prescriptions
Accidental death by fatal drug overdose is a rising trend in the United States. What can you do to help?
This dataset contains summaries of prescription records for 250 common opioid and non-opioid drugs written by 25,000 unique licensed medical professionals in 2014 in the United States for citizens covered under Class D Medicare as well as some metadata about the doctors themselves. This is a small subset of data that was sourced from cms.gov. The full dataset contains almost 24 million prescription instances in long format. I have cleaned and compiled this data here in a format with 1 row per prescriber and limited the approximately 1 million total unique prescribers down to 25,000 to keep it manageable. If you are interested in more data, you can get the script I used to assemble the dataset here and run it yourself. The main data is in prescriber-info.csv. There is also opioids.csv that contains the names of all opioid drugs included in the data and overdoses.csv that contains information on opioid related drug overdose fatalities.
The increase in overdose fatalities is a well-known problem, and the search for possible solutions is an ongoing effort. My primary interest in this dataset is detecting sources of significant quantities of opiate prescriptions. However, there is plenty of other studies to perform, and I am interested to see what other Kagglers will come up with, or if they can improve the model I have already built.
The data consists of the following characteristics for each prescriber
NPI – unique National Provider Identifier number
Gender - (M/F)
State - U.S. State by abbreviation
Credentials - set of initials indicative of medical degree
Specialty - description of type of medicinal practice
A long list of drugs with numeric values indicating the total number of prescriptions written for the year by that individual
Opioid.Prescriber - a boolean label indicating whether or not that individual prescribed opiate drugs more than 10 times in the year"
US Consumer Finance Complaints,US consumer complaints on financial products and company responses,Consumer Financial Protection Bureau,73,"Version 1,2016-04-27",finance,CSV,361 MB,Other,"28,190 views","3,448 downloads",76 kernels,,https://www.kaggle.com/cfpb/us-consumer-finance-complaints,"Each week the CFPB sends thousands of consumers’ complaints about financial products and services to companies for response. Those complaints are published here after the company responds or after 15 days, whichever comes first. By adding their voice, consumers help improve the financial marketplace."
Demonetization in India Twitter Data,Data extracted from Twitter regarding the recent currency demonetization,Amandeep Rathee,72,"Version 3,2017-04-22|Version 2,2016-11-24|Version 1,2016-11-24","finance
twitter
human-computer interaction
internet",CSV,5 MB,Other,"30,471 views","3,258 downloads",168 kernels,4 topics,https://www.kaggle.com/arathee2/demonetization-in-india-twitter-data,"Context
The demonetization of ₹500 and ₹1000 banknotes was a step taken by the Government of India on 8 November 2016, ceasing the usage of all ₹500 and ₹1000 banknotes of the Mahatma Gandhi Series as a form of legal tender in India from 9 November 2016.
The announcement was made by the Prime Minister of India Narendra Modi in an unscheduled live televised address to the nation at 20:15 Indian Standard Time (IST) the same day. In the announcement, Modi declared circulation of all ₹500 and ₹1000 banknotes of the Mahatma Gandhi Series as invalid and announced the issuance of new ₹500 and ₹2000 banknotes of the Mahatma Gandhi New Series in exchange for the old banknotes.
Content
The data contains 6000 most recent tweets on #demonetization. There are 6000 rows(one for each tweet) and 14 columns.
Metadata:
Text (Tweets)
favorited
favoriteCount
replyToSN
created
truncated
replyToSID
id
replyToUID
statusSource
screenName
retweetCount
isRetweet
retweeted
Acknowledgement
The data was collected using the ""twitteR"" package in R using the twitter API.
Past Research
I have performed my own analysis on the data. I only did a sentiment analysis and formed a word cloud.
Click here to see the analysis on GitHub
Inspiration
What percentage of tweets are negative, positive or neutral ?
What are the most famous/re-tweeted tweets ?"
Open Exoplanet Catalogue,Characteristics of all discovered extrasolar planets,Megan Risdal,71,"Version 2,2017-06-09|Version 1,2016-09-11","astronomy
space",CSV,455 KB,Other,"15,670 views","1,541 downloads",90 kernels,,https://www.kaggle.com/mrisdal/open-exoplanet-catalogue,"Our first glimpse at planets outside of the solar system we call home came in 1992 when several terrestrial-mass planets were detected orbiting the pulsar PSR B1257+12. In this dataset, you can become a space explorer too by analyzing the characteristics of all discovered exoplanets (plus some familiar faces like Mars, Saturn, and even Earth). Data fields include planet and host star attributes, discovery methods, and (of course) date of discovery.
Data was originally collected and continues to be updated by Hanno Rein at the Open Exoplanet Catalogue Github repository. If you discover any new exoplanets, please submit a pull request there.
Constants
Jupiter mass: 1.8991766e+27 kg
Solar mass: 1.9891e+30 kg
Jupiter radius: 69911000 m
Solar radius: 6.96e+08 m
License
The database is licensed under an MIT license. If you use it for a scientific publication, please include a reference to the Open Exoplanet Catalogue on GitHub or to this arXiv paper."
Diamonds,"Analyze diamonds by their cut, color, clarity, price, and other attributes",shivamagrawal,71,"Version 1,2017-05-25","clothing
finance",CSV,3 MB,Other,"19,685 views","3,349 downloads",53 kernels,3 topics,https://www.kaggle.com/shivam2503/diamonds,"Context
This classic dataset contains the prices and other attributes of almost 54,000 diamonds. It's a great dataset for beginners learning to work with data analysis and visualization.
Content
price price in US dollars (\$326--\$18,823)
carat weight of the diamond (0.2--5.01)
cut quality of the cut (Fair, Good, Very Good, Premium, Ideal)
color diamond colour, from J (worst) to D (best)
clarity a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))
x length in mm (0--10.74)
y width in mm (0--58.9)
z depth in mm (0--31.8)
depth total depth percentage = z / mean(x, y) = 2 * z / (x + y) (43--79)
table width of top of diamond relative to widest point (43--95)"
Red Wine Quality,Simple and clean practice dataset for regression or classification modelling,UCI Machine Learning,71,"Version 2,2017-11-28|Version 1,2017-11-12","food and drink
beginner
regression analysis",CSV,99 KB,ODbL,"15,724 views","3,359 downloads",32 kernels,4 topics,https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009,"Context
The two datasets are related to red and white variants of the Portuguese ""Vinho Verde"" wine. For more details, consult the reference [Cortez et al., 2009]. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).
These datasets can be viewed as classification or regression tasks. The classes are ordered and not balanced (e.g. there are much more normal wines than excellent or poor ones).
This dataset is also available from the UCI machine learning repository, https://archive.ics.uci.edu/ml/datasets/wine+quality , I just shared it to kaggle for convenience. (If I am mistaken and the public license type disallowed me from doing so, I will take this down if requested.)
Content
For more information, read [Cortez et al., 2009].
Input variables (based on physicochemical tests):
1 - fixed acidity
2 - volatile acidity
3 - citric acid
4 - residual sugar
5 - chlorides
6 - free sulfur dioxide
7 - total sulfur dioxide
8 - density
9 - pH
10 - sulphates
11 - alcohol
Output variable (based on sensory data):
12 - quality (score between 0 and 10)
Tips
What might be an interesting thing to do, is aside from using regression modelling, is to set an arbitrary cutoff for your dependent variable (wine quality) at e.g. 7 or higher getting classified as 'good/1' and the remainder as 'not good/0'. This allows you to practice with hyper parameter tuning on e.g. decision tree algorithms looking at the ROC curve and the AUC value. Without doing any kind of feature engineering or overfitting you should be able to get an AUC of .88 (without even using random forest algorithm)
KNIME is a great tool (GUI) that can be used for this.
1 - File Reader (for csv) to linear correlation node and to interactive histogram for basic EDA.
2- File Reader to 'Rule Engine Node' to turn the 10 point scale to dichtome variable (good wine and rest), the code to put in the rule engine is something like this:
- $quality$ > 6.5 => ""good""
- TRUE => ""bad""
3- Rule Engine Node output to input of Column Filter node to filter out your original 10point feature (this prevent leaking)
4- Column Filter Node output to input of Partitioning Node (your standard train/tes split, e.g. 75%/25%, choose 'random' or 'stratified')
5- Partitioning Node train data split output to input of Train data split to input Decision Tree Learner node and
6- Partitioning Node test data split output to input Decision Tree predictor Node
7- Decision Tree learner Node output to input Decision Tree Node input
8- Decision Tree output to input ROC Node.. (here you can evaluate your model base on AUC value)
Inspiration
Use machine learning to determine which physiochemical properties make a wine 'good'!
Acknowledgements
This dataset is also available from the UCI machine learning repository, https://archive.ics.uci.edu/ml/datasets/wine+quality , I just shared it to kaggle for convenience. (I am mistaken and the public license type disallowed me from doing so, I will take this down at first request. I am not the owner of this dataset.
Please include this citation if you plan to use this database: P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.
Relevant publication
P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009."
Keras Pretrained models,This dataset helps to use pretrained keras models in Kernels.,beluga,70,"Version 11,2017-11-17|Version 10,2017-11-17|Version 9,2017-10-13|Version 8,2017-10-13|Version 7,2017-10-05|Version 6,2017-10-04|Version 5,2017-10-04|Version 4,2017-10-04|Version 3,2017-10-04|Version 2,2017-10-04|Version 1,2017-10-03","artificial intelligence
pre-trained model",Other,943 MB,CC4,"7,870 views","1,401 downloads",27 kernels,,https://www.kaggle.com/gaborfodor/keras-pretrained-models,"Context
Kaggle has more and more computer vision challenges. Although Kernel resources were increased recently we still can not train useful CNNs without GPU. The other main problem is that Kernels can't use network connection to download pretrained keras model weights. This dataset helps you to apply your favorite pretrained model in the Kaggle Kernel environment.
Happy data exploration and transfer learning!
Content
Model (Top-1 Accuracy | Top -5 Accuracy)
Xception (0.790 | 0.945)
VGG16 (0.715 | 0.901)
VGG19 (0.727 | 0.910)
ResNet50 (0.759 | 0.929)
InceptionV3 (0.788 | 0.944)
InceptionResNetV2 (0.804 | 0.953) (could not upload due to 500 MB limit)
For more information see https://keras.io/applications/
Acknowledgements
Thanks to François Chollet for collecting these models and for the awesome keras."
"Significant Earthquakes, 1965-2016","Date, time, and location of all earthquakes with magnitude of 5.5 or higher",US Geological Survey,70,"Version 1,2017-01-27",earth sciences,CSV,2 MB,CC0,"24,603 views","4,104 downloads",121 kernels,,https://www.kaggle.com/usgs/earthquake-database,"Context
The National Earthquake Information Center (NEIC) determines the location and size of all significant earthquakes that occur worldwide and disseminates this information immediately to national and international agencies, scientists, critical facilities, and the general public. The NEIC compiles and provides to scientists and to the public an extensive seismic database that serves as a foundation for scientific research through the operation of modern digital national and global seismograph networks and cooperative international agreements. The NEIC is the national data center and archive for earthquake information.
Content
This dataset includes a record of the date, time, location, depth, magnitude, and source of every earthquake with a reported magnitude 5.5 or higher since 1965.
Start a new kernel"
2016 New Coder Survey,"A survey of 15,000+ people who are new to software development",Free Code Camp,70,"Version 1,2016-06-03","employment
computing and society",CSV,10 MB,ODbL,"31,112 views","3,282 downloads",159 kernels,6 topics,https://www.kaggle.com/freecodecamp/2016-new-coder-survey-,"Context
Free Code Camp is an open source community where you learn to code and build projects for nonprofits. CodeNewbie.org is the most supportive community of people learning to code. Together, we surveyed more than 15,000 people who are actively learning to code. We reached them through the twitter accounts and email lists of various organizations that help people learn to code. Our goal was to understand these people's motivations in learning to code, how they're learning to code, their demographics, and their socioeconomic background. We've written in depth about this dataset.
In May 2017 we just released an even bigger open dataset with our 2017 survey results."
Twitter User Gender Classification,Predict user gender based on Twitter profile information,Crowdflower,70,"Version 1,2016-11-21","gender
twitter
internet",CSV,8 MB,CC0,"32,808 views","3,851 downloads",61 kernels,5 topics,https://www.kaggle.com/crowdflower/twitter-user-gender-classification,"This data set was used to train a CrowdFlower AI gender predictor. You can read all about the project here. Contributors were asked to simply view a Twitter profile and judge whether the user was a male, a female, or a brand (non-individual). The dataset contains 20,000 rows, each with a user name, a random tweet, account profile and image, location, and even link and sidebar color.
Inspiration
Here are a few questions you might try to answer with this dataset:
how well do words in tweets and profiles predict user gender?
what are the words that strongly predict male or female gender?
how well do stylistic factors (like link color and sidebar color) predict user gender?
Acknowledgments
Data was provided by the Data For Everyone Library on Crowdflower.
Our Data for Everyone library is a collection of our favorite open data jobs that have come through our platform. They're available free of charge for the community, forever.
The Data
The dataset contains the following fields:
_unit_id: a unique id for user
_golden: whether the user was included in the gold standard for the model; TRUE or FALSE
_unit_state: state of the observation; one of finalized (for contributor-judged) or golden (for gold standard observations)
_trusted_judgments: number of trusted judgments (int); always 3 for non-golden, and what may be a unique id for gold standard observations
_last_judgment_at: date and time of last contributor judgment; blank for gold standard observations
gender: one of male, female, or brand (for non-human profiles)
gender:confidence: a float representing confidence in the provided gender
profile_yn: ""no"" here seems to mean that the profile was meant to be part of the dataset but was not available when contributors went to judge it
profile_yn:confidence: confidence in the existence/non-existence of the profile
created: date and time when the profile was created
description: the user's profile description
fav_number: number of tweets the user has favorited
gender_gold: if the profile is golden, what is the gender?
link_color: the link color on the profile, as a hex value
name: the user's name
profile_yn_gold: whether the profile y/n value is golden
profileimage: a link to the profile image
retweet_count: number of times the user has retweeted (or possibly, been retweeted)
sidebar_color: color of the profile sidebar, as a hex value
text: text of a random one of the user's tweets
tweet_coord: if the user has location turned on, the coordinates as a string with the format ""[latitude, longitude]""
tweet_count: number of tweets that the user has posted
tweet_created: when the random tweet (in the text column) was created
tweet_id: the tweet id of the random tweet
tweet_location: location of the tweet; seems to not be particularly normalized
user_timezone: the timezone of the user"
CT Medical Image Analysis Tutorial,CT images from cancer imaging archive with contrast and patient age,Kevin Mader,69,"Version 6,2017-05-23|Version 5,2017-05-23|Version 4,2017-04-20|Version 3,2017-03-20|Version 2,2017-03-20|Version 1,2017-03-20","healthcare
tutorial
image data",Other,437 MB,Other,"20,239 views","1,732 downloads",24 kernels,4 topics,https://www.kaggle.com/kmader/siim-medical-image-analysis-tutorial,"Overview
The dataset is designed to allow for different methods to be tested for examining the trends in CT image data associated with using contrast and patient age. The basic idea is to identify image textures, statistical patterns and features correlating strongly with these traits and possibly build simple tools for automatically classifying these images when they have been misclassified (or finding outliers which could be suspicious cases, bad measurements, or poorly calibrated machines)
Data
The data are a tiny subset of images from the cancer imaging archive. They consist of the middle slice of all CT images taken where valid age, modality, and contrast tags could be found. This results in 475 series from 69 different patients.
TCIA Archive Link - https://wiki.cancerimagingarchive.net/display/Public/TCGA-LUAD
License
http://creativecommons.org/licenses/by/3.0/
After the publication embargo period ends these collections are freely available to browse, download, and use for commercial, scientific and educational purposes as outlined in the Creative Commons Attribution 3.0 Unported License. Questions may be directed to help@cancerimagingarchive.net. Please be sure to acknowledge both this data set and TCIA in publications by including the following citations in your work:
Data Citation
Albertina, B., Watson, M., Holback, C., Jarosz, R., Kirk, S., Lee, Y., … Lemmerman, J. (2016). Radiology Data from The Cancer Genome Atlas Lung Adenocarcinoma [TCGA-LUAD] collection. The Cancer Imaging Archive. http://doi.org/10.7937/K9/TCIA.2016.JGNIHEP5
TCIA Citation
Clark K, Vendt B, Smith K, Freymann J, Kirby J, Koppel P, Moore S, Phillips S, Maffitt D, Pringle M, Tarbox L, Prior F. The Cancer Imaging Archive (TCIA): Maintaining and Operating a Public Information Repository, Journal of Digital Imaging, Volume 26, Number 6, December, 2013, pp 1045-1057. (paper)"
International football results from 1872 to 2018,"An up-to-date dataset of nearly 40,000 international football results",Mart Jürisoo,69,"Version 14,2018-01-27|Version 13,2017-11-18|Version 12,2017-11-18|Version 11,2017-11-16|Version 10,2017-11-16|Version 9,2017-11-16|Version 8,2017-11-16|Version 7,2017-11-11|Version 6,2017-11-11|Version 5,2017-11-11|Version 4,2017-11-10|Version 3,2017-11-10|Version 2,2017-11-10|Version 1,2017-11-10","association football
countries
sports
+ 2 more...",CSV,475 KB,CC0,"9,293 views","1,394 downloads",8 kernels,4 topics,https://www.kaggle.com/martj42/international-football-results-from-1872-to-2017,"Context
Well, basically what happened was I was looking for a semi-definite easy to read list of international football matches and couldn't find anything decent. So I took it upon myself to collect it for my own use. I might as well share it.
Content
This dataset includes 38,759 results of international football matches starting from the very first official match in 1972 up to 2018. The matches range from World Cup to Baltic Cup to regular friendly matches. The matches are strictly men's full internationals and the data does not include Olympic Games or matches where at least one of the teams was the nation's B-team, U-23 or a league select team.
results.csv includes the following columns:
date
home_team
away_team
home_score
away_score
tournament
city
country
Acknowledgements
The data is gathered from several sources including but not limited to wikipedia, fifa.com, rsssf.com and individual football associations' websites.
Inspiration
Some directions to take when exploring the data:
Which teams dominated different eras of football
Who is the best team of all time
What trends have there been in international football throughout the ages - home advantage, total goals scored, distribution of teams' strength etc
Can we say anything about geopolitics from football fixtures - how has the number of countries changed, which teams like to play each other
Which countries host the most matches where they themselves are not participating in
How much, if at all, does hosting a major tournament help a country's chances in said tournament
Which teams are the most active in playing friendlies and friendly tournaments - does it help or harm them
Do you dare to make any predictions for 2018 World Cup based on this data?
and so on...
The world's your oyster, my friend."
Spotify's Worldwide Daily Song Ranking,The 200 daily most streamed songs in 53 countries,Eduardo,69,"Version 3,2018-01-13|Version 2,2018-01-12|Version 1,2017-08-21",music,CSV,43 MB,Other,"11,139 views","1,522 downloads",5 kernels,0 topics,https://www.kaggle.com/edumucelli/spotifys-worldwide-daily-song-ranking,"Context
Music streaming is ubiquitous. Currently, Spotify plays an important part on that. This dataset enable us to explore how artists and songs' popularity varies in time.
Content
This dataset contains the daily ranking of the 200 most listened songs in 53 countries from 2017 and 2018 by Spotify users. It contains more than 2 million rows, which comprises 6629 artists, 18598 songs for a total count of one hundred five billion streams count.
The data spans from 1st January 2017 to 9th January 2018 and will be kept up-to-date on following versions. It has been collected from Spotify's regional chart data.
Inspiration
Can you predict what is the rank position or the number of streams a song will have in the future?
How long does songs ""resist"" on the top 3, 5, 10, 20 ranking?
What are the signs of a song that gets into the top rank to stay?
Do continents share same top ranking artists or songs?
Are people listening to the very same top ranking songs on countries far away from each other?
How long time does a top ranking song takes to get into the ranking of neighbor countries?
Example
To start out, you can take a look into a simple Kernel I have made in order to read the data, filter data from a song, plot is temporal tendency per country than make a simple forecast of the its streams count here.
Crawler
The crawler used to collect this data can be found here."
Smart meters in London,Smart meter data from London area,Jean-Michel D.,69,"Version 10,2017-12-13|Version 9,2017-12-12|Version 8,2017-11-29|Version 7,2017-11-22|Version 6,2017-11-14|Version 5,2017-11-10|Version 4,2017-11-09|Version 3,2017-11-07|Version 2,2017-11-07|Version 1,2017-11-05","weather
home
demographics
energy",CSV,1 GB,ODbL,"11,025 views","1,060 downloads",2 kernels,6 topics,https://www.kaggle.com/jeanmidev/smart-meters-in-london,"Context
To better follow the energy consumption, the government wants energy suppliers to install smart meters in every home in England, Wales and Scotland. There are more than 26 million homes for the energy suppliers to get to, with the goal of every home having a smart meter by 2020.
This roll out of meter is lead by the European Union who asked all member governments to look at smart meters as part of measures to upgrade our energy supply and tackle climate change. After an initial study, the British government decided to adopt smart meters as part of their plan to update our ageing energy system.
In this dataset, you will find a refactorised version of the data from the London data store, that contains the energy consumption readings for a sample of 5,567 London Households that took part in the UK Power Networks led Low Carbon London project between November 2011 and February 2014. The data from the smart meters seems associated only to the electrical consumption.
There is infomations on the ACORN classification details that you can find in this report or the website of CACI.
I added weather data for London area, I used the darksky api to collect this data.
Content
There is 19 files in this dataset :
informations_households.csv : this file that contains all the information on the households in the panel (their acorn group, their tariff) and in which block.csv.gz file their data are stored
halfhourly_dataset.zip: Zip file that contains the block files with the half-hourly smart meter measurement
daily_dataset.zip: Zip file that contains the block files with the daily information like the number of measures, minimum, maximum, mean, median, sum and std.
acorn_details.csv : Details on the acorn groups and their profile of the people in the group, it's come from this xlsx spreadsheet.The first three columns are the attributes studied, the ACORN-X is the index of the attribute. At a national scale, the index is 100 if for one column the value is 150 it means that there are 1.5 times more people with this attribute in the ACORN group than at the national scale. You can find an explanation on the CACI website
weather_daily_darksky.csv : that contains the daily data from darksky api. You can find more details about the parameters in the documentation of the api
weather_hourly_darksky.csv : that contains the hourly data from darksky api. You can find more details about the parameters in the documentation of the api
Acknowledgements
All the big work of data collection has been done by the UK power networks for the smart meter data.
The details related at the acorn group are provided by the CACI.
The weather data are from darksky.
Inspiration
For me some ideas to analyze the data:
Segmentation of the consumption daily pattern
Disaggregation of the electricity load curve
Cross the consumption result and the acorn information
Forecast the electricity consumption of a household, I wrote an article on this subject
What if I add electrical heating system ? an EV battery system ?
Forecast at a global scale (London consumption)"
LEGO Database,The LEGO Parts/Sets/Colors and Inventories of every official LEGO set,Rachael Tatman,69,"Version 1,2017-07-15",games and toys,CSV,12 MB,CC0,"21,060 views","3,013 downloads",34 kernels,,https://www.kaggle.com/rtatman/lego-database,"Context:
LEGO is a popular brand of toy building bricks. They are often sold in sets with in order to build a specific object. Each set contains a number of parts in different shapes, sizes and colors. This database contains information on which parts are included in different LEGO sets. It was originally compiled to help people who owned some LEGO sets already figure out what other sets they could build with the pieces they had.
Content:
This dataset contains the LEGO Parts/Sets/Colors and Inventories of every official LEGO set in the Rebrickable database. These files are current as of July 2017. If you need it to be more recent data, you can use Rebrickable’s API which provides up to date data, and additional features.
Acknowledgements:
This dataset was compiled by Rebrickable, which is a website to help identify what LEGO sets can be built given bricks and pieces from other LEGO sets. You can use these files for any purpose.
Inspiration:
This is a very rich dataset that offers lots of rooms for exploration, especially since the “sets” file includes the year in which a set was first released.
How have the size of sets changed over time?
What colors are associated with witch themes? Could you predict which theme a set is from just by the bricks it contains?
What sets have the most-used pieces in them? What sets have the rarest pieces in them?
Have the colors of LEGOs included in sets changed over time?"
1.6 million UK traffic accidents,Visualise and analyse traffic demographics,Dave Fisher-Hickey,69,"Version 10,2017-09-17|Version 9,2017-09-17|Version 8,2017-09-16|Version 7,2017-09-16|Version 6,2017-09-16|Version 5,2017-09-15|Version 4,2017-09-14|Version 3,2017-09-14|Version 2,2017-09-13|Version 1,2017-09-11","climate
automobiles
road transport
taxi services",CSV,621 MB,ODbL,"23,349 views","2,474 downloads",10 kernels,2 topics,https://www.kaggle.com/daveianhickey/2000-16-traffic-flow-england-scotland-wales,"Context
The UK government amassed traffic data from 2000 and 2016, recording over 1.6 million accidents in the process and making this one of the most comprehensive traffic data sets out there. It's a huge picture of a country undergoing change.
Note that all the contained accident data comes from police reports, so this data does not include minor incidents.
Content
ukTrafficAADF.csv tracks how much traffic there was on all major roads in the given time period (2000 through 2016). AADT, the core statistic included in this file, stands for ""Average Annual Daily Flow"", and is a measure of how activity a road segment based on how many vehicle trips traverse it. The AADT page on Wikipedia is a good reference on the subject.
Accidents data is split across three CSV files: accidents_2005_to_2007.csv, accidents_2009_to_2011.csv, and accidents_2012_to_2014.csv. These three files together constitute 1.6 million traffic accidents. The total time period is 2005 through 2014, but 2008 is missing.
A data dictionary for the raw dataset at large is available from the UK Department of Transport website here. For descriptions of individual columns, see the column metadata.
Acknowledgements
The license for this dataset is the Open Givernment Licence used by all data on data.gov.uk (here). The raw datasets are available from the UK Department of Transport website here.
Inspiration
How has changing traffic flow impacted accidents?
Can we predict accident rates over time? What might improve accident rates?
Plot interactive maps of changing trends, e.g. How has London has changed for cyclists? Busiest roads in the nation?
Which areas never change and why? Identify infrastructure needs, failings and successes.
How have Rural and Urban areas differed (see RoadCategory)? How about the differences between England, Scotland, and Wales?
The UK government also like to look at miles driven. You can do this by multiplying the AADF by the corresponding length of road (link length) and by the number of days in the years. What does this tell you about UK roads?"
Adult Census Income,Predict whether income exceeds $50K/yr based on census data,UCI Machine Learning,69,"Version 3,2016-10-08|Version 2,2016-10-08|Version 1,2016-10-07","employment
demographics",CSV,4 MB,CC0,"42,472 views","5,017 downloads",119 kernels,6 topics,https://www.kaggle.com/uciml/adult-census-income,"This data was extracted from the 1994 Census bureau database by Ronny Kohavi and Barry Becker (Data Mining and Visualization, Silicon Graphics). A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1) && (HRSWK>0)). The prediction task is to determine whether a person makes over $50K a year.
Description of fnlwgt (final weight)
The weights on the Current Population Survey (CPS) files are controlled to independent estimates of the civilian noninstitutional population of the US. These are prepared monthly for us by Population Division here at the Census Bureau. We use 3 sets of controls. These are:
A single cell estimate of the population 16+ for each state.
Controls for Hispanic Origin by age and sex.
Controls by Race, age and sex.
We use all three sets of controls in our weighting program and ""rake"" through them 6 times so that by the end we come back to all the controls we used. The term estimate refers to population totals derived from CPS by creating ""weighted tallies"" of any specified socio-economic characteristics of the population. People with similar demographic characteristics should have similar weights. There is one important caveat to remember about this statement. That is that since the CPS sample is actually a collection of 51 state samples, each with its own probability of selection, the statement only applies within state.
Relevant papers
Ron Kohavi, ""Scaling Up the Accuracy of Naive-Bayes Classifiers: a Decision-Tree Hybrid"", Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, 1996. (PDF)"
Horses For Courses,"Daily horse racing (thoroughbred) data, machine learning for fun and profit",lukebyrne,68,"Version 18,2017-01-11|Version 17,2016-12-12|Version 16,2016-11-21|Version 15,2016-11-07|Version 14,2016-10-17|Version 13,2016-10-17|Version 12,2016-10-05|Version 11,2016-09-27|Version 10,2016-09-21|Version 9,2016-09-12|Version 8,2016-09-05|Version 7,2016-09-05|Version 6,2016-08-29|Version 5,2016-08-29|Version 4,2016-08-29|Version 3,2016-08-25|Version 2,2016-08-22|Version 1,2016-08-20",horse racing,CSV,26 MB,CC4,"36,251 views","3,270 downloads",122 kernels,17 topics,https://www.kaggle.com/lukebyrne/horses-for-courses,"Context: Daily horse racing (thoroughbred) information that has(is) being actively collected and aggregated from a variety of sources. Years covered are just 2016, country is irrelevant to the dataset.
Acknowledgements: This data has(is) being actively collected and aggregated from a variety of sources, all in the public domain.
Past Research: None of merit, data is used currently to influence some betting decisions but no solid machine learning model(s) have been developed.
Have thrown various versions of the data into:
Google Prediction
Amazon Machine Learning
Azure Machine Learning
Watson Analytics
as a way to learn how these systems work.
Inspiration: Probably one of the hardest things to do is pick stocks and horses. I have been involved in the stocks and horses industry for many years and through publishing previous libraries and software I have met many interesting people and also one of my long term clients/friends.
I am currently trying enhance my software development skills by learning data science / machine learning.
I have a done a few tutorials and I am hoping that by publishing this data I can learn and collaborate with members of the Kaggle Community.
Content:
markets.csv
id
start_time
what time did the race start, datetime in UTC
venue_id
race_number
distance(m)
condition_id
track condition, see conditions.csv
weather_id
weather on day, see weathers.csv
total_pool_win_one
rough $ amount wagered across all runners for win market
total_pool_place_one
rough $ amount wagered across all runners for place market
total_pool_win_two
total_pool_place_two
total_pool_win_three
total_pool_place_three
runners.csv
id
collected
what time was this row created/data collected, datetime in UTC
market_id
position
THIS IS THE FIELD WE WANT TO PREDICT!!!!
Will either be 1,2,3,4,5,6 etc or 0/null if the horse was scratched or failed to finish
If all positions for a market_id are null it means we were unable to match up the positional data for this market
place_paid
Will either be 1/0 or null
If you see a race that only has 2 booleans of 1 it means that the race only paid out places on the first two positions
margin
If the runner didnt win, how many lengths behind the 1st place was it
horse_id
see horses.csv
trainer_id
rider_id
see riders.csv
handicap_weight
number
barrier
blinkers
emergency
did it come into the race at the last minute
form_rating_one
form_rating_two
form_rating_three
last_five_starts
favourite_odds_win
from one of the odds sources, will it win - true/false
favourite_odds_place
from one of the odds sources, will it win - true/false
favourite_pool_win
favourite_pool_place
tip_one_win
from a tipster, will it win - true/false
tip_one_place
from a tipster, will it place - true/false
tip_two_win
tip_two_place
tip_three_win
tip_three_place
tip_four_win
tip_four_place
tip_five_win
tip_five_place
tip_six_win
tip_six_place
tip_seven_win
tip_seven_place
tip_eight_win
tip_eight_place
tip_nine_win
tip_nine_place
odds.csv (collected for every runner 10 minutes out from race start until race starts)
runner_id
collected
what time was this row created/data collected, datetime in UTC
odds_one_win
from odds source, win odds
odds_one_win_wagered
from odds source, rough $ amount wagered on win
odds_one_place
from odds source, place odds
odds_one_place_wagered
from odds source, rough $ amount wagered on place
odds_two_win
odds_two_win_wagered
odds_two_place
odds_two_place_wagered
odds_three_win
odds_three_win_wagered
odds_three_place
odds_three_place_wagered
odds_four_win
odds_four_win_wagered
odds_four_place
odds_four_place_wagered
forms.csv
collected
what time was this row created/data collected, datetime in UTC
market_id
horse_id
runner_number
last_twenty_starts
e.g. f9x726x753x92222x35
f = failed to finish, 7 = finished 7th, 6 = finished 6th, 7 = finished 7th, x = runner was scratched
class_level_id
1 = eq (in same class as other horses)
2 = up (up in class)
3 = dn (down in class)
field_strength
days_since_last_run
runs_since_spell
overall_starts
overall_wins
overall_places
track_starts
track_wins
track_places
firm_starts
firm_wins
firm_places
good_starts
good_wins
good_places
dead_starts
dead_wins
dead_places
slow_starts
slow_wins
slow_places
soft_starts
soft_wins
soft_places
heavy_starts
heavy_wins
heavy_places
distance_starts
distance_wins
distance_places
class_same_starts
class_same_wins
class_same_places
class_stronger_starts
class_stronger_wins
class_stronger_places
first_up_starts
first_up_wins
first_up_places
second_up_starts
second_up_wins
second_up_places
track_distance_starts
track_distance_wins
track_distance_places
conditions.csv
id
name
weathers.csv
id
name
riders.csv (jockeys)
id
sex
horses.csv
id
age
sex_id
see horse_sexes.csv
sire_id
not related to horses.id, there is another table called horse_sires that is not present here
dam_id
not related to horses.id, there is another table called horse_dams that is not present here
prize_money
total aggregate prize money
horse_sexes.csv
id
name"
Cancer Inhibitors,Predict small molecules' activity targeting protein kinase,Kelvin Xiao,68,"Version 4,2016-10-28|Version 3,2016-10-26|Version 2,2016-09-03|Version 1,2016-09-02",healthcare,Other,2 GB,CC4,"18,371 views","1,912 downloads",34 kernels,6 topics,https://www.kaggle.com/xiaotawkaggle/inhibitors,"Outline
It was reported that an estimated 4292,000 new cancer cases and 2814,000 cancer deaths would occur in China in 2015. Chen, W., etc. (2016), Cancer statistics in China, 2015.
Small molecules play an non-trivial role in cancer chemotherapy. Here I focus on inhibitors of 8 protein kinases(name: abbr):
Cyclin-dependent kinase 2: cdk2
Epidermal growth factor receptor erbB1: egfr_erbB1
Glycogen synthase kinase-3 beta: gsk3b
Hepatocyte growth factor receptor: hgfr
MAP kinase p38 alpha: map_k_p38a
Tyrosine-protein kinase LCK: tpk_lck
Tyrosine-protein kinase SRC: tpk_src
Vascular endothelial growth factor receptor 2: vegfr2
For each protein kinase, several thousand inhibitors are collected from chembl database, in which molecules with IC50 lower than 10 uM are usually considered as inhibitors, otherwise non-inhibitors.
Challenge
Based on those labeled molecules, build your model, and try to make the right prediction.
Additionally, more than 70,000 small molecules are generated from pubchem database. And you can screen these molecules to find out potential inhibitors. P.S. the majority of these molecules are non-inhibitors.
DataSets(hdf5 version)
There are 8 protein kinase files and 1 pubchem negative samples file. Taking ""cdk2.h5"" as an example:
import h5py
from scipy import sparse
hf = h5py.File(""../input/cdk2.h5"", ""r"")
ids = hf[""chembl_id""].value # the name of each molecules
ap = sparse.csr_matrix((hf[""ap""][""data""], hf[""ap""][""indices""], hf[""ap""][""indptr""]), shape=[len(hf[""ap""][""indptr""]) - 1, 2039])
mg = sparse.csr_matrix((hf[""mg""][""data""], hf[""mg""][""indices""], hf[""mg""][""indptr""]), shape=[len(hf[""mg""][""indptr""]) - 1, 2039])
tt = sparse.csr_matrix((hf[""tt""][""data""], hf[""tt""][""indices""], hf[""tt""][""indptr""]), shape=[len(hf[""tt""][""indptr""]) - 1, 2039])
features = sparse.hstack([ap, mg, tt]).toarray() # the samples' features, each row is a sample, and each sample has 3*2039 features
labels = hf[""label""].value # the label of each molecule"
Food choices,College students' food and cooking preferences,BoraPajo,67,"Version 5,2017-04-23|Version 4,2017-04-23|Version 3,2017-04-18|Version 2,2017-04-18|Version 1,2017-04-18","food and drink
health",Other,5 MB,CC0,"23,149 views","5,125 downloads",41 kernels,0 topics,https://www.kaggle.com/borapajo/food-choices,"Food choices and preferences of college students
This dataset includes information on food choices, nutrition, preferences, childhood favorites, and other information from college students. There are 126 responses from students. Data is raw and uncleaned. Cleaning is in the process and as soon as that is done, additional versions of the data will be posted. Acknowledgements
Thank you to all the students of Mercyhurst University who agreed to participate in this survey.
Inspiration
How important is nutrition information for today's college kids? Is their taste in food defined by their food preferences when they were children? Are kids of parents who cook more likely to make better food choices than others? Are these kids likely to have a different taste compared to others? There a number of open ended questions included in this dataset such as: What is your favorite comfort food? What is your favorite cuisine? that could work well for natural language processing"
Fifa 18 More Complete Player Dataset,FIFA 18 Player Data++.,KevinH,66,"Version 5,2017-12-26|Version 4,2017-12-23|Version 3,2017-11-05|Version 2,2017-11-05|Version 2,2017-11-05|Version 1,2017-11-03","popular culture
video games
association football
sports",CSV,5 MB,CC0,"11,400 views","2,009 downloads",9 kernels,5 topics,https://www.kaggle.com/kevinmh/fifa-18-more-complete-player-dataset,"Context
This dataset is an extension of that found here. It contains several extra fields and is pre-cleaned to a much greater extent. After talking with the creator of the original dataset, he and I agreed that merging our work would require making breaking changes to the original, and that this should be published as a new dataset.
Content
185 fields for every player in FIFA 18.
Player info such as age, club, league, nationality, salary and physical attributes
All playing attributes, such as finishing and dribbling
Special attributes like skill moves and international reputation
Traits and specialities
Overall, potential, and ratings for each position
Differences
Here are the columns in this dataset that aren't in the original:
birth_date
eur_release_clause
height_cm
weight_kg
body_type
real_face
league
Headline attributes: pac, sho, pas, dri, def, and phy. These are what appear on Ultimate Team cards
international_reputation
skill_moves
weak_foot
work_rate_att
work_rate_def
preferred_foot
all traits and specialities as dummy variables
all position preferences as dummy variables
Acknowledgements
Credit goes to Aman Shrivastava for building the original dataset. And thanks of course to https://sofifa.com for not banning my IP when I scraped over 18000 pages to get this data.
Inspiration
What insights can this data give us, not only into FIFA 18 but into real-world football? The kernels on last year's dataset are a good place to find ideas.
Contributing
Contributions to the GitHub project are more than welcome. Do let me know if you think of ways to improve either the code or the dataset!"
First GOP Debate Twitter Sentiment,Analyze tweets on the first 2016 GOP Presidential Debate,Crowdflower,65,"Version 2,2016-10-06|Version 1,2015-12-28","politics
internet",SQLite,8 MB,CC4,"29,918 views","3,990 downloads",96 kernels,3 topics,https://www.kaggle.com/crowdflower/first-gop-debate-twitter-sentiment,"This data originally came from Crowdflower's Data for Everyone library.
As the original source says,
We looked through tens of thousands of tweets about the early August GOP debate in Ohio and asked contributors to do both sentiment analysis and data categorization. Contributors were asked if the tweet was relevant, which candidate was mentioned, what subject was mentioned, and then what the sentiment was for a given tweet. We've removed the non-relevant messages from the uploaded dataset.
The data we're providing on Kaggle is a slightly reformatted version of the original source. It includes both a CSV file and SQLite database. The code that does these transformations is available on GitHub"
NIPS 2015 Papers,Explore and analyze this year's NIPS papers,Ben Hamner,64,"Version 3,2017-05-02|Version 2,2016-02-29|Version 1,2015-12-09","writing
linguistics
artificial intelligence",CSV,28 MB,ODbL,"45,351 views","2,323 downloads",76 kernels,5 topics,https://www.kaggle.com/benhamner/nips-2015-papers,"Neural Information Processing Systems (NIPS) is one of the top machine learning conferences in the world. It covers topics ranging from deep learning and computer vision to cognitive science and reinforcement learning.
This year, Kaggle is hosting the NIPS 2015 paper dataset to facilitate and showcase exploratory analytics on the NIPS data. We've extracted the paper text from the raw PDF files and are releasing that both in CSV files and as a SQLite database. Here's a quick script that gives an overview of what's included in the data.
We encourage you to explore this data and share what you find through Kaggle Scripts!
Data Description
Overview of the data in Kaggle Scripts.
nips-2015-papers-release-*.zip (downloadable from the link above) contains the below files/folders. All this data's available through Kaggle Scripts as well, and you can create a new script to immediately start exploring the data in R, Python, Julia, or SQLite.
This dataset is available in two formats: three CSV files and a single SQLite database (consisting of three tables with content identical to the CSV files).
You can see the code used to create this dataset on Github.
Papers.csv
This file contains one row for each of the 403 NIPS papers from this year's conference. It includes the following fields
Id - unique identifier for the paper (equivalent to the one in NIPS's system)
Title - title of the paper
EventType - whether it's a poster, oral, or spotlight presentation
PdfName - filename for the PDF document
Abstract - text for the abstract (scraped from the NIPS website)
PaperText - raw text from the PDF document (created using the tool pdftotext)
Authors.csv
This file contains id's and names for each of the authors on this year's NIPS papers.
Id - unique identifier for the author (equivalent to the one in NIPS's system)
Name - author's name
PaperAuthors.csv
This file links papers to their corresponding authors.
Id - unique identifier
PaperId - id for the paper
AuthorId - id for the author
database.sqlite
This SQLite database contains the tables with equivalent data and formatting as the Papers.csv, Authors.csv, and PaperAuthors.csv files.
pdfs
This folder contains the raw pdf files for each of the papers."
Hearthstone Cards,Explore the entire collection of Hearthstone cards,Jerad Rose,64,"Version 3,2017-01-05|Version 2,2016-11-01|Version 1,2016-10-05","games and toys
video games",CSV,2 MB,CC0,"14,964 views","1,245 downloads",31 kernels,0 topics,https://www.kaggle.com/jeradrose/hearthstone-cards,"This dataset contains data for the entire collection of cards for Hearthstone, the popular online card game by Blizzard. Launching to the public on March 11, 2011 after being under development for almost 5 years, Hearthstone has gained popularity as a freemium game, launching into eSports across the globe, and the source of many Twitch channels.
The data in this dataset was extracted from hearthstonejson.com, and the documentation for all the data can be found on the cards.json documentation page.
The original data was extracted from the actual card data files used in the game, so all of the data should be here, enabling explorations like:
Card strengths and weaknesses
Card strengths relative to cost and rarity
Comparisons across player classes, bosses, and sets
Whether a set of optimal cards can be determined per class
The cards can be explored in one of four ways:
cards.json: The raw JSON pulled from hearthstonejson.com
cards_flat.csv: A flat CSV containing a row for each card, and any n:m data stored as arrays in single fields
database.sqlite: A SQLite database containing relational data of the cards
cards.csv, mechanics.csv, dust_costs.csv, play_requirements.csv, and entourages.csv: the normalized data in CSV format.
This dataset will be updated as new releases and expansions are made to Hearthstone.
Currently, any localized string values are in en-us, but I may look into adding other languages if the demand seems to be there."
Detailed NFL Play-by-Play Data 2015,An NFL dataset generated by the nflscrapR R-package & primed for analysis,Max Horowitz,64,"Version 2,2016-10-03|Version 1,2016-05-21",american football,CSV,15 MB,Other,"33,699 views","3,321 downloads",86 kernels,8 topics,https://www.kaggle.com/maxhorowitz/nflplaybyplay2015,"Introduction
The lack of publicly available National Football League (NFL) data sources has been a major obstacle in the creation of modern, reproducible research in football analytics. While clean play-by-play data is available via open-source software packages in other sports (e.g. nhlscrapr for hockey; PitchF/x data in baseball; the NBA API for basketball), the equivalent datasets are not freely available for researchers interested in the statistical analysis of the NFL. To solve this issue, a group of Carnegie Mellon University statistical researchers led by recent graduate, Maksim Horowitz, built and released nflscrapR an R package which uses an API maintained by the NFL to scrape, clean, parse, and output clean datasets at the individual play, player, game, and season levels. These datasets allow for the advancement of NFL research in the public domain by allowing analysts to develop from a common source in order to create reproducible NFL research, similar to what is being done currently in other professional sports.
2015 NFL Play-by-Play Dataset
The dataset made available on Kaggle contains all the regular season plays from the 2015-2016 NFL season. The dataset contain 46,129 rows and 63 columns. Each play is broken down into great detail containing information on; game situation, players involved and results. Detailed information about the dataset can be found in the nflscrapR documentation.
Downloading and Installing nflscrapR:
Use the following code in your R console:
# Must install the devtools package using the below code
install.packages('devtools')
library(devtools)
# For now you must install nflscrapR from github
if (!is.element(""nflscrapR"", installed.packages())) {
    # Print Installing nflscrapR
    devtools::install_github(repo = ""maksimhorowitz/nflscrapR"")
}

library(nflscrapR)"
Pokémon for Data Mining and Machine Learning,(Almost) all Pokémon stats until generation 6: 21 variables per Pokémon,alopez247,64,"Version 2,2017-03-05|Version 1,2017-01-08",video games,CSV,800 KB,CC4,"69,501 views","2,256 downloads",30 kernels,0 topics,https://www.kaggle.com/alopez247/pokemon,"Context
With the rise of the popularity of machine learning, this is a good opportunity to share a wide database of the even more popular video-game Pokémon by Nintendo, Game freak, and Creatures, originally released in 1996.
Pokémon started as a Role Playing Game (RPG), but due to its increasing popularity, its owners ended up producing many TV series, manga comics, and so on, as well as other types of video-games (like the famous Pokémon Go!).
This dataset is focused on the stats and features of the Pokémon in the RPGs. Until now (08/01/2017) seven generations of Pokémon have been published. All in all, this dataset does not include the data corresponding to the last generation, since 1) I created the databased when the seventh generation was not released yet, and 2) this database is a modification+extension of the database ""721 Pokemon with stats"" by Alberto Barradas (https://www.kaggle.com/abcsds/pokemon), which does not include (of course) the latest generation either.
Content
This database includes 21 variables per each of the 721 Pokémon of the first six generations, plus the Pokémon ID and its name. These variables are briefly described next:
Number. Pokémon ID in the Pokédex.
Name. Name of the Pokémon.
Type_1. Primary type.
Type_2. Second type, in case the Pokémon has it.
Total. Sum of all the base stats (Health Points, Attack, Defense, Special Attack, Special Defense, and Speed).
HP. Base Health Points.
Attack. Base Attack.
Defense. Base Defense.
Sp_Atk. Base Special Attack.
Sp_Def. Base Special Defense.
Speed. Base Speed.
Generation. Number of the generation when the Pokémon was introduced.
isLegendary. Boolean that indicates whether the Pokémon is Legendary or not.
Color. Color of the Pokémon according to the Pokédex.
hasGender. Boolean that indicates if the Pokémon can be classified as female or male.
Pr_male. In case the Pokémon has Gender, the probability of its being male. The probability of being female is, of course, 1 minus this value.
Egg_Group_1. Egg Group of the Pokémon.
Egg_Group_2. Second Egg Group of the Pokémon, in case it has two.
hasMegaEvolution. Boolean that indicates whether the Pokémon is able to Mega-evolve or not.
Height_m. Height of the Pokémon, in meters.
Weight_kg. Weight of the Pokémon, in kilograms.
Catch_Rate. Catch Rate.
Body_Style. Body Style of the Pokémon according to the Pokédex.
Notes
Please note that many Pokémon are multi-form, and also some of them can Mega-evolve. I wanted to keep the structure of the dataset as simple and general as possible, as well as the Number variable (the ID of the Pokémon) unique. Hence, in the cases of the multi-form Pokémon, or the ones capable of Mega-evolve, I just chose one of the forms, the one I (and my brother) considered the standard and/or the most common. The specific choice for each of this Pokémon are shown below:
Mega-Evolutions are not considered as Pokémon.
Kyogre, Groudon. Primal forms not considered.
Deoxis. Only normal form considered.
Wormadam. Only plant form considered.
Rotom. Only normal form considered, the one with types Electric and Ghost.
Giratina. Origin form considered.
Shaymin. Land form considered.
Darmanitan. Standard mode considered.
Tornadus, Thundurus, Landorus. Incarnate form considered.
Kyurem. Normal form considered, not white or black forms.
Meloetta. Aria form considered.
Mewstic. Both female and male forms are equal in the considered variables.
Aegislash. Shield form considered.
Pumpkaboo, Gourgeist. Average size considered.
Zygarde. 50% form considered.
Hoopa. Confined form considered.
Acknowledgements
As said at the beginning, this database was based on the Kaggle database ""721 Pokemon with stats"" by Alberto Barradas (https://www.kaggle.com/abcsds/pokemon). The other resources I mainly used are listed below:
WikiDex (http://es.pokemon.wikia.com/wiki/WikiDex).
Bulbapedia, the community driven Pokémon encyclopedia (http://bulbapedia.bulbagarden.net/wiki/Main_Page).
Smogon University (http://www.smogon.com/).
Possible future work
This dataset can be used with different objectives, such as, Pokémon clustering, trying to find relations or dependencies between the variables, and also for supervised classification purposes, where the class could be the Primary Type, but also many of the other variables.
Author
Asier López Zorrilla"
Ships in Satellite Imagery,Classify ships in San Franciso Bay using Planet satellite imagery,rhammell,64,"Version 7,2017-11-14|Version 6,2017-10-16|Version 5,2017-10-11|Version 4,2017-10-11|Version 3,2017-10-10|Version 2,2017-10-09|Version 1,2017-10-09",business,Other,148 MB,CC4,"8,227 views",954 downloads,9 kernels,3 topics,https://www.kaggle.com/rhammell/ships-in-satellite-imagery,"Context
Satellite imagery provides unique insights into various markets, including agriculture, defense and intelligence, energy, and finance. New commercial imagery providers, such as Planet and BlackSky, are using constellations of small satellites to exponentially increase the amount of images of the earth captured every day.
This flood of new imagery is outgrowing the ability for organizations to manually look at each image that gets captured, and there is a need for machine learning and computer vision algorithms to help automate the analysis process.
The aim of this dataset is to help address the difficult task of detecting the location of large ships in satellite images. Automating this process can be applied to many issues including monitoring port activity levels and supply chain analysis.
Continusouly updates will be made to this dataset as new Planet imagery released. Current images were collected as late as September 2017.
Content
The dataset consists of image chips extracted from Planet satellite imagery collected over the San Franciso Bay area. It includes 2800 80x80 RGB images labeled with either a ""ship"" or ""no-ship"" classification. Image chips were derived from PlanetScope full-frame visual scene products, which are orthorectified to a 3 meter pixel size.
Provided is a zipped directory shipsnet.7z that contains the entire dataset as .png image chips. Each individual image filename follows a specific format: {label} __ {scene id} __ {longitude} _ {latitude}.png
label: Valued 1 or 0, representing the ""ship"" class and ""no-ship"" class, respectively.
scene id: The unique identifier of the PlanetScope visual scene the image chip was extracted from. The scene id can be used with the Planet API to discover and download the entire scene.
longitude_latitude: The longitude and latitude coordinates of the image center point, with values separated by a single underscore.
The dataset is also distributed as a JSON formatted text file shipsnet.json. The loaded object contains data, label, scene_ids, and location lists.
The pixel value data for each 80x80 RGB image is stored as a list of 19200 integers within the data list. The first 6400 entries contain the red channel values, the next 6400 the green, and the final 6400 the blue. The image is stored in row-major order, so that the first 80 entries of the array are the red channel values of the first row of the image.
The list values at index i in labels, scene_ids, and locations each correspond to the i-th image in the data list.
Class Labels
The ""ship"" class includes 700 images. Images in this class are near-centered on the body of a single ship. Ships of different ship sizes, orientations, and atmospheric collection conditions are included. Example images from this class are shown below.
The ""no-ship"" class includes 2100 images. A third of these are a random sampling of different landcover features - water, vegetion, bare earth, buildings, etc. - that do not include any portion of an ship. The next third are ""partial ships"" that contain only a portion of an ship, but not enough to meet the full definition of the ""ship"" class. The last third are images that have previously been mislabeled by machine learning models, typically caused by bright pixels or strong linear features. Example images from this class are shown below.
Acknowledgements
Satellite imagery used to build this dataset is made available through Planet's Open California dataset, which is openly licensed. As such, this dataset is also available under the same CC-BY-SA license. Users can sign up for a free Planet account to search, view, and download thier imagery and gain access to their API."
Historical Air Quality,Air Quality Data Collected at Outdoor Monitors Across the US,US Environmental Protection Agency,64,"Version 1,2017-12-01","pollution
bigquery",BigQuery,323 GB,CC0,"13,461 views",0 downloads,14 kernels,,https://www.kaggle.com/epa/epa-historical-air-quality,"The AQS Data Mart is a database containing all of the information from AQS. It has every measured value the EPA has collected via the national ambient air monitoring program. It also includes the associated aggregate values calculated by EPA (8-hour, daily, annual, etc.). The AQS Data Mart is a copy of AQS made once per week and made accessible to the public through web-based applications. The intended users of the Data Mart are air quality data analysts in the regulatory, academic, and health research communities. It is intended for those who need to download large volumes of detailed technical data stored at EPA and does not provide any interactive analytical tools. It serves as the back-end database for several Agency interactive tools that could not fully function without it: AirData, AirCompare, The Remote Sensing Information Gateway, the Map Monitoring Sites KML page, etc.
AQS must maintain constant readiness to accept data and meet high data integrity requirements, thus is limited in the number of users and queries to which it can respond. The Data Mart, as a read only copy, can allow wider access.
The most commonly requested aggregation levels of data (and key metrics in each) are:
Sample Values (2.4 billion values back as far as 1957, national consistency begins in 1980, data for 500 substances routinely collected) The sample value converted to standard units of measure (generally 1-hour averages as reported to EPA, sometimes 24-hour averages) Local Standard Time (LST) and GMT timestamps Measurement method Measurement uncertainty, where known Any exceptional events affecting the data NAAQS Averages NAAQS average values (8-hour averages for ozone and CO, 24-hour averages for PM2.5) Daily Summary Values (each monitor has the following calculated each day) Observation count Observation per cent (of expected observations) Arithmetic mean of observations Max observation and time of max AQI (air quality index) where applicable Number of observations > Standard where applicable Annual Summary Values (each monitor has the following calculated each year) Observation count and per cent Valid days Required observation count Null observation count Exceptional values count Arithmetic Mean and Standard Deviation 1st - 4th maximum (highest) observations Percentiles (99, 98, 95, 90, 75, 50) Number of observations > Standard Site and Monitor Information FIPS State Code (the first 5 items on this list make up the AQS Monitor Identifier) FIPS County Code Site Number (unique within the county) Parameter Code (what is measured) POC (Parameter Occurrence Code) to distinguish from different samplers at the same site Latitude Longitude Measurement method information Owner / operator / data-submitter information Monitoring Network to which the monitor belongs Exemptions from regulatory requirements Operational dates City and CBSA where the monitor is located Quality Assurance Information Various data fields related to the 19 different QA assessments possible
Querying BigQuery tables
You can use the BigQuery Python client library to query tables in this dataset in Kernels. Note that methods available in Kernels are limited to querying data. Tables are at bigquery-public-data.epa_historical_air_quality.[TABLENAME]. Fork this kernel to get started.
Acknowledgements
Data provided by the US Environmental Protection Agency Air Quality System Data Mart."
Question Pairs Dataset,Can you identify duplicate questions?,Quora,64,"Version 2,2017-02-02|Version 1,2017-01-31","languages
linguistics
artificial intelligence",CSV,58 MB,Other,"18,252 views","1,473 downloads",80 kernels,4 topics,https://www.kaggle.com/quora/question-pairs-dataset,"Context
Quora's first public dataset is related to the problem of identifying duplicate questions. At Quora, an important product principle is that there should be a single question page for each logically distinct question. For example, the queries “What is the most populous state in the USA?” and “Which state in the United States has the most people?” should not exist separately on Quora because the intent behind both is identical. Having a canonical page for each logically distinct query makes knowledge-sharing more efficient in many ways: for example, knowledge seekers can access all the answers to a question in a single location, and writers can reach a larger readership than if that audience was divided amongst several pages.
The dataset is based on actual data from Quora and will give anyone the opportunity to train and test models of semantic equivalence.
Content
There are over 400,000 lines of potential question duplicate pairs. Each line contains IDs for each question in the pair, the full text for each question, and a binary value that indicates whether the line truly contains a duplicate pair.
Acknowledgements
For more information on this dataset, check out Quora's first dataset release page.
License
This data is subject to Quora's Terms of Service, allowing for non-commercial use."
Weather Data for Recruit Restaurant Competition,Data From Weather Stations in Japan and the Recruit Restaurant Competition Data,Hunter McGushion,64,"Version 5,2018-01-05|Version 4,2018-01-04|Version 3,2018-01-03|Version 2,2017-12-21|Version 1,2017-12-20","food and drink
time series
geography",CSV,11 MB,CC4,"5,944 views","1,900 downloads",5 kernels,9 topics,https://www.kaggle.com/huntermcgushion/rrv-weather-data,"Version 5 Description
TLDR
The directory 1-1-16_5-31-17_Weather contains 1663 files (one for each of the 1663 stations in Japan)
As its name implies, the data is from the same date window as the competition's date_info file
If a station file's name ends with four underscores and a date, the date indicates when the station was terminated
Please, read on...
Context
This dataset contains the dataset from the Recruit Restaurant Visitor Forecasting competition (active from 11-28-17 to 2-6-18).
The focus is on using data about reservations made at various restaurants throughout Japan, along with restaurant location and genre information to predict the actual number of visitors a restaurant will have on a given day.
This dataset augments the above with the addition of information about the weather at various locations in Japan over time to produce an exciting, multi-faceted dataset that deals with time, geography, weather, and delicious food.
Thank you for your interest in this dataset! Please let me know if you have any suggestions, questions or problems!
Content
The core of the dataset comprises the files in the following directory:
1-1-16_5-31-17_Weather (1663 .csv files):
This directory contains translated weather data for the time period denoted by the directory’s name (from 1-1-16 through 5-31-17).
Each .csv file in this directory is of shape (517, 15), and is named according to the id values in the below weather_stations file.
There are a few reasons why there may seem to be a lot of null values:
The primary reason is that different types of stations/sensors are used, and some just don't capture as much data as others
Questionable data is sometimes removed by the Agency
If the station was terminated, its values are null
These are the features for all translated weather files (I won’t hazard a description for the features that aren’t already self-explanatory because I’m no meteorologist, and I’d hate for you to get that impression):
calendar_date - the observation date, formatted thusly ""yyyy-mm-dd""
avg_temperature
high_temperature
low_temperature
precipitation
hours_sunlight
solar_radiation
deepest_snowfall
total_snowfall
avg_wind_speed
avg_vapor_pressure
avg_local_pressure
avg_humidity
avg_sea_pressure
cloud_cover
This dataset adds the following .csv files regarding weather stations, and their relations to the competition data:
weather_stations.csv (1663, 8):
This file contains the location and termination dates for 1,663 weather stations in Japan.
id - the join of a station’s prefecture, first_name, and second_name, with ""__"" (double underscores)
Note: If date_terminated is not null, id will end with four underscores and the date_terminated
prefecture - the prefecture in which this station is located (see note 1)
first_name - the first name given to specify a location (see note 2)
second_name - the second name given to specify a location (see note 2)
latitude - latitude of the station, converted from degrees, minutes, seconds to decimal degrees for consistency
longitude - longitude of the station, converted from degrees, minutes, seconds to decimal degrees for consistency
altitude - altitude of the station
date_terminated - If the station was terminated, the date of its termination (formatted thusly ""yyyy-mm-dd"") else null
nearby_active_stations.csv (62, 8):
This file is a subset of weather_stations.csv (above) selected via the following criteria:
    1) the station was not terminated, and
    2) the station was the closest station to at least one store in air_store_info or hpg_store_info
As you can see, there is a lot of overlap here, because while the weather stations seem to generally be scattered throughout Japan, the store locations tend to be clustered around several areas.
Column names and descriptions are identical to those of weather_stations.
feature_manifest.csv (1663, 15):
This file contains information about each station's ""coverage"" of each weather feature.
Values of 0.0 for any of the below features except id mean that station collected no data on that feature.
Values of 1.0 for any of the below features except id mean that station collected data on that feature for every day.
id - the id of this weather station
avg_temperature - ratio of non-null values for this feature at this station
high_temperature - ratio of non-null values for this feature at this station
low_temperature - ratio of non-null values for this feature at this station
precipitation - ratio of non-null values for this feature at this station
hours_sunlight - ratio of non-null values for this feature at this station
solar_radiation - ratio of non-null values for this feature at this station
deepest_snowfall - ratio of non-null values for this feature at this station
total_snowfall - ratio of non-null values for this feature at this station
avg_wind_speed - ratio of non-null values for this feature at this station
avg_vapor_pressure - ratio of non-null values for this feature at this station
avg_local_pressure - ratio of non-null values for this feature at this station
avg_humidity - ratio of non-null values for this feature at this station
avg_sea_pressure - ratio of non-null values for this feature at this station
cloud_cover - ratio of non-null values for this feature at this station
air_station_distances.csv (1663, 111):
This file contains the Vincenty distance from every weather station to every unique latitude/longitude pair in the air system.
station_id - the id of this weather station
station_latitude - station latitude (in decimal degrees)
station_longitude - station longitude (in decimal degrees)
<<UNIQUE AIR COORDINATE PAIRS>> - The remaining 108 columns are stringified versions of all unique air coordinate pairs
They are formatted thusly: (##.###, ##.###), where the first float is the store latitude and the second is the store longitude (see note 3)
EX) The first of these columns is: (34.6951242, 135.1978525)
hpg_station_distances.csv (1663, 132):
This file contains the Vincenty distance from every weather station to every unique latitude/longitude pair in the hpg system.
station_id - the id of this weather station
station_latitude - station latitude (in decimal degrees)
station_longitude - station longitude (in decimal degrees)
<<UNIQUE HPG COORDINATE PAIRS>> - The remaining 129 columns are stringified versions of all unique hpg coordinate pairs
They are formatted thusly: (##.###, ##.###), where the first float is the store latitude and the second is the store longitude (see note 3)
EX) The first of these columns is: (35.6436746642265, 139.668220854814)
air_store_info_with_nearest_active_station.csv (829, 12):
hpg_store_info_with_nearest_active_station.csv (4690, 12):
These two files are supplemented versions of air_store_info and hpg_store_info, and they contain the original competition data for the store_info file specified in the file’s name, plus the following features:
latitude_str - a stringified version of this store's latitude for lookup in air_station_distances and hpg_station_distances
longitude_str - a stringified version of this store's longitude for lookup in air_station_distances and hpg_station_distances
station_id - the id of the weather station nearest to this store
station_latitude - the latitude (in decimal degrees) of the weather station nearest to this store
station_longitude - the longitude (in decimal degrees) of the weather station nearest to this store
station_vincenty - the Vincenty distance between this store and the station to which it is closest
station_great_circle - the Great Circle distance between this store and the station to which it is closest
Other Available Data
Date ranges outside of the one used for this competition
Records seem to go fairly far back, but of course the data we can actually get will be subject to the activation and termination dates of each individual station
Observation periods other than daily values
Hourly, every “x” days, monthly, seasonal
Option to compare data with average from the past 30 years
Other features:
Day’s maximum instantaneous wind speed
Day’s maximum wind direction
Day’s minimum relative humidity
Day’s minimum sea pressure
Option to show the time at which the observation period’s maximum and minimum values occurred
Acknowledgements
All weather data contained herein came from the Japan Meteorological Agency
All data pertaining to restaurants, reservations, and visitors came from the Recruit Restaurant Visitor Forecasting competition
Disclaimers
The site has one station (that I know of) that I did not include in the list of all stations. That station is located in the Antarctic, and I didn’t include it because the Antarctic is not located in Japan
I have not tested the feasibility of actually gathering and processing the “other available data” listed above
Below is a list of my qualifications for processing weather data for Japan, from a site written in Japanese:
1
2
3
Look upon the list and see that it is empty. I’m just a guy that knows how to get stuff done and enjoys a challenge
Again, read all translations with several grains of salt
Plans/Ideas to Expand This Dataset (Tell Me If These Interest You)
Getting data from previous years to get a better idea of how the weather should be on a given day (next version)
Averaging data from multiple stations to maybe create some representation of the weather for that prefecture/area
For stores that are particularly far from their nearest station, doing a similar sort of averaging as above to attempt to minimize any risk posed by using weather data too far from the target store
Notes
1) The site separates the prefecture ""Hokkaido"" into its 14 subprefectures:
Therefore, when you might expect prefecture to be ""hokkaido"", you will actually see ""hokkaido_<subprefecture>""
i.e., the concatenation of the three strings ""hokkaido"", ""_"" (single underscore), and the subprefecture name
2) Formatting of first_name and last_name:
Spaces were replaced by ""-"" (single dash)
If a value was not provided or could not be translated, the value is ""NONE""
I would suggest you don’t rely too heavily on these features, as they are the result of several different translation APIs, which were rarely in agreement on the translation. These features are provided more as a convenience, and in an effort to give you as much data as possible
3) Column formatting in air_station_distances.csv and hpg_station_distances.csv:
These files show the coordinates with the precision shown in the original air_store_info.csv and hpg_store_info.csv files; however, Pandas, doesn't read these values in with the same precision
If you plan on using these files, you must use the latitude_str and longitude_str features in the appropriate ..._store_info_with_nearest_active_station file to look up coordinates
For more information, see this discussion post"
"380,000+ lyrics from MetroLyrics","Lyrics, Artist , Genre, Year",GyanendraMishra,63,"Version 2,2017-01-11|Version 1,2017-01-09","writing
music
linguistics",CSV,310 MB,CC4,"11,838 views","1,624 downloads",18 kernels,5 topics,https://www.kaggle.com/gyani95/380000-lyrics-from-metrolyrics,"Context
I tried to gather as many lyrics as I could. I ran my code on a a free ec2 instance and ran out of storage space. I have attached the code below so if any one wants to try out it and get all lyrics, please do.
Content
There are around 380,000+ lyrics in the data set from a lot of different artists from a lot of different genres arranged by year. Structure is artist/year/song. Every artist folder has a genre.txt that tells what is the genre of the musician. Find the crawler here.
Acknowledgements
I would like to thank Shruti Jasoria, SJasoria on GitHub for writing the multi-threaded version.
Inspiration
I wanted to find out what genre and what artist abuses what substance. Do rapstars like cocaine or liquor? If liquor then what Liquor? Does Eminem prefer Hennesy over Jack Daniels? Do Rockstars love pot?"
Loan Data,This dataset includes customers who have paid off their loans or not,Zhijin,63,"Version 1,2017-04-11",finance,CSV,43 KB,CC0,"21,936 views","3,470 downloads",47 kernels,2 topics,https://www.kaggle.com/zhijinzhai/loandata,"Context
This data set includes customers who have paid off their loans, who have been past due and put into collection without paying back their loan and interests, and who have paid off only after they were put in collection. The financial product is a bullet loan that customers should pay off all of their loan debt in just one time by the end of the term, instead of an installment schedule. Of course, they could pay off earlier than their pay schedule.
Content
Loan_id A unique loan number assigned to each loan customers
Loan_status Whether a loan is paid off, in collection, new customer yet to payoff, or paid off after the collection efforts
Principal Basic principal loan amount at the origination
terms Can be weekly (7 days), biweekly, and monthly payoff schedule
Effective_date When the loan got originated and took effects
Due_date Since it’s one-time payoff schedule, each loan has one single due date
Paidoff_time The actual time a customer pays off the loan
Pastdue_days How many days a loan has been past due
Age, education, gender A customer’s basic demographic information"
The Complete Pokemon Dataset,Data on more than 800 Pokemon from all 7 Generations.,Rounak Banik,63,"Version 1,2017-09-30","popular culture
video games",CSV,157 KB,CC0,"15,580 views","1,544 downloads",16 kernels,0 topics,https://www.kaggle.com/rounakbanik/pokemon,"Context
This dataset contains information on all 802 Pokemon from all Seven Generations of Pokemon. The information contained in this dataset include Base Stats, Performance against Other Types, Height, Weight, Classification, Egg Steps, Experience Points, Abilities, etc. The information was scraped from http://serebii.net/
Content
name: The English name of the Pokemon
japanese_name: The Original Japanese name of the Pokemon
pokedex_number: The entry number of the Pokemon in the National Pokedex
percentage_male: The percentage of the species that are male. Blank if the Pokemon is genderless.
type1: The Primary Type of the Pokemon
type2: The Secondary Type of the Pokemon
classification: The Classification of the Pokemon as described by the Sun and Moon Pokedex
height_m: Height of the Pokemon in metres
weight_kg: The Weight of the Pokemon in kilograms
capture_rate: Capture Rate of the Pokemon
base_egg_steps: The number of steps required to hatch an egg of the Pokemon
abilities: A stringified list of abilities that the Pokemon is capable of having
experience_growth: The Experience Growth of the Pokemon
base_happiness: Base Happiness of the Pokemon
against_?: Eighteen features that denote the amount of damage taken against an attack of a particular type
hp: The Base HP of the Pokemon
attack: The Base Attack of the Pokemon
defense: The Base Defense of the Pokemon
sp_attack: The Base Special Attack of the Pokemon
sp_defense: The Base Special Defense of the Pokemon
speed: The Base Speed of the Pokemon
generation: The numbered generation which the Pokemon was first introduced
is_legendary: Denotes if the Pokemon is legendary.
Acknowledgements
The data was scraped from http://serebii.net/.
Inspiration
Pokemon holds a very special place in my heart as it is probably the only video game I have judiciously followed for more than 10 years. With this dataset, I wanted to be able to answer the following questions:
Is it possible to build a classifier to identify legendary Pokemon?
How does height and weight of a Pokemon correlate with its various base stats?
What factors influence the Experience Growth and Egg Steps? Are these quantities correlated?
Which type is the strongest overall? Which is the weakest?
Which type is the most likely to be a legendary Pokemon?
Can you build a Pokemon dream team? A team of 6 Pokemon that inflicts the most damage while remaining relatively impervious to any other team of 6 Pokemon."
Amazon Reviews for Sentiment Analysis,A few million Amazon reviews in fastText format,Adam Mathias Bittlingmayer,62,"Version 2,2017-05-24|Version 1,2017-05-23","business
linguistics
internet",Other,493 MB,Other,"27,962 views","3,335 downloads",9 kernels,,https://www.kaggle.com/bittlingmayer/amazonreviews,"This dataset consists of a few million Amazon customer reviews (input text) and star ratings (output labels) for learning how to train fastText for sentiment analysis.
The idea here is a dataset is more than a toy - real business data on a reasonable scale - but can be trained in minutes on a modest laptop.
Content
The fastText supervised learning tutorial requires data in the following format:
__label__<X> __label__<Y> ... <Text>
where X and Y are the class names. No quotes, all on one line.
In this case, the classes are __label__1 and __label__2, and there is only one class per row.
__label__1 corresponds to 1- and 2-star reviews, and __label__2 corresponds to 4- and 5-star reviews.
(3-star reviews i.e. reviews with neutral sentiment were not included in the original),
The review titles, followed by ':' and a space, are prepended to the text.
Most of the reviews are in English, but there are a few in other languages, like Spanish.
Source
The data was lifted from Xiang Zhang's Google Drive dir, but it was in .csv format, not suitable for fastText.
Training and Testing
Follow the basic instructions at fastText supervised learning tutorial to set up the directory.
To train:
./fasttext supervised -input train.ft.txt -output model_amzn
This should take a few minutes.
To test:
./fasttext test model_amzn.bin test.ft.txt
Expect precision and recall of 0.916 if all is in order.
You can also train and test in Python, see Kernel."
"Govt. of India Census, 2001 District-Wise","One billion hearts, a single CSV",PreetSinghKhalsa,62,"Version 1,2017-01-18","demographics
sociology",CSV,355 KB,ODbL,"14,182 views","2,104 downloads",71 kernels,,https://www.kaggle.com/bazuka/census2001,"Context
Census of India is a rich database which can tell stories of over a billion Indians. It is important not only for research point of view, but commercially as well for the organizations that want to understand India's complex yet strongly knitted heterogeneity. However, nowhere on the web, there exists a single database that combines the district- wise information of all the variables (most include no more than 4-5 out of over 50 variables!). Extracting and using data from Census of India 2001 is quite a laborious task since all data is made available in scattered PDFs district wise. Individual PDFs can be extracted from http://www.censusindia.gov.in/(S(ogvuk1y2e5sueoyc5eyc0g55))/Tables_Published/Basic_Data_Sheet.aspx.
Content
This database has been extracted from Census of 2001 and includes data of 590 districts, having around 80 variables each.
In case of confusion regarding the context of the variable, refer to the following PDF and you will be able to make sense out of it: http://censusindia.gov.in/Dist_File/datasheet-2923.pdf
All the extraction work can be found @ https://github.com/preetskhalsa97/census2001auto The final CSV can be found at finalCSV/all.csv
The subtle hack that was used to automate extraction to a great extent was the the URLs of all the PDFs were same except the four digits (that were respective state and district codes).
A few abbreviations used for states:
AN- Andaman and Nicobar CG- Chhattisgarh D_D- Daman and Diu D_N_H- Dadra and Nagar Haveli JK- Jammu and Kashmir MP- Madhya Pradesh TN- Tamil Nadu UP- Uttar Pradesh WB- West Bengal
A few variables for clarification: Growth..1991...2001- population growth from 1991 to 2001 X0..4 years- People in age group 0 to 4 years SC1- Scheduled Class with highest population
Acknowledgements
Inspiration
This is a massive dataset which can be used to explain the interplay between education, caste, development, gender and much more. It really can explain a lot about India and propel data driven research. Happy Number Crunching!"
Indian Startup Funding,Funding details of the startups in India,SRK,61,"Version 2,2017-08-11|Version 1,2017-08-11","india
finance
lending",CSV,305 KB,CC0,"10,948 views","1,873 downloads",22 kernels,3 topics,https://www.kaggle.com/sudalairajkumar/indian-startup-funding,"Context
Interested in the Indian startup ecosystem just like me? Wanted to know what type of startups are getting funded in the last few years? Wanted to know who are the important investors? Wanted to know the hot fields that get a lot of funding these days? This dataset is a chance to explore the Indian start up scene. Deep dive into funding data and derive insights into the future!
Content
This dataset has funding information of the Indian startups from January 2015 to August 2017. It includes columns with the date funded, the city the startup is based out of, the names of the funders, and the amount invested (in USD).
For more information on the values of individual fields, check out the Column Metadata.
Acknowledgements
Thanks to trak.in who are generous enough to share the data publicly for free.
Inspiration
Possible questions which could be answered are:
How does the funding ecosystem change with time?
Do cities play a major role in funding?
Which industries are favored by investors for funding?
Who are the important investors in the Indian Ecosystem?
How much funds does startups generally get in India?"
Retailrocket recommender system dataset,"Ecommerce data: web events, item properties (with texts), category tree",Retailrocket,61,"Version 4,2017-03-24|Version 3,2017-03-24|Version 2,2017-03-24|Version 1,2017-03-24","business
internet",CSV,942 MB,CC4,"23,263 views","2,357 downloads",16 kernels,6 topics,https://www.kaggle.com/retailrocket/ecommerce-dataset,"Context
The dataset consists of three files: a file with behaviour data (events.csv), a file with item properties (item_properties.сsv) and a file, which describes category tree (category_tree.сsv). The data has been collected from a real-world ecommerce website. It is raw data, i.e. without any content transformations, however, all values are hashed due to confidential issues. The purpose of publishing is to motivate researches in the field of recommender systems with implicit feedback.
Content
The behaviour data, i.e. events like clicks, add to carts, transactions, represent interactions that were collected over a period of 4.5 months. A visitor can make three types of events, namely “view”, “addtocart” or “transaction”. In total there are 2 756 101 events including 2 664 312 views, 69 332 add to carts and 22 457 transactions produced by 1 407 580 unique visitors. For about 90% of events corresponding properties can be found in the “item_properties.csv” file.
For example:
“1439694000000,1,view,100,” means visitorId = 1, clicked the item with id = 100 at 1439694000000 (Unix timestamp)
“1439694000000,2,transaction,1000,234” means visitorId = 2 purchased the item with id = 1000 in transaction with id = 234 at 1439694000000 (Unix timestamp)
The file with item properties (item_properties.csv) includes 20 275 902 rows, i.e. different properties, describing 417 053 unique items. File is divided into 2 files due to file size limitations. Since the property of an item can vary in time (e.g., price changes over time), every row in the file has corresponding timestamp. In other words, the file consists of concatenated snapshots for every week in the file with the behaviour data. However, if a property of an item is constant over the observed period, only a single snapshot value will be present in the file. For example, we have three properties for single item and 4 weekly snapshots, like below:
timestamp,itemid,property,value
1439694000000,1,100,1000
1439695000000,1,100,1000
1439696000000,1,100,1000
1439697000000,1,100,1000
1439694000000,1,200,1000
1439695000000,1,200,1100
1439696000000,1,200,1200
1439697000000,1,200,1300
1439694000000,1,300,1000
1439695000000,1,300,1000
1439696000000,1,300,1100
1439697000000,1,300,1100
After snapshot merge it would looks like:
1439694000000,1,100,1000
1439694000000,1,200,1000
1439695000000,1,200,1100
1439696000000,1,200,1200
1439697000000,1,200,1300
1439694000000,1,300,1000
1439696000000,1,300,1100
Because property=100 is constant over time, property=200 has different values for all snapshots, property=300 has been changed once.
Item properties file contain timestamp column because all of them are time dependent, since properties may change over time, e.g. price, category, etc. Initially, this file consisted of snapshots for every week in the events file and contained over 200 millions rows. We have merged consecutive constant property values, so it's changed from snapshot form to change log form. Thus, constant values would appear only once in the file. This action has significantly reduced the number of rows in 10 times.
All values in the “item_properties.csv” file excluding ""categoryid"" and ""available"" properties were hashed. Value of the ""categoryid"" property contains item category identifier. Value of the ""available"" property contains availability of the item, i.e. 1 means the item was available, otherwise 0. All numerical values were marked with ""n"" char at the beginning, and have 3 digits precision after decimal point, e.g., ""5"" will become ""n5.000"", ""-3.67584"" will become ""n-3.675"". All words in text values were normalized (stemming procedure: https://en.wikipedia.org/wiki/Stemming) and hashed, numbers were processed as above, e.g. text ""Hello world 2017!"" will become ""24214 44214 n2017.000""
The category tree file has 1669 rows. Every row in the file specifies a child categoryId and the corresponding parent. For example:
Line “100,200” means that categoryid=1 has parent with categoryid=200
Line “300,” means that categoryid hasn’t parent in the tree
Acknowledgements
Retail Rocket (retailrocket.io) helps web shoppers make better shopping decisions by providing personalized real-time recommendations through multiple channels with over 100MM unique monthly users and 1000+ retail partners over the world.
Inspiration
How to use item properties and category tree data to improve collaborative filtering model?
Recurrent Neural Networks with Top-k Gains for Session-based Recommendations https://github.com/hidasib/GRU4Rec and paper https://arxiv.org/abs/1706.03847
https://www.researchgate.net/publication/280538158_Application_of_Kullback-Leibler_divergence_for_short-term_user_interest_detection
https://pdfs.semanticscholar.org/66dc/1724c4ed1e74fe6b22e636b52031a33c8ebe.pdf https://www.slideshare.net/LukasLerche/adaptation-and-evaluation-of-recommendationsfor-shortterm-shopping-goals Adaptation and Evaluation of Recommendations for Short-term Shopping Goals
Tasks
Task 1
When a customer comes to an e-commerce site, he looks for a product with particular properties: price range, vendor, product type and etc. These properties are implicit, so it's hard to determine them through clicks log.
Try to create an algorithm which predicts properties of items in ""addtocart"" event by using data from ""view"" events for any visitor in the published log.
Task 2
Description:
Process of analyzing ecommerce data include very important part of data cleaning. Researchers noticed that in some cases browsing data include up to 40% of abnormal traffic.
Firstly, abnormal users add a lot of noise into data and make recommendation system less effective. In order to increase efficiency of recommendation system, abnormal users should be removed from the raw data.
Secondly, abnormal users add bias to results of split tests, so this type of users should be removed also from split test data.
Goals:
The main goal is to find abnormal users of e-shop.
Subgoals:
Generate features
Build a model
Create a metric that helps to evaluate quality of the model"
Social Power NBA,"NBA on the court performance with Social Influence, Popularity and Power",Noah Gift,61,"Version 1,2017-08-01","basketball
social groups",CSV,8 MB,CC4,"10,136 views","1,914 downloads",37 kernels,,https://www.kaggle.com/noahgift/social-power-nba,"Context
This data set contains combined on-court performance data for NBA players in the 2016-2017 season, alongside salary, Twitter engagement, and Wikipedia traffic data.
Further information can be found in a series of articles for IBM Developerworks: ""Explore valuation and attendance using data science and machine learning"" and ""Exploring the individual NBA players"".
Acknowledgement
Data sources include ESPN, Basketball-Reference, Twitter, Five-ThirtyEight, and Wikipedia. The source code for this dataset (in Python and R) can be found on GitHub. Links to more writing can be found at noahgift.com.
Inspiration
Do NBA fans know more about who the best players are, or do owners?
What is the true worth of the social media presence of athletes in the NBA?"
Fruits 360 dataset,A dataset with 60 fruits and 38409 images,Mihai Oltean,61,"Version 13,2018-02-08|Version 12,2018-02-07|Version 11,2018-02-05|Version 10,2018-01-27|Version 9,2018-01-26|Version 8,2018-01-19|Version 7,2018-01-14|Version 6,2018-01-11|Version 5,2018-01-11|Version 4,2018-01-02|Version 3,2017-12-30|Version 2,2017-12-27|Version 1,2017-12-02","food and drink
image data
multiclass classification",Other,180 MB,CC4,"7,198 views","1,075 downloads",3 kernels,0 topics,https://www.kaggle.com/moltean/fruits,"Context
A high-quality, dataset of images containing fruits. The following fruits are included: Apples - (different varieties: Golden, Golden-Red, Granny Smith, Red, Red Delicious), Apricot, Avocado, Avocado ripe, Banana (Yellow, Red), Cactus fruit, Carambula, Cherry, Clementine, Cocos, Dates, Granadilla, Grape (Pink, White, White2), Grapefruit (Pink, White), Guava, Huckleberry, Kiwi, Kaki, Kumsquats, Lemon (normal, Meyer), Lime, Litchi, Mandarine, Mango, Maracuja, Nectarine, Orange, Papaya, Passion fruit, Peach, Pepino, Pear (different varieties, Abate, Monster, Williams), Pineapple, Pitahaya Red, Plum, Pomegranate, Quince, Raspberry, Salak, Strawberry, Tamarillo, Tangelo.
Dataset properties
Training set size: 28736 images.
Validation set size: 9673 images.
Number of classes: 60 (fruits).
Image size: 100x100 pixels.
Filename format: image_index_100.jpg (e.g. 32_100.jpg) or r_image_index_100.jpg (e.g. r_32_100.jpg). ""r"" stands for rotated fruit. ""100"" comes from image size (100x100 pixels).
Different varieties of the same fruit (apple for instance) are shown having different labels.
Content
Fruits were planted in the shaft of a low speed motor (3 rpm) and a short movie of 20 seconds was recorded.
A Logitech C920 camera was used for filming the fruits. This is one of the best webcams available.
Behind the fruits we placed a white sheet of paper as background.
However due to the variations in the lighting conditions, the background was not uniform and we wrote a dedicated algorithm which extract the fruit from the background. This algorithm is of flood fill type: we start from each edge of the image and we mark all pixels there, then we mark all pixels found in the neighborhood of the already marked pixels for which the distance between colors is less than a prescribed value. We repeat the previous step until no more pixels can be marked.
All marked pixels are considered as being background (which is then filled with white) and the rest of pixels are considered as belonging to the object.
The maximum value for the distance between 2 neighbor pixels is a parameter of the algorithm and is set (by trial and error) for each movie.
How to cite
Horea Muresan, Mihai Oltean, Fruit recognition from images using deep learning, Technical Report, Babes-Bolyai University, 2017
Alternate download
This dataset is also available for download from GitHub: Fruits-360 dataset
History
Fruits were filmed at the dates given below:
2017.02.25 - Apple (golden).
2017.02.28 - Apple (red-yellow, red, golden2), Kiwi, Pear, Grapefruit, Lemon, Orange, Strawberry, Banana.
2017.03.05 - Apple (golden3, Braeburn, Granny Smith, red2).
2017.03.07 - Apple (red3).
2017.05.10 - Plum, Peach, Peach flat, Apricot, Nectarine, Pomegranate.
2017.05.27 - Avocado, Papaya, Grape, Cherrie.
2017.12.25 - Carambula, Cactus fruit, Granadilla, Kaki, Kumsquats, Passion fruit, Avocado ripe, Quince.
2017.12.28 - Clementine, Cocos, Mango, Lime, Litchi.
2017.12.31 - Apple Red Delicious, Pear Monster, Grape White.
2018.01.14 - Ananas, Grapefruit Pink, Mandarine, Pineapple, Tangelo.
2018.01.19 - Huckleberry, Raspberry.
2018.01.26 - Dates, Maracuja, Salak, Tamarillo.
2018.02.05 - Guava, Grape White 2, Lemon Meyer
2018.02.07 - Banana Red, Pepino, Pitahaya Red.
2018.02.08 - Pear Abate, Pear Williams."
Online Courses from Harvard and MIT,What subjects or courses are the most popular on edX?,edX,61,"Version 1,2017-01-27",education,CSV,65 KB,Other,"15,138 views","2,046 downloads",59 kernels,0 topics,https://www.kaggle.com/edx/course-study,"Context
In 2012, the Massachusetts Institute of Technology (MIT) and Harvard University launched open online courses on edX, a non-profit learning platform co-founded by the two institutions. Four years later, what have we learned about these online “classrooms” and the global community of learners who take them?
Content
This report provides data on 290 Harvard and MIT online courses, 250 thousand certifications, 4.5 million participants, and 28 million participant hours on the edX platform since 2012.
Acknowledgements
Isaac Chuang, a professor at MIT, and Andrew Ho, a professor at Harvard University, published this data as an appendix to their paper ""HarvardX and MITx: Four Years of Open Online Courses""."
goodbooks-10k,"Ten thousand books, one million ratings. Also books marked to read, and tags.",Foxtrot,60,"Version 5,2017-09-02|Version 4,2017-08-29|Version 3,2017-08-24|Version 2,2017-08-22|Version 1,2017-08-14",books,CSV,41 MB,CC4,"16,076 views","1,803 downloads",19 kernels,6 topics,https://www.kaggle.com/zygmunt/goodbooks-10k,"This version of the dataset is obsolete. It contains duplicate ratings (same user_id,book_id), as reported by Philipp Spachtholz in his illustrious notebook.
The current version has duplicates removed, and more ratings (six million), sorted by time. Book and user IDs are the same.
It is available at https://github.com/zygmuntz/goodbooks-10k.
There have been good datasets for movies (Netflix, Movielens) and music (Million Songs) recommendation, but not for books. That is, until now.
This dataset contains ratings for ten thousand popular books. As to the source, let's say that these ratings were found on the internet. Generally, there are 100 reviews for each book, although some have less - fewer - ratings. Ratings go from one to five.
Both book IDs and user IDs are contiguous. For books, they are 1-10000, for users, 1-53424. All users have made at least two ratings. Median number of ratings per user is 8.
There are also books marked to read by the users, book metadata (author, year, etc.) and tags.
Contents
ratings.csv contains ratings and looks like that:
book_id,user_id,rating
1,314,5
1,439,3
1,588,5
1,1169,4
1,1185,4
to_read.csv provides IDs of the books marked ""to read"" by each user, as user_id,book_id pairs.
books.csv has metadata for each book (goodreads IDs, authors, title, average rating, etc.).
The metadata have been extracted from goodreads XML files, available in the third version of this dataset as books_xml.tar.gz. The archive contains 10000 XML files. One of them is available as sample_book.xml. To make the download smaller, these files are absent from the current version. Download version 3 if you want them.
book_tags.csv contains tags/shelves/genres assigned by users to books. Tags in this file are represented by their IDs.
tags.csv translates tag IDs to names.
See the notebook for some basic stats of the dataset.
goodreads IDs
Each book may have many editions. goodreads_book_id and best_book_id generally point to the most popular edition of a given book, while goodreads work_id refers to the book in the abstract sense.
You can use the goodreads book and work IDs to create URLs as follows:
https://www.goodreads.com/book/show/2767052
https://www.goodreads.com/work/editions/2792775"
Top 500 Indian Cities,What story do the top 500 cities of India tell to the world?,Arijit Mukherjee,60,"Version 4,2016-12-21|Version 3,2016-12-21|Version 2,2016-12-17|Version 1,2016-12-17","cities
sociology",CSV,73 KB,CC0,"23,972 views","2,823 downloads",171 kernels,9 topics,https://www.kaggle.com/zed9941/top-500-indian-cities,"Context
I created this data set merging the census 2011 of Indian Cities with Population more than 1 Lac and City wise number of Graduates from the Census 2011, to create a visualization of where the future cities of India stands today, I will try to add more columns [ fertility rate, religion distribution, health standards, number of schools, Mortality rate ] in the future, hope people will contribute.
Content
Data of 500 Cities with population more than 1 Lac by Census 2011
'name_of_city'                  : Name of the City 
'state_code'                    : State Code of the City
'state_name'                    : State Name of the City
'dist_code'                     : District Code where the city belongs ( 99 means multiple district ) 
'population_total'              : Total Population
'population_male'               : Male Population 
'population_female'             : Female Population
'0-6_population_total'          : 0-6 Age Total Population
'0-6_population_male'           : 0-6 Age Male Population
'0-6_population_female'         : 0-6 Age Female Population
'literates_total'               : Total Literates
'literates_male'                : Male Literates
'literates_female'              : Female Literates 
'sex_ratio'                     : Sex Ratio 
'child_sex_ratio'               : Sex ratio in 0-6
'effective_literacy_rate_total' : Literacy rate over Age 7 
'effective_literacy_rate_male'  : Male Literacy rate over Age 7 
'effective_literacy_rate_female': Female Literacy rate over Age 7 
'location'                      : Lat,Lng
'total_graduates'               : Total Number of Graduates
'male_graduates'                : Male Graduates 
'female_graduates'              : Female Graduates
Acknowledgements
Census 2011
http://censusindia.gov.in/2011-prov-results/paper2/data_files/India2/Table_2_PR_Cities_1Lakh_and_Above.xls
Google Geocoder for Location Fetching.
Graduation Data Census 2011
http://www.censusindia.gov.in/2011census/C-series/DDWCT-0000C-08.xlsx
Inspiration
What story do the top 500 cities of India tell to the world? I wrote a post in my blog about the dataset ."
Detailed NFL Play-by-Play Data 2009-2016,nflscrapR generated NFL dataset wiith expected points and win probability,Max Horowitz,59,"Version 3,2018-01-10|Version 2,2017-07-28|Version 1,2017-07-26","american football
sports",CSV,67 MB,Other,"11,493 views","1,149 downloads",2 kernels,7 topics,https://www.kaggle.com/maxhorowitz/nflplaybyplay2009to2016,"Introduction
The lack of publicly available National Football League (NFL) data sources has been a major obstacle in the creation of modern, reproducible research in football analytics. While clean play-by-play data is available via open-source software packages in other sports (e.g. nhlscrapr for hockey; PitchF/x data in baseball; the Basketball Reference for basketball), the equivalent datasets are not freely available for researchers interested in the statistical analysis of the NFL. To solve this issue, a group of Carnegie Mellon University statistical researchers including Maksim Horowitz, Ron Yurko, and Sam Ventura, built and released nflscrapR an R package which uses an API maintained by the NFL to scrape, clean, parse, and output clean datasets at the individual play, player, game, and season levels. Using the data outputted by the package, the trio went on to develop reproducible methods for building expected point and win probability models for the NFL. The outputs of these models are included in this dataset and can be accessed using the nflscrapR package.
Content
The dataset made available on Kaggle contains all the regular season plays from the 2009-2016 NFL seasons. The dataset has 356,768 rows and 100 columns. Each play is broken down into great detail containing information on: game situation, players involved, results, and advanced metrics such as expected point and win probability values. Detailed information about the dataset can be found at the following web page, along with more NFL data: https://github.com/ryurko/nflscrapR-data.
Acknowledgements
This dataset was compiled by Ron Yurko, Sam Ventura, and myself. Special shout-out to Ron for improving our current expected points and win probability models and compiling this dataset. All three of us are proud founders of the Carnegie Mellon Sports Analytics Club.
Inspiration
This dataset is meant to both grow and bring together the community of sports analytics by providing clean and easily accessible NFL data that has never been availabe on this scale for free."
The Simpsons Characters Data,Image dataset of 20 characters from The Simpsons,alexattia,58,"Version 3,2017-07-01|Version 2,2017-06-20|Version 1,2017-06-15","popular culture
image data
object detection",Other,588 MB,CC4,"16,589 views","2,701 downloads",5 kernels,5 topics,https://www.kaggle.com/alexattia/the-simpsons-characters-dataset,"Feel free to check and recommend my Medium post Part 1 on a classification model and Part 2 on a detection model (Faster R-CNN) about this dataset and what I am doing with it.
You can also find the related GitHub repo here .
Context
As a big Simpsons fan, I have watched a lot (and still watching) of The Simpson episodes -multiple times each- over the years. I wanted to build a neural network which can recognize characters
Content
I am still building this dataset (labeling pictures), I will upload new versions of this dataset. Please check the files there are descriptions and explanations.
File simpson-set.tar.gz : This is an image dataset: 20 folders (one for each character) with 400-2000 pictures in each folder.
File simpson-test-set.zip. : Preview of the image dataset
File weights.best.h5 : Weights computed, in order to predict in Kernels.
File annotation.txt : Annotation file for bounding boxes for each character
Help me to build this dataset
If someone wants to contribute and make this dataset bigger and more relevant, any help will be appreciated.
Acknowledgements
Data is directly taken and labeled from TV show episodes."
Glass Classification,Can you correctly identify glass type?,UCI Machine Learning,58,"Version 1,2017-01-28","chemistry
artificial intelligence",CSV,10 KB,ODbL,"28,823 views","3,249 downloads",222 kernels,3 topics,https://www.kaggle.com/uciml/glass,"Context
This is a Glass Identification Data Set from UCI. It contains 10 attributes including id. The response is glass type(discrete 7 values)
Content
Attribute Information:
Id number: 1 to 214 (removed from CSV file)
RI: refractive index
Na: Sodium (unit measurement: weight percent in corresponding oxide, as are attributes 4-10)
Mg: Magnesium
Al: Aluminum
Si: Silicon
K: Potassium
Ca: Calcium
Ba: Barium
Fe: Iron
Type of glass: (class attribute) -- 1 building_windows_float_processed -- 2 building_windows_non_float_processed -- 3 vehicle_windows_float_processed -- 4 vehicle_windows_non_float_processed (none in this database) -- 5 containers -- 6 tableware -- 7 headlamps
Acknowledgements
https://archive.ics.uci.edu/ml/datasets/Glass+Identification Source:
Creator: B. German Central Research Establishment Home Office Forensic Science Service Aldermaston, Reading, Berkshire RG7 4PN
Donor: Vina Spiehler, Ph.D., DABFT Diagnostic Products Corporation (213) 776-0180 (ext 3014)
Inspiration
Data exploration of this dataset reveals two important characteristics : 1) The variables are highly corelated with each other including the response variables: So which kind of ML algorithm is most suitable for this dataset Random Forest , KNN or other? Also since dataset is too small is there any chance of applying PCA or it should be completely avoided?
2) Highly Skewed Data: Is scaling sufficient or are there any other techniques which should be applied to normalize data? Like BOX-COX Power transformation?"
Python Questions from Stack Overflow,Full text of Stack Overflow Q&A about the Python programming language,Stack Overflow,58,"Version 1,2016-10-21","internet
programming languages",CSV,2 GB,Other,"16,001 views","1,534 downloads",48 kernels,,https://www.kaggle.com/stackoverflow/pythonquestions,"Context
Full text of all questions and answers from Stack Overflow that are tagged with the python tag. Useful for natural language processing and community analysis. See also the dataset of R questions.
Content
This dataset is organized as three tables:
Questions contains the title, body, creation date, score, and owner ID for each Python question.
Answers contains the body, creation date, score, and owner ID for each of the answers to these questions. The ParentId column links back to the Questions table.
Tags contains the tags on each question besides the Python tag.
Questions may be deleted by the user who posted them. They can also be closed by community vote, if the question is deemed off-topic for instance. Such questions are not included in this dataset.
The dataset contains questions all questions asked between August 2, 2008 and Ocotober 19, 2016.
License
All Stack Overflow user contributions are licensed under CC-BY-SA 3.0 with attribution required."
Biodiversity in National Parks,Plant and animal species found in the American national park system,National Park Service,58,"Version 3,2017-01-20|Version 2,2017-01-20|Version 1,2017-01-17","ecology
animals
plants",CSV,17 MB,CC0,"9,304 views","1,631 downloads",16 kernels,3 topics,https://www.kaggle.com/nationalparkservice/park-biodiversity,"Context
The National Park Service publishes a database of animal and plant species identified in individual national parks and verified by evidence — observations, vouchers, or reports that document the presence of a species in a park. All park species records are available to the public on the National Park Species portal; exceptions are made for sensitive, threatened, or endangered species when widespread distribution of information could pose a risk to the species in the park.
Content
National Park species lists provide information on the presence and status of species in our national parks. These species lists are works in progress and the absence of a species from a list does not necessarily mean the species is absent from a park. The time and effort spent on species inventories varies from park to park, which may result in data gaps. Species taxonomy changes over time and reflects regional variations or preferences; therefore, records may be listed under a different species name.
Each park species record includes a species ID, park name, taxonomic information, scientific name, one or more common names, record status, occurrence (verification of species presence in park), nativeness (species native or foreign to park), abundance (presence and visibility of species in park), seasonality (season and nature of presence in park), and conservation status (species classification according to US Fish & Wildlife Service). Taxonomic classes have been translated from Latin to English for species categorization; order, family, and scientific name (genus, species, subspecies) are in Latin.
Acknowledgements
The National Park Service species list database is managed and updated by staff at individual national parks and the systemwide Inventory and Monitoring department."
Formula 1 Race Data,Race data from 1950 to 2017,Chris G,57,"Version 1,2017-11-29","auto racing
sports",CSV,6 MB,CC4,"7,684 views","1,572 downloads",,2 topics,https://www.kaggle.com/cjgdev/formula-1-race-data-19502017,"Context
Formula One (also Formula 1 or F1 and officially the FIA Formula One World Championship) is the highest class of single-seat auto racing that is sanctioned by the Fédération Internationale de l'Automobile (FIA). The FIA Formula One World Championship has been one of the premier forms of racing around the world since its inaugural season in 1950.
Content
This dataset contains data from 1950 all the way through the 2017 season, and consists of tables describing constructors, race drivers, lap times, pit stops and more.
Acknowledgements
The data was downloaded from http://ergast.com/mrd/ at the conclusion of the 2017 season. The data was originally gathered and published to the public domain by Chris Newell.
Inspiration
I think this dataset offers an exciting insight into a $ billion industry, enjoyed by hundreds of millions of viewers all over the world. So please, explore and enjoy!"
Prescription-based prediction,Predicting doctor attributes from prescription behavior,Roam Analytics,56,"Version 1,2016-10-17","healthcare
artificial intelligence",{}JSON,156 MB,CC4,"16,920 views","1,518 downloads",38 kernels,,https://www.kaggle.com/roamresearch/prescriptionbasedprediction,"This is the dataset used in the Roam blog post Prescription-based prediction. It is derived from a variety of US open health datasets, but the bulk of the data points come from the Medicare Part D dataset and the National Provider Identifier dataset.
The prescription vector for each doctor tells a rich story about that doctor's attributes, including specialty, gender, age, and region. There are 239,930 doctors in the dataset.
The file is in JSONL format (one JSON record per line):
{
    'provider_variables': 
        {
            'brand_name_rx_count': int,
            'gender': 'M' or 'F',
            'generic_rx_count': int,
            'region': 'South' or 'MidWest' or 'Northeast' or 'West',
            'settlement_type': 'non-urban' or 'urban'
            'specialty': str
            'years_practicing': int
        },
     'npi': str,
     'cms_prescription_counts':
        {
            `drug_name`: int, 
            `drug_name`: int, 
            ...
        }
}
The brand/generic classifications behind brand_name_rx_count and generic_rx_count are defined heuristically. For more details, see the blog post or go directly to the associated code."
Meteorite Landings,Data on over 45k meteorites that have struck Earth,NASA,56,"Version 2,2016-11-06|Version 1,2016-11-05","astronomy
space",CSV,4 MB,CC0,"18,161 views","2,212 downloads",60 kernels,2 topics,https://www.kaggle.com/nasa/meteorite-landings,"The Meteoritical Society collects data on meteorites that have fallen to Earth from outer space. This dataset includes the location, mass, composition, and fall year for over 45,000 meteorites that have struck our planet.
Notes on missing or incorrect data points:
a few entries here contain date information that was incorrectly parsed into the NASA database. As a spot check: any date that is before 860 CE or after 2016 are incorrect; these should actually be BCE years. There may be other errors and we are looking for a way to identify them.
a few entries have latitude and longitude of 0N/0E (off the western coast of Africa, where it would be quite difficult to recover meteorites). Many of these were actually discovered in Antarctica, but exact coordinates were not given. 0N/0E locations should probably be treated as NA.
The starter kernel for this dataset has a quick way to filter out these observations using dplyr in R, provided here for convenience:
meteorites.geo <- meteorites.all %>%
filter(year>=860 & year<=2016) %>% # filter out weird years
filter(reclong<=180 & reclong>=-180 & (reclat!=0 | reclong!=0)) # filter out weird locations
The Data
Note that a few column names start with ""rec"" (e.g., recclass, reclat, reclon). These are the recommended values of these variables, according to The Meteoritical Society. In some cases, there were historical reclassification of a meteorite, or small changes in the data on where it was recovered; this dataset gives the currently recommended values.
The dataset contains the following variables:
name: the name of the meteorite (typically a location, often modified with a number, year, composition, etc)
id: a unique identifier for the meteorite
nametype: one of:
-- valid: a typical meteorite
-- relict: a meteorite that has been highly degraded by weather on Earth
recclass: the class of the meteorite; one of a large number of classes based on physical, chemical, and other characteristics (see the Wikipedia article on meteorite classification for a primer)
mass: the mass of the meteorite, in grams
fall: whether the meteorite was seen falling, or was discovered after its impact; one of:
-- Fell: the meteorite's fall was observed
-- Found: the meteorite's fall was not observed
year: the year the meteorite fell, or the year it was found (depending on the value of fell)
reclat: the latitude of the meteorite's landing
reclong: the longitude of the meteorite's landing
GeoLocation: a parentheses-enclose, comma-separated tuple that combines reclat and reclong
What can we do with this data?
Here are a couple of thoughts on questions to ask and ways to look at this data:
how does the geographical distribution of observed falls differ from that of found meteorites? -- this would be great overlaid on a cartogram or alongside a high-resolution population density map
are there any geographical differences or differences over time in the class of meteorites that have fallen to Earth?
Acknowledgements
This dataset was downloaded from NASA's Data Portal, and is based on The Meteoritical Society's Meteoritical Bulletin Database (this latter database provides additional information such as meteorite images, links to primary sources, etc.)."
NIPS Papers,"Titles, authors, abstracts, and extracted text for all NIPS papers (1987-2017)",Ben Hamner,55,"Version 2,2017-12-06|Version 1,2016-12-04","linguistics
artificial intelligence",CSV,142 MB,ODbL,"20,790 views","1,519 downloads",68 kernels,0 topics,https://www.kaggle.com/benhamner/nips-papers,"Neural Information Processing Systems (NIPS) is one of the top machine learning conferences in the world. It covers topics ranging from deep learning and computer vision to cognitive science and reinforcement learning.
This dataset includes the title, authors, abstracts, and extracted text for all NIPS papers to date (ranging from the first 1987 conference to the current 2016 conference). I've extracted the paper text from the raw PDF files and are releasing that both in CSV files and as a SQLite database. The code to scrape and create this dataset is on GitHub. Here's a quick RMarkdown exploratory overview of what's in the data. We encourage you to explore this data and share what you find through Kaggle Kernels!"
New York City Taxi with OSRM,Helpful dataset for New York Taxi Playground,oscarleo,55,"Version 5,2017-08-06|Version 4,2017-07-30|Version 3,2017-07-29|Version 2,2017-07-27|Version 1,2017-07-26",taxi services,CSV,2 GB,ODbL,"12,073 views","3,702 downloads",45 kernels,10 topics,https://www.kaggle.com/oscarleo/new-york-city-taxi-with-osrm,"Context
I created this data set to help with the New York City Taxi Trip Duration playground. I used OSRM to extract information about the fastest routes for each data point.
I think it will be very useful for anyone doing work in that competition. Please try it out and tell me what you want me to add. I intend to improve and add features.
Content
starting_street
The street where the taxi-trip starts. In version 1 this field contained a lot of empty values. That has been dealt with.
end_street
The street where the taxi-trip ends. In version 1 this field contained a lot of empty values. That has been dealt with.
total_distance
The total distance is measured between the pickup coordinates and the drop-off coordinates in train.csv and test.csv. The unit is meters.
total_travel_time
The total travel time for that data point in seconds
number_of_steps
The number of steps on that trip. One step consists of some driving and an action the taxi needs to perform. It can be something like a turn or going on to a highway. See step_maneuvers for more information.
street_for_each_step
A list of streets where each step occurs. Multiple steps can be performed on the same street. Therefore there might the same street might occur multiple times.
(The values are stored as a string separated by '|')
distance_per_step
The distance for each step.
(The values are stored as a string separated by '|')
travel_time_per_step
The travel time for each step
(The values are stored as a string separated by '|')
step_maneuvers
The action (or maneuver) performed in each step. The possible maneuvers are:
turn: a basic turn
new name: no turn is taken/possible, but the road name changes.
depart: The trip starts
arrive: The trip ends
merge: Merge onto a street (e.g. getting on the highway from a ramp)
on ramp: Entering a highway (direction given my modifier )
off ramp: Exiting a highway
fork: The road forks
end of road: The road ends in a T intersection
continue: Turn to stay on the same road
roundabout: A roundabout
rotary A traffic circle (a bigger roundabout)
roundabout turn: A small roundabout that can be treated as a regular turn
(The values are stored as a string separated by '|')
step_direction
The direction for each action (or maneuver)
step_location_list
The coordinates for each action (or maneuver)"
NIH Chest X-rays,"Over 112,000 Chest X-ray images from more than 30,000 unique patients",National Institutes of Health Chest X-Ray Dataset,55,"Version 3,2018-02-22|Version 2,2017-12-06|Version 1,2017-12-02","medicine
machine learning",Other,42 GB,CC0,"8,668 views","2,051 downloads",8 kernels,0 topics,https://www.kaggle.com/nih-chest-xrays/data,"NIH Chest X-ray Dataset
National Institutes of Health Chest X-Ray Dataset
Chest X-ray exams are one of the most frequent and cost-effective medical imaging examinations available. However, clinical diagnosis of a chest X-ray can be challenging and sometimes more difficult than diagnosis via chest CT imaging. The lack of large publicly available datasets with annotations means it is still very difficult, if not impossible, to achieve clinically relevant computer-aided detection and diagnosis (CAD) in real world medical sites with chest X-rays. One major hurdle in creating large X-ray image datasets is the lack resources for labeling so many images. Prior to the release of this dataset, Openi was the largest publicly available source of chest X-ray images with 4,143 images available.
This NIH Chest X-ray Dataset is comprised of 112,120 X-ray images with disease labels from 30,805 unique patients. To create these labels, the authors used Natural Language Processing to text-mine disease classifications from the associated radiological reports. The labels are expected to be >90% accurate and suitable for weakly-supervised learning. The original radiology reports are not publicly available but you can find more details on the labeling process in this Open Access paper: ""ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases."" (Wang et al.)
Link to paper

Data limitations:
The image labels are NLP extracted so there could be some erroneous labels but the NLP labeling accuracy is estimated to be >90%.
Very limited numbers of disease region bounding boxes (See BBox_list_2017.csv)
Chest x-ray radiology reports are not anticipated to be publicly shared. Parties who use this public dataset are encouraged to share their “updated” image labels and/or new bounding boxes in their own studied later, maybe through manual annotation

File contents
Image format: 112,120 total images with size 1024 x 1024
images_001.zip: Contains 4999 images
images_002.zip: Contains 10,000 images
images_003.zip: Contains 10,000 images
images_004.zip: Contains 10,000 images
images_005.zip: Contains 10,000 images
images_006.zip: Contains 10,000 images
images_007.zip: Contains 10,000 images
images_008.zip: Contains 10,000 images
images_009.zip: Contains 10,000 images
images_010.zip: Contains 10,000 images
images_011.zip: Contains 10,000 images
images_012.zip: Contains 7,121 images
README_ChestXray.pdf: Original README file
BBox_list_2017.csv: Bounding box coordinates. Note: Start at x,y, extend horizontally w pixels, and vertically h pixels
Image Index: File name
Finding Label: Disease type (Class label)
Bbox x
Bbox y
Bbox w
Bbox h
Data_entry_2017.csv: Class labels and patient data for the entire dataset
Image Index: File name
Finding Labels: Disease type (Class label)
Follow-up #
Patient ID
Patient Age
Patient Gender
View Position: X-ray orientation
OriginalImageWidth
OriginalImageHeight
OriginalImagePixelSpacing_x
OriginalImagePixelSpacing_y

Class descriptions
There are 15 classes (14 diseases, and one for ""No findings""). Images can be classified as ""No findings"" or one or more disease classes:
Atelectasis
Consolidation
Infiltration
Pneumothorax
Edema
Emphysema
Fibrosis
Effusion
Pneumonia
Pleural_thickening
Cardiomegaly
Nodule Mass
Hernia

Full Dataset Content
There are 12 zip files in total and range from ~2 gb to 4 gb in size. Additionally, we randomly sampled 5% of these images and created a smaller dataset for use in Kernels. The random sample contains 5606 X-ray images and class labels.
Sample: sample.zip

Modifications to original data
Original TAR archives were converted to ZIP archives to be compatible with the Kaggle platform
CSV headers slightly modified to be more explicit in comma separation and also to allow fields to be self-explanatory

Citations
Wang X, Peng Y, Lu L, Lu Z, Bagheri M, Summers RM. ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases. IEEE CVPR 2017, ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.pdf
NIH News release: NIH Clinical Center provides one of the largest publicly available chest x-ray datasets to scientific community
Original source files and documents: https://nihcc.app.box.com/v/ChestXray-NIHCC/folder/36938765345

Acknowledgements
This work was supported by the Intramural Research Program of the NClinical Center (clinicalcenter.nih.gov) and National Library of Medicine (www.nlm.nih.gov)."
US Permanent Visa Applications,Detailed information on 374k visa decisions,Jacob Boysen,54,"Version 1,2017-08-25","government agencies
immigration",CSV,285 MB,CC0,"8,287 views","1,278 downloads",7 kernels,0 topics,https://www.kaggle.com/jboysen/us-perm-visas,"Context:
A permanent labor certification issued by the Department of Labor (DOL) allows an employer to hire a foreign worker to work permanently in the United States. In most instances, before the U.S. employer can submit an immigration petition to the Department of Homeland Security's U.S. Citizenship and Immigration Services (USCIS), the employer must obtain a certified labor certification application from the DOL's Employment and Training Administration (ETA). The DOL must certify to the USCIS that there are not sufficient U.S. workers able, willing, qualified and available to accept the job opportunity in the area of intended employment and that employment of the foreign worker will not adversely affect the wages and working conditions of similarly employed U.S. workers.
Content:
Data covers 2012-2017 and includes information on employer, position, wage offered, job posting history, employee education and past visa history, associated lawyers, and final decision.
Acknowledgements:
This data was collected and distributed by the US Department of Labor.
Inspiration:
Can you predict visa decisions based on employee/employer/wage?
How does this data compare to H1B decisions in this dataset?"
League of Legends Ranked Matches,180000 ranked games of League of Legends starting from 2014,Paolo Campanelli,54,"Version 9,2017-10-26|Version 8,2017-10-20|Version 7,2017-10-20|Version 6,2017-10-20|Version 5,2017-10-18|Version 4,2017-10-18|Version 3,2017-10-18|Version 2,2017-10-18|Version 1,2017-10-18",video games,CSV,696 MB,Other,"8,653 views","1,104 downloads",6 kernels,7 topics,https://www.kaggle.com/paololol/league-of-legends-ranked-matches,"League of Legends Ranked Matches
Data about 184070 League of Legends ranked solo games, spanning across several years
Content
Matches
Player and team stats
Bans
Acknowledgements
I found this data on a SQL database and exported it to CSV. All data belongs ultimately to Riot Games and their data policies applies. These files are presented only as a simpler way to obtain a large dataset without stressing the Riot API and are in no way associated with Riot Games. The data is provided as-is without any warranty on its correctness. If your algorithm catches fire, don't blame me or Riot. If you are Rito and are opposed to sharing this data here, contact me and it will be removed immediately.
Possible questions
Can we predict the winner given the teams?
Can ranked matchmaking be assumed to be unbiased (or adjusted for red-side advantage)?
Does the region affect significantly win rates?
Can we compare the data in relation to competitive data (also available on Kaggle)?
Can we assess information on the different metas?"
Steam Video Games,Recommend video games from 200k steam user interactions.,Tamber,54,"Version 3,2017-03-09|Version 2,2017-03-04|Version 1,2017-03-04",video games,CSV,9 MB,ODbL,"16,123 views","1,832 downloads",35 kernels,2 topics,https://www.kaggle.com/tamber/steam-video-games,"Context
Steam is the world's most popular PC Gaming hub, with over 6,000 games and a community of millions of gamers. With a massive collection that includes everything from AAA blockbusters to small indie titles, great discovery tools are a highly valuable asset for Steam. How can we make them better?
Content
This dataset is a list of user behaviors, with columns: user-id, game-title, behavior-name, value. The behaviors included are 'purchase' and 'play'. The value indicates the degree to which the behavior was performed - in the case of 'purchase' the value is always 1, and in the case of 'play' the value represents the number of hours the user has played the game.
Acknowledgements
This dataset is generated entirely from public Steam data, so we want to thank Steam for building such an awesome platform and community!
Inspiration
The dataset is formatted to be compatible with Tamber. Build a Tamber engine and take it for a spin!
Combine our collaborative filter's results with your favorite Machine Learning techniques with Ensemble Learning, or make Tamber do battle with something else you've built.
Have fun, The Tamber Team"
SF Bay Area Bike Share,Anonymized bike trip data from August 2013 to August 2015,Ben Hamner,53,"Version 1,2016-06-14","cycling
road transport",CSV,4 GB,Other,"30,915 views","5,130 downloads",155 kernels,2 topics,https://www.kaggle.com/benhamner/sf-bay-area-bike-share,"The Bay Area Bike Share enables quick, easy, and affordable bike trips around the San Francisco Bay Area. They make regular open data releases (this dataset is a transformed version of the data from this link), plus maintain a real-time API.
Exploration Ideas
How does weather impact bike trips?
How do bike trip patterns vary by time of day and the day of the week?"
eCommerce Item Data,500 SKUs and their descriptions. Great for content engine training!,cclark,53,"Version 1,2016-08-18",business,CSV,553 KB,Other,"38,272 views","4,575 downloads",34 kernels,4 topics,https://www.kaggle.com/cclark/product-item-data,500 actual SKUs from an outdoor apparel brand's product catalog. It's somewhat rare to get real item level data in a real-world format. Very useful for testing things like recommendation engines. In fact...maybe I'll publish some code along with this :)
Star Cluster Simulations,Direct N-body simulation of a star cluster: Position and velocities of stars,Mario Pasquato,53,"Version 1,2017-01-07","astronomy
space",CSV,109 MB,CC0,"12,745 views","1,327 downloads",21 kernels,2 topics,https://www.kaggle.com/mariopasquato/star-cluster-simulations,"Context
Stars mostly form in clusters and associations rather than in isolation. Milky Way star clusters are easily observable with small telescopes, and in some cases even with the naked eye. Depending on a variety of conditions, star clusters may dissolve quickly or be very long lived. The dynamical evolution of star clusters is a topic of very active research in astrophysics. Some popular models of star clusters are the so-called direct N-body simulations [1, 2], where every star is represented by a point particle that interacts gravitationally with every other particle. This kind of simulation is computationally expensive, as it scales as O(N^2) where N is the number of particles in the simulated cluster. In the following, the words ""particle"" and ""star"" are used interchangeably.
Content
This dataset contains the positions and velocities of simulated stars (particles) in a direct N-body simulation of a star cluster. In the cluster there are initially 64000 stars distributed in position-velocity space according to a King model [3]. Each .csv file named c_xxxx.csv corresponds to a snapshot of the simulation at time t = xxxx. For example, c_0000.csv contains the initial conditions (positions and velocities of stars at time t=0). Times are measured in standard N-body units [4]. This is a system of units where G = M = −4E = 1 (G is the gravitational constant, M the total mass of the cluster, and E its total energy).
x, y, z Columns 1, 2, and 3 of each file are the x, y, z positions of the stars. They are also expressed in standard N-body units [4]. You can switch to units of the median radius of the cluster by finding the cluster center and calculating the median distance of stars from it, and then dividing x, y, and z by this number. In general, the median radius changes in time. The initial conditions are approximately spherically symmetric (you can check) so there is no particular physical meaning attached to the choice of x, y, and z.
vx, vy, vz Columns 4, 5, and 6 contain the x, y, and z velocity, also in N-body units. A scale velocity for the stars can be obtained by taking the standard deviation of velocity along one direction (e.g. z). You may check that the ratio between the typical radius (see above) and the typical velocity is of order unity.
m Column 7 is the mass of each star. For this simulation this is identically 1.5625e-05, i.e. 1/64000. The total mass of the cluster is initially 1. More realistic simulations (coming soon) have a spectrum of different masses and live stelar evolution, that results in changes in the mass of stars. This simulation is a pure N-body problem instead.
Star id number The id numbers of each particle are listed in the last column (8) of the files under the header ""id"". The ids are unique and can be used to trace the position and velocity of a star across all files. There are initially 64000 particles. At end of the simulation there are 63970. This is because some particles escape the cluster.
Acknowledgements
This simulation was run on a Center for Galaxy Evolution Research (CGER) workstation at Yonsei University (Seoul, Korea), using the NBODY6 software (https://www.ast.cam.ac.uk/~sverre/web/pages/nbody.htm).
Inspiration
Some stars hover around the center of the cluster, while some other get kicked out to the cluster outskirts or even leave the cluster altogether. Can we predict where a star will be at any given time based on its initial position and velocity? Can we predict its velocity?
How correlated are the motions of stars? Can we predict the velocity of a given star based on the velocity of its neighbours?
The size of the cluster can be measured by defining a center (see below) and finding the median distance of stars from it. This is called the three-dimensional effective radius. Can we predict how it evolves over time? What are its properties as a time series? What can we say about other quantiles of the radius?
How to define the cluster center? Just as the mode of a KDE of the distribution of stars? How does it move over time and how to quantify the properties of its fluctuations? Is the cluster symmetric around this center?
Some stars leave the cluster: over time they exchange energy in close encounters with other stars and reach the escape velocity. This can be seen by comparing later snapshots with the initial one: some IDs are missing and there is overall a lower number of stars. Can we predict which stars are more likely to escape? When will a given star escape?
References
[1] Heggie, D., Hut, P. 2003, The Gravitational Million-Body Problem: A Multidisciplinary Approach to Star Cluster Dynamics ~ Cambridge University Press, 2003
[2] Aarseth, S.~J. 2003, Gravitational N-Body Simulations - Cambridge University Press, 2003
[3] King, I. 1966, AJ, 71, 64
[4] Heggie, D. C., Mathieu, R. D. 1986, Lecture Notes in Physics, Vol. 267, The Use of Supercomputers in Stellar Dynamics, Berlin, Springer"
Global Shark Attacks,Data compiled by the global shark attack file,toby jolly,53,"Version 6,2018-01-02|Version 5,2017-06-21|Version 4,2017-06-21|Version 3,2016-09-30|Version 2,2016-09-30|Version 1,2016-09-29",oceans,CSV,543 KB,Other,"21,471 views","3,450 downloads",80 kernels,4 topics,https://www.kaggle.com/teajay/global-shark-attacks,"Context
This is a table of shark attack incidents compiled by the Global Shark Attack File: Please see their website for more details on where this data comes from.
Acknowledgements
This data was downloaded with permission from the Global Shark Attack File's website"
Pakistan Drone Attacks,"Most authentic count of drone strikes in Pakistan, 2004-2016",Zeeshan-ul-hassan Usmani,52,"Version 8,2017-12-01|Version 7,2017-10-26|Version 6,2017-10-26|Version 5,2017-10-10|Version 4,2017-05-02|Version 3,2017-03-23|Version 2,2017-01-29|Version 1,2017-01-25",military,CSV,475 KB,CC0,"11,241 views",975 downloads,44 kernels,3 topics,https://www.kaggle.com/zusmani/pakistandroneattacks,"Context
Pakistan Drone Attacks (2004-2016)
The United States has targeted militants in the Federally Administered Tribal Areas [FATA] and the province of Khyber Pakhtunkhwa [KPK] in Pakistan via its Predator and Reaper drone strikes since year 2004. Pakistan Body Count (www.PakistanBodyCount.org) is the oldest and most accurate running tally of drone strikes in Pakistan. The given database (PakistanDroneAttacks.CSV) has been populated by using majority of the data from Pakistan Body Count, and building up on it by canvassing open source newspapers, media reports, think tank analyses, and personal contacts in media and law enforcement agencies. We provide a count of the people killed and injured in drone strikes, including the ones who died later in hospitals or homes due to injuries caused or aggravated by drone strikes, making it the most authentic source for drone related data in this region.
We will keep releasing the updates every quarter at this page.
Content
Geography: Pakistan
Time period: 2004-2016
Unit of analysis: Attack
Dataset: The dataset contains detailed information of 397 drone attacks in Pakistan that killed an estimated 3,558 and injured 1,333 people including 2,539 civilians.
Variables: The dataset contains Serial No, Incident Day & Date, Approximate Time of the attack, Specific Location, City, Province, Number of people killed who claimed to be from Al-Qaeeda, Number of people killed who claimed to be from Taliban, minimum and maximum count of foreigners killed, minimum and maximum count of civilians killed, minimum and maximum count of civilians injured, special mention (more details) and comments about the attack, longitude and latitude of the location. Sources: Unclassified media articles, hospital reports, think tank analysis and reports, and government official press releases.
Acknowledgements & References
Pakistan Body Count has been leveraged extensively in scholarly publications, reports, media articles and books. The website and the dataset has been collected and curated by the founder Zeeshan-ul-hassan Usmani. Users are allowed to use, copy, distribute and cite the dataset as follows: “Zeeshan-ul-hassan Usmani, Pakistan Body Count, Drone Attacks Dataset, Kaggle Dataset Repository, Jan 25, 2017.”
Past Research
Zeeshan-ul-hassan Usmani and Hira Bashir, “The Impact of Drone Strikes in Pakistan”, Cost of War Project, Brown University, December 16, 2014
Inspiration
Some ideas worth exploring:
• How many people got killed and injured per year in last 12 years?
• How many attacks involved killing of actual terrorists from Al-Qaeeda and Taliban?
• How many attacks involved women and children?
• Visualize drone attacks on timeline
• Find out any correlation with number of drone attacks with specific date and time, for example, do we have more drone attacks in September?
• Find out any correlation with drone attacks and major global events (US funding to Pakistan and/or Afghanistan, Friendly talks with terrorist outfits by local or foreign government?)
• The number of drone attacks in Bush Vs Obama tenure?
• The number of drone attacks versus the global increase/decrease in terrorism?
• Correlation between number of drone strikes and suicide bombings in Pakistan
Questions?
For detailed visit www.PakistanBodyCount.org
Or contact Pakistan Body Count staff at info@pakistanbodycount.org"
South Park Dialogue,"More than 70,000 lines of dialogue by season, episode, and character",Ksenia Sukhova,52,"Version 1,2017-03-15","popular culture
linguistics",CSV,5 MB,Other,"9,836 views","1,061 downloads",37 kernels,4 topics,https://www.kaggle.com/tovarischsukhov/southparklines,"South Park cartoon lines
+70k lines, annotated with season, episode and speaker
It is interesting to practice NLP with ML techniques in order to guess who is speaking. Later there will be file with pre-proccesed data to train"
2015 Traffic Fatalities,National Highway Traffic Safety Administration,NHTSA,52,"Version 3,2016-11-26|Version 2,2016-11-26|Version 1,2016-11-21",road transport,CSV,88 MB,ODbL,"23,938 views","3,337 downloads",188 kernels,5 topics,https://www.kaggle.com/nhtsa/2015-traffic-fatalities,"Quick Start
For a quick introduction to this Dataset, take a look at the Kernel Traffic Fatalities Getting Started.
See the Fatality Analysis Reporting System FARS User’s Manual for understanding the column abbreviations and possible values.
Also, see the following reference
Original source of this data containing all files can be obtained here
Below are the files released by the (NHTSA) National Highway Traffic Safety Administration, in their original format. Additional files can be found in the extra folder. Reference Traffic Fatalities Getting Started. for how to access this extra folder with contents.
Data Compared to 2014
A few interesting notes about this data compared to 2014
Pedalcyclist fatalities increased by 89 (12.2 percent)
Motorcyclist fatalities increased by 382 (8.3-percent increase)
Alcohol-impaired driving fatalities increased by 3.2 percent, from 9,943 in 2014 to 10,265 in 2015
Vehicle miles traveled (VMT) increased by 3.5 percent from 2014 to 2015, the largest increase since 1992, nearly 25 years ago.
See TRAFFIC SAFETY FACTS for more detail on the above findings."
World of Warcraft Avatar History,Track the players of this popular online game,Myles O'Neill,51,"Version 3,2016-06-14|Version 2,2016-05-21|Version 1,2016-05-20","games and toys
video games",CSV,614 MB,CC0,"22,550 views","1,634 downloads",35 kernels,5 topics,https://www.kaggle.com/mylesoneill/warcraft-avatar-history,"Overview
The World of Warcraft Avatar History Dataset is a collection of records that detail information about player characters in the game over time. It includes information about their character level, race, class, location, and social guild. The Kaggle version of this dataset includes only the information from 2008 (and the dataset in general only includes information from the 'Horde' faction of players in the game from a single game server).
Full Dataset Source and Information: http://mmnet.iis.sinica.edu.tw/dl/wowah/
Code used to clean the data: https://github.com/myles-oneill/WoWAH-parser
Ideas for Using the Dataset
From the perspective of game system designers, players' behavior is one of the most important factors they must consider when designing game systems. To gain a fundamental understanding of the game play behavior of online gamers, exploring users' game play time provides a good starting point. This is because the concept of game play time is applicable to all genres of games and it enables us to model the system workload as well as the impact of system and network QoS on users' behavior. It can even help us predict players' loyalty to specific games.
Open Questions
Understand user gameplay behavior (game sessions, movement, leveling)
Understand user interactions (guilds)
Predict players unsubscribing from the game based on activity
What are the most popular zones in WoW, what level players tend to inhabit each?
Wrath of the Lich King
An expansion to World of Warcraft, ""Wrath of the Lich King"" (Wotlk) was released on November 13, 2008. It introduced new zones for players to go to, a new character class (the death knight), and a new level cap of 80 (up from 70 previously). This event intersects nicely with the dataset and is probably interesting to investigate.
Map
This dataset doesn't include a shapefile (if you know of one that exists, let me know!) to show where the zones the dataset talks about are. Here is a list of zones an information from this version of the game, including their recommended levels: http://wowwiki.wikia.com/wiki/Zones_by_level_(original) .
Update (Version 3): dmi3kno has generously put together some supplementary zone information files which have now been included in this dataset. Some notes about the files:
Note that some zone names contain Chinese characters. Unicode names are preserved as a key to the original dataset. What this addition will allow is to understand properties of the zones a bit better - their relative location to each other, competititive properties, type of gameplay and, hopefully, their contribution to character leveling. Location coordinates contain some redundant (and possibly duplicate) records as they are collected from different sources. Working with uncleaned location coordinate data will allow users to demonstrate their data wrangling skills (both working with strings and spatial data)."
"18,393 Pitchfork Reviews","Pitchfork reviews from Jan 5, 1999 to Jan 8, 2017",Nolan Conaway,51,"Version 1,2017-01-13","critical theory
music",SQLite,80 MB,Other,"8,565 views",928 downloads,13 kernels,,https://www.kaggle.com/nolanbconaway/pitchfork-data,"Context
Pitchfork is a music-centric online magazine. It was started in 1995 and grew out of independent music reviewing into a general publication format, but is still famed for its variety music reviews. I scraped over 18,000 Pitchfork reviews (going back to January 1999). Initially, this was done to satisfy a few of my own curiosities, but I bet Kagglers can come up with some really interesting analyses!
Content
This dataset is provided as a sqlite database with the following tables: artists, content, genres, labels, reviews, years. For column-level information on specific tables, refer to the Metadata tab.
Inspiration
Do review scores for individual artists generally improve over time, or go down?
How has Pitchfork's review genre selection changed over time?
Who are the most highly rated artists? The least highly rated artists?
Acknowledgements
Gotta love Beautiful Soup!"
Retail Data Analytics,Historical sales data from 45 stores,Manjeet Singh,51,"Version 2,2017-09-01|Version 1,2017-09-01",,CSV,13 MB,CC0,"18,458 views","2,866 downloads",4 kernels,,https://www.kaggle.com/manjeetsingh/retaildataset,"Context
The Challenge - One challenge of modeling retail data is the need to make decisions based on limited history. Holidays and select major events come once a year, and so does the chance to see how strategic decisions impacted the bottom line. In addition, markdowns are known to affect sales – the challenge is to predict which departments will be affected and to what extent.
Content
You are provided with historical sales data for 45 stores located in different regions - each store contains a number of departments. The company also runs several promotional markdown events throughout the year. These markdowns precede prominent holidays, the four largest of which are the Super Bowl, Labor Day, Thanksgiving, and Christmas. The weeks including these holidays are weighted five times higher in the evaluation than non-holiday weeks.
Within the Excel Sheet, there are 3 Tabs – Stores, Features and Sales
Stores
Anonymized information about the 45 stores, indicating the type and size of store
Features
Contains additional data related to the store, department, and regional activity for the given dates.
Store - the store number
Date - the week
Temperature - average temperature in the region
Fuel_Price - cost of fuel in the region
MarkDown1-5 - anonymized data related to promotional markdowns. MarkDown data is only available after Nov 2011, and is not available for all stores all the time. Any missing value is marked with an NA
CPI - the consumer price index
Unemployment - the unemployment rate
IsHoliday - whether the week is a special holiday week
Sales
Historical sales data, which covers to 2010-02-05 to 2012-11-01. Within this tab you will find the following fields:
Store - the store number
Dept - the department number
Date - the week
Weekly_Sales -  sales for the given department in the given store
IsHoliday - whether the week is a special holiday week
The Task
Predict the department-wide sales for each store for the following year
Model the effects of markdowns on holiday weeks
Provide recommended actions based on the insights drawn, with prioritization placed on largest business impact"
Health Nutrition and Population Statistics,State of human health across the world,World Bank,51,"Version 1,2016-11-18","nutrition
health
demographics",CSV,43 MB,Other,"19,575 views","2,935 downloads",22 kernels,,https://www.kaggle.com/theworldbank/health-nutrition-and-population-statistics,"Context
HealthStats provides key health, nutrition and population statistics gathered from a variety of international sources. Themes include population dynamics, nutrition, reproductive health, health financing, medical resources and usage, immunization, infectious diseases, HIV/AIDS, DALY, population projections and lending. HealthStats also includes health, nutrition and population statistics by wealth quintiles.
Content
This dataset includes 345 indicators, such as immunization rates, malnutrition prevalence, and vitamin A supplementation rates across 263 countries around the world. Data was collected on a yearly basis from 1960-2016.
Inspiration
In your opinion, what are some of the more surprising indicators? Are there any you would consider adding?
Is there a relationship between condom use and rates of children born with HIV? How do these rates compare over time?
Which countries have the highest consumption of iodized salt? Has this indicator changed over time, and if so, in which countries? Are there any other indicators that seem to correlate with this one?
Acknowledgements
Data was acquired from the World Bank, and can be accessed in multiple formats here."
World Cities Database,A database of coordinates for countries and cities,Max Mind,51,"Version 3,2017-08-24|Version 2,2017-08-24|Version 1,2017-08-18","world
cities
countries",Other,157 MB,Other,"8,575 views","1,045 downloads",5 kernels,0 topics,https://www.kaggle.com/max-mind/world-cities-database,"Context
This dataset is meant to be used with other datasets that have features like country and city but no latitude/longitude. It is simply a list of cities in the world. Being able to put cities on a map will help people tell their stories more effectively. Another way to think about it is that you can use this make more pretty graphs!
Content
Fields:
city
region
country
population
latitude
longitude
Acknowledgements
These data come from Maxmind.com and have not been altered. The original source can be found by clicking here
Additionally, the Maxmind sharing license has been included.
Inspiration
I wanted to analyze a dataset and make a map, but I was only given a city name without any latitude or longotude coordinates. I found this dataset very helpful and I hoe you do too!"
Customer Support on Twitter,Over 3 million tweets and replies from the biggest brands on Twitter,Thought Vector,51,"Version 10,2017-12-04|Version 9,2017-11-30|Version 8,2017-11-28|Version 7,2017-11-22|Version 6,2017-11-14|Version 5,2017-11-10|Version 4,2017-11-09|Version 3,2017-11-08|Version 2,2017-11-07|Version 1,2017-11-07","business
communication
linguistics
twitter",Other,167 MB,CC4,"10,715 views",708 downloads,7 kernels,,https://www.kaggle.com/thoughtvector/customer-support-on-twitter,"The Customer Support on Twitter dataset is a large, modern corpus of tweets and replies to aid innovation in natural language understanding and conversational models, and for study of modern customer support practices and impact.
Context
Natural language remains the densest encoding of human experience we have, and innovation in NLP has accelerated to power understanding of that data, but the datasets driving this innovation don't match the real language in use today. The Customer Support on Twitter dataset offers a large corpus of modern English (mostly) conversations between consumers and customer support agents on Twitter, and has three important advantages over other conversational text datasets:
Focused - Consumers contact customer support to have a specific problem solved, and the manifold of problems to be discussed is relatively small, especially compared to unconstrained conversational datasets like the reddit Corpus.
Natural - Consumers in this dataset come from a much broader segment than those in the Ubuntu Dialogue Corpus and have much more natural and recent use of typed text than the Cornell Movie Dialogs Corpus.
Succinct - Twitter's brevity causes more natural responses from support agents (rather than scripted), and to-the-point descriptions of problems and solutions. Also, its convenient in allowing for a relatively low message limit size for recurrent nets.
Inspiration
The size and breadth of this dataset inspires many interesting questions:
Can we predict company responses? Given the bounded set of subjects handled by each company, the answer seems like yes!
Do requests get stale? How quickly do the best companies respond, compared to the worst?
Can we learn high quality dense embeddings or representations of similarity for topical clustering?
How does tone affect the customer support conversation? Does saying sorry help?
Can we help companies identify new problems, or ones most affecting their customers?
Content
The dataset is a CSV, where each row is a tweet. The different columns are described below. Every conversation included has at least one request from a consumer and at least one response from a company. Which user IDs are company user IDs can be calculated using the inbound field.
tweet_id
A unique, anonymized ID for the Tweet. Referenced by response_tweet_id and in_response_to_tweet_id.
author_id
A unique, anonymized user ID. @s in the dataset have been replaced with their associated anonymized user ID.
inbound
Whether the tweet is ""inbound"" to a company doing customer support on Twitter. This feature is useful when re-organizing data for training conversational models.
created_at
Date and time when the tweet was sent.
text
Tweet content. Sensitive information like phone numbers and email addresses are replaced with mask values like __email__.
response_tweet_id
IDs of tweets that are responses to this tweet, comma-separated.
in_response_to_tweet_id
ID of the tweet this tweet is in response to, if any.
Contributing
Know of other brands the dataset should include? Found something that needs to be fixed? Start a discussion, or email me directly at $FIRSTNAME@$LASTNAME.com!
Acknowledgements
A huge thank you to my friends who helped bootstrap the list of companies that do customer support on Twitter! There are many rocks that would have been left un-turned were it not for your suggestions!
Relevant Resources
NLTK - casual_tokenize for social media text tokenizing, vader sentiment analysis for social media text
SciKit Learn - BoW Count Vectorizer, Multinomial Naive Bayes Classifier
Topic Modeling via Phrase detection with gensim
facebook research - fastText text classifier"
Complete Historical Cryptocurrency Financial Data,Top 200 Cryptocurrencies by Marketcap,pmohun,51,"Version 2,2018-02-12|Version 1,2018-01-05","business
finance
money
internet",CSV,2 MB,CC0,"5,077 views",843 downloads,,,https://www.kaggle.com/philmohun/cryptocurrency-financial-data,"Context
Recent growing interest in cryptocurrencies, specifically as a speculative investment vehicle, has sparked global conversation over the past 12 months. Although this data is available across various sites, there is a lack of understanding as to what is driving the exponential rise of many individual currencies. This data set is intended to be a starting point for a detailed analysis into what is driving price action, and what can be done to predict future movement.
Content
Consolidated financial information for the top 200 cryptocurrencies by marketcap. Pulled from CoinMarketCap.com. Attributes include:
Currency name (e.g. bitcoin)
Date
Open
High
Low
Close
Volume
Marketcap
Inspiration
For the past few months I have been searching for a reliable source for historical price information related to cryptocurrencies. I wasn't able to find anything that I could use to my liking, so I built my own data set.
I've written a small script that scrapes historical price information for the top 200 coins by market cap as listed on CoinMarketCap.com.
I plan to run some basic analysis on it to answer questions that I have a ""gut"" feeling about, but no quantitative evidence (yet!).
Questions such as:
What is the correlation between bitcoin and alt coin prices?
What is the average age of the top 10 coins by market cap?
What day of the week is best to buy/sell?
Which coins in the top two hundred are less than 6 months old?
Which currencies are the most volatile?
What the hell happens when we go to bed and Asia starts trading?
Feel free to use this for your own purposes! I just ask that you share your results with the group when complete. Happy hunting!"
The Gravitational Waves Discovery Data,The GW150914 Gravitational Waves event data,Elena Cuoco,50,"Version 1,2016-06-09","physics
space",Other,10 MB,Other,"17,399 views","1,410 downloads",12 kernels,2 topics,https://www.kaggle.com/elenacuoco/the-gravitational-waves-discovery-data,"On February 11th 2016 LIGO-Virgo collaboration gave the announce of the discovery of Gravitational Waves, just 100 years after the Einstein’s paper on their prediction. The LIGO Scientific Collaboration (LSC) and the Virgo Collaboration prepared a web page to inform the broader community about a confirmed astrophysical event observed by the gravitational-wave detectors, and to make the data around that time available for others to analyze: https://losc.ligo.org/events/GW150914/
You can find much more information on the LOSC web site, and a good starting tutorial at the following link:
https://losc.ligo.org/tutorial00/
These data sets contain 32 secs of data sampled at 4096Hz an 16384Hz around the GW event detected on 14/09/2015.
Longer sets of data can be downloaded here
https://losc.ligo.org/s/events/GW150914/H-H1_LOSC_4_V1-1126257414-4096.hdf5
https://losc.ligo.org/s/events/GW150914/L-L1_LOSC_4_V1-1126257414-4096.hdf5
https://losc.ligo.org/s/events/GW150914/H-H1_LOSC_16_V1-1126257414-4096.hdf5
https://losc.ligo.org/s/events/GW150914/L-L1_LOSC_16_V1-1126257414-4096.hdf5
How to acknowledge use of this data: If your research used data from one of the data releases, please cite as:
LIGO Scientific Collaboration, ""LIGO Open Science Center release of S5"", 2014, DOI 10.7935/K5WD3XHR
LIGO Scientific Collaboration, ""LIGO Open Science Center release of S6"", 2015, DOI 10.7935/K5RN35SD
LIGO Scientific Collaboration, ""LIGO Open Science Center release of GW150914"", 2016, DOI10.7935/K5MW2F23
and please include the statement ""This research has made use of data, software and/or web tools obtained from the LIGO Open Science Center (https://losc.ligo.org), a service of LIGO Laboratory and the LIGO Scientific Collaboration. LIGO is funded by the U.S. National Science Foundation.""
If you would also like to cite a published paper, M Vallisneri et al. ""The LIGO Open Science Center"", proceedings of the 10th LISA Symposium, University of Florida, Gainesville, May 18-23, 2014; also arxiv:1410.4839
Publications We request that you let the LOSC team know if you publish (or intend to publish) a paper using data released from this site. If you would like, we may be able to review your work prior to publication, as we do for our colleagues in the LIGO Scientific Collaboration. Credits LOSC Development: The LOSC Team and The LIGO Scientific Collaboration
The data products made available through the LOSC web service are created and maintained by LIGO Lab and the LIGO Scientific Collaboration. The development of this web page was a team effort, with all members of the LOSC team making contributions in most areas. In addition to the team members listed below, a large number of individuals in the LIGO Scientific Collaboration have contributed content and advice. The LOSC team includes:
Alan Weinstein: LOSC Director
Roy Williams: LOSC Developer, web services and data base architecture
Jonah Kanner: LOSC Developer, tutorials, documentation, data set curation
Michele Vallisneri: LOSC Developer, data quality curation
Branson Stephens: LOSC Developer, event database and web site architecture
Please send any comments, questions, or concerns to: losc@ligo.org"
Drosophila Melanogaster Genome,Explore the annotated genome of the common fruit fly,Myles O'Neill,50,"Version 1,2016-05-26","biology
medicine",CSV,460 MB,CC0,"12,357 views",668 downloads,21 kernels,3 topics,https://www.kaggle.com/mylesoneill/drosophila-melanogaster-genome,"Drosophila Melanogaster
Drosophila Melanogaster, the common fruit fly, is a model organism which has been extensively used in entymological research. It is one of the most studied organisms in biological research, particularly in genetics and developmental biology.
When its not being used for scientific research, D. melanogaster is a common pest in homes, restaurants, and anywhere else that serves food. They are not to be confused with Tephritidae flys (also known as fruit flys).
https://en.wikipedia.org/wiki/Drosophila_melanogaster
About the Genome
This genome was first sequenced in 2000. It contains four pairs of chromosomes (2,3,4 and X/Y). More than 60% of the genome appears to be functional non-protein-coding DNA.
The genome is maintained and frequently updated at FlyBase. This dataset is sourced from the UCSC Genome Bioinformatics download page. It uses the August 2014 version of the D. melanogaster genome (dm6, BDGP Release 6 + ISO1 MT). http://hgdownload.soe.ucsc.edu/downloads.html#fruitfly
Files were modified by Kaggle to be a better fit for analysis on Scripts. This primarily involved turning files into CSV format, with a header row, as well as converting the genome itself from 2bit format into a FASTA sequence file.
Bioinformatics
Genomic analysis can be daunting to data scientists who haven't had much experience with bioinformatics before. We have tried to give basic explanations to each of the files in this dataset, as well as links to further reading on the biological basis for each. If you haven't had the chance to study much biology before, some light reading (ie wikipedia) on the following topics may be helpful to understand the nuances of the data provided here: Genetics, Genomics (Sequencing/Genome Assembly), Chromosomes, DNA, RNA (mRNA/miRNA), Genes, Alleles, Exons, Introns, Transcription, Translation, Peptides, Proteins, Gene Regulation, Mutation, Phylogenetics, and SNPs.
Of course, if you've got some idea of the basics already - don't be afraid to jump right in!
Learning Bioinformatics
There are a lot of great resources for learning bioinformatics on the web. One cool site is Rosalind - a platform that gives you bioinformatic coding challenges to complete. You can use Kaggle Scripts on this dataset to easily complete the challenges on Rosalind (and see Myles' solutions here if you get stuck). We have set up Biopython on Kaggle's docker image which is a great library to help you with your analyses. Check out their tutorial here and we've also created a python notebook with some of the tutorial applied to this dataset as a reference.
Files in this Dataset
Drosophila Melanogaster Genome
genome.fa
The assembled genome itself is presented here in FASTA format. Each chromosome is a different sequence of nucleotides. Repeats from RepeatMasker and Tandem Repeats Finder (with period of 12 or less) are show in lower case; non-repeating sequence is shown in upper case.
Meta Information
There are 3 additional files with meta information about the genome.
meta-cpg-island-ext-unmasked.csv
This file contains descriptive information about CpG Islands in the genome.
https://en.wikipedia.org/wiki/CpG_site
meta-cytoband.csv
This file describes the positions of cytogenic bands on each chromosome.
https://en.wikipedia.org/wiki/Cytogenetics
meta-simple-repeat.csv
This file describes simple tandem repeats in the genome.
https://en.wikipedia.org/wiki/Repeated_sequence_(DNA) https://en.wikipedia.org/wiki/Tandem_repeat
Drosophila Melanogaster mRNA Sequences
Messenger RNA (mRNA) is an intermediate molecule created as part of the cellular process of converting genomic information into proteins. Some mRNA are never translated into proteins and have functional roles in the cell on their own. Collectively, organism mRNA information is known as a Transcriptome. mRNA files included in this dataset give insight into the activity of genes in the organism.
https://en.wikipedia.org/wiki/Messenger_RNA
mrna-genbank.fa
This file includes all mRNA sequences from GenBank associated with Drosophila Melanogaster.
http://www.ncbi.nlm.nih.gov/genbank/
mrna-refseq.fa
This file includes all mRNA sequences from RefSeq associated with Drosophila Melanogaster.
http://www.ncbi.nlm.nih.gov/refseq/
Gene Predictions
A gene is a segment of DNA on the genome which, through mRNA, is used to create proteins in the organism. Knowing which parts of DNA are coding (genes) or non-coding is difficult, and a number of different systems for prediction exist. This dataset includes a number of different gene prediction systems applied to the drosophila melanogaster genome.
https://en.wikipedia.org/wiki/Gene_prediction
genes-augustus.csv
AUGUSTUS is a piece of software that predicts genes ab initio using Hidden Markov Models. http://www.ncbi.nlm.nih.gov/pmc/articles/PMC441517/
genes-genscan.csv
GENSCAN is an older ab initio software for predicting genes. http://genes.mit.edu/GENSCANinfo.html
genes-ensembl.csv
ensembl-gtp.csv
ensembl-pep.csv
ensembl-source.csv
ensembl-to-gene-name.csv
Ensembl provides gene annotation generated by their software Genebuild. This process combines automatic annotation alongside manual curation. http://uswest.ensembl.org/info/genome/genebuild/genome_annotation.html
We have also included some supplementary files for these, including predicted protein peptide sequences for each predicted gene.
genes-refseq.csv
genes-xeno-refseq.csv
refseq-link.csv
refseq-summary.csv
We have included two RefSeq gene predictions in this dataset. The first is based solely on information from the drosophila melanogaster genome. The second (genes-xeno-refseq.csv) uses genes from other organisms as a basis for predicting genes in drosophila melanogaster.
RefSeq RNAs were aligned against the D. melanogaster genome using blat; those with an alignment of less than 15% were discarded. When a single RNA aligned in multiple places, the alignment having the highest base identity was identified. Only alignments having a base identity level within 0.1% of the best and at least 96% base identity with the genomic sequence were kept.
We have also included supplementary files for these which include information about the genes that have been identified.
http://www.ncbi.nlm.nih.gov/refseq/
What can you do with this data?
Genomic data is the foundation of bioinformatics, and there is an incredible array of things you can do with this data. A good place to start is to look at some of the meta supplementary files alongside the genomic sequence itself.
We have a number of different gene prediction systems in the dataset, how do they compare to each other? How do they compare to the mRNA data?
Working back from the refseq-summary.csv file, you can look at genes that code for particular proteins - can you find these genes in the genome?
How much of the genome codes for the mRNA's found in our mRNA data? Of the mRNA's we have, how many map to the predicted genes and the predicted peptided sequence data? How much of the mRNA seems to be protein-coding vs how much looks like it is miRNA? Can you find pre-mRNA or splice variants within the mRNA data? Does meta information like cytogenic bands or CpG sites correspond with splice variants or a lack of mRNA altogether?
Those are just some of many ideas that could get you started.
Looking for Feedback
This is the first genomic dataset on Kaggle and we are looking for feedback from our community about how interesting this dataset is to them, or if there are ways we could improve it to better suit analysis. Please post suggestions for supplementary data, future genomes we could host, bioinformatics packages we should include on scripts, and any other feedback on the dataset forum."
All the news,"143,000 articles from 15 American publications",Andrew Thompson,50,"Version 4,2017-08-20|Version 3,2017-08-17|Version 2,2017-08-17|Version 1,2017-08-16",journalism,CSV,639 MB,Other,"9,357 views","1,170 downloads",5 kernels,5 topics,https://www.kaggle.com/snapcrack/all-the-news,"Context
I wanted to see how articles clustered together if the articles were rendered into document-term matrices---would there be greater affinity among political affiliations, or medium, subject matter, etc. The data was scraped using BeautifulSoup and stored in Sqlite, but I've chopped it up into three separate CSVs here, because the entire Sqlite database came out to about 1.2 gb, beyond Kaggle's max.
Content
Each row contains:
an id for the Sqlite database
author name
full date
month
year
title
publication name
article url (not available for all articles)
full article content
The publications include the New York Times, Breitbart, CNN, Business Insider, the Atlantic, Fox News, Talking Points Memo, Buzzfeed News, National Review, New York Post, the Guardian, NPR, Reuters, Vox, and the Washington Post. Sampling wasn't quite scientific; I chose publications based on my familiarity of the domain and tried to get a range of political alignments, as well as a mix of print and digital publications. By count, the publications break down accordingly:
It's not entirely even---this was something of a collect-it-all approach, and some sites are more prolific than others, and some have data that maintains integrity after scraping more easily than others.
For each publication, I used archive.org to grab the past year-and-a-half of either home-page headlines or RSS feeds and ran those links through the scraper. That is, the articles are not the product of scraping an entire site, but rather their more prominently placed articles. For example, CNN's articles from 5/6/16 were what appeared on the homepage of CNN.com proper, not everything within the CNN.com domain. Vox's articles from 5/6/16 were everything that appeared in the Vox RSS reader. on 5/6/16, and so on. RSS readers are a breeze to scrape, and so I used them when possible, but not every publication uses them or makes them easy to find.
The data primarily falls between the years of 2016 and July 2017, although there is a not-insignificant number of articles from 2015, and a possibly insignificant number from before then.
A note: there are some stray spaces between non-word characters at times as well as some other minor blemishes and imperfections here and there, the result of cleaning a very messy dataset.
Acknowledgements
Thanks mostly go to the maesters of Stack Overflow."
Zoo Animal Classification,Use Machine Learning Methods to Correctly Classify Animals Based Upon Attributes,UCI Machine Learning,50,"Version 1,2016-12-25",animals,CSV,5 KB,ODbL,"21,525 views","2,660 downloads",41 kernels,,https://www.kaggle.com/uciml/zoo-animal-classification,"This dataset consists of 101 animals from a zoo. There are 16 variables with various traits to describe the animals. The 7 Class Types are: Mammal, Bird, Reptile, Fish, Amphibian, Bug and Invertebrate
The purpose for this dataset is to be able to predict the classification of the animals, based upon the variables. It is the perfect dataset for those who are new to learning Machine Learning.
zoo.csv
Attribute Information: (name of attribute and type of value domain)
animal_name: Unique for each instance
hair Boolean
feathers Boolean
eggs Boolean
milk Boolean
airborne Boolean
aquatic Boolean
predator Boolean
toothed Boolean
backbone Boolean
breathes Boolean
venomous Boolean
fins Boolean
legs Numeric (set of values: {0,2,4,5,6,8})
tail Boolean
domestic Boolean
catsize Boolean
class_type Numeric (integer values in range [1,7])
class.csv
This csv describes the dataset
Class_Number Numeric (integer values in range [1,7])
Number_Of_Animal_Species_In_Class Numeric
Class_Type character -- The actual word description of the class
Animal_Names character -- The list of the animals that fall in the category of the class
Acknowledgements
UCI Machine Learning: https://archive.ics.uci.edu/ml/datasets/Zoo
Source Information -- Creator: Richard Forsyth -- Donor: Richard S. Forsyth 8 Grosvenor Avenue Mapperley Park Nottingham NG3 5DX 0602-621676 -- Date: 5/15/1990
Inspiration
What are the best machine learning ensembles/methods for classifying these animals based upon the variables given?"
515K Hotel Reviews Data in Europe,Can you make your trip more cozy by using data science?,Jason Liu,49,"Version 1,2017-08-22","life
leisure
hotels
+ 2 more...",CSV,227 MB,CC0,"9,762 views","1,312 downloads",10 kernels,3 topics,https://www.kaggle.com/jiashenliu/515k-hotel-reviews-data-in-europe,"Acknowledgements
The data was scraped from Booking.com. All data in the file is publicly available to everyone already. Data is originally owned by Booking.com. Please contact me through my profile if you want to use this dataset somewhere else.
Data Context
This dataset contains 515,000 customer reviews and scoring of 1493 luxury hotels across Europe. Meanwhile, the geographical location of hotels are also provided for further analysis.
Data Content
The csv file contains 17 fields. The description of each field is as below:
Hotel_Address: Address of hotel.
Review_Date: Date when reviewer posted the corresponding review.
Average_Score: Average Score of the hotel, calculated based on the latest comment in the last year.
Hotel_Name: Name of Hotel
Reviewer_Nationality: Nationality of Reviewer
Negative_Review: Negative Review the reviewer gave to the hotel. If the reviewer does not give the negative review, then it should be: 'No Negative'
Review_Total_Negative_Word_Counts: Total number of words in the negative review.
Positive_Review: Positive Review the reviewer gave to the hotel. If the reviewer does not give the negative review, then it should be: 'No Positive'
Review_Total_Positive_Word_Counts: Total number of words in the positive review.
Reviewer_Score: Score the reviewer has given to the hotel, based on his/her experience
Total_Number_of_Reviews_Reviewer_Has_Given: Number of Reviews the reviewers has given in the past.
Total_Number_of_Reviews: Total number of valid reviews the hotel has.
Tags: Tags reviewer gave the hotel.
days_since_review: Duration between the review date and scrape date.
Additional_Number_of_Scoring: There are also some guests who just made a scoring on the service rather than a review. This number indicates how many valid scores without review in there.
lat: Latitude of the hotel
lng: longtitude of the hotel
In order to keep the text data clean, I removed unicode and punctuation in the text data and transform text into lower case. No other preprocessing was performed.
Inspiration
The dataset is large and informative, I believe you can have a lot of fun with it! Let me put some ideas below to futher inspire kagglers!
Fit a regression model on reviews and score to see which words are more indicative to a higher/lower score
Perform a sentiment analysis on the reviews
Find correlation between reviewer's nationality and scores.
Beautiful and informative visualization on the dataset.
Clustering hotels based on reviews
Simple recommendation engine to the guest who is fond of a special characteristic of hotel.
The idea is unlimited! Please, have a look into data, generate some ideas and leave a master kernel here! I am ready to upvote your ideas and kernels! Cheers!"
Top 100 Cryptocurrency Historical Data,Historical prices for each of the leading cryptocurrencies,Nate,49,"Version 3,2017-10-10|Version 2,2017-10-08|Version 1,2017-10-06","finance
internet",Other,5 MB,Other,"21,916 views","1,837 downloads",,,https://www.kaggle.com/natehenderson/top-100-cryptocurrency-historical-data,"This dataset contains historical prices as tracked by www.coinmarketcap.com for the top 100 cryptocurrencies by market capitalization as of September 22, 2017, and is current to that date.
Each CSV file is named by its cryptocurrency as named on www.coinmarketcap.com, with the sole exception of ""I-O Coin"" in place of I/O Coin for ease of importing.
Also accompanying the zip of the top 100 CSVs is a CSV named ""Top 100.csv"", which is a list of the top 100, ordered 1 to 100 with Bitcoin at the beginning and GridCoin at the end. The second row of this CSV is the Market Cap as of September 22, 2017.
Row descriptions - Date, string, e.g. ""Sep 22, 2017"" - Open, float (2 decimal places), e.g. 1234.00 - High, float (2 decimal places), e.g. 1234.00 - Low, float (2 decimal places), e.g. 1234.00 - Close, float (2 decimal places), e.g. 1234.00 - Volume [traded in 24 hours], string, e.g. ""1,234,567,890"" - Market Cap [Market capitalization], string, e.g. ""1,234,567,890""
This is my first dataset and I would greatly appreciate your feedback. Thanks and enjoy!"
US county-level mortality,United States Mortality Rates by County 1980-2014,IHME,49,"Version 6,2016-12-28|Version 5,2016-12-28|Version 4,2016-12-27|Version 3,2016-12-27|Version 2,2016-12-27|Version 1,2016-12-27",demographics,Other,24 MB,Other,"11,632 views","1,353 downloads",38 kernels,3 topics,https://www.kaggle.com/IHME/us-countylevel-mortality,"Context
IHME United States Mortality Rates by County 1980-2014: National - All. (Deaths per 100,000 population)
To quickly get started creating maps, like the one below, see the Quick Start R kernel.
How the Dataset was Created
This Dataset was created from the Excel Spreadsheet, which can be found in the download. Or, you can view the source here. If you take a look at the row for United States, for the column Mortality Rate, 1980*, you'll see the set of numbers 1.52 (1.44, 1.61). Numbers in parentheses are 95% uncertainty. The 1.52 is an age-standardized mortality rate for both sexes combined (deaths per 100,000 population).
In this Dataset 1.44 will be placed in the named column Mortality Rage, 1989 (Min)* and 1.61 is in column named Mortality Rate, 1980 (Max)* . For information on how these Age-standardized mortality rates were calculated, see the December JAMA 2016 article, which you can download for free.
Reference
JAMA Full Article
Video Describing this Study (Short and this is worth viewing)
Data Resources
How Americans Die May Depend On Where They Live, by Anna Maria Barry-Jester (FiveThirtyEight)
Interactive Map from healthdata.org
IHME Data
Acknowledgements
This Dataset was provided by IHME
Institute for Health Metrics and Evaluation 2301 Fifth Ave., Suite 600, Seattle, WA 98121, USA Tel: +1.206.897.2800 Fax: +1.206.897.2899 © 2016 University of Washington"
Health Analytics,26 health indicators (642 variables) from 9 states and 284 districts of India,Rajanand Ilangovan / இராஜ்ஆனந்த் இளங்கோவன்,48,"Version 1,2017-08-10","india
public health
health",CSV,2 MB,CC4,"16,860 views","1,787 downloads",,,https://www.kaggle.com/rajanand/key-indicators-of-annual-health-survey,"Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks.
Context
India - Annual Health Survey(AHS) 2012-13:
The survey was conducted in Empowered Action Group (EAG) states Uttarakhand, Rajasthan, Uttar Pradesh, Bihar, Jharkhand, Odisha, Chhattisgarh & Madhya Pradesh and Assam. These nine states, which account for about 48 percent of the total population, 59 percent of Births, 70 percent of Infant Deaths, 75 percent of Under 5 Deaths and 62 percent of Maternal Deaths in the country, are the high focus States in view of their relatively higher fertility and mortality.
A representative sample of about 21 million population and 4.32 million households were covered 20k+ sample units which is spread across rural and urban area of these 9 states.
The objective of the AHS is to yield a comprehensive, representative and reliable dataset on core vital indicators including composite ones like Infant Mortality Rate, Maternal Mortality Ratio and Total Fertility Rate along with their co-variates (process and outcome indicators) at the district level and map the changes therein on an annual basis. These benchmarks would help in better and holistic understanding and timely monitoring of various determinants on well-being and health of population particularly Reproductive and Child Health. Source
Content
This dataset contains the data about the below 26 key indicators.
AA. Sample Particulars
Sample Units
Households
Population
Ever Married Women (aged 15-49 years)
Currently Married Women (aged 15-49 years)
Children 12-23 months
BB. Household Characteristics
Average Household Size
SC
ST
All
Population below age 15 years (%)
Dependency Ratio
Currently Married Illiterate Women aged 15-49 years (%)
CC. Sex Ratio
Sex Ratio at Birth
Sex Ratio (0- 4 years)
Sex Ratio (All ages)
DD. Effective Literacy Rate
EE. Marriage
Marriages among Females below legal age (18 years) (%)
Marriages among Males below legal age (21 years) (%)
Currently Married Women aged 20-24 years married before legal age (18 years) (%)
Currently Married Men aged 25-29 years married before legal age (21 years) (%)
Mean age at Marriage# - Male
Mean age at Marriage# - Female
FF. Schooling Status
Children currently attending school (Age 6-17 years) (%)
Children attended before / Drop out (Age 6-17 years) (%)
GG. Work Status
Children aged 5-14 years engaged in work (%)
Work Participation Rate (15 years and above)
HH. Disability
Prevalence of any type of Disability (Per 100,000 Population)
II. Injury
Number of Injured Persons by type of Treatment received (Per 100,000 Population)
Severe
Major
Minor
JJ. Acute Illness
Persons suffering from Acute Illness (Per 100,000 Population)
Diarrhoea/Dysentery
Acute Respiratory Infection (ARI)
Fever (All Types)
Any type of Acute Illness
Persons suffering from Acute Illness and taking treatment from Any Source (%)
Persons suffering from Acute Illness and taking treatment from Government Source (%)
KK. Chronic Illness
Having any kind of Symptoms of Chronic Illness (Per 100,000 Population)
Having any kind of Symptoms of Chronic Illness and sought Medical Care (%)
Having diagnosed for Chronic Illness (Per 100,000 Population)
Diabetes
Hypertension
Tuberculosis (TB)
Asthma / Chronic Respiratory Disease
Arthritis
Any kind of Chronic Illness
Having diagnosed for any kind of Chronic Illness and getting Regular Treatment (%)
Having diagnosed for any kind of Chronic Illness and getting Regular Treatment from Government Source (%)
LL. Fertility
Crude Birth Rate (CBR)
Natural Growth Rate
Total Fertility Rate
Women aged 20-24 reporting birth of order 2 & above (%)
Women reporting birth of order 3 & above (%)
Women with two children wanting no more children (%)
Women aged 15-19 years who were already mothers or pregnant at the time of survey (%)
Median age at first live birth of Women aged 15-49 years
Median age at first live birth of Women aged 25-49 years
Live Births taking place after an interval of 36 months (%)
Mean number of children ever born to Women aged 15-49 years
Mean number of children surviving to Women aged 15-49 years
Mean number of children ever born to Women aged 45-49 years
MM. Abortion
Pregnancy to Women aged 15-49 years resulting in abortion (%)
Women who received any ANC before abortion (%)
Women who went for Ultrasound before abortion (%)
Average Month of pregnancy at the time of abortion
Abortion performed by skilled health personnel (%)
Abortion taking place in Institution (%)
NN. Family Planning Practices (Cmw Aged 15-49 Years)
Current Usage
Any Method (%)
Any Modern Method (%)
Female Sterilization (%)
Male Sterilization (%)
Copper-T/IUD (%)
Pills (%)
Condom/Nirodh (%)
Emergency Contraceptive Pills (%)
Any Traditional Method (%)
Periodic Abstinence (%)
Withdrawal (%)
LAM (%)
OO. Unmet Need For Family Planning
Unmet need for Spacing (%)
Unmet need for Limiting (%)
Total Unmet need (%)
PP. Ante Natal Care
Currently Married Pregnant Women aged 15-49 years registered for ANC (%)
Mothers who received any Antenatal Check-up (%)
Mothers who had Antenatal Check-up in First Trimester (%)
Mothers who received 3 or more Antenatal Care (%)
Mothers who received at least one Tetanus Toxoid (TT) injection (%)
Mothers who consumed IFA for 100 days or more (%)
Mothers who had Full Antenatal Check-up (%)
Mothers who received ANC from Govt. Source (%)
Mothers whose Blood Pressure (BP) taken (%)
Mothers whose Blood taken for Hb (%)
Mothers who underwent Ultrasound (%)
QQ. Delivery Care
Institutional Delivery (%)
Delivery at Government Institution (%)
Delivery at Private Institution (%)
Delivery at Home (%)
Delivery at home conducted by skilled health personnel (%)
Safe Delivery (%)
Caesarean out of total delivery taken place in Government Institutions (%)
Caesarean out of total delivery taken place in Private Institutions (%)
RR. Post Natal Care
Less than 24 hrs. stay in institution after delivery (%)
Mothers who received Post-natal Check-up within 48 hrs. of delivery (%)
Mothers who received Post-natal Check-up within 1 week of delivery (%)
Mothers who did not receive any Post-natal Check-up (%)
New borns who were checked up within 24 hrs. of birth (%)
SS. Janani Suraksha Yojana (JSY)
Mothers who availed financial assistance for delivery under JSY (%)
Mothers who availed financial assistance for institutional delivery under JSY (%)
Mothers who availed financial assistance for Government Institutional delivery under JSY (%)
TT. Immunization, Vitamin A & Iron Supplement And Birth Weight
Children aged 12-23 months having Immunization Card (%)
Children aged 12-23 months who have received BCG (%)
Children aged 12-23 months who have received 3 doses of Polio vaccine (%)
Children aged 12-23 months who have received 3 doses of DPT vaccine (%)
Children aged 12-23 months who have received Measles vaccine (%)
Children aged 12-23 months Fully Immunized (%)
Children who have received Polio dose at birth (%)
Children who did not receive any vaccination (%)
Children (aged 6-35 months) who received at least one Vitamin A dose during last six months (%)
Children (aged 6-35 months) who received IFA tablets/syrup during last 3 months (%)
Children whose birth weight was taken (%)
Children with birth weight less than 2.5 Kg. (%)
UU. Childhood Diseases
Children suffering from Diarrhoea (%)
Children suffering from Diarrhoea who received HAF/ORS/ORT (%)
Children suffering from Acute Respiratory Infection (%)
Children suffering from Acute Respiratory Infection who sought treatment (%)
Children suffering from Fever (%)
Children suffering from Fever who sought treatment (%)
VV. Breastfeeding And Supplementation
Children breastfed within one hour of birth (%)
Children (aged 6-35 months) exclusively breastfed for at least six months (%)
Children Who Received Foods Other Than Breast Milk During First 6 Months
Water (%)
Animal/Formula Milk (%)
Semi-Solid Mashed Food (%)
Solid (Adult) Food (%)
Vegetables/Fruits (%)
Average Month By Which Children Received Foods Other Than Breast Milk
Water (%)
Animal/Formula Milk (%)
Semi-Solid Mashed Food (%)
Solid (Adult) Food (%)
Vegetables/Fruits (%)
WW. Birth Registration
Birth Registered (%)
Children Whose Birth Was Registered And Received Birth Certificate (%)
XX. Awareness On Hiv/Aids, Rti/Sti, Haf/Ors/Ort/Zinc And Ari/Pneumonia
Women Who Are Aware of:
HIV/AIDS
RTI/STI
HAF/ORS/ORT/ZINC
Danger Signs Of ARI/Pneumonia (%)
YY. Mortality (unit level data of mortality is available here)
Crude Death Rate (CDR)
Infant Mortality Rate (IMR)
Neo-Natal Mortality Rate
Under Five Mortality Rate (U5MR)
ZZ. Confidence Interval (95%) For Some Important Indicators
Crude Birth Rate - (Lower and Upper Limit)
Crude Death Rate - (Lower and Upper Limit)
Infant Mortality Rate - (Lower and Upper Limit)
Under Five Mortality Rate (U5MR) - (Lower and Upper Limit)
Sex Ratio At Birth - (Lower and Upper Limit)
Acknowledgements
Department of Health and Family Welfare, Govt. of India has published this data in Open Govt Data Platform India portal under Govt. Open Data License - India."
Paradise-Panama-Papers,Data Scientists United Against Corruption,Zeeshan-ul-hassan Usmani,48,"Version 2,2017-11-21|Version 1,2017-11-08","money
banking",CSV,134 MB,CC3,"7,055 views",520 downloads,5 kernels,3 topics,https://www.kaggle.com/zusmani/paradisepanamapapers,"Context
The Paradise Papers is a cache of some 13GB of data that contains 13.4 million confidential records of offshore investment by 120,000 people and companies in 19 tax jurisdictions (Tax Heavens - an awesome video to understand this); that was published by the International Consortium of Investigative Journalists (ICIJ) on November 5, 2017. Here is a brief video about the leak. The people include Queen Elizabeth II, the President of Columbia (Juan Manuel Santos), Former Prime Minister of Pakistan (Shaukat Aziz), U.S Secretary of Commerce (Wilbur Ross) and many more. According to an estimate by the Boston Consulting Group, the amount of money involved is around $10 trillion. The leak contains many famous companies, including Facebook, Apple, Uber, Nike, Walmart, Allianz, Siemens, McDonald’s and Yahoo.
It also contains a lot of U. S President Donald Trump allies including Rax Tillerson, Wilbur Ross, Koch Brothers, Paul Singer, Sheldon Adelson, Stephen Schwarzman, Thomas Barrack and Steve Wynn etc. The complete list of Politicians involve is avaiable here.
The Panama Papers in the cache of 38GB of data from the national corporate registry of Bahamas. It contains world’s top politicians and influential persons as head and director of offshore companies registered in Bahamas.
Offshore Leaks details 13,000 offshore accounts in a report.
I am calling all data scientists to help me stop the corruption and reveal the patterns and linkages invisible for the untrained eye.
Content
The data is the effort of more than 100 journalists from 60+ countries
The original data is available under creative common license and can be downloaded from this link.
I will keep updating the datasets with more leaks and data as it’s available
Acknowledgements
International Consortium of Investigative Journalists (ICIJ)
Paradise Papers Update
Paradise Papers data has been uploaded as released by ICIJ on Nov 21, 2017. You can find Paradise Papers zip file and six extracted files in CSV format, all starting with a prefix of Paradise. Happy Coding!
Inspiration
Some ideas worth exploring:
How many companies and individuals are there in all of the leaks data
How many countries involved
Total money involved
What is the biggest best tax heaven
Can we compare the corruption with human development index and make an argument that would correlate corruption with bad conditions in that country
Who are the biggest cheaters and where they live
What role Fortune 500 companies play in this game
I need your help to make this world corruption free in the age of NLP and Big Data"
U.S. Incomes by Occupation and Gender,Analyze gender gap and differences in industry's incomes,Jean-Phillipe,48,"Version 1,2016-09-23","gender
employment
income",CSV,31 KB,CC0,"14,922 views","2,591 downloads",21 kernels,0 topics,https://www.kaggle.com/jonavery/incomes-by-career-and-gender,"Many people say the gender gap in income levels is overstated in the United States, where some say that inequality in the labor force is a thing of the past. Is there a gender gap at all? Is it stronger in some industries than in others?
This dataset, retrieved from the Bureau of Labor Statistics, shows the median weekly incomes for 535 different occupations. The data encompasses information for all working American citizens as of January 2015. The incomes are broken into male and female statistics, preceded by the total median income when including both genders. The data has been re-formatted from the original PDF-friendly arrangement to make it easier to clean and analyze.
Analysis thus far has found that there is indeed a sizable gender gap between male and female incomes. Use of this dataset should cite the Bureau of Labor Statistics as per their copyright information:
The Bureau of Labor Statistics (BLS) is a Federal government agency and everything that we publish, both in hard copy and electronically, is in the public domain, except for previously copyrighted photographs and illustrations. You are free to use our public domain material without specific permission, although we do ask that you cite the Bureau of Labor Statistics as the source."
OSMI Mental Health in Tech Survey 2016,Data on prevalence and attitudes towards mental health among tech workers,"Open Sourcing Mental Illness, LTD",48,"Version 1,2016-11-14","mental health
employment",Other,80 MB,CC4,"12,698 views","2,090 downloads",24 kernels,0 topics,https://www.kaggle.com/osmi/mental-health-in-tech-2016,"OSMI Mental Health in Tech Survey 2016
Currently over 1400 responses, the ongoing 2016 survey aims to measure attitudes towards mental health in the tech workplace, and examine the frequency of mental health disorders among tech workers.
How Will This Data Be Used?
We are interested in gauging how mental health is viewed within the tech/IT workplace, and the prevalence of certain mental health disorders within the tech industry. The Open Sourcing Mental Illness team of volunteers will use this data to drive our work in raising awareness and improving conditions for those with mental health disorders in the IT workplace."
National Health and Nutrition Examination Survey,NHANES datasets from 2013-2014,Centers for Disease Control and Prevention,48,"Version 1,2017-01-27","healthcare
health",CSV,31 MB,Other,"14,654 views","2,146 downloads",14 kernels,2 topics,https://www.kaggle.com/cdc/national-health-and-nutrition-examination-survey,"Context
The National Health and Nutrition Examination Survey (NHANES) is a program of studies designed to assess the health and nutritional status of adults and children in the United States. The survey is unique in that it combines interviews and physical examinations. NHANES is a major program of the National Center for Health Statistics (NCHS). NCHS is part of the Centers for Disease Control and Prevention (CDC) and has the responsibility for producing vital and health statistics for the Nation.
The NHANES program began in the early 1960s and has been conducted as a series of surveys focusing on different population groups or health topics. In 1999, the survey became a continuous program that has a changing focus on a variety of health and nutrition measurements to meet emerging needs. The survey examines a nationally representative sample of about 5,000 persons each year. These persons are located in counties across the country, 15 of which are visited each year.
The NHANES interview includes demographic, socioeconomic, dietary, and health-related questions. The examination component consists of medical, dental, and physiological measurements, as well as laboratory tests administered by highly trained medical personnel.
To date, thousands of research findings have been published using the NHANES data.
Content
The 2013-2014 NHANES datasets include the following components:
Demographics dataset:
A complete variable dictionary can be found here
Examinations dataset, which contains:
Blood pressure
Body measures
Muscle strength - grip test
Oral health - dentition
Taste & smell
A complete variable dictionary can be found here
Dietary data - total nutrient intake, first day:
A complete variable dictionary can be found here
Laboratory dataset, which includes:
Albumin & Creatinine - Urine
Apolipoprotein B
Blood Lead, Cadmium, Total Mercury, Selenium, and Manganese
Blood mercury: inorganic, ethyl and methyl
Cholesterol - HDL
Cholesterol - LDL & Triglycerides
Cholesterol - Total
Complete Blood Count with 5-part Differential - Whole Blood
Copper, Selenium & Zinc - Serum
Fasting Questionnaire
Fluoride - Plasma
Fluoride - Water
Glycohemoglobin
Hepatitis A
Hepatitis B Surface Antibody
Hepatitis B: core antibody, surface antigen, and Hepatitis D antibody
Hepatitis C RNA (HCV-RNA) and Hepatitis C Genotype
Hepatitis E: IgG & IgM Antibodies
Herpes Simplex Virus Type-1 & Type-2
HIV Antibody Test
Human Papillomavirus (HPV) - Oral Rinse
Human Papillomavirus (HPV) DNA - Vaginal Swab: Roche Cobas & Roche Linear Array
Human Papillomavirus (HPV) DNA Results from Penile Swab Samples: Roche Linear Array
Insulin
Iodine - Urine
Perchlorate, Nitrate & Thiocyanate - Urine
Perfluoroalkyl and Polyfluoroalkyl Substances (formerly Polyfluoroalkyl Chemicals - PFC)
Personal Care and Consumer Product Chemicals and Metabolites
Phthalates and Plasticizers Metabolites - Urine
Plasma Fasting Glucose
Polycyclic Aromatic Hydrocarbons (PAH) - Urine
Standard Biochemistry Profile
Tissue Transglutaminase Assay (IgA-TTG) & IgA Endomyseal Antibody Assay (IgA EMA)
Trichomonas - Urine
Two-hour Oral Glucose Tolerance Test
Urinary Chlamydia
Urinary Mercury
Urinary Speciated Arsenics
Urinary Total Arsenic
Urine Flow Rate
Urine Metals
Urine Pregnancy Test
Vitamin B12
A complete data dictionary can be found here
Questionnaire dataset, which includes information on:
Acculturation
Alcohol Use
Blood Pressure & Cholesterol
Cardiovascular Health
Consumer Behavior
Current Health Status
Dermatology
Diabetes
Diet Behavior & Nutrition
Disability
Drug Use
Early Childhood
Food Security
Health Insurance
Hepatitis
Hospital Utilization & Access to Care
Housing Characteristics
Immunization
Income
Medical Conditions
Mental Health - Depression Screener
Occupation
Oral Health
Osteoporosis
Pesticide Use
Physical Activity
Physical Functioning
Preventive Aspirin Use
Reproductive Health
Sexual Behavior
Sleep Disorders
Smoking - Cigarette Use
Smoking - Household Smokers
Smoking - Recent Tobacco Use
Smoking - Secondhand Smoke Exposure
Taste & Smell
Weight History
Weight History - Youth
A complete variable dictionary can be found here
Medication dataset, which includes prescription medications:
A complete variable dictionary can be found here
Acknowledgements
Original data and additional documents related to the datasets or NHANES can be found here."
Hillary Clinton and Donald Trump Tweets,Tweets from the major party candidates for the 2016 US Presidential Election,Ben Hamner,47,"Version 1,2016-09-28","politics
internet",CSV,5 MB,Other,"25,143 views","2,923 downloads",81 kernels,3 topics,https://www.kaggle.com/benhamner/clinton-trump-tweets,"Twitter has played an increasingly prominent role in the 2016 US Presidential Election. Debates have raged and candidates have risen and fallen based on tweets.
This dataset provides ~3000 recent tweets from Hillary Clinton and Donald Trump, the two major-party presidential nominees."
2016 March ML Mania Predictions,Forecasting the 2016 NCAA Basketball Tournament,William Cukierski,47,"Version 2,2017-11-16|Version 1,2016-03-16","basketball
artificial intelligence",CSV,27 MB,CC4,"12,482 views","1,485 downloads",73 kernels,2 topics,https://www.kaggle.com/wcukierski/2016-march-ml-mania,"Kaggle’s March Machine Learning Mania competition challenged data scientists to predict winners and losers of the men's 2016 NCAA basketball tournament. This dataset contains the 1070 selected predictions of all Kaggle participants. These predictions were collected and locked in prior to the start of the tournament.
How can this data be used? You can pivot it to look at both Kaggle and NCAA teams alike. You can look at who will win games, which games will be close, which games are hardest to forecast, or which Kaggle teams are gambling vs. sticking to the data.
The NCAA tournament is a single-elimination tournament that begins with 68 teams. There are four games, usually called the “play-in round,” before the traditional bracket action starts. Due to competition timing, these games are included in the prediction files but should not be used in analysis, as it’s possible that the prediction was submitted after the play-in round games were over.
Data Description
Each Kaggle team could submit up to two prediction files. The prediction files in the dataset are in the 'predictions' folder and named according to:
TeamName_TeamId_SubmissionId.csv
The file format contains a probability prediction for every possible game between the 68 teams. This is necessary to cover every possible tournament outcome. Each team has a unique numerical Id (given in Teams.csv). Each game has a unique Id column created by concatenating the year and the two team Ids. The format is the following:
Id,Pred
2016_1112_1114,0.6
2016_1112_1122,0
...
The team with the lower numerical Id is always listed first. “Pred” represents the probability that the team with the lower Id beats the team with the higher Id. For example, ""2016_1112_1114,0.6"" indicates team 1112 has a 0.6 probability of beating team 1114.
For convenience, we have included the data files from the 2016 March Mania competition dataset in the Scripts environment (you may find TourneySlots.csv and TourneySeeds.csv useful for determining matchups, see the documentation). However, the focus of this dataset is on Kagglers' predictions."
Annotated Corpus for Named Entity Recognition,Corpus (CoNLL 2002) annotated with IOB and POS tags,Abhinav Walia,47,"Version 4,2017-09-21|Version 3,2017-07-11|Version 2,2017-04-02|Version 1,2017-03-20",linguistics,CSV,164 MB,ODbL,"16,016 views","1,705 downloads",6 kernels,4 topics,https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus,"Context: Annotated Corpus for Named Entity Recognition using GMB(Groningen Meaning Bank) corpus for entity classification with enhanced and popular features by Natural Language Processing applied to the data set.
Tip: Use Pandas Dataframe to load dataset if using Python for convenience.
Content: This is the extract from GMB corpus which is tagged, annotated and built specifically to train the classifier to predict named entities such as name, location, etc.
Number of tagged entities:
'O': 1146068', geo-nam': 58388, 'org-nam': 48034, 'per-nam': 23790, 'gpe-nam': 20680, 'tim-dat': 12786, 'tim-dow': 11404, 'per-tit': 9800, 'per-fam': 8152, 'tim-yoc': 5290, 'tim-moy': 4262, 'per-giv': 2413, 'tim-clo': 891, 'art-nam': 866, 'eve-nam': 602, 'nat-nam': 300, 'tim-nam': 146, 'eve-ord': 107, 'per-ini': 60, 'org-leg': 60, 'per-ord': 38, 'tim-dom': 10, 'per-mid': 1, 'art-add': 1
Essential info about entities:
geo = Geographical Entity
org = Organization
per = Person
gpe = Geopolitical Entity
tim = Time indicator
art = Artifact
eve = Event
nat = Natural Phenomenon
Total Words Count = 1354149 Target Data Column: ""tag""
Inspiration: This dataset is getting more interested because of more features added to the recent version of this dataset. Also, it helps to create a broad view of Feature Engineering with respect to this dataset.
Why this dataset is helpful or playful?
It might not sound so interested for earlier versions, but when you are able to pick intent and custom named entities from your own sentence with more features then, it is getting interested and helps you solve real business problems(like picking entities from Electronic Medical Records, etc)
Please, feel free to ask questions, do variations and let's play together!"
Computer Parts (CPUs and GPUs),How did computer specifications and performance evolve over time?,ilias sekkaf,47,"Version 7,2017-09-30|Version 6,2017-09-14|Version 5,2017-09-14|Version 4,2017-09-14|Version 3,2017-09-13|Version 2,2017-09-13|Version 1,2017-09-13","computers
computer architecture
internet",CSV,1 MB,Other,"9,959 views","1,530 downloads",6 kernels,,https://www.kaggle.com/iliassekkaf/computerparts,"Contents
This dataset contains detailed specifications, release dates, and release prices of computer parts.
The dataset contains two CSV files: gpus.csv for Graphics Processing Units (GPUs), and cpus.csv for Central Processing Units (CPUs). Each table has its own list of unique entries, but the list of features includes: clock speeds, maximum temperatures, display resolutions, power draws, number of threads, release dates, release prices, die size, virtualization support, and many other similar fields. For more specific column-level metadata refer to the Column Metadata.
Looking for inspiration? Try starting by reading ""Using regression to predict the GPUs of the future"".
Inspiration
How did performance over price ratio evolve over time?
How about general computing power?
Are there any manufacturers that are known for some specific range of performance & price?
Acknowledgements
The data given here belongs mainly to Intel, Game-Debate, and the companies involved in producing the part. I do not own the data I uploaded it solely for informative purposes, under their original license."
Synchronized Brainwave Dataset,Brainwave recordings from a group presented with a shared audio-visual stimulus,BioSENSE @ UC Berkeley School of Information,47,"Version 4,2016-10-27|Version 3,2016-10-25|Version 2,2016-10-25|Version 1,2016-10-22","healthcare
human-computer interaction",CSV,101 MB,CC4,"13,355 views","1,195 downloads",62 kernels,0 topics,https://www.kaggle.com/berkeley-biosense/synchronized-brainwave-dataset,"Context
EEG devices are becoming cheaper and more inconspicuous, but few applications leverage EEG data effectively, in part because there are few large repositories of EEG data. The MIDS class at the UC Berkeley School of Information is sharing a dataset collected using consumer-grade brainwave-sensing headsets, along with the software code and visual stimulus used to collect the data. The dataset includes all subjects' readings during the stimulus presentation, as well as readings from before the start and after the end of the stimulus.
Content
We presented two slightly different stimuli to two different groups. Stimuli 1 is available here, and stimuli 2 is available here.
For both stimuli, a group of about 15 people saw the stimuli at the same time, while EEG data was being collected. The stimuli each person saw is available in the session field of subject-metadata.csv. (Subjects who saw stimulus 2 left the room during stimulus 1, and vice versa).
Find the synchronized times for both stimuli in stimulus-timing.csv.
For each participant, we also anonymously collected some other metadata: (1) whether or not they had previously seen the video displayed during the stimulus (a superbowl ad), (2) gender, (3) whether or not they saw hidden icons displayed during the color counting exercise, and (4) their chosen color during the color counting exercise. All of these can be found in subject-metadata.csv.
We also collected the timing (in indra_time) of all stimulus events for both session 1 and session 2. These times are included in stimulus-times.csv.
The server receives one data packet every second from each Mindwave Mobile device, and stores the data in one row entry.
Acknowledgements
Please use the following citation if you publish your research results using this dataset or software code or stimulus file:
John Chuang, Nick Merrill, Thomas Maillart, and Students of the UC Berkeley Spring 2015 MIDS Immersion Class. ""Synchronized Brainwave Recordings from a Group Presented with a Common Audio-Visual Stimulus (May 9, 2015)."" May 2015."
Segmenting Soft Tissue Sarcomas,A challenge to automate tumor segmentation,4Quant,47,"Version 2,2016-12-09|Version 1,2016-12-09",healthcare,Other,379 MB,Other,"11,570 views","1,110 downloads",27 kernels,,https://www.kaggle.com/4quant/soft-tissue-sarcoma,"Summary
The data is a preprocessed subset of the TCIA Study named Soft Tissue Sarcoma. The data have been converted from DICOM folders of varying resolution and data types to 3D HDF5 arrays with isotropic voxel size. This should make it easier to get started and test out various approaches (NN, RF, CRF, etc) to improve segmentations.
TCIA Summary
This collection contains FDG-PET/CT and anatomical MR (T1-weighted, T2-weighted with fat-suppression) imaging data from 51 patients with histologically proven soft-tissue sarcomas (STSs) of the extremities. All patients had pre-treatment FDG-PET/CT and MRI scans between November 2004 and November 2011. (Note: date in the TCIA images have been changed in the interest of de-identification; the same change was applied across all images, preserving the time intervals between serial scans). During the follow-up period, 19 patients developed lung metastases. Imaging data and lung metastases development status were used in the following study:
Vallières, M. et al. (2015). A radiomics model from joint FDG-PET and MRI texture features for the prediction of lung metastases in soft-tissue sarcomas of the extremities. Physics in Medicine and Biology, 60(14), 5471-5496. doi:10.1088/0031-9155/60/14/5471.
Imaging data, tumor contours (RTstruct DICOM objects), clinical data and source code is available for this study. See the DOI below for more details and links to access the whole dataset. Please contact Martin Vallières (mart.vallieres@gmail.com) of the Medical Physics Unit of McGill University for any scientific inquiries about this dataset.
Acknowledgements
We would like to acknowledge the individuals and institutions that have provided data for this collection: McGill University, Montreal, Canada - Special thanks to Martin Vallières of the Medical Physics Unit
License
This collection is freely available to browse, download, and use for commercial, scientific and educational purposes as outlined in the Creative Commons Attribution 3.0 Unported License. See TCIA's Data Usage Policies and Restrictions for additional details. Questions may be directed to help@cancerimagingarchive.net.
Citation
Data Citation
Vallières, Martin, Freeman, Carolyn R., Skamene, Sonia R., & El Naqa, Issam. (2015). A radiomics model from joint FDG-PET and MRI texture features for the prediction of lung metastases in soft-tissue sarcomas of the extremities. The Cancer Imaging Archive. http://doi.org/10.7937/K9/TCIA.2015.7GO2GSKS
Publication Citation
Vallières, M., Freeman, C. R., Skamene, S. R., & Naqa, I. El. (2015, June 29). A radiomics model from joint FDG-PET and MRI texture features for the prediction of lung metastases in soft-tissue sarcomas of the extremities. Physics in Medicine and Biology. IOP Publishing. http://doi.org/10.1088/0031-9155/60/14/5471
TCIA Citation
Clark K, Vendt B, Smith K, Freymann J, Kirby J, Koppel P, Moore S, Phillips S, Maffitt D, Pringle M, Tarbox L, Prior F. The Cancer Imaging Archive (TCIA): Maintaining and Operating a Public Information Repository, Journal of Digital Imaging, Volume 26, Number 6, December, 2013, pp 1045-1057. (paper)"
"Zillow Rent Index, 2010-Present",Which city has the highest median price or price per square foot?,Zillow,47,"Version 1,2017-03-04","cities
home
real estate",CSV,10 MB,Other,"11,687 views","1,794 downloads",16 kernels,,https://www.kaggle.com/zillow/rent-index,"Context
Zillow operates an industry-leading economics and analytics bureau led by Zillow’s Chief Economist, Dr. Stan Humphries. At Zillow, Dr. Humphries and his team of economists and data analysts produce extensive housing data and analysis covering more than 500 markets nationwide. Zillow Research produces various real estate, rental and mortgage-related metrics and publishes unique analyses on current topics and trends affecting the housing market.
At Zillow’s core is our living database of more than 100 million U.S. homes, featuring both public and user-generated information including number of bedrooms and bathrooms, tax assessments, home sales and listing data of homes for sale and for rent. This data allows us to calculate, among other indicators, the Zestimate, a highly accurate, automated, estimated value of almost every home in the country as well as the Zillow Home Value Index and Zillow Rent Index, leading measures of median home values and rents.
Content
The Zillow Rent Index is the median estimated monthly rental price for a given area, and covers multifamily, single family, condominium, and cooperative homes in Zillow’s database, regardless of whether they are currently listed for rent. It is expressed in dollars and is seasonally adjusted. The Zillow Rent Index is published at the national, state, metro, county, city, neighborhood, and zip code levels.
Zillow produces rent estimates (Rent Zestimates) based on proprietary statistical and machine learning models. Within each county or state, the models observe recent rental listings and learn the relative contribution of various home attributes in predicting prevailing rents. These home attributes include physical facts about the home, prior sale transactions, tax assessment information and geographic location as well as the estimated market value of the home (Zestimate). Based on the patterns learned, these models estimate rental prices on all homes, including those not presently for rent. Because of the availability of Zillow rental listing data used to train the models, Rent Zestimates are only available back to November 2010; therefore, each ZRI time series starts on the same date.
Acknowledgements
The rent index data was calculated from Zillow's proprietary Rent Zestimates and published on its website.
Inspiration
What city has the highest and lowest rental prices in the country? Which metropolitan area is the most expensive to live in? Where have rental prices increased in the past five years and where have they remained the same? What city or state has the lowest cost per square foot?"
OpenAQ,Global Air Pollution Measurements,Open AQ,47,"Version 1,2017-12-02","pollution
bigquery",BigQuery,2 MB,CC4,"8,181 views",0 downloads,978 kernels,3 topics,https://www.kaggle.com/open-aq/openaq,"OpenAQ is an open-source project to surface live, real-time air quality data from around the world. Their “mission is to enable previously impossible science, impact policy and empower the public to fight air pollution.” The data includes air quality measurements from 5490 locations in 47 countries.
Scientists, researchers, developers, and citizens can use this data to understand the quality of air near them currently. The dataset only includes the most current measurement available for the location (no historical data).
Update Frequency: Weekly
Querying BigQuery tables
You can use the BigQuery Python client library to query tables in this dataset in Kernels. Note that methods available in Kernels are limited to querying data. Tables are at bigquery-public-data.openaq.[TABLENAME]. Fork this kernel to get started.
Acknowledgements
Dataset Source: openaq.org
Use: This dataset is publicly available for anyone to use under the following terms provided by the Dataset Source and is provided ""AS IS"" without any warranty, express or implied."
News Aggregator Dataset,Headlines and categories of 400k news stories from 2014,UCI Machine Learning,46,"Version 1,2016-11-01","news agencies
linguistics",CSV,98 MB,CC0,"22,506 views","2,284 downloads",45 kernels,2 topics,https://www.kaggle.com/uciml/news-aggregator-dataset,"This dataset contains headlines, URLs, and categories for 422,937 news stories collected by a web aggregator between March 10th, 2014 and August 10th, 2014.
News categories included in this dataset include business; science and technology; entertainment; and health. Different news articles that refer to the same news item (e.g., several articles about recently released employment statistics) are also categorized together.
Content
The columns included in this dataset are:
ID : the numeric ID of the article
TITLE : the headline of the article
URL : the URL of the article
PUBLISHER : the publisher of the article
CATEGORY : the category of the news item; one of: -- b : business -- t : science and technology -- e : entertainment -- m : health
STORY : alphanumeric ID of the news story that the article discusses
HOSTNAME : hostname where the article was posted
TIMESTAMP : approximate timestamp of the article's publication, given in Unix time (seconds since midnight on Jan 1, 1970)
Acknowledgments
This dataset comes from the UCI Machine Learning Repository. Any publications that use this data should cite the repository as follows:
Lichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.
This specific dataset can be found in the UCI ML Repository at this URL
Inspiration
What kinds of questions can we explore using this dataset? Here are a few possibilities:
can we predict the category (business, entertainment, etc.) of a news article given only its headline?
can we predict the specific story that a news article refers to, given only its headline?"
Extinct Languages,"Number of endangered languages in the world, and their likelihood of extinction",The Guardian,46,"Version 1,2016-12-07","languages
linguistics",CSV,737 KB,Other,"12,447 views","1,399 downloads",71 kernels,2 topics,https://www.kaggle.com/the-guardian/extinct-languages,"Context
A recent Guardian blog post asks: ""How many endangered languages are there in the World and what are the chances they will die out completely?"" The United Nations Education, Scientific and Cultural Organisation (UNESCO) regularly publishes a list of endangered languages, using a classification system that describes its danger (or completion) of extinction.
Content
The full detailed dataset includes names of languages, number of speakers, the names of countries where the language is still spoken, and the degree of endangerment. The UNESCO endangerment classification is as follows:
Vulnerable: most children speak the language, but it may be restricted to certain domains (e.g., home)
Definitely endangered: children no longer learn the language as a 'mother tongue' in the home
Severely endangered: language is spoken by grandparents and older generations; while the parent generation may understand it, they do not speak it to children or among themselves
Critically endangered: the youngest speakers are grandparents and older, and they speak the language partially and infrequently
Extinct: there are no speakers left
Acknowledgements
Data was originally organized and published by The Guardian, and can be accessed via this Datablog post.
Inspiration
How can you best visualize this data?
Which rare languages are more isolated (Sicilian, for example) versus more spread out? Can you come up with a hypothesis for why that is the case?
Can you compare the number of rare speakers with more relatable figures? For example, are there more Romani speakers in the world than there are residents in a small city in the United States?"
International Energy Statistics,Global energy trade & production 1990-2014,United Nations,46,"Version 2,2017-11-16|Version 1,2017-11-16","economics
energy",Other,7 MB,Other,"5,960 views",901 downloads,6 kernels,,https://www.kaggle.com/unitednations/international-energy-statistics,"Curious about the growth of wind energy? The extent to which the decline of coal is an American or international trend? Interested in using energy consumption as an alternate method of comparing national economies? This dataset has you covered.
The Energy Statistics Database contains comprehensive energy statistics on the production, trade, conversion and final consumption of primary and secondary; conventional and non-conventional; and new and renewable sources of energy.
Acknowledgements
This dataset was kindly published by the United Nations Statistics Division on the UNData site. You can find the original dataset here.
License
Per the UNData terms of use: all data and metadata provided on UNdata’s website are available free of charge and may be copied freely, duplicated and further distributed provided that UNdata is cited as the reference."
Mobile phone activity in a city,"Hourly phone calls, SMS and Internet communication of an entire city",Marco De Nadai,45,"Version 4,2016-11-11|Version 3,2016-11-11|Version 2,2016-11-10|Version 1,2016-11-10","cities
internet
telecommunications",CSV,1 GB,ODbL,"25,619 views","2,596 downloads",39 kernels,2 topics,https://www.kaggle.com/marcodena/mobile-phone-activity,"Introduction
The Mobile phone activity dataset is composed by one week of Call Details Records (CDRs) from the city of Milan and the Province of Trentino (Italy).
Description of the dataset
Every time a user engages a telecommunication interaction, a Radio Base Station (RBS) is assigned by the operator and delivers the communication through the network. Then, a new CDR is created recording the time of the interaction and the RBS which handled it. The following activities are present in the dataset:
received SMS
sent SMS
incoming calls
outgoing calls
Internet activity
In particular, Internet activity is generated each time a user starts an Internet connection or ends an Internet connection. Moreover, during the same connection a CDR is generated if the connection lasts for more than 15 min or the user transferred more than 5 MB.
The datasets is spatially aggregated in a square cells grid. The area of Milan is composed of a grid overlay of 1,000 (squares with size of about 235×235 meters. This grid is projected with the WGS84 (EPSG:4326) standard. For more details we link the original paper http://go.nature.com/2fcOX5E
The data provides CellID, CountryCode and all the aforementioned telecommunication activities aggregated every 60 minutes.
Original datasource
The Mobile phone activity dataset is a part of the Telecom Italia Big Data Challenge 2014, which is a rich and open multi-source aggregation of telecommunications, weather, news, social networks and electricity data from the city of Milan and the Province of Trentino (Italy). The original dataset has been created by Telecom Italia in association with EIT ICT Labs, SpazioDati, MIT Media Lab, Northeastern University, Polytechnic University of Milan, Fondazione Bruno Kessler, University of Trento and Trento RISE. In order to make it easy-to-use, here we provide a subset of telecommunications data that allows researchers to design algorithms able to exploit an enormous number of behavioral and social indicators. The complete version of the dataset is available at the following link: http://go.nature.com/2fz4AFr
Relevant, external, data sources
The presented datasets can be enriched by using census data provided by the Italian National Institute of Statistics (ISTAT) (http://www.istat.it/en/), a public research organization and the main provider of official statistics in Italy. The census data have been released for 1999, 2001 and 2011. The dataset (http://www.istat.it/it/archivio/104317), released in Italian, is composed of four parts: Territorial Bases (Basi Territoriali), Administrative Boundaries (Confini Amministrativi), Census Variables (Variabili Censuarie) and data about Toponymy (Dati Toponomastici).
Motivational video: https://www.youtube.com/watch?v=_d2_RWMsUKc
Relevant papers
Blondel, Vincent D., Adeline Decuyper, and Gautier Krings. ""A survey of results on mobile phone datasets analysis."" EPJ Data Science 4, no. 1 (2015): 1.
Francesco Calabrese, Laura Ferrari, and Vincent D. Blondel. 2014. Urban Sensing Using Mobile Phone Network Data: A Survey of Research. ACM Comput. Surv. 47, 2, Article 25 (November 2014), 20 pages.
Eagle, Nathan, Michael Macy, and Rob Claxton. ""Network diversity and economic development."" Science 328, no. 5981 (2010): 1029-1031.
Lenormand, Maxime, Miguel Picornell, Oliva G. Cantú-Ros, Thomas Louail, Ricardo Herranz, Marc Barthelemy, Enrique Frías-Martínez, Maxi San Miguel, and José J. Ramasco. ""Comparing and modelling land use organization in cities."" Royal Society open science 2, no. 12 (2015): 150449.
Louail, Thomas, Maxime Lenormand, Oliva G. Cantu Ros, Miguel Picornell, Ricardo Herranz, Enrique Frias-Martinez, José J. Ramasco, and Marc Barthelemy. ""From mobile phone data to the spatial structure of cities."" Scientific reports 4 (2014).
De Nadai, Marco, Jacopo Staiano, Roberto Larcher, Nicu Sebe, Daniele Quercia, and Bruno Lepri. ""The Death and Life of Great Italian Cities: A Mobile Phone Data Perspective."" WWW, 2016.
Citation
We kindly ask people who use this dataset to cite the following paper, where this aggregation comes from:
Barlacchi, Gianni, Marco De Nadai, Roberto Larcher, Antonio Casella, Cristiana Chitic, Giovanni Torrisi, Fabrizio Antonelli, Alessandro Vespignani, Alex Pentland, and Bruno Lepri. ""A multi-source dataset of urban life in the city of Milan and the Province of Trentino."" Scientific data 2 (2015)."
Magic The Gathering Cards,Analyze cards from this classic trading card game,Myles O'Neill,45,"Version 1,2016-09-27","games and toys
card games",{}JSON,53 MB,Other,"12,840 views",618 downloads,36 kernels,,https://www.kaggle.com/mylesoneill/magic-the-gathering-cards,"Magic The Gathering (MTG, or just Magic) is a trading card game first published in 1993 by Wizards of the Coast. This game has seen immense popularity and new cards are still released every few months. The strength of different cards in the game can vary wildly and as a result some cards now sell on secondary markets for as high as thousands of dollars.
MTG JSON has an excellent collection of every single Magic Card - stored in JSON data. Version 3.6 (collected September 21, 2016) of their database is provided here.
Full documentation for the data is provided here: http://mtgjson.com/documentation.html
Also, if you want to include images of the cards in your writeups, you can grab them from the official Wizards of the Coast website using the following URL:
http://gatherer.wizards.com/Handlers/Image.ashx?multiverseid=180607&type=card
Just replace the multiverse ID with the one provided in the mtgjson file."
Epicurious - Recipes with Rating and Nutrition,"Recipes from Epicurious by rating, nutritional content, and categories",HugoDarwood,45,"Version 2,2017-02-21|Version 1,2016-12-25","food and drink
nutrition",Other,86 MB,Other,"16,850 views","2,477 downloads",268 kernels,,https://www.kaggle.com/hugodarwood/epirecipes,"Context
I created this dataset to explore different factors affecting people's enjoyment of food and/or cooking!
Content
Over 20k recipes listed by recipe rating, nutritional information and assigned category (sparse). I may later upload a version binned by recipe creation date and also including recipe ingredients.
Use the 'full_format_recipes.json' file to interact with all recipe data, 'epi_r.csv' drops ingredients and directions in favour of sparse category dummies.
Acknowledgements
Recipe information lifted from: http://www.epicurious.com/recipes-menus"
Funding Successful Projects on Kickstarter,Predict if a project will get successfully funded or not using labeled data,Ashok Lathwal,45,"Version 1,2017-06-21","business
finance",CSV,57 MB,Other,"8,273 views",999 downloads,6 kernels,5 topics,https://www.kaggle.com/codename007/funding-successful-projects,"Problem Statement
Kickstarter is a community of more than 10 million people comprising of creative, tech enthusiasts who help in bringing creative project to life. Till now, more than $3 billion dollars have been contributed by the members in fueling creative projects. The projects can be literally anything – a device, a game, an app, a film etc.
Kickstarter works on all or nothing basis i.e if a project doesn’t meet it goal, the project owner gets nothing. For example: if a projects’s goal is $500. Even if it gets funded till $499, the project won’t be a success.
Recently, Kickstarter released its public data repository to allow researchers and enthusiasts like us to help them solve a problem. Will a project get fully funded ?
In this challenge, you have to predict if a project will get successfully funded or not.
Data Description
There are three files given to download: train.csv, test.csv and sample_submission.csv The train data consists of sample projects from the May 2009 to May 2015. The test data consists of projects from June 2015 to March 2017."
2016 Election Polls,Collection of Presidential Election Polls from 2015-2016,FiveThirtyEight,45,"Version 1,2016-11-03","news agencies
politics",CSV,3 MB,Other,"18,154 views","2,251 downloads",83 kernels,2 topics,https://www.kaggle.com/fivethirtyeight/2016-election-polls,"Dataset Information
This dataset is a collection of state and national polls conducted from November 2015-November 2016 on the 2016 presidential election. Data on the raw and weighted poll results by state, date, pollster, and pollster ratings are included.
Content
There are 27 variables:
cycle
branch
type
matchup
forecastdate
state:
startdate
enddate
pollster
grade
samplesize
populaion
poll_wt
rawpoll_clinton
rawpoll_trump
rawpoll_johnson
rawpoll_mcmullin
adjpoll_clinton
adjpoll_trump
adjpoll_johnson
adjpoll_mcmullin
multiversions
url
poll_id
question_id
createddate
timestamp
Inspiration
Some questions for exploring this dataset are:
What are the trends of the polls over time (by day/week/month)?
How do the trends vary by state?
What is the probability that Trump/Clinton will win the 2016 election?
Acknowledgements
The original dataset is from the FiveThirtyEight 2016 Election Forecast and can be downloaded from here. Poll results were aggregated from HuffPost Pollster, RealClearPolitics, polling firms and news reports."
MovieLens 20M Dataset,Over 20 Million Movie Ratings and Tagging Activities Since 1995,GroupLens,45,"Version 2,2016-11-08|Version 1,2016-11-07",film,CSV,885 MB,Other,"15,733 views","2,146 downloads",28 kernels,0 topics,https://www.kaggle.com/grouplens/movielens-20m-dataset,"Context
The datasets describe ratings and free-text tagging activities from MovieLens, a movie recommendation service. It contains 20000263 ratings and 465564 tag applications across 27278 movies. These data were created by 138493 users between January 09, 1995 and March 31, 2015. This dataset was generated on October 17, 2016.
Users were selected at random for inclusion. All selected users had rated at least 20 movies.
Content
No demographic information is included. Each user is represented by an id, and no other information is provided.
The data are contained in six files.
tag.csv that contains tags applied to movies by users:
userId
movieId
tag
timestamp
rating.csv that contains ratings of movies by users:
userId
movieId
rating
timestamp
movie.csv that contains movie information:
movieId
title
genres
link.csv that contains identifiers that can be used to link to other sources:
movieId
imdbId
tmbdId
genome_scores.csv that contains movie-tag relevance data:
movieId
tagId
relevance
genome_tags.csv that contains tag descriptions:
tagId
tag
Acknowledgements
The original datasets can be found here. To acknowledge use of the dataset in publications, please cite the following paper:
F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages. DOI=http://dx.doi.org/10.1145/2827872
Inspiration
Some ideas worth exploring:
Which genres receive the highest ratings? How does this change over time?
Determine the temporal trends in the genres/tagging activity of the movies released"
Stanford Question Answering Dataset,"New Reading Comprehension Dataset on 100,000+ Question-Answer Pairs",Stanford University,45,"Version 1,2016-11-15","languages
linguistics",{}JSON,34 MB,CC4,"12,777 views",884 downloads,13 kernels,0 topics,https://www.kaggle.com/stanfordu/stanford-question-answering-dataset,"Context
The Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset consisting of questions posed by crowdworkers on a set of Wikipedia articles. The answer to every question is a segment of text, or span, from the corresponding reading passage. There are 100,000+ question-answer pairs on 500+ articles.
Content
There are two files to help you get started with the dataset and evaluate your models:
*train-v1.1.json*fasdf
dev-v1.1.json
Acknowledgements
The original datasets can be found here.
Inspiration
Can you build a prediction model that can accurately predict answers to different types of questions?
You can also explore SQuAD here"
Amazon Reviews: Unlocked Mobile Phones,"More than 400,000 reviews from Amazon's unlocked mobile phone category",PromptCloud,45,"Version 1,2017-01-11","business
internet
telecommunications",CSV,126 MB,CC0,"19,623 views","2,350 downloads",93 kernels,4 topics,https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones,"Context
PromptCloud extracted 400 thousand reviews of unlocked mobile phones sold on Amazon.com to find out insights with respect to reviews, ratings, price and their relationships.
Content
Given below are the fields:
Product Title
Brand
Price
Rating
Review text
Number of people who found the review helpful
Data was acquired in December, 2016 by the crawlers build to deliver our data extraction services.
Initial Analysis
It can be accessed here: http://www.kdnuggets.com/2017/01/data-mining-amazon-mobile-phone-reviews-interesting-insights.html"
US ACS Financial Hedging Features,+30 Features To Build Models & Make Money,Golden Oak Research Group,45,"Version 4,2018-02-06|Version 3,2017-08-28|Version 2,2017-08-26|Version 1,2017-08-26","finance
demographics",Other,2 MB,Other,"4,826 views",605 downloads,3 kernels,5 topics,https://www.kaggle.com/goldenoakresearch/us-acs-mortgage-equity-loans-rent-statistics,"Background:
The underlying concept behind hedging strategies is simple, create a model, and make money doing it. The hardest part is finding the features that matter. For a more in-depth look at hedging strategies, I have attached one of my graduate papers to get you started.
Mortgage-Backed Securities
Geographic Business Investment
Real Estate Analysis
For any questions, you may reach us at research_development@goldenoakresearch.com. For immediate assistance, you may reach me on at 585-626-2965. Please Note: the number is my personal number and email is preferred
Statistical Fields:
Note: All interpolated statistical data include Mean, Median, and Standard Deviation Statistics. For more information view the variable definitions document.
Monthly Mortgage & Owner Costs: Sum of mortgage payments, home equity loans, utilities, property taxes
Monthly Owner Costs: Sum of utilities, property taxes
Gross Rent: contract rent plus the estimated average monthly cost of utilities
Household Income: sum of the householder and all other individuals +15 years who reside in the household
Family Income: Sum of incomes of all members +15 years of age related to the householder.
Location Fields:
Note: The location fields were derived from a variety of sources. The zip code, area code, and city were derived using a heuristic. All other locations used the census Geo ID to cross reference data between multiple datasets.
State Name, Abbreviation, Number: reported by the U.S. Census Bureau
County Name: reported by the U.S. Census Bureau
Location Type: Specifies Classification of location {City, Village, Town, CPD, ..., etc.}
Area Code: Defined via heuristic.
Zip Code: Defined via heuristic.
City: Defined via heuristic.
Access All 325,258 Location of Our Most Complete Database Ever:
Monetize Risk and Optimize your portfolio instantly at an unbeatable price. Don't settle. Go big and win big. Access all gross rent records and more on a scale roughly equivalent to a neighborhood, see link below:
Full Dataset: View Full Dataset
Real Estate Research: View Research"
Every song you have heard (almost)!,"Over 500,000 song lyrics, urls for over a million artists",Soumitra Agarwal,44,"Version 1,2017-08-20","languages
music
internet",CSV,601 MB,ODbL,"5,629 views",621 downloads,3 kernels,,https://www.kaggle.com/artimous/every-song-you-have-heard-almost,"Dataset for people who double on Music and Data Science
How it began
One fine day, when someone with the idea of self sufficient AI capable of writing it's own poems wanted data, he stumbled upon the idea of using songs as a source. The journey wasn't easy, since each song has it's own page with the lyrics and scraping pages one at a time (when there are over a million of them) is a slow task. I worked up some optimisations here and there. For people interested in going through the process:
GITHUB PROJECT.
Content
There is a lot to play with here, though all of it vertically. There isn't a lot of variation in the types of fields until you look deep enough.
The main dataset is split up into 2 files, each containing ~250,000 songs with their artists and lyrics. The files are titled Lyrics1 and Lyrics2. The fields include band name, song name and lyrics.
Go through the exploration scripts for how to use the lyrics data set and other interesting observations.
Another file containing urls of description pages of different artists is also provided, titled ArtistUrls.
Acknowledgements
After seeing the response for the FIFA PLAYER DATASET it was exciting as ever to get this one off.
Soon it was realised, not a lot of places exist where extracting data is straightforward.
Then one stumbles upon the perfectly indexed page : https://www.lyrics.com/. They deserve major credit for the existence of this dataset.
Inspiration
This started off as a source for some kind of intelligent poet which writes poems on it's own. It would be great to see what the artificially intelligent world has to express once it knows enough, and beautifully if at all?"
Pokemon Sun and Moon (Gen 7) Stats,Explore all 802 Pokemon from the newly released 7th Generation,Myles O'Neill,44,"Version 3,2016-11-19|Version 2,2016-11-18|Version 1,2016-11-17","games and toys
video games",CSV,2 MB,Other,"13,528 views","1,311 downloads",36 kernels,0 topics,https://www.kaggle.com/mylesoneill/pokemon-sun-and-moon-gen-7-stats,"Pokemon Sun and Moon (released November 18th, 2016) are the latest games in the widely popular Pokemon video game franchise. Pokemon games are usually released in pairs (red and blue, gold and silver, x and y, etc.) and collectively each pair that introduces new pokemon to the game is known as a Generation. Pokemon Sun and Moon are the 7th Generation, adding new pokemon to the franchise as well as adjusting the stats and movesets of some of the older pokemon. (Please note that the recently popular PokemonGo games are unrelated to the Pokemon Video Games).
This dataset contains a full set of in-game statistics for all 802 pokemon in the Sun and Moon. It also includes full information on which pokemon can learn which moves (movesets.csv), what moves can do (moves.csv), and how damage is modified by pokemon type (type-chart.csv).
Pokemon Battle Simulation
With the level of detail in the data provided here it is possible to almost fully simulate pokemon battles using the information provided (status effects and some other nuances are still missing). If you are interested in simulating how Pokemon battles would pan out between different opponents make sure to read up on the math behind how Pokemon battles work."
"French employment, salaries, population per town",Some data to show equality and inequalities in France,Etienne LQ,44,"Version 7,2017-10-27|Version 6,2017-10-27|Version 5,2017-10-25|Version 4,2017-10-24|Version 3,2017-10-18|Version 2,2017-10-13|Version 1,2017-10-13","employment
demographics",CSV,344 MB,CC0,"8,148 views","1,070 downloads",7 kernels,3 topics,https://www.kaggle.com/etiennelq/french-employment-by-town,"Context
INSEE is the official french institute gathering data of many types around France. It can be demographic (Births, Deaths, Population Density...), Economic (Salary, Firms by activity / size...) and more.
It can be a great help to observe and measure inequality in the french population.
Content
Four files are in the dataset :
base_etablissement_par_tranche_effectif : give information on the number of firms in every french town, categorized by size , come from INSEE.
CODGEO : geographique code for the town (can be joined with *code_insee* column from ""name_geographic_information.csv')
LIBGEO : name of the town (in french)
REG : region number
DEP : depatment number
E14TST : total number of firms in the town
E14TS0ND : number of unknown or null size firms in the town
E14TS1 : number of firms with 1 to 5 employees in the town
E14TS6 : number of firms with 6 to 9 employees in the town
E14TS10 : number of firms with 10 to 19 employees in the town
E14TS20 : number of firms with 20 to 49 employees in the town
E14TS50 : number of firms with 50 to 99 employees in the town
E14TS100 : number of firms with 100 to 199 employees in the town
E14TS200 : number of firms with 200 to 499 employees in the town
E14TS500 : number of firms with more than 500 employees in the town
name_geographic_information : give geographic data on french town (mainly latitude and longitude, but also region / department codes and names )
EU_circo : name of the European Union Circonscription
code_région : code of the region attached to the town
nom_région : name of the region attached to the town
chef.lieu_région : name the administrative center around the town
numéro_département : code of the department attached to the town
nom_département : name of the department attached to the town
préfecture : name of the local administrative division around the town
numéro_circonscription : number of the circumpscription
nom_commune : name of the town
codes_postaux : post-codes relative to the town
code_insee : unique code for the town
latitude : GPS latitude
longitude : GPS longitude
éloignement : i couldn't manage to figure out what was the meaning of this number
net_salary_per_town_per_category : salaries around french town per job categories, age and sex
CODGEO : unique code of the town
LIBGEO : name of the town
SNHM14 : mean net salary
SNHMC14 : mean net salary per hour for executive
SNHMP14 : mean net salary per hour for middle manager
SNHME14 : mean net salary per hour for employee
SNHMO14 : mean net salary per hour for worker
SNHMF14 : mean net salary for women
SNHMFC14 : mean net salary per hour for feminin executive
SNHMFP14 : mean net salary per hour for feminin middle manager
SNHMFE14 : mean net salary per hour for feminin employee
SNHMFO14 : mean net salary per hour for feminin worker
SNHMH14 : mean net salary for man
SNHMHC14 : mean net salary per hour for masculin executive
SNHMHP14 : mean net salary per hour for masculin middle manager
SNHMHE14 : mean net salary per hour for masculin employee
SNHMHO14 : mean net salary per hour for masculin worker
SNHM1814 : mean net salary per hour for 18-25 years old
SNHM2614 : mean net salary per hour for 26-50 years old
SNHM5014 : mean net salary per hour for >50 years old
SNHMF1814 : mean net salary per hour for women between 18-25 years old
SNHMF2614 : mean net salary per hour for women between 26-50 years old
SNHMF5014 : mean net salary per hour for women >50 years old
SNHMH1814 : mean net salary per hour for men between 18-25 years old
SNHMH2614 : mean net salary per hour for men between 26-50 years old
SNHMH5014 : mean net salary per hour for men >50 years old
population : demographic information in France per town, age, sex and living mode
NIVGEO : geographic level (arrondissement, communes...)
CODGEO : unique code for the town
LIBGEO : name of the town (might contain some utf-8 errors, this information has better quality name_geographic_information)
MOCO : cohabitation mode : [list and meaning available in Data description]
AGE80_17 : age category (slice of 5 years) | ex : 0 -> people between 0 and 4 years old
SEXE : sex, 1 for men | 2 for women
NB : Number of people in the category
departments.geojson : contains the borders of french departments. From Gregoire David (github)
These datasets can be merged by : CODGEO = code_insee
Acknowledgements
The entire dataset has been created (and actualized) by INSEE, I just uploaded it on Kaggle after doing some jobs and checks on it. I haven't seen INSEE on Kaggle yet but I think it would be great to bring the organization in as a Kaggle actor.
Inspiration
First aim I had creating that dataset was to provide a map of french towns with the number of firm that are settled in by size.
Now my goal is to explore inequality between men and women, youngsters and elders, working / social classes.
Population can also be a great filter to explain some phenomenons on the maps."
2015 Notebook UX Survey,Understand user perspectives on Jupyter Notebooks,Project Jupyter,44,"Version 2,2017-05-02|Version 1,2016-01-21",human-computer interaction,CSV,748 KB,CC4,"12,070 views",813 downloads,20 kernels,0 topics,https://www.kaggle.com/jupyter/2015-notebook-ux-survey,"At the end of 2015, the Jupyter Project conducted a UX Survey for Jupyter Notebook users. This dataset, Survey.csv, contains the raw responses.
See the Google Group Thread for more context around this dataset."
My Complete Genome,"6,000 Base-Pairs of Phenotype SNPs - Complete Raw Data",Zeeshan-ul-hassan Usmani,44,"Version 4,2017-01-30|Version 3,2017-01-28|Version 2,2017-01-28|Version 1,2017-01-26",human genetics,CSV,15 MB,CC0,"12,160 views",749 downloads,8 kernels,2 topics,https://www.kaggle.com/zusmani/mygenome,"Context
Zeeshan-ul-hassan Usmani’s Genome Phenotype SNPs Raw Data
Genomics is a branch of molecular biology that involves structure, function, variation, evolution and mapping of genomes. There are several companies offering next generation sequencing of human genomes from complete 3 billion base-pairs to a few thousand Phenotype SNPs. I’ve used 23andMe (using Illumina HumanOmniExpress-24) for my DNA’s Phenotype SNPs. I am sharing the entire raw dataset here for the international research community for following reasons:
I am a firm believer in open dataset, transparency, and the right to learn, research, explores, and educate. I do not want to restrict the knowledge flow for mere privacy concerns. Hence, I am offering my entire DNA raw data for the world to use for research without worrying about privacy. I call it copyleft dataset.
Most of available test datasets for research come from western world and we don’t see much from under-developing countries. I thought to share my data to bridge the gap and I expect others to follow the trend.
I would be the happiest man on earth, if a life can be saved, knowledge can be learned, an idea can be explore, or a fact can be found using my DNA data. Please use it the way you will
Content
Name: Zeeshan-ul-hassan Usmani
Age: 38 Years
Country of Birth: Pakistan
Country of Ancestors: India (Utter Pradesh - UP)
File: GenomeZeeshanUsmani.csv
Size: 15 MB
Sources: 23andMe Personalized Genome Report
The research community is still progressively working in this domain and it is agreed upon by professionals that genomics is still in its infancy. You now have the chance to explore this novel domain via the dataset and become one of the few genomics early adopters.
The data-set is a complete genome extracted from www.23andme.com and is represented as a sequence of SNPs represented by the following symbols: A (adenine), C (cytosine), G (guanine), T (thymine), D (base deletions), I (base insertions), and '_' or '-' if the SNP for particular location is not accessible. It contains Chromosomes 1-22, X, Y, and mitochondrial DNA.
A complete list of the exact SNPs (base pairs) available and their data-set index can be found at https://api.23andme.com/res/txt/snps.b4e00fe1db50.data
For more information about how the data-set was extracted follow https://api.23andme.com/docs/reference/#genomes
Moreover, for a more detailed understanding of the data-set content please acquaint yourself with the description of https://api.23andme.com/docs/reference/#genotypes
Acknowledgements
Users are allowed to use, copy, distribute and cite the dataset as follows: “Zeeshan-ul-hassan Usmani, Genome Phenotype SNPS Raw Data File by 23andMe, Kaggle Dataset Repository, Jan 25, 2017.”
Useful Links
You may use the following human genome database sites for help:
GenBank - https://www.ncbi.nlm.nih.gov/genbank/
The Human Genome Project - https://www.genome.gov/hgp/
Genomes OnLine Database (GOLD) - https://gold.jgi.doe.gov
Complete Genomics - http://www.completegenomics.com/public-data/
Inspiration
Some ideas worth exploring:
Is the individual in question more susceptible to cancer?
Does he tend to gain weight?
Where is his place of origin?
Which gene determines certain biological feature (cancer susceptibility, fat generation rate, hair color etc.
How does this phenotype SNPs compare with other similar datasets from the western-world?
What would be the likely cause of death for this person?
What are the most likely diseases/illnesses this person is going to face in lifetime?
What is unique about this dataset?
What else you can extract from this dataset when it comes to personal trait, intelligence level, ancestry and body makeup?
Sample Reports
Please check out following reports to understand what can be done with this data
Ancestry – https://www.23andme.com/published-report/eeb4f9bbd6b5474f/?share_id=f6c5562848e84586
Weight Report - https://you.23andme.com/published/reports/65c9af9f8223456d/?share_id=0126f129e4f3458b"
"United States Energy, Census, and GDP 2010-2014",Examining the relationships between various data collected by the US government,Lislejoem,44,"Version 4,2017-03-25|Version 3,2017-03-21|Version 2,2017-02-03|Version 1,2016-11-20",energy,CSV,74 KB,CC0,"12,191 views","1,587 downloads",26 kernels,,https://www.kaggle.com/lislejoem/us_energy_census_gdp_10-14,"The purpose of this data set is to allow exploration between various types of data that is commonly collected by the US government across the states and the USA as a whole. The data set consists of three different types of data:
Census and Geographic Data;
Energy Data; and
Economic Data.
When creating the data set, I combined data from many different types of sources, all of which are cited below. I have also provided the fields included in the data set and what they represent below. I have not performed any research on the data yet, but am going to dive in soon. I am particularly interested in the relationships between various types of data (i.e. GDP or birth rate) in prediction algorithms. Given that I have compiled 5 years’ worth of data, this data set was primarily constructed with predictive algorithms in mind.
An additional note before you delve into the fields: * There could have been many more variables added across many different fields of metrics. I have stopped here, but it could potentially be beneficial to observe the interaction of these variables with others (i.e. the GDP of certain industries, the average age in a state, the male/female gender ratio, etc.) to attempt to find additional trends.
Census and Geographic Data
StateCodes: The state 2-letter abbreviations. Note that I added ""US"" for the United States.
Region: The number corresponding to the region the state lies within, according to the 2010 census. (1 = Northeast, 2 = Midwest, 3 = South, 4 = West)
Division: The number corresponding to the division the state lies within, according to the 2010 census. (1 = New England, 2 = Middle Atlantic, 3 = East North Central, 4 = West North Central, 5 = South Atlantic, 6 = East South Central, 7 = West South Central, 8 = Mountain, 9 = Pacific)
Coast: Whether the state shares a border with an ocean. (1 = Yes, 0 = No)
Great Lakes: Whether the state shares a border with a great lake. (1 = Yes, 0 = No
CENSUS2010POP: 4/1/2010 resident total Census 2010 population
POPESTIMATE{year}: 7/1/{year} resident total population estimate
RBIRTH{year}: Birth rate in period 7/1/{year - 1} to 6/30/{year}
RDEATH{year}: Death rate in period 7/1/{year - 1} to 6/30/{year}
RNATURALINC{year}: Natural increase rate in period 7/1/{year - 1} to 6/30/{year}
RINTERNATIONALMIG{year}: Net international migration rate in period 7/1/{year - 1} to 6/30/{year}
RDOMESTICMIG{year}: Net domestic migration rate in period 7/1/{year - 1} to 6/30/{year}
RNETMIG{year}: Net migration rate in period 7/1/{year - 1} to 6/30/{year}
As noted from the census:
Net international migration for the United States includes the international migration of both native and foreign-born populations. Specifically, it includes: (a) the net international migration of the foreign born, (b) the net migration between the United States and Puerto Rico, (c) the net migration of natives to and from the United States, and (d) the net movement of the Armed Forces population between the United States and overseas. Net international migration for Puerto Rico includes the migration of native and foreign-born populations between the United States and Puerto Rico.
Codes for most of the data, information about the geographic terms and coditions, and more information about the methodology behind the population estimates can be found on the US Census website.
Energy Data
TotalC{year}: Total energy consumption in billion BTU in given year.
TotalP{year}: Total energy production in billion BTU in given year.
TotalE{year}: Total Energy expenditures in million USD in given year.
TotalPrice{year}: Total energy average price in USD/million BTU in given year.
TotalC{first year}–{second year}: The first year’s total energy consumption divided by the second year’s total energy consumption, times 100. (The percent change between years in total energy consumption.)
TotalP{first year}–{second year}: The first year’s total energy production divided by the second year’s total energy production, times 100. (The percent change between years in total energy production.)
TotalE{first year}–{second year}: The first year’s total energy expenditure divided by the second year’s total energy expenditure, times 100. (The percent change between years in total energy expenditure.)
TotalPrice{first year}–{second year}: The first year’s total energy average price divided by the second year’s total energy average price, times 100. (The percent change between years in total energy average price.)
BiomassC{year}: Biomass total consumption in billion BTU in given year.
CoalC{year}: Coal total consumption in billion BTU in given year.
CoalP{year}: Coal total production in billion BTU in given year.
CoalE{year}: Coal total expenditures in million USD in given year.
CoalPrice{year}: Coal average price in USD per million BTU in given year.
ElecC{year}: Electricity total consumption in billion BTU in given year.
ElecE{year}: Electricity total expenditures in million USD in given year.
ElecPrice{year}: Electricity average price in USD per million BTU in given year.
FossFuelC{year}: Fossil fuels total consumption in billion BTU in given year.
GeoC{year}: Geothermal energy total consumption in billion BTU in given year.
GeoP{year}: Geothermal energy net generation in the electric power sector in million kilowatt hours in given year.
HydroC{year}: Hydropower total consumption in billion BTU in given year.
HydroP{year}: Hydropower total net generation in million kilowatt hours in given year.
NatGasC{year}: Natural gas total consumption (including supplemental gaseous fuels) in billion BTU in given year.
NatGasE{year}: Natural gas total expenditures in million USD in given year.
NatGasPrice{year}: Natural gas average price in USD per million BTU in given year.
LPGC{year}: LPG total consumption in billion BTU in given year.
LPGE{year}: LPG total expenditures in million USD in given year.
LPGPrice{year}: LPG average price in USD per million BTU in given year.
Notes:
BTU stands for British Thermal Unit and is a unit of measurement for energy. One BTU is equal to the amount of energy used to raise the temperature of one pound of water on degree Fahrenheit.
Many other types of energy and their associated consumption, production, expenditure, and price totals can be found from the EIA; this is where I received the data I used in compiling this dataset.
Economic Data
GDP{year}{quarter}: The GDP in the provided quarter of the given year (in million USD).
GDP{year}: The average GDP throughout the given year (in million USD).
Notes:
The GDP is reported by the Bureau of Economic Analysis from the U.S. Department of Commerce and measures the value of the goods and services produced by the economy in a given period.
The quarterly GDP data can be downloaded from the BEA.
The yearly GDP data can be downloaded from the BEA.
Image credit: http://www.freelargeimages.com/wp-content/uploads/2014/11/Map_of_united_states-3.jpg"
Crowdedness at the Campus Gym,Number of attendees every 10 minutes from the last year in the gym,Nick Rose,44,"Version 2,2017-03-19|Version 1,2016-12-14","health
demographics
sociology",CSV,3 MB,ODbL,"16,879 views","1,609 downloads",78 kernels,7 topics,https://www.kaggle.com/nsrose7224/crowdedness-at-the-campus-gym,"Background
When is my university campus gym least crowded, so I know when to work out? We measured how many people were in this gym once every 10 minutes over the last year. We want to be able to predict how crowded the gym will be in the future.
Goals
Given a time of day (and maybe some other features, including weather), predict how crowded the gym will be.
Figure out which features are actually important, which are redundant, and what features could be added to make the predictions more accurate.
Data
The dataset consists of 26,000 people counts (about every 10 minutes) over the last year. In addition, I gathered extra info including weather and semester-specific information that might affect how crowded it is. The label is the number of people, which I'd like to predict given some subset of the features.
Label:
Number of people
Features:
date (string; datetime of data)
timestamp (int; number of seconds since beginning of day)
day_of_week (int; 0 [monday] - 6 [sunday])
is_weekend (int; 0 or 1) [boolean, if 1, it's either saturday or sunday, otherwise 0]
is_holiday (int; 0 or 1) [boolean, if 1 it's a federal holiday, 0 otherwise]
temperature (float; degrees fahrenheit)
is_start_of_semester (int; 0 or 1) [boolean, if 1 it's the beginning of a school semester, 0 otherwise]
month (int; 1 [jan] - 12 [dec])
hour (int; 0 - 23)
Acknowledgements
This data was collected with the consent of the university and the gym in question."
ATP Men's Tour,Results of the ATP tour competitions since 2000,Jordan Goblet,43,"Version 2,2016-09-27|Version 1,2016-09-26",tennis,CSV,9 MB,Other,"15,319 views","1,794 downloads",72 kernels,3 topics,https://www.kaggle.com/jordangoblet/atp-tour-20002016,"Results for the men's ATP tour date back to January 2000, including Grand Slams, Masters Series, Masters Cup and International Series competitions. Historical head-to-head betting odds go back to 2001.
See here for more details about the metadata : http://www.tennis-data.co.uk/notes.txt
Source - http://www.tennis-data.co.uk/data.php
There is a lot you can do with this data set. The ultimate goal is obviously to predict the outcome of the game or to build an efficient betting strategy based on your model(s).
You can also
compare the betting agencies (Bet365, Bet&Win, Ladbrokes, Pinnacles...) in terms of predictions quality or measure the progress of these betting agencies over the years
discover if it's possible to predict specific events (3 sets match, retirement, walk over...) or the evolution of players."
3D MNIST,A 3D version of the MNIST database of handwritten digits,David de la Iglesia Castro,43,"Version 13,2016-11-10|Version 12,2016-11-05|Version 11,2016-11-05|Version 10,2016-10-26|Version 9,2016-10-26|Version 8,2016-10-25|Version 7,2016-10-25|Version 6,2016-10-20|Version 5,2016-10-18|Version 4,2016-10-18|Version 3,2016-10-18|Version 2,2016-10-18|Version 1,2016-10-13","writing
artificial intelligence",Other,244 MB,Other,"14,269 views",830 downloads,62 kernels,2 topics,https://www.kaggle.com/daavoo/3d-mnist,"Context
The aim of this dataset is to provide a simple way to get started with 3D computer vision problems such as 3D shape recognition.
Accurate 3D point clouds can (easily and cheaply) be adquired nowdays from different sources:
RGB-D devices: Google Tango, Microsoft Kinect, etc.
Lidar.
3D reconstruction from multiple images.
However there is a lack of large 3D datasets (you can find a good one here based on triangular meshes); it's especially hard to find datasets based on point clouds (wich is the raw output from every 3D sensing device).
This dataset contains 3D point clouds generated from the original images of the MNIST dataset to bring a familiar introduction to 3D to people used to work with 2D datasets (images).
In the 3D_from_2D notebook you can find the code used to generate the dataset.
You can use the code in the notebook to generate a bigger 3D dataset from the original.
Content
full_dataset_vectors.h5
The entire dataset stored as 4096-D vectors obtained from the voxelization (x:16, y:16, z:16) of all the 3D point clouds.
In adition to the original point clouds, it contains randomly rotated copies with noise.
The full dataset is splitted into arrays:
X_train (10000, 4096)
y_train (10000)
X_test(2000, 4096)
y_test (2000)
Example python code reading the full dataset:
 with h5py.File(""../input/train_point_clouds.h5"", ""r"") as hf:    
     X_train = hf[""X_train""][:]
     y_train = hf[""y_train""][:]    
     X_test = hf[""X_test""][:]  
     y_test = hf[""y_test""][:]  
train_point_clouds.h5 & test_point_clouds.h5
5000 (train), and 1000 (test) 3D point clouds stored in HDF5 file format. The point clouds have zero mean and a maximum dimension range of 1.
Each file is divided into HDF5 groups
Each group is named as its corresponding array index in the original mnist dataset and it contains:
""points"" dataset: x, y, z coordinates of each 3D point in the point cloud.
""normals"" dataset: nx, ny, nz components of the unit normal associate to each point.
""img"" dataset: the original mnist image.
""label"" attribute: the original mnist label.
Example python code reading 2 digits and storing some of the group content in tuples:
with h5py.File(""../input/train_point_clouds.h5"", ""r"") as hf:    
    a = hf[""0""]
    b = hf[""1""]    
    digit_a = (a[""img""][:], a[""points""][:], a.attrs[""label""]) 
    digit_b = (b[""img""][:], b[""points""][:], b.attrs[""label""]) 
voxelgrid.py
Simple Python class that generates a grid of voxels from the 3D point cloud. Check kernel for use.
plot3D.py
Module with functions to plot point clouds and voxelgrid inside jupyter notebook. You have to run this locally due to Kaggle's notebook lack of support to rendering Iframes. See github issue here
Functions included:
array_to_color Converts 1D array to rgb values use as kwarg color in plot_points()
plot_points(xyz, colors=None, size=0.1, axis=False)
plot_voxelgrid(v_grid, cmap=""Oranges"", axis=False)
Acknowledgements
Website of the original MNIST dataset
Website of the 3D MNIST dataset
Have fun!"
US Household Income Statistics,"+32,000 records, with grandularity on a neighborhood scale (mean, median, Stdev)",Golden Oak Research Group,43,"Version 6,2017-08-21|Version 5,2017-08-14|Version 4,2017-08-14|Version 3,2017-08-14|Version 2,2017-08-10|Version 1,2017-08-09","geography
income
finance
+ 2 more...",Other,6 MB,Other,"6,803 views","1,210 downloads",3 kernels,2 topics,https://www.kaggle.com/goldenoakresearch/us-household-income-stats-geo-locations,"New Upload:
Added +32,000 more locations. For information on data calculations please refer to the methodology pdf document. Information on how to calculate the data your self is also provided as well as how to buy data for $1.29 dollars.
What you get:
The database contains 32,000 records on US Household Income Statistics & Geo Locations. The field description of the database is documented in the attached pdf file. To access, all 348,893 records on a scale roughly equivalent to a neighborhood (census tract) see link below and make sure to up vote. Up vote right now, please. Enjoy!
Household & Geographic Statistics:
Mean Household Income (double)
Median Household Income (double)
Standard Deviation of Household Income (double)
Number of Households (double)
Square area of land at location (double)
Square area of water at location (double)
Geographic Location:
Longitude (double)
Latitude (double)
State Name (character)
State abbreviated (character)
State_Code (character)
County Name (character)
City Name (character)
Name of city, town, village or CPD (character)
Primary, Defines if the location is a track and block group.
Zip Code (character)
Area Code (character)
Abstract
The dataset originally developed for real estate and business investment research. Income is a vital element when determining both quality and socioeconomic features of a given geographic location. The following data was derived from over +36,000 files and covers 348,893 location records.
License
Only proper citing is required please see the documentation for details. Have Fun!!!
Golden Oak Research Group, LLC. “U.S. Income Database Kaggle”. Publication: 5, August 2017. Accessed, day, month year.
Sources, don't have 2 dollars? Get the full information yourself!
2011-2015 ACS 5-Year Documentation was provided by the U.S. Census Reports. Retrieved August 2, 2017, from https://www2.census.gov/programs-surveys/acs/summary_file/2015/data/5_year_by_state/
Found Errors?
Please tell us so we may provide you the most accurate data possible. You may reach us at: research_development@goldenoakresearch.com
for any questions you can reach me on at 585-626-2965
please note: it is my personal number and email is preferred
Check our data's accuracy: Census Fact Checker
Access all 348,893 location records and more:
Don't settle. Go big and win big. Optimize your potential. Overcome limitation and outperform expectation. Access all household income records on a scale roughly equivalent to a neighborhood, see link below:
Website: Golden Oak Research Kaggle Deals all databases $1.29 Limited time only
A small startup with big dreams, giving the every day, up and coming data scientist professional grade data at affordable prices It's what we do."
Crime Statistics for South Africa,A history of crime statistics from 2004 to 2015 per province and station,Stephan Wessels,42,"Version 8,2016-11-02|Version 7,2016-11-02|Version 6,2016-10-27|Version 5,2016-10-27|Version 4,2016-10-27|Version 3,2016-10-26|Version 2,2016-10-26|Version 1,2016-10-19",crime,Other,23 MB,Other,"15,787 views","2,050 downloads",85 kernels,6 topics,https://www.kaggle.com/slwessels/crime-statistics-for-south-africa,"Context
CRIME STATISTICS: INTEGRITY
The South African Police Service (SAPS) has accepted a new and challeging objective of ensuring that its crime statistics are in line with international best practice. This will be achieved through a Memorandum of Understanding with Statistics South Africa (Stats SA), aimed at further enhancing the quality and integrity of the South African crime statistics.
The crime statistics generated by SAPS are an important link in the value chain of the statistics system informs policy development and planning in the criminal justice system. The collaboration with StatsSA will go a long way in enhancing the integrity of the SAPS crime statistics and ensuring that policy-makers have quality data to assist them with making policy decisions.
Content
The dataset contains South African crime statistics, broken down per province, station and crime type.
Acknowledgements
Data as published from:
http://www.saps.gov.za/resource_centre/publications/statistics/crimestats/2015/crime_stats.php
Further sources:
http://www.saps.gov.za/services/crimestats.php
An overview presentation:
http://www.saps.gov.za/services/final-crime-stats-release-02september2016.pdf"
CS:GO Competitive Matchmaking Data,"Damage/Grenade entries on over 35,000 rounds played in competitive matchmaking",KP,42,"Version 5,2017-10-12|Version 4,2017-09-29|Version 3,2017-09-26|Version 2,2017-09-26|Version 1,2017-09-23","video games
internet",Other,366 MB,CC4,"7,863 views",628 downloads,5 kernels,3 topics,https://www.kaggle.com/skihikingkevin/csgo-matchmaking-damage,"Introduction
Video games are a rich area for data extraction due to their digital nature. Notable examples such as the complex EVE Online economy, World of Warcraft corrupted blood incident and even Grand Theft Auto self-driving cars tells us that fiction is closer to reality than we really think. Data scientists can gain insight on the logic and decision-making that the players face when put in hypothetical and virtual scenarios.
In this Kaggle Dataset, I provide just over 1400 competitive matchmaking matches from Valve's game Counter-strike: Global Offensive (CS:GO). The data was extracted from competitive matchmaking replays submitted to csgo-stats. I intend for this data-set to be purely exploratory, however users are free to create their own predictive models they see fit.
About Counter-Strike: Global Offensive
Counter-Strike: Global Offensive is a first-person shooter game pitting two teams of 5 players against each other. Within a maximum of 30 rounds, the two teams find themselves on either side as a Counter Terrorist or Terrorist. Both sides are tasked with eliminating the opposition or, as the terrorist team, planting the C4 bomb at a bomb site and allowing it to explode. Rounds are played out until either of those two objectives or if the maximum time is reached (in which the counter terrorists then win by default). At the end of the 15th round, the two teams switch sides and continue until one team reaches 16 round wins first. CS:GO is widely known for its competitive aspect of technical skill, teamwork and in-game strategies. Players are constantly rewarded with the efforts they put it in training and learning through advancing in rank.
Click here to read more about the competitive mechanics of CS:GO.
Content
This dataset within the 1400 matches provides every successful entry of duels (or battle) that took place for a player. That is, each row documents an event when a player is hurt by another player (or World e.g fall damage). There are over 900,000 entries within more than 31500 rounds.
mm_master_demos.csv contains information on rounds fired, while mm_grenades_demos.csv contains information on grenades thrown. The fields in the two datasets are similar: highlights include shooters and victims, event coordinates, and timestamps. The datasets also includes static information on the match winner, player ranks before and after the match, and other miscellaneous match-level metadata.
For further information on individual fields in the dataset, refer to the Column Metadata.
Interpreting Positional Data
This dataset also includes a selection of the game's official radar maps, as well as a table, map_data.csv, to aid in mapping data over them. The X,Y coordinates included in the dataset are all in in-game coordinates and need to be linearly scaled to be plotted on any official radar maps. See converting to map coordinates for more information.
Acknowledgements
Definitely the guys from csgo-stats, without them, this wouldn't have been possible! :)
/r/globaloffensive for many years of lulz
Akiver of CSGO Demo Manager for spending so much time perfecting his demo parser."
US Census Demographic Data,Demographic and Economic Data for Tracts and Counties,MuonNeutrino,42,"Version 1,2017-12-14","united states
demographics",CSV,6 MB,CC0,"7,173 views","1,517 downloads",3 kernels,,https://www.kaggle.com/muonneutrino/us-census-demographic-data,"Context
This dataset expands on my earlier New York City Census Data dataset. It includes data from the entire country instead of just New York City. The expanded data will allow for much more interesting analyses and will also be much more useful at supporting other data sets.
Content
The data here are taken from the DP03 and DP05 tables of the 2015 American Community Survey 5-year estimates. The full datasets and much more can be found at the American Factfinder website. Currently, I include two data files:
acs2015_census_tract_data.csv: Data for each census tract in the US, including DC and Puerto Rico.
acs2015_county_data.csv: Data for each county or county equivalent in the US, including DC and Puerto Rico.
The two files have the same structure, with just a small difference in the name of the id column. Counties are political subdivisions, and the boundaries of some have been set for centuries. Census tracts, however, are defined by the census bureau and will have a much more consistent size. A typical census tract has around 5000 or so residents.
The Census Bureau updates the estimates approximately every year. At least some of the 2016 data is already available, so I will likely update this in the near future.
Acknowledgements
The data here were collected by the US Census Bureau. As a product of the US federal government, this is not subject to copyright within the US.
Inspiration
There are many questions that we could try to answer with the data here. Can we predict things such as the state (classification) or household income (regression)? What kinds of clusters can we find in the data? What other datasets can be improved by the addition of census data?"
Aviation Accident Database & Synopses,The NTSB aviation accident dataset,Kheirallah Samaha,41,"Version 6,2018-01-15|Version 5,2017-01-10|Version 4,2017-01-09|Version 3,2017-01-05|Version 2,2016-12-12|Version 1,2016-12-11",aviation,CSV,4 MB,CC0,"18,163 views","2,487 downloads",87 kernels,,https://www.kaggle.com/khsamaha/aviation-accident-database-synopses,"Content
The NTSB aviation accident database contains information from 1962 and later about civil aviation accidents and selected incidents within the United States, its territories and possessions, and in international waters.
Acknowledgements
Generally, a preliminary report is available online within a few days of an accident. Factual information is added when available, and when the investigation is completed, the preliminary report is replaced with a final description of the accident and its probable cause. Full narrative descriptions may not be available for dates before 1993, cases under revision, or where NTSB did not have primary investigative responsibility.
Inspiration
Hope it will teach us how to improve the quality and safety of traveling by Airplane."
Forest Cover Type Dataset,Tree types found in the Roosevelt National Forest in Colorado,UCI Machine Learning,41,"Version 1,2016-11-03","botany
ecology
plants",CSV,72 MB,Other,"13,386 views","1,303 downloads",66 kernels,0 topics,https://www.kaggle.com/uciml/forest-cover-type-dataset,"Context
This dataset contains tree observations from four areas of the Roosevelt National Forest in Colorado. All observations are cartographic variables (no remote sensing) from 30 meter x 30 meter sections of forest. There are over half a million measurements total!
Content
This dataset includes information on tree type, shadow coverage, distance to nearby landmarks (roads etcetera), soil type, and local topography.
Acknowledgement
This dataset is part of the UCI Machine Learning Repository, and the original source can be found here. The original database owners are Jock A. Blackard, Dr. Denis J. Dean, and Dr. Charles W. Anderson of the Remote Sensing and GIS Program at Colorado State University.
Inspiration
Can you build a model that predicts what types of trees grow in an area based on the surrounding characteristics? A past Kaggle competition project on this topic can be found here.
What kinds of trees are most common in the Roosevelt National Forest?
Which tree types can grow in more diverse environments? Are there certain tree types that are sensitive to an environmental factor, such as elevation or soil type?"
Cryptocoins Historical Prices,A collection of prices for cryptocurrencies,ValerioVaccaro,40,"Version 8,2018-01-12|Version 7,2018-01-06|Version 6,2017-12-01|Version 5,2017-11-13|Version 4,2017-11-08|Version 3,2017-10-22|Version 2,2017-10-22|Version 1,2017-10-18",,CSV,20 MB,CC0,"6,216 views",856 downloads,10 kernels,,https://www.kaggle.com/valeriovaccaro/cryptocoinshistoricalprices,"Introduction
This file contains the values of the price for more than 1000 different cryptocurrencies (including scams) recorded on daily base, I decide to include all coins in order to analyze exotic coins and compare with well knows cryptocurrencies. All this dataset come from coinmarketcap historical pages, grabbed using just an R script. Thanks coinmarketcap to making this data available for free (and for every kind of usage).
The dataset
Available columns in the dataset:
Date - the day of recorded values
Open - the opening price (in USD)
High - the highest price (in USD)
Low - the lowest price (in USD)
Close - the closing price (in USD)
Volume - total exchanged volume (in USD)
Market.Cap - the total market capitalization for the coin (in USD)
coin - the name of the coin
Delta - calculated as (Close - Open) / Open"
2012 and 2016 Presidential Elections,"Election results with county information on race, income and education",Joel Wilson,40,"Version 2,2016-11-28|Version 1,2016-11-23",politics,CSV,3 MB,Other,"15,634 views","2,711 downloads",39 kernels,3 topics,https://www.kaggle.com/joelwilson/2012-2016-presidential-elections,"These data files contain election results for both the 2012 and 2016 US Presidential Elections, include proportions of votes cast for Romney, Obama (2012) and Trump, Clinton (2016).
The election results were obtained from this Git repository: https://github.com/tonmcg/County_Level_Election_Results_12-16
The county facts data was obtained from another Kaggle election data set: https://www.kaggle.com/benhamner/2016-us-election"
Emoji sentiment,Are people that use emoji happier?,Jose Berengueres,39,"Version 2,2017-10-01|Version 1,2017-10-01","linguistics
internet",CSV,152 MB,GPL,"7,772 views",771 downloads,,0 topics,https://www.kaggle.com/harriken/emoji-sentiment,"Are people that use emoji happier?
paper --> https://arxiv.org/abs/1710.00888
At ASONAM2017, PydataDubai vol 1.0 @ AWOK, PyDataBCN2017 @ EASDE we have presented the paper Happiness inside a job?... Many people in the various audiences asked why we avoid using emojis to predict and profile employees. The answer is that we prefer to use links of likes because they are more authentic than words or emojis. In the same way that google page rank is more effective when it looks at links between pages rather than content inside the pages. ... Still people keep asking about it. But there is one thing emoji are good at estimating: author sentiment and that is just possible thanks to the unique characteristics of the dataset at hand.
Previous research has traditionally analyzed emoji sentiment from the point of view of the reader of the content not the author. Here, we analyze emoji sentiment from the author point of view and present a benchmark that was built from an employee happiness dataset where emoji happen to be annotated with daily happiness of the author of the comment. We also found out that people that use emoji are happier!?, muuch happier... But the question is, what did we miss?
Content
The main table contains columns named after emoji hex codes, a 1 means the emoji appears one time in the comment (row). This dataset is an expanded version of this one, but has different formats, columns and one different table, that is why we decided to release it as separate dataset. as he scripts are not compatible.
Other stuff
The R script written on MAC OS does not work in the kaggle platform (because numbers become factors and other little changes in how the code is interpreted...), the full working script (tested on R studio MAC OS) can be found at https://github.com/orioli/emoji-writer-sentiment
Thank you to Lewis Michel"
Human Resources Data Set,Dataset used for learning data visualization and basic regression,Dr. Rich,39,"Version 4,2018-02-15|Version 3,2017-07-24|Version 2,2017-07-21|Version 1,2017-07-20","employment
business",CSV,176 KB,CC4,"12,756 views","2,047 downloads",5 kernels,2 topics,https://www.kaggle.com/rhuebner/human-resources-data-set,"Context
HR data can be hard to come by, and HR professionals generally lag behind with respect to analytics and data visualization competency. Thus, Dr. Carla Patalano and I set out to create our own HR-related dataset, which is used in one of our graduate MSHRM courses called HR Metrics and Analytics, at New England College of Business. We created this data set ourselves.
Content
There are multiple worksheets within the Excel workbook. These include
Core data set
Production staff
Sales analysis
Salaries
Recruiting sources
The Excel workbook revolves around a fictitious company, called Dental Magic, and the core data set contains names, DOBs, age, gender, marital status, date of hire, reasons for termination, department, whether they are active or terminated, position title, pay rate, manager name, and performance score.
Acknowledgements
Dr. Carla Patalano provided many suggestions for creating this synthetic data set, which has been used now by over 30 Human Resource Management students at the college. Students in the course learn data visualization techniques with Tableau Desktop and use this data set to complete a series of assignments.
Inspiration
Is there any relationship between who a person works for and their performance score?
What is the overall diversity profile of the organization?
What are our best recruiting sources if we want to ensure a diverse organization?
There are so many other interesting questions that could be addressed through this interesting data set. Dr. Patalano and I look forward to seeing what we can come up with."
Text Similarity,Natural Language Processing on Stock data,Rishi Sankineni,39,"Version 1,2017-03-19","languages
finance
linguistics",CSV,202 KB,ODbL,"11,418 views",866 downloads,13 kernels,,https://www.kaggle.com/rishisankineni/text-similarity,"Context:
Natural Language Processing(NLP), Text Similarity(lexical and semantic)
Content:
In each row of the included datasets(train.csv and test.csv), products X(description_x) and Y(description_y) are considered to refer to the same security(same_security) if they have the same ticker(ticker_x,ticker_y), even if the descriptions don't exactly match. You can make use of these descriptions to predict whether each pair in the test set also refers to the same security.
Dataset info:
Train - description_x, description_y, ticker_x, ticker_y, same_security. Test - description_x, description_y, same_security(to be predicted)
Past Research:
This dataset is pretty similar to the Quora Question Pairs . You can also check out my kernel for dataset exploration and n-gram analysis N-gram analysis on stock data.
How to Approach:
There are several good ways to approach this, check out this algorithm, and see how far you can go with it: https://en.wikipedia.org/wiki/Tf–idf http://scikit learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html. You can also try doing n-gram analysis(check out my kernel). I would suggest using log-loss as your evaluation metric since it gives you a number between 0 and 1 instead of binary classification, which is not so effective in this case.
Acknowledgements:
Quovo stock data."
2016 U.S. Presidential Election Memes,"Analyze 45,000+ U.S. Presidential Election Memes",SIZZLE,39,"Version 4,2016-11-02|Version 3,2016-10-28|Version 2,2016-10-27|Version 1,2016-10-26","popular culture
politics
internet",CSV,26 MB,CC4,"21,453 views","1,353 downloads",53 kernels,3 topics,https://www.kaggle.com/SIZZLE/2016electionmemes,"We’re releasing 30,000+ OCR’d political memes and their captions . With the election just days away, we hope to contribute to the pre and post-election analysis of the most meme-orable election in modern history.
v2 of the dataset includes 8 csv’s:
Bern.csv: 146 rows
Bernie.csv: 2100 rows
Clinton.csv: 4362 rows
Donald.csv: 6499 rows
Gary_Johnston.csv: 140 rows
Hillary.csv: 7398 rows
Jill Stein.csv: 96 rows
Trump.csv: 12139 rows
with the following columns:
timestamp (date published)
id (our unique identifier)
link (post url)
caption (meme caption via ocr)
author
network
likes/upvotes
Let us know any revisions you'd like to see. And of course, if you find errors in the data, do let us know."
Cycle Share Dataset,Bicycle Trip Data from Seattle's Cycle Share System,Pronto Cycle Share,39,"Version 1,2016-11-07","cycling
road transport",CSV,46 MB,Other,"14,437 views","2,009 downloads",41 kernels,3 topics,https://www.kaggle.com/pronto/cycle-share-dataset,"Context
The Pronto Cycle Share system consists of 500 bikes and 54 stations located in Seattle. Pronto provides open data on individual trips, stations, and daily weather.
Content
There are 3 datasets that provide data on the stations, trips, and weather from 2014-2016.
Station dataset
station_id: station ID number
name: name of station
lat: station latitude
long: station longitude
install_date: date that station was placed in service
install_dockcount: number of docks at each station on the installation date
modification_date: date that station was modified, resulting in a change in location or dock count
current_dockcount: number of docks at each station on 8/31/2016
decommission_date: date that station was placed out of service
Trip dataset
trip_id: numeric ID of bike trip taken
starttime: day and time trip started, in PST
stoptime: day and time trip ended, in PST
bikeid: ID attached to each bike
tripduration: time of trip in seconds
from_station_name: name of station where trip originated
to_station_name: name of station where trip terminated
from_station_id: ID of station where trip originated
to_station_id: ID of station where trip terminated
usertype: ""Short-Term Pass Holder"" is a rider who purchased a 24-Hour or 3-Day Pass; ""Member"" is a rider who purchased a Monthly or an Annual Membership
gender: gender of rider
birthyear: birth year of rider
Weather dataset contains daily weather information in the service area
Acknowledgements
The original datasets can be downloaded here.
Inspiration
Some ideas worth exploring:
What is the most popular bike route?
How are bike uses or routes affected by user characteristics, station features, and weather?"
Daily Sea Ice Extent Data,Total sea ice extent from 1978 to present,National Snow and Ice Data Center,39,"Version 2,2017-06-09|Version 1,2016-12-07","environment
climate",CSV,4 MB,Other,"10,577 views","1,224 downloads",27 kernels,3 topics,https://www.kaggle.com/nsidcorg/daily-sea-ice-extent-data,"Context
The National Snow and Ice Data Center (NSIDC) supports research into our world’s frozen realms: the snow, ice, glaciers, frozen ground, and climate interactions that make up Earth’s cryosphere. NSIDC manages and distributes scientific data, creates tools for data access, supports data users, performs scientific research, and educates the public about the cryosphere.
Content
The dataset provides the total extent for each day for the entire time period (1978-2015). There are 7 variables:
Year
Month
Day
Extent: unit is 10^6 sq km
Missing: unit is 10^6 sq km
Source: Source data product web site: http://nsidc.org/data/nsidc-0051.html
hemisphere
Acknowledgements
The original datasets can be found here and here.
Inspiration
Can you visualize the change in sea ice over time?
Do changes in sea ice differ between the two hemispheres?"
World Bank Youth Unemployment Rates,Youth Unemployment rates by country from 2010-2014,SovBoc2018,38,"Version 1,2016-11-05","employment
finance",CSV,17 KB,Other,"16,609 views","2,443 downloads",129 kernels,0 topics,https://www.kaggle.com/sovannt/world-bank-youth-unemployment,"Context A data driven look into answering the common question while travelling overseas: ""how easy is it to get a job in your country?""
Content This dataset contains youth unemployment rates (% of total labor force ages 15-24) (modeled ILO estimate) Latest data available from 2010 to 2014.
Acknowledgements International Labour Organization.
http://data.worldbank.org/indicator/SL.UEM.TOTL.ZS
Released under Open license."
Crime in India,State-wise data from 2001 is classified according to 40+factors. (75+ csv files),Rajanand Ilangovan / இராஜ்ஆனந்த் இளங்கோவன்,38,"Version 4,2017-09-01|Version 3,2017-08-24|Version 2,2017-08-07|Version 1,2017-08-07","india
crime",CSV,12 MB,CC4,"9,932 views","1,669 downloads",5 kernels,,https://www.kaggle.com/rajanand/crime-in-india,"Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks.
Context
This dataset contains complete information about various aspects of crimes happened in India from 2001. There are many factors that can be analysed from this dataset. Over all, I hope this dataset helps us to understand better about India.
Content
I : Cases Reported and their Disposal by Police and Court
Indian Penal Code
Special & Local Laws
IA : SC/ST Cases Reported and their Disposal by Police and Court
Crime against SCs
Crime against STs
IB : Children Cases Reported and their Disposal by Police and Court
Abetment of Suicide (Section 305 IPC)
Buying of Girls for Prostitution (Section 373 IPC)
Child Marriage Restraint Act, 1929
Exposure and Abandonment (Section 317 IPC)
Foeticide (Section 315 and 316 IPC)
Infanticide (Section 315 IPC)
Kidnapping & Abduction (Section 360,361,363,363-A, 363 read with Section 384, 366, 367 & 369 IPC)
Murder (Section 302, 315 IPC)
Other Crimes against Children
Other Murder of Children (Section 302 IPC)
Procuration of Minor Girls (Section 366-A IPC)
Rape (Section 376 IPC)
Selling of Girls for Prostitution (Section 372 IPC)
Total Crimes against Children
II : Persons Arrested and their Disposal by Police and Court
Indian Penal Code
Special and Local Laws
IIA : SC/ST Persons Arrested and their Disposal by Police and Court
Crime against SCs
Crime against STs
IIB : Children Persons Arrested and their Disposal by Police and Court
Abetment of suicide (Section 305 IPC)
Buying of girls for prostitution (Section 373 IPC)
Child Marriage Restraint Act, 1929
Exposure and Abandonment (Section 317 IPC)
Foeticide (Section 315 and 316 IPC)
Kidnapping & Abduction (Section 360,361,363,363-A, 366, 367 & 369 IPC)
Murder - Infanticide (Section 315 IPC)
Murder - Other Murder of Children
Murder (Section 302, 315 IPC)
Other Crimes against Children
Procuration of minor girls (Section 366-A IPC)
Rape (Section 376 IPC)
Selling of girls for prostitution (Section 372 IPC)
Total Crimes against Children
IV : Persons Arrested by Sex and Age Group
Indian Penal Code
Special & Local Laws
V : Juveniles Apprehended
Indian Penal Code
Special & Local Laws
VI : Juveniles Arrested and their Disposal
VII : Property Stolen & Recovered (Crime Head)
Dacoity
Robbery
Burglary
Theft
Criminal Breach of Trust
Other Property
Total Property Stolen & Recovered
VIII : Property Stolen & Recovered (Nature of Property)
Communation and Electricity Wire
Cattle
Cycle
Motor Vehicles
Motor Vehicles - Motor Cycle/Scooters
Motor Vehicles - Motor Car/Taxi/Jeep
Motor Vehicles - Other Motor Vehicles
Fire Arms
Explosives/Explosive Substances
Electronic Components
Cultural Property including Antiques
Other kinds of Property
Total Property Stolen & Recovered
IX : Police Strength (Actual & Sanctioned)
A) Actual Civil Police (Incl. District Armed Police and Women Police)
A) Acual Armed Police (Incl. Women Police)
A) Actual Police Strength (Incl. Women)
B) Acual Women Civil Police (Incl. District Armed Force)
B) Actual Women Armed Police
B) Actual Women Police Strength
C) Sanctioned Civil Police (Incl. District Armed Police)
C) Santioned Armed Police (Incl. Women Police)
C) Santioned Police Strength (Incl. Women)
D) Sanctioned Women Civil Police (Incl. District Armed Police)
D) Sanctioned Women Armed Police
D) Sanctioned Women Police Strength
X : Police Personnel Killed or Injured on duty
Constables
Head Constables
Assistant Sub-Inspectos
Sub-Inspectors
Inspectors
Gazetted Officers
Total Police Killed or Injured
X-B : Age Profile of Police Personnel Killed on Duty
X-C : Natural Deaths and Suicides of Police Personnel
Natural Deaths of Police Personnel (while in service)
Police Personnel Committed Suicide
XI : Casualties under Police Firing and LathiCharge
Riot Control
Anti Dacoity Operations
Against Extremists & Terrorists
Against Others
Total Casualties
XII : Cases Reported Value of Property Stolen under Dacoity, Robbery, Burglary and Theft by Place of Occurance
Residential Premises
Highways
River and Sea
Railways 4.1 In Running Trains 4.2 Others
Banks
Commercial Establishments (Shops etc.)
Other Places
Total
XIII : Particulars of Juveniles Arrested
Education
Economic Setup
Family Background
Recidivism
XIV : Motive/Cause of Murder and Culpable Homicide not Amounting to Murder
XV : Victims of Rape(Age Group-wise)
Incest Rape Cases
Other Rape Cases (Otherthan Incest)
Total Rape Cases
XV-A : Rape Offenders relation, nearness to Rape Victims
XVI : Persons Arrested under Recidivism
XVII : Anti Corruption - Cases
XVIII : Anti Corruption - Arrests
XIX : Complaints/Cases Against Police Personnel
Complaints Received/Cases Registered
Police Personnel Involved/Action Taken
Departmental Action/Punishments
*XX : Police Budget and Infrastructure
Equipments and Transport Support
Distribution of Police Stations by Crime Incidences
Distribution of Police Stations by Police Strength
Organisational Set Up
SCs/STs and Muslims in Police Force (Actual)
XXI : 1. Nature of Complaints Received by Police
XXI : 2. Trial of Violent Crimes by Courts
Murder
Attempt to Murder
C H Not Amounting to Murder
Rape
Kidnapping & Abduction 5.1 Kidnapping & Abduction of Women & Girls 5.2 Kidnapping & Abduction of Others
Dacoity
Preparation & Assembly for Dacoity
Robbery
Riots
Arson
Dowry Deaths
Total Trials (Sum of 1-11 Above)
XXI : 3. Period of Trials by Courts
District/Session Judge
Additional Session Judge
Chief Judicial Magistrate
Judicial Magistrate (I)
Judicial Magistrate (II)
Special Judicial Magistrate
Other courts
Total Trials (Sum of 1-7 Above)
XXI : 4.1 Autho Theft (Stolen & Recovered)
Motor Cycles/ Scooters
Motor Car/Taxi/Jeep
Buses
Goods carrying vehicles (Trucks/Tempo etc)
Other Motor vehicles
Total (Sum of 1-5 Above)
XXI : 4.2 Serious Fraud
Criminal Breach of Trust
Cheating
XXI : 5.1 Victims of Murder (Age & Sex-Wise)
Male Victims
Female Victims
Total
XXI : 5.2 Victims of CH not Amounting to Murder (Age & Sex-wise)
Male Victims
Female Victims
Total
XXI : 5.3 Use of FireArms in Murder Cases
XXI : 6. Human Rights Violation by Police
Disappearance of Persons
Illegal Detention/Arrests
Fake Encounter Killings
Violation Against Terrorists/Extremists
Extortion
Torture
False Implication
Failure in Taking Action
Indignity to Women
Atrocities on SC/ST
Others
Total (Sum of 1-11 Above)
XXI : 7. Police Housing
For Officers (Dy.SP & Above)
Upper SubOrdinates (ASI to Inspectos)
Lower SubOrdinates (Constables, Head Constables & Class-IV Subordinate Staff)
XXI : 8. Home Guards and Auxilliary force
XXI : 9. Unidentified Deadbodies Recovered & Inquest conducted
XXI : 10. Victims of Kidnapping & Abduction for Specific Purpose
For Adoption
For Begging
for Camel Racing
For Illicit Intercourse
For Marriage
For Prostitution
For Ransom
For Revenge
For Sale
For Selling Bodyparts
For Slavery
For Unlawful Activity
Other Purposes
Total (Sum of 1-13 Above)
XXI : 11. Custodial Deaths
Deaths in Custody/Lockup of Persons Remanded to Police Custody by Court
Deaths in Custody/Lockup of Persons Not Remanded to Police Custody by Court
Deaths in Custody during production/process in courts/journey connected with investigation
Deaths during Hospitalisation/Treatment
Deaths due to Other Reasons
XXI : 12. Escapes from Police Custody
Cases under Crime Against Women
Rape
Kidnapping & Abduction of Women & Girls
Dowry Deaths
Molestation
Sexual Harassment
Cruelty by Husband and Relatives
Importation of Girls
Immoral Traffic Prevention Act, 1956
Dowry Prohibition Act, 1961
Indecent Representation of Women(Prohibition) Act, 1986
Sati Prevention Act, 1987
Total Crimes Against Women
Arrests under Crime Against Women
Rape
Kidnapping & Abduction of Women & Girls
Dowry Deaths
Molestation
Sexual Harassment
Cruelty by Husband and Relatives
Importation of Girls
Immoral Traffic Prevention Act, 1956
Dowry Prohibition, 1961
Indecent Representation of Women(Prohibition) Act, 1986
Sati Prevention Act, 1987
Total Crimes Against Women
Some of the data contains district level data. The districts are police districts and also include special police unit. Therefore these may be different from revenue districts. Most of the data is from 2001 to 2010. But there are few files which has data only from 2011 and few are having 2001-14.
Inspiration
There could be many things one can understand by analyzing this dataset. Few inspirations for you to start with.
What is the major reason people being kidnapped in each and every state?
Offenders relation to the rape victim
Juveniles family background, education and economic setup.
Which state has more crime against children and women?
Age group wise murder victim
Crime by place of occurrence.
Anti corruption cases vs arrests.
Which state has more number of complaints against police?
Which state is the safest for foreigners?
Acknowledgements
National Crime Records Bureau (NCRB), Govt of India has published this dataset on their website and also has shared on Open Govt Data Platform India portal under Govt. Open Data License - India."
80 Cereals,Nutrition data on 80 cereal products,Chris Crawford,38,"Version 2,2017-10-25|Version 1,2017-08-19",food and drink,CSV,5 KB,CC3,"11,553 views","1,995 downloads",286 kernels,,https://www.kaggle.com/crawford/80-cereals,"Context
If you like to eat cereal, do yourself a favor and avoid this dataset at all costs. After seeing these data it will never be the same for me to eat Fruity Pebbles again.
Content
Fields in the dataset:
Name: Name of cereal
mfr: Manufacturer of cereal
A = American Home Food Products;
G = General Mills
K = Kelloggs
N = Nabisco
P = Post
Q = Quaker Oats
R = Ralston Purina
type:
cold
hot
calories: calories per serving
protein: grams of protein
fat: grams of fat
sodium: milligrams of sodium
fiber: grams of dietary fiber
carbo: grams of complex carbohydrates
sugars: grams of sugars
potass: milligrams of potassium
vitamins: vitamins and minerals - 0, 25, or 100, indicating the typical percentage of FDA recommended
shelf: display shelf (1, 2, or 3, counting from the floor)
weight: weight in ounces of one serving
cups: number of cups in one serving
rating: a rating of the cereals (Possibly from Consumer Reports?)
Acknowledgements
These datasets have been gathered and cleaned up by Petra Isenberg, Pierre Dragicevic and Yvonne Jansen. The original source can be found here
This dataset has been converted to CSV
Inspiration
Eat too much sugary cereal? Ruin your appetite with this dataset!"
1000 Netflix Shows,Understand the rating distributions of Netflix shows,Chase Willden,38,"Version 1,2017-06-11","film
internet",CSV,87 KB,CC0,"11,602 views","1,660 downloads",13 kernels,2 topics,https://www.kaggle.com/chasewillden/netflix-shows,"Context
Netflix in the past 5-10 years has captured a large populate of viewers. With more viewers, there most likely an increase of show variety. However, do people understand the distribution of ratings on Netflix shows?
Content
Because of the vast amount of time it would take to gather 1,000 shows one by one, the gathering method took advantage of the Netflix’s suggestion engine. The suggestion engine recommends shows similar to the selected show. As part of this data set, I took 4 videos from 4 ratings (totaling 16 unique shows), then pulled 53 suggested shows per video. The ratings include: G, PG, TV-14, TV-MA. I chose not to pull from every rating (e.g. TV-G, TV-Y, etc.).
Acknowledgements
The data set and the research article can be found at The Concept Center
Inspiration
I was watching Netflix with my wife and we asked ourselves, why are there so many R and TV-MA rating shows?"
Who eats the food we grow?,"Worldwide food\feed production and distribution, 1961-2013",Dor Oppenheim,38,"Version 7,2017-11-30|Version 6,2017-11-21|Version 5,2017-11-21|Version 4,2017-11-21|Version 3,2017-11-17|Version 2,2017-11-17|Version 1,2017-11-17","animals
agriculture",CSV,874 KB,CC0,"4,114 views",630 downloads,2 kernels,3 topics,https://www.kaggle.com/dorbicycle/world-foodfeed-production,"Context
Our world population is expected to grow from 7.3 billion today to 9.7 billion in the year 2050. Finding solutions for feeding the growing world population has become a hot topic for food and agriculture organizations, entrepreneurs and philanthropists. These solutions range from changing the way we grow our food to changing the way we eat. To make things harder, the world's climate is changing and it is both affecting and affected by the way we grow our food – agriculture. This dataset provides an insight on our worldwide food production - focusing on a comparison between food produced for human consumption and feed produced for animals.
Content
The Food and Agriculture Organization of the United Nations provides free access to food and agriculture data for over 245 countries and territories, from the year 1961 to the most recent update (depends on the dataset). One dataset from the FAO's database is the Food Balance Sheets. It presents a comprehensive picture of the pattern of a country's food supply during a specified reference period, the last time an update was loaded to the FAO database was in 2013. The food balance sheet shows for each food item the sources of supply and its utilization. This chunk of the dataset is focused on two utilizations of each food item available:
Food - refers to the total amount of the food item available as human food during the reference period.
Feed - refers to the quantity of the food item available for feeding to the livestock and poultry during the reference period.
Dataset's attributes:
Area code - Country name abbreviation
Area - County name
Item - Food item
Element - Food or Feed
Latitude - geographic coordinate that specifies the north–south position of a point on the Earth's surface
Longitude - geographic coordinate that specifies the east-west position of a point on the Earth's surface
Production per year - Amount of food item produced in 1000 tonnes
Acknowledgements
This dataset was meticulously gathered, organized and published by the Food and Agriculture Organization of the United Nations.
Inspiration
Animal agriculture and factory farming is a a growing interest of the public and of world leaders.
Can you find interesting outliers in the data?
What are the fastest growing countries in terms of food production\consumption?
Compare between food and feed consumption."
Did it rain in Seattle? (1948-2017),"More than 25,000 consecutive days of Seattle weather data",Rachael Tatman,38,"Version 1,2017-12-21","united states
north america
climate
weather",CSV,744 KB,CC0,"4,636 views",921 downloads,16 kernels,0 topics,https://www.kaggle.com/rtatman/did-it-rain-in-seattle-19482017,"Context:
Besides coffee, grunge and technology companies, one of the things that Seattle is most famous for is how often it rains. This dataset contains complete records of daily rainfall patterns from January 1st, 1948 to December 12, 2017.
Content
This data was collected at the Seattle-Tacoma International Airport. The dataset contains five columns:
DATE = the date of the observation
PRCP = the amount of precipitation, in inches
TMAX = the maximum temperature for that day, in degrees Fahrenheit
TMIN = the minimum temperature for that day, in degrees Fahrenheit
RAIN = TRUE if rain was observed on that day, FALSE if it was not
Acknowledgements:
This dataset was compiled by NOAA and is in the public domain.
Inspiration:
Can you use this dataset to build a model of whether it will rain on a specific day given information on the previous days?
Is there a correlation between the minimum and maximum temperature? Can you predict one given the other?
Can you model changes in the amount of precipitation over time? Is there seasonality?"
Boston Airbnb Open Data,"A sneak peek into the Airbnb activity in Boston, MA, USA",Airbnb,38,"Version 1,2016-11-17","united states
home
hotels",CSV,72 MB,CC0,"26,681 views","3,426 downloads",70 kernels,2 topics,https://www.kaggle.com/airbnb/boston,"Context
Since 2008, guests and hosts have used Airbnb to travel in a more unique, personalized way. As part of the Airbnb Inside initiative, this dataset describes the listing activity of homestays in Boston, MA.
Content
The following Airbnb activity is included in this Boston dataset: * Listings, including full descriptions and average review score * Reviews, including unique id for each reviewer and detailed comments * Calendar, including listing id and the price and availability for that day
Inspiration
Can you describe the vibe of each Boston neighborhood using listing descriptions?
What are the busiest times of the year to visit Boston? By how much do prices spike?
Is there a general upward trend of both new Airbnb listings and total Airbnb visitors to Boston?
For more ideas, visualizations of all Boston datasets can be found here.
Acknowledgement
This dataset is part of Airbnb Inside, and the original source can be found here."
Random Sample of NIH Chest X-ray Dataset,"5,606 images and labels sampled from the NIH Chest X-ray Dataset",National Institutes of Health Chest X-Ray Dataset,38,"Version 4,2017-11-23|Version 3,2017-11-17|Version 2,2017-11-16|Version 1,2017-11-16","healthcare
health
biotechnology
+ 2 more...",Other,2 GB,CC0,"5,282 views",707 downloads,4 kernels,,https://www.kaggle.com/nih-chest-xrays/sample,"NIH Chest X-ray Dataset Sample
National Institutes of Health Chest X-Ray Dataset
Chest X-ray exams are one of the most frequent and cost-effective medical imaging examinations available. However, clinical diagnosis of a chest X-ray can be challenging and sometimes more difficult than diagnosis via chest CT imaging. The lack of large publicly available datasets with annotations means it is still very difficult, if not impossible, to achieve clinically relevant computer-aided detection and diagnosis (CAD) in real world medical sites with chest X-rays. One major hurdle in creating large X-ray image datasets is the lack resources for labeling so many images. Prior to the release of this dataset, Openi was the largest publicly available source of chest X-ray images with 4,143 images available.
This NIH Chest X-ray Dataset is comprised of 112,120 X-ray images with disease labels from 30,805 unique patients. To create these labels, the authors used Natural Language Processing to text-mine disease classifications from the associated radiological reports. The labels are expected to be >90% accurate and suitable for weakly-supervised learning. The original radiology reports are not publicly available but you can find more details on the labeling process in this Open Access paper: ""ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases."" (Wang et al.)
Link to paper

File contents - This is a random sample (5%) of the full dataset:
sample.zip: Contains 5,606 images with size 1024 x 1024
sample_labels.csv: Class labels and patient data for the entire dataset
Image Index: File name
Finding Labels: Disease type (Class label)
Follow-up #
Patient ID
Patient Age
Patient Gender
View Position: X-ray orientation
OriginalImageWidth
OriginalImageHeight
OriginalImagePixelSpacing_x
OriginalImagePixelSpacing_y

Class descriptions
There are 15 classes (14 diseases, and one for ""No findings"") in the full dataset, but since this is drastically reduced version of the full dataset, some of the classes are sparse with the labeled as ""No findings""
Hernia - 13 images
Pneumonia - 62 images
Fibrosis - 84 images
Edema - 118 images
Emphysema - 127 images
Cardiomegaly - 141 images
Pleural_Thickening - 176 images
Consolidation - 226 images
Pneumothorax - 271 images
Mass - 284 images
Nodule - 313 images
Atelectasis - 508 images
Effusion - 644 images
Infiltration - 967 images
No Finding - 3044 images

Full Dataset Content
The full dataset can be found here. There are 12 zip files in total and range from ~2 gb to 4 gb in size.

Data limitations:
The image labels are NLP extracted so there could be some erroneous labels but the NLP labeling accuracy is estimated to be >90%.
Very limited numbers of disease region bounding boxes (See BBox_list_2017.csv)
Chest x-ray radiology reports are not anticipated to be publicly shared. Parties who use this public dataset are encouraged to share their “updated” image labels and/or new bounding boxes in their own studied later, maybe through manual annotation

Modifications to original data
Original TAR archives were converted to ZIP archives to be compatible with the Kaggle platform
CSV headers slightly modified to be more explicit in comma separation and also to allow fields to be self-explanatory

Citations
Wang X, Peng Y, Lu L, Lu Z, Bagheri M, Summers RM. ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases. IEEE CVPR 2017, ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.pdf
NIH News release: NIH Clinical Center provides one of the largest publicly available chest x-ray datasets to scientific community
Original source files and documents: https://nihcc.app.box.com/v/ChestXray-NIHCC/folder/36938765345

Acknowledgements
This work was supported by the Intramural Research Program of the NClinical Center (clinicalcenter.nih.gov) and National Library of Medicine (www.nlm.nih.gov)."
History of Hearthstone,"346,242 decks representing more than 3 years of gameplay!",romainvincent,37,"Version 3,2017-07-06|Version 2,2017-07-03|Version 1,2017-06-27",video games,CSV,78 MB,Other,"7,991 views",481 downloads,5 kernels,,https://www.kaggle.com/romainvincent/history-of-hearthstone,"Context
Hearthstone is a very popular collectible card game published by Blizzard Entertainment in 2014. The goal of the game consists in building a 30 cards deck in order to beat your opponent. A few weeks ago, I decided to download all the decks posted by players at Hearthpwn. The code to download the data is available here.
Content
This upload is composed of two files :
data.json / data.csv
Contains the actual Hearthstone deck records. Each record features :
date (str) : the date of publication (or last update) of the deck.
user (str) : the user who uploaded the deck.
deck_class (str) : one of the nine character class in Hearthstone (Druid, Priest, ...).
deck_archetype (str) : the theme of deck labelled by the user (Aggro Druid, Dragon Priest, ...).
deck_format (str) : the game format of the deck on the day data was recorded (W for ""Wild"" or S for ""Standard"").
deck_set (str) : the latest expansion published prior the deck publication (Naxxramas, TGT Launch, ...).
deck_id (int) : the ID of the deck.
deck_type (str) : the type of the deck labelled by the user :
Ranked Deck : a deck played on ladder.
Theorycraft : a deck built with unreleased cards to get a gist of the future metagame.
PvE Adventure : a deck built to beat the bosses in adventure mode.
Arena : a deck built in arena mode.
Tavern Brawl : a deck built for the weekly tavern brawl mode.
Tournament : a deck brought at tournament by a pro-player.
None : the game type was not mentioned.
rating (int) : the number of upvotes received by that deck.
title (str) : the name of the deck.
craft_cost (int) : the amount of dust (in-game craft material) required to craft the deck.
cards (list) : a list of 30 card ids. Each ID can be mapped to the card description using the reference file.
refs.json
Contains the reference to the cards played in Hearthstone. This file was originally proposed on HearthstoneJSON. Each record features a lot of informations about the cards, I'll list the most important :
dbfId (int) : the id of the card (the one used in data.json).
rarity (str) : the rarity of the card (EPIC, RARE, ...).
cardClass (str) : the character class (WARLOCK, PRIEST, ...).
artist (str) : the artist behind the card's art.
collectible (bool) : whether or not the card can be collected.
cost (int) : the card play cost.
health (int) : the card health (if it's a minion).
attack (int) : the card attack (if it's a minion).
name (str) : the card name.
flavor (str) : the card's flavor text.
set (str) : the set / expansion which featured this card.
text (int) : the card's text.
type (str) : the card's type (MINION, SPELL, ...).
race (str) : the card's race (if it's a minion).
set (str) : the set / expansion which featured this card.
...
If you need help cleaning the data take a look at my start over kernel!
What you could do :
Try to predict the deck archetype based on the cards features in the deck.
Seek relationships between the cost of the deck and it's popularity.
Describe the evolution of the meta-game over-time.
Find out unbalanced (overplayed) cards"
Pakistan Suicide Bombing Attacks,Most Authentic Count of Suicide Bombing Attacks in Pakistan (1995-2016),Zeeshan-ul-hassan Usmani,37,"Version 6,2017-12-01|Version 5,2017-10-10|Version 4,2017-07-19|Version 3,2017-03-23|Version 2,2017-01-29|Version 1,2017-01-26",crime,CSV,226 KB,CC0,"11,927 views",943 downloads,27 kernels,,https://www.kaggle.com/zusmani/pakistansuicideattacks,"Context
Pakistan Suicide Bombing Attacks (1995-2016)
Suicide bombing is an operational method in which the very act of the attack is dependent upon the death of the perpetrator. Though only 3% of all terrorist attacks around the world can be classified as suicide bombing attacks these account for 48% of the casualties. Explosions and suicide bombings have become the modus operandi of terrorist organizations throughout the world. The world is full of unwanted explosives, brutal bombings, accidents, and violent conflicts, and there is a need to understand the impact of these explosions on one’s surroundings, the environment, and most importantly on human bodies. From 1980 to 2001 (excluding 9/11/01) the average number of deaths per incident for suicide bombing attacks was 13. This number is far above the average of less than one death per incident across all types of terrorist attacks over the same time period. Suicide bombers, unlike any other device or means of destruction, can think and therefore detonate the charge at an optimal location with perfect timing to cause maximum carnage and destruction. Suicide bombers are adaptive and can quickly change targets if forced by security risk or the availability of better targets. Suicide attacks are relatively inexpensive to fund and technologically primitive, as IEDs can be readily constructed.
World has seen more than 3,600 suicide bombing attacks in over 40 countries since 1982. Suicide Bombing has wreaked havoc in Pakistan in the last decade or so. From only a couple of attacks before 2000, it kept escalating after the US Operation Enduring Freedom in Afghanistan, promiscuously killing hundreds of people each year, towering as one of the most prominent security threats that every single Pakistani faces today. The conundrum of suicide bombing in Pakistan has obliterated 6,982 clean-handed civilians and injured another 17,624 in a total of 475 attacks since 1995. More than 94% of these attacks have taken place after year 2006. From 2007 to 2013 the country witnessed a suicide bombing attack on every 6th day that increased to every 4th day in 2013. Counting the dead and the injured, each attack victimizes 48 people in Pakistan.
Pakistan Body Count (www.PakistanBodyCount.org) is the oldest and most accurate running tally of suicide bombings in Pakistan. The given database (PakistanSuicideAttacks.CSV) has been populated by using majority of the data from Pakistan Body Count, and building up on it by canvassing open source newspapers, media reports, think tank analyses, and personal contacts in media and law enforcement agencies. We provide a count of the people killed and injured in suicide attacks, including the ones who died later in hospitals or homes due to injuries caused or aggravated by these attacks (second and tertiary blast injuries), making it the most authentic source for suicide attacks related data in this region.
We will keep releasing the updates every quarter at this page.
Content
Geography: Pakistan
Time period: 1995-2016
Unit of analysis: Attack
Dataset: The dataset contains detailed information of 475 suicide bombing attacks in Pakistan that killed an estimated 6,982 and injured 17,624 people.
Variables: The dataset contains Serial No, Incident Date, Islamic Date (based on Islamic lunar calendar), approximate Time, Long-Lat, City, Province, Location, Location Sensitivity & Type, Target type and Sect, Open/Close Space (as it will change the impact of blast waves due to reflection), min and max number of people killed and injured, number of suicide bombers, amount of explosive being used and the name of hospitals where victims went for treatment.
Sources: Unclassified media articles, hospital reports, think tank analysis and reports, and government official press releases.
Acknowledgements & References
Pakistan Body Count has been leveraged extensively in scholarly publications, reports, media articles and books. The website and the dataset has been collected and curated by the founder Zeeshan-ul-hassan Usmani.
Users are allowed to use, copy, distribute and cite the dataset as follows: “Zeeshan-ul-hassan Usmani, Pakistan Body Count, Pakistan Suicide Bombing Attacks Dataset, Kaggle Dataset Repository, Jan 25, 2017.”
Past Work
Zeeshan-ul-hassan Usmani and Daniel Kirk, “Simulation of Suicide Bombing – Using Computers to Save Lives”, I-Universe, New York, NY, April 2011
Zeeshan-ul-hassan Usmani and Daniel Kirk, “Modeling and Simulation of Explosion Effectiveness as a Function of Blast and Crowd Characteristics”, The Journal of Defense Modeling and Simulation: Applications, Methodology, Technology, Sage Publications with Society of Simulation, Vol. 6, No. 2, pp. 79-95, Vista, CA, USA, October 2009
Muhammad Irfan and Zeeshan-ul-hassan Usmani, “Suicide Terrorism and its New Target –Pakistan”, in Wars, Insurgencies and Terrorist Attacks: A Psychosocial Perspective from The Muslim World, by Unaiza Niaz, Oxford University Press, Canada, July 2010
Sana Rasheed, Data Science for Suicide Bombings, I-Universe, New York, NY, December 2016
Zeeshan-ul-hassan Usmani and Sana Rasheed, “Terrorism: What Data Sciences Can Do?”, 20th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 2014, Data Framework Track) at Bloomberg, New York, NY, USA (August 24-27, 2014)
Zeeshan-ul-hassan Usmani, “Suicide Bombing Forecaster – Novel Techniques to Predict Patterns of Suicide Bombing in Pakistan”, 2012 Conference on Homeland Security, part of 2012 Autumn Simulation Multi-Conference, San Diego, CA, USA, October 28 – 31, 2012
Zeeshan-ul-hassan Usmani, “BlastSim – Simulation to Save Lives”, IEEE/SIC Winter Simulation Conference, PhD Colloquium, Austin, Texas, December 13-16, 2009
Zeeshan-ul-hassan Usmani, Fawzi Alghamdi, and Daniel Kirk, “BlastSim – Multi-agent Simulation of Suicide Bombing“, IEEE Symposium: Computational Intelligence for Security and Defense Applications (CISDA), Ottawa, Canada, July 8-10, 2009
Zeeshan-ul-hassan Usmani, Eyosias Imana and Daniel Kirk, “Virtual Iraq – Simulation of Insurgent Attacks”, IEEE Workshop on Computational Intelligence in Virtual Environments (CIVE), March 30-April 2, 2009
Zeeshan-ul-hassan Usmani, Eyosias Imana, and Daniel Kirk, “Random Walk in Extreme Conditions – An Agent Based Simulation of Suicide Bombing”, IEEE Symposium on Intelligent Agents, March, 2009
Zeeshan-ul-hassan Usmani, Eyosias Imana and Daniel Kirk, “Escaping Death – Geometrical Recommendations for High Value Targets”, IEEE International Joint Conferences on Computer, Information and Systems Sciences and Engineering (CIS2E 08), Bridgeport, CT, December 5–13, 2008
Zeeshan-ul-hassan Usmani and Daniel Kirk, “Extreme Conditions for Intelligent Agents”, IEEE 2008 WI-IAT Doctoral Workshop, Sydney, Australia, December 9-12, 2008
Zeeshan-ul-hassan Usmani, Andrew English & Richard Griffith, “The Effects of a Suicide Bombing: Crowd Formations”, Inter-service/Industry Training, Simulation, and Education Conference (I/ITSEC), Orlando, FL, Nov 26-29 2007
Inspiration
Some ideas worth exploring:
How many people got killed and injured per year?
Visualize suicide attacks on timeline
Find out any correlation with number of suicide bombing attacks with drone attacks
Find out any correlation with suicide bombing attacks with influencing events given in the dataset
Can we predict the next suicide bombing attack?
Find the correlation between blast/explosive weight and number of people killed and injured
Find the impact of holiday type on number of blast victims
Find the correlation between Islamic date and blast day/time/size/number of victims
Find the Top 10 locations of blasts
Find the names of hospitals sorted by number of victims
Questions?
For detailed visit www.PakistanBodyCount.org
Or contact Pakistan Body Count staff at info@pakistanbodycount.org"
R Questions from Stack Overflow,Full text of Stack Overflow Q&A about the R statistical programming language,Stack Overflow,37,"Version 3,2017-09-26|Version 2,2016-10-21|Version 1,2016-10-20","linguistics
internet
programming languages",CSV,516 MB,Other,"15,609 views",956 downloads,39 kernels,,https://www.kaggle.com/stackoverflow/rquestions,"Full text of questions and answers from Stack Overflow that are tagged with the r tag, useful for natural language processing and community analysis.
This is organized as three tables:
Questions contains the title, body, creation date, score, and owner ID for each R question.
Answers contains the body, creation date, score, and owner ID for each of the answers to these questions. The ParentId column links back to the Questions table.
Tags contains the tags on each question besides the R tag.
For space reasons only non-deleted and non-closed content are included in the dataset. The dataset contains questions up to 24 September 2017 (UTC).
License
All Stack Overflow user contributions are licensed under CC-BY-SA 3.0 with attribution required."
Eating & Health Module Dataset,American Time Use Survey (ATUS) Eating & Health Module Files from 2014,US Bureau of Labor Statistics,37,"Version 2,2016-11-10|Version 1,2016-11-09","nutrition
demographics",CSV,19 MB,Other,"16,422 views","1,820 downloads",88 kernels,2 topics,https://www.kaggle.com/bls/eating-health-module-dataset,"Context
The ATUS Eating & Health (EH) Module was fielded from 2006 to 2008 and again in 2014 to 2016. The EH Module data files contain additional information related to eating, meal preparation and health.
Data for 2015 currently are being processed and have not yet been released. Data collection is planned to run through December 2016.
Content
There are 3 datasets from 2014:
The EH Respondent file, which contains information about EH respondents, including general health and body mass index. There are 37 variables.
The EH Activity file, which contains information such as the activity number, whether secondary eating occurred during the activity, and the duration of secondary eating. There are 5 variables.
The EH Replicate weights file, which contains miscellaneous EH weights. There are 161 variables.
The data dictionary can be found here.
Acknowledgements
The original datasets can be found here.
Inspiration
Some ideas for exploring the datasets are:
What is the relationship between weight or BMI and meal preparation patterns, consumption of fresh/fast food, or snacking patterns?
Do grocery shopping patterns differ by income?"
"The Academy Awards, 1927-2015",What actors and films have received the most Oscars?,Academy of Motion Picture Arts and Sciences,37,"Version 1,2017-02-14",film,CSV,775 KB,Other,"12,072 views","1,835 downloads",31 kernels,0 topics,https://www.kaggle.com/theacademy/academy-awards,"Context
Each January, the entertainment community and film fans around the world turn their attention to the Academy Awards. Interest and anticipation builds to a fevered pitch leading up to the Oscar telecast in February, when hundreds of millions of movie lovers tune in to watch the glamorous ceremony and learn who will receive the highest honors in filmmaking.
Achievements in up to 25 regular categories will be honored on February 26, 2017, at the 89th Academy Awards presentation at the Dolby Theatre at Hollywood & Highland Center.
Content
The Academy Awards Database contains the official record of past Academy Award winners and nominees. The data is complete through the 2015 (88th) Academy Awards, presented on February 28, 2016.
Acknowledgements
The awards data was scraped from the Official Academy Awards Database; nominees were listed with their name first and film following in some categories, such as Best Actor/Actress, and in the reverse for others.
Inspiration
Do the Academy Awards reflect the diversity of American films or are the #OscarsSoWhite? Which actor/actress has received the most awards overall or in a single year? Which film has received the most awards in a ceremony? Can you predict who will receive the 2016 awards?"
UK Car Accidents 2005-2015,Data from the UK Department for Transport,silicon99,36,"Version 3,2017-02-21|Version 2,2017-02-01|Version 1,2017-02-01",road transport,CSV,534 MB,CC0,"19,343 views","3,315 downloads",29 kernels,2 topics,https://www.kaggle.com/silicon99/dft-accident-data,"Context
UK police forces collect data on every vehicle collision in the uk on a form called Stats19. Data from this form ends up at the DfT and is published at https://data.gov.uk/dataset/road-accidents-safety-data
Content
There are 3 CSVs in this set. Accidents is the primary one and has references by Accident_Index to the casualties and vehicles tables. This might be better done as a database.
Inspiration
Questions to ask of this data -
combined with population data, how do different areas compare?
what trends are there for accidents involving different road users eg motorcycles, peds, cyclists
are road safety campaigns effective?
likelihood of accidents for different groups / vehicles
many more..
Manifest
dft05-15.tgz - tar of Accidents0515.csv, Casualties0515.csv and Vehicles0515.csv tidydata.sh - script to get and tidy data."
Board Game Data,Data is a collection of board game information from Board Game Geek,mrpantherson,36,"Version 5,2018-02-16|Version 4,2017-06-16|Version 3,2017-06-08|Version 2,2017-04-29|Version 1,2017-04-07",board games,CSV,818 KB,CC0,"11,059 views","1,193 downloads",39 kernels,6 topics,https://www.kaggle.com/mrpantherson/board-game-data,"Context
Being a fan of board games, I wanted to see if there was any correlation with a games rating and any particular quality, the first step was to collect of this data.
Content
The data was collected in March of 2017 from the website https://boardgamegeek.com/, this site has an API to retrieve game information (though sadly XML not JSON).
Acknowledgements
Mainly I want to thank the people who run the board game geek website for maintaining such a great resource for those of us in the hobby.
Inspiration
I wish I had some better questions to ask of the data, perhaps somebody else can think of some good ways to get some insight of this dataset."
San Francisco Library Usage,"Anonymized library usage data by over 420,000 patrons",DataSF,36,"Version 2,2017-01-07|Version 1,2017-01-05",libraries,CSV,33 MB,Other,"7,141 views",771 downloads,28 kernels,,https://www.kaggle.com/datasf/sf-library-usage-data,"Context
San Francisco's Integrated Library System (ILS) is composed of bibliographic records including inventoried items, patron records, and circulation data. The data is used in the daily operation of the library, including circulation, online public catalog, cataloging, acquisitions, collection development, processing, and serials control. This dataset represents the usage of inventoried items by patrons (~420K records).
Content
The dataset includes approximately 420,000 records, with each record representing an anonymized library patron. Individual columns include statistics on the type code and age of the patron, the year the patron registered (only since 2003), and how heavily the patron has been utilizing the library system (in terms of number of checkouts) since first registering.
For more information on specific columns refer to the official data dictionary and the information in the Column Metadata on the /Data tab.
Acknowledgements
The data is provided by SF Public Library via the San Francisco Open Data Portal, under the PDDL 1.0 ODC Public Domain Dedication and Licence (PDDL).
Photo via Flickr Kolya Miller (CC BY-NC-SA 2.0).
Inspiration
What attributes are most associated with library activity (# of checkouts, # of renewals)?
Can you group the data into type of patrons? What classifiers would you use to predict patron type?"
Solar and Lunar Eclipses,"Date, time, and location of every eclipse in five thousand years",NASA,36,"Version 1,2017-02-09","astronomy
space",CSV,2 MB,CC0,"5,630 views",684 downloads,15 kernels,,https://www.kaggle.com/nasa/solar-eclipses,"Context
Eclipses of the sun can only occur when the moon is near one of its two orbital nodes during the new moon phase. It is then possible for the Moon's penumbral, umbral, or antumbral shadows to sweep across Earth's surface thereby producing an eclipse. There are four types of solar eclipses: a partial eclipse, during which the moon's penumbral shadow traverses Earth and umbral and antumbral shadows completely miss Earth; an annular eclipse, during which the moon's antumbral shadow traverses Earth but does not completely cover the sun; a total eclipse, during which the moon's umbral shadow traverses Earth and completely covers the sun; and a hybrid eclipse, during which the moon's umbral and antumbral shadows traverse Earth and annular and total eclipses are visible in different locations. Earth will experience 11898 solar eclipses during the five millennium period -1999 to +3000 (2000 BCE to 3000 CE).
Eclipses of the moon can occur when the moon is near one of its two orbital nodes during the full moon phase. It is then possible for the moon to pass through Earth's penumbral or umbral shadows thereby producing an eclipse. There are three types of lunar eclipses: a penumbral eclipse, during which the moon traverses Earth's penumbral shadow but misses its umbral shadow; a partial eclipse, during which the moon traverses Earth's penumbral and umbral shadows; and a total eclipse, during which the moon traverses Earth's penumbral and umbral shadows and passes completely into Earth's umbra. Earth will experience 12064 lunar eclipses during the five millennium period -1999 to +3000 (2000 BCE to 3000 CE).
Acknowledgements
Lunar eclipse predictions were produced by Fred Espenak from NASA's Goddard Space Flight Center."
"Fatal Police Shootings, 2015-Present",Civilians shot and killed by on-duty police officers in United States,The Washington Post,36,"Version 3,2017-03-11|Version 2,2017-01-03|Version 1,2016-11-26","crime
demographics",CSV,192 KB,CC4,"10,025 views","1,737 downloads",48 kernels,,https://www.kaggle.com/washingtonpost/police-shootings,"The Washington Post is compiling a database of every fatal shooting in the United States by a police officer in the line of duty since January 1, 2015.
In 2015, The Post began tracking more than a dozen details about each killing — including the race of the deceased, the circumstances of the shooting, whether the person was armed and whether the victim was experiencing a mental-health crisis — by culling local news reports, law enforcement websites and social media and by monitoring independent databases such as Killed by Police and Fatal Encounters.
The Post is documenting only those shootings in which a police officer, in the line of duty, shot and killed a civilian — the circumstances that most closely parallel the 2014 killing of Michael Brown in Ferguson, Missouri, which began the protest movement culminating in Black Lives Matter and an increased focus on police accountability nationwide. The Post is not tracking deaths of people in police custody, fatal shootings by off-duty officers or non-shooting deaths.
The FBI and the Centers for Disease Control and Prevention log fatal shootings by police, but officials acknowledge that their data is incomplete. In 2015, The Post documented more than two times more fatal shootings by police than had been recorded by the FBI.
The Post’s database is updated regularly as fatal shootings are reported and as facts emerge about individual cases. The Post is seeking assistance in making the database as comprehensive as possible. To provide information about fatal police shootings, send us an email at policeshootingsfeedback@washpost.com.
CREDITS
Research and Reporting: Julie Tate, Jennifer Jenkins and Steven Rich
Production and Presentation: John Muyskens, Kennedy Elliott and Ted Mellnik"
"Olympic Sports and Medals, 1896-2014",Which countries and athletes have won the most medals at the Olympic games?,The Guardian,36,"Version 1,2017-01-24",olympic games,CSV,3 MB,CC4,"15,684 views","3,899 downloads",27 kernels,,https://www.kaggle.com/the-guardian/olympic-games,"Content
Which Olympic athletes have the most gold medals? Which countries are they from and how has it changed over time?
More than 35,000 medals have been awarded at the Olympics since 1896. The first two Olympiads awarded silver medals and an olive wreath for the winner, and the IOC retrospectively awarded gold, silver, and bronze to athletes based on their rankings. This dataset includes a row for every Olympic athlete that has won a medal since the first games.
Acknowledgements
Data was provided by the IOC Research and Reference Service and published by The Guardian's Datablog."
Google Text Normalization Challenge,"Text-to-speech synthesis text normalization data, from Sproat & Jaitly 2016",Google Natural Language Understanding Research,36,"Version 1,2017-04-27","languages
linguistics",CSV,9 GB,CC4,"10,298 views","1,539 downloads",7 kernels,0 topics,https://www.kaggle.com/google-nlu/text-normalization,"Challenge Description
This dataset and accompanying paper present a challenge to the community: given a large corpus of written text aligned to its normalized spoken form, train an RNN to learn the correct normalization function. That is, a date written ""31 May 2014"" is spoken as ""the thirty first of may twenty fourteen."" We present a dataset of general text where the normalizations were generated using an existing text normalization component of a text-to-speech (TTS) system. This dataset was originally released open-source here and is reproduced on Kaggle for the community.
The Data
The data in this directory are the English language training, development and test data used in Sproat and Jaitly (2016).
The following divisions of data were used:
Training: output_1 through output_21 (corresponding to output-000[0-8]?-of-00100 in the original dataset)
Runtime eval: output_91 (corresponding to output-0009[0-4]-of-00100 in the original dataset)
Test data: output_96 (corresponding to output-0009[5-9]-of-00100 in the original dataset)
In practice for the results reported in the paper only the first 100,002 lines of output-00099-of-00100 were used (for English).
Lines with """" in two columns are the end of sentence marker, otherwise there are three columns, the first of which is the ""semiotic class"" (Taylor, 2009), the second is the input token and the third is the output, following the paper cited above.
All text is from Wikipedia. All data were extracted on 2016/04/08, and run through the Google Kestrel TTS text normalization system (Ebden and Sproat, 2015), so that the notion of ""token"", ""semiotic class"" and reference output are all Kestrel's notion.
Our Research
In this paper, we present our own experiments with this data set with a variety of different RNN architectures. While some of the architectures do in fact produce very good results when measured in terms of overall accuracy, the errors that are produced are problematic, since they would convey completely the wrong message if such a system were deployed in a speech application. On the other hand, we show that a simple FST-based filter can mitigate those errors, and achieve a level of accuracy not achievable by the RNN alone.
Though our conclusions are largely negative on this point, we are actually not arguing that the text normalization problem is intractable using an pure RNN approach, merely that it is not going to be something that can be solved merely by having huge amounts of annotated text data and feeding that to a general RNN model. And with open-source data, we provide a novel data set for sequence-to-sequence modeling in the hopes that the the community can find better solutions.
Disclaimer
This is not an official Google product.
References
Ebden, Peter and Sproat, Richard. 2015. The Kestrel TTS text normalization system. Natural Language Engineering. 21(3).
Richard Sproat and Navdeep Jaitly. 2016. RNN Approaches to Text Normalization: A Challenge. Released on arXiv.org: https://arxiv.org/abs/1611.00068
Taylor, Paul. 2009. Text-to-Speech Synthesis. Cambridge University Press, Cambridge."
NFL Draft Outcomes,All players selected in the NFL Draft from 1985 to 2015 with outcome statistics,Ron Graf,35,"Version 1,2016-12-19",american football,CSV,780 KB,Other,"7,327 views",975 downloads,23 kernels,0 topics,https://www.kaggle.com/ronaldjgrafjr/nfl-draft-outcomes,"Context
Consolidated draft data from http://www.pro-football-reference.com/ for all drafts from 1985 to 2015.
Content
Pro-Football-Reference AV: Approximate Value is PFR's attempt to attach a single number to every player-season since 1960. Methodology can be found here: http://www.pro-football-reference.com/blog/indexd961.html?page_id=8061
Player_Id Pro Football Reference Player Id
Year Draft Year
Rnd Draft Round
Pick Draft Pick
Tm Team
Player Player first and last name
Pos Position unfiltered
Position Standard Position standardized to one of the following QB, LB, WR, T, DE, RB, DB, DT, C, C, G, TE, FB, P, LS, K
First4AV AV accumulated for this player's first four seasons
Age Age at time of draft
To Year of last season played
AP1 # of first team all-pro selections
PB # of pro-bowl selections
St # of years as a primary starter in their primary position
CarAV Weighted Career AV - 100% of best season, 95% of second best season, 90% of third best season, and so on
DrAV AV accumulated for team that drafted this player
G Games played
Cmp Pass completions
Pass_Att Pass attempts
Pass_Yds Yards gained by passing
Pass_TD Passing touchdowns
Pass_Int Interceptions thrown
Rush_Att Rushing attempts
Rush_Yds Rushing yards gained
Rush_TDs Rushing touchdowns
Rec Receptions
Rec_Yds Receiving yards gained
Rec_Tds Receiving touchdowns
Tkl Tackles
Def_Int Defensive interceptions
Sk Sacks
College/Univ College/University attended by player
Acknowledgements
http://www.pro-football-reference.com/"
Horse Racing - Tipster Bets,Thirty nine thousand bets from thirty one tipsters,gunner38,35,"Version 3,2016-09-14|Version 2,2016-09-14|Version 1,2016-09-13",horse racing,CSV,3 MB,Other,"16,607 views","1,060 downloads",19 kernels,4 topics,https://www.kaggle.com/gunner38/horseracing,"Horse Racing - A different and profitable approach
The traditional approach in attempting to make a profit from horse-racing, using machine learning techniques, is to use systems involving dozens and dozens of variables. These systems include the following types of variables:
Horse - Name, Sex, Age, Pedigree, Weight, Speed over various distances, race data with finishing times and positions - etc. Trainer info. Jockey info. Track info - Track, track conditions - etc.
And a whole lot more.
Finding, compiling, maintaining and updating this data is a massive task for the individual. Unless you have access to a database of such data - where would you even start?
We have a different approach.
We collect, maintain and use data from various 'Tipsters'. The tipsters use their skill to study the horses and make a prediction - that they think a particular horse will win a particular race. We take those tipsters predictions and put them through a machine learning algorithm (microsoft azure) asking it to predict a 'win' or 'lose' based upon the tipsters performance history.
We have a database of approx. 39,000 bets using 31 odd tipsters. Fifteen tipsters are active and sixteen tipsters are inactive The betting history for the inactive tipsters is used in the dataset as it appears to add 'weight' to the system when considering active tips.
We have been using this system live for three months now and although it has it's ups and downs - it makes money! One bookmaker has already closed our account.
We are looking to further optimize the system to reach it's maximum efficiency coupled with a betting strategy to increase profitability. We ask for your help. If you can produce an 'Azure' system more efficient than ours - then further information will be shared with you.
Questions
Are bets from inactive tipsters critical to performance?
Is it better to have all the tipsters 'stacked on top of each other' in one large dataset or is the system better served by separating them out?
Predicting Bets
When we ask the system to predict if a bet will Win or Lose for say Tipster A - we take the last ID number for that Tipster and add one to it - making it a new ID - outside the systems experience. That ID is used for all that Tipsters bets until the system is updated. The system is updated once a week.
Good hunting.
Gunner38"
NFL Statistics,"Basic NFL statistics, career statistics, and game logs",KendallGillies,35,"Version 1,2017-06-09",american football,CSV,93 MB,Other,"10,675 views","1,587 downloads",3 kernels,3 topics,https://www.kaggle.com/kendallgillies/nflstatistics,"NFL-Statistics-Scrape
Here are the basic statistics, career statistics and game logs provided by the NFL on their website (http://www.nfl.com) for all players past and present.
Summary
The data was scraped using a Python code. The code can be located at Github: https://github.com/kendallgillies/NFL-Statistics-Scrape
Explanation of Data
The first main group of statistics is the basic statistics provided for each player. This data is stored in the CSV file titled Basic_Stats.csv along with the player’s name and URL identifier. If available the data pulled for each player is as follows:
Number
Position
Current Team
Height
Weight
Age
Birthday
Birth Place
College Attended
High School Attended
High School Location
Experience
The second main group of statistics gathered for each player are their career statistics. While each player has a main position they play, they will have statistics in other areas; therefore, the career statistics are divided into statistics types. The statistics are then stored in CSV files based on statistic type along with the player name, URL identifier and position (if available). The following are the career statistics types and accompanying CSV file names:
Defensive Statistics – Career_Stats_Defensive.csv
Field Goal Kickers - Career_Stats_Field_Goal_Kickers.csv
Fumbles - Career_Stats_Fumbles.csv
Kick Return - Career_Stats_Kick_Return.csv
Kickoff - Career_Stats_Kickoff.csv
Offensive Line - Career_Stats_Offensive_Line.csv
Passing - Career_Stats_Passing.csv
Punt Return - Career_Stats_Punt_Return.csv
Punting - Career_Stats_Punting.csv
Receiving - Career_Stats_Receiving.csv
Rushing - Career_Stats_Rushing.csv
The final group of statistics is the game logs for each player. The game logs are stored by position and have the player name, URL identifier and position (if available). The following are the game log types and accompanying CSV file names:
Quarterback – Game_Logs_Quarterback.csv
Running back – Game_Logs_Runningback.csv
Wide Receiver and Tight End – Game_Logs_Wide_Receiver_and_Tight_End.csv
Offensive Line – Game_Logs_Offensive_Line.csv
Defensive Lineman – Game_Logs_Defensive_Lineman.csv
Kickers – Game_Logs_Kickers.csv
Punters – Game_Logs_Punters.csv
Glossary
While most of the abbreviations used by the NFL have been translated in the table headers in the data files, there are still a couple of abbreviations used.
FG: Field Goal
TD: Touchdown
Int: Interception"
Tree Census in New York City,What tree species are thriving on the streets of each NYC borough?,NYC Parks and Recreation,35,"Version 2,2017-07-17|Version 1,2017-02-07","plants
forestry",CSV,475 MB,CC0,"5,243 views",657 downloads,7 kernels,0 topics,https://www.kaggle.com/nycparks/tree-census,"Context
New York City’s trees shade us in the summer, beautify our neighborhoods, help reduce noise, and support urban wildlife. Beyond these priceless benefits, our urban forest provides us a concrete return on the financial investment we put into it. This return includes stormwater interception, energy conservation, air pollutant removal, and carbon dioxide storage. Our publicly owned trees are as much of an asset to us as our streets, sewers, bridges, and public buildings.
Content
This dataset includes a record for every tree in New York City and includes the tree's location by borough and latitude/longitude, species by Latin name and common names, size, health, and issues with the tree's roots, trunk, and branches.
Acknowledgements
The 2015, 2005, and 1995 tree censuses were conducted by NYC Parks and Recreation staff, TreesCount! program staff, and hundreds of volunteers."
Daily Happiness & Employee Turnover,Is There a Relationship Between Employee Happiness and Job Turnover?,Jose Berengueres,35,"Version 3,2017-08-07|Version 2,2017-08-07|Version 1,2017-08-05",economics,CSV,49 MB,ODbL,"8,454 views",927 downloads,9 kernels,3 topics,https://www.kaggle.com/harriken/employeeturnover,"Can Happiness Predict Employee Turnover, or is it the Other Way Around?
It is the summer of 2016. I am in Barcelona and it is hot and humid. By chance I go to a talk where Alex Rios - the ceo of myhappyforce.com explains his product. He has built an app where employees report daily happiness levels at work. This app is used by companies to track happiness of the workforce. After the talk I ask him if he would opensource the (anonymized data) so we can better understad the phenomenon of employee turnover. Here is what we did, we developed a model that predicts which employees will churn. Then we looked at the features (used by the model) that are common to employees that churn. The top feautures of employees that churn are:
low ratio of likes received (likeability)
low posting frequency (engagement),
low relative happiness (employee happiness normalized by company mean).
Surprisingly, a priori expected explanatory features such as mean happiness level and the ratio of likes (positivity), were not significant. Precision@50 = 80% out of a test set with 116 churns, sample size N=2k. Another surprise was that raw happiness is a bad predictor of churn. But, the question is, What did we miss? Can you find more insights?
Starter script
R starter script https://www.kaggle.com/harriken/how-many-unlikes-it-takes-to-get-fired
Content
The data consists of four tables: votes, comments, interactions and churn. A vote was obtained when an employee opened the app and answered the question: How happy are you at work today? To vote the employee indicates their feeling by touching one of four icons that appeared on the screen. After the employee indicates their happiness level, a second screen appears where they can input a text explanation (usually a complaint, suggestion or comment), this is the comments table. Out of 4,356 employees, 2,638 employees commented at least once. Finally, in a third screen the employee can see their peers’ comments and like or dislike them, this data is stored in the interactions table. 3,516 employees liked or disliked at least one of their peers’ comments. The churn table contains when an employee churned (quit or was fired).
Acknowledgements
Python script version with social graph features: http://bit.ly/2v2sEZg
More detailed R scripts: https://github.com/orioli/e3
The paper which was presented at ASONAM 2017 Sydney
Slides https://www.slideshare.net/harriken/ieee-happiness-an-inside-job-asoman-2017
Inspiration
The cost of employee turnover has been pointed out extensively in the literature. A high turnover rate not only increases human resource costs, which can reach up to 150% of the annual salary per replaced employee, but it also has social costs, as it is correlated with lower wages, lower productivity per employee, and not surprisingly, a less loyal workforce 1. For reference, in 2006, turnover at Walmart’s Sam’s Club was 44% with an average hourly pay of $10.11, while at Costco it was a much lower 17% with a higher $17.0 hourly wage 2. In addition, a more recent study correlated companies with low turnover with a series of socially positive characteristics dubbed high-involvement work practices 3. On the other hand, research on employee turnover (churn) is not a prolific topic in the engineering community. In IEEE publications, one can find just over 278 publications with titles containing the keyword churn, and the bulk of those focus on customer churn, and specifically churn in the telecommunications industry, while on the topic of employee churn there is just one title indexed 4. The goal is to clarify the characteristics of employees that will churn (or that are at risk of churning), to help companies understand the causes so they can reduce the turnover rate."
RSNA Bone Age,Predict Age from X-Rays,Kevin Mader,35,"Version 2,2018-01-24|Version 1,2018-01-24","healthcare
orthopedic surgery
pediatrics
image data",Other,9 GB,Other,"3,306 views",431 downloads,5 kernels,,https://www.kaggle.com/kmader/rsna-bone-age,"Context
At RSNA 2017 there was a contest to correctly identify the age of a child from an X-ray of their hand. This is the dataset on Kaggle making it easier to experiment with and do educational demos. Additionally maybe there are some new ideas for building smarter models for handling X-ray images.
Content
A number of folders full of images (digital and scanned) with a CSV containing the age (what is to be predicted) and the gender (useful additional information)
Acknowledgements
The dataset was originally published on CloudApp as an RSNA challenge.
Original Dataset Acknowledgements
The Radiological Society of North America (RSNA) Radiology Informatics Committee (RIC) Pediatric Bone Age Machine Learning Challenge Organizing Committee:
Kathy Andriole, Massachusetts General Hospital
Brad Erickson, Mayo Clinic
Adam Flanders, Thomas Jefferson University
Safwan Halabi, Stanford University
Jayashree Kalpathy-Cramer, Massachusetts General Hospital
Marc Kohli, University of California - San Francisco
Luciano Prevedello, The Ohio State University
Data sets used in the Pediatric Bone Age Challenge have been contributed by Stanford University, the University of Colorado and the University of California - Los Angeles.
The MedICI platform (built CodaLab) used for the challenge is provided by Jayashree Kalpathy-Cramer, supported through NIH grants (U24CA180927) and a contract from Leidos.
Inspiration
Can you predict with better than 4.2 months accuracy?
Is identifying the joints an important step?
What algorithms work best?
What do the algorithms focus on?
Is gender a necessary piece of information or can it be automatically derived from the image?"
GloVe: Global Vectors for Word Representation,Pre-trained word vectors from Wikipedia 2014 + Gigaword 5,Rachael Tatman,35,"Version 1,2017-08-05","languages
linguistics",Other,1 GB,Other,"5,055 views",591 downloads,15 kernels,0 topics,https://www.kaggle.com/rtatman/glove-global-vectors-for-word-representation,"Context
GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.
Content
This dataset contains English word vectors pre-trained on the combined Wikipedia 2014 + Gigaword 5th Edition corpora (6B tokens, 400K vocab). All tokens are in lowercase. This dataset contains 50-dimensional, 100-dimensional and 200-dimensional pre trained word vectors. For 300-dimensional word vectors and additional information, please see the project website.
Acknowledgements
This data has been released under the Open Data Commons Public Domain Dedication and License. If you use this dataset in your work, please cite the following paper:
Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation. URL: https://nlp.stanford.edu/pubs/glove.pdf
Inspiration
GloVe embeddings have been used in more than 2100 papers, and counting! You can use these pre-trained embeddings whenever you need a way to quantify word co-occurrence (which also captures some aspects of word meaning.)"
An Open Dataset for Human Activity Analysis,"Data collected using Smartphone, Smartwatch and Smartglasses",Sasan Jafarnejad,35,"Version 3,2017-09-01|Version 2,2017-08-31|Version 1,2017-08-31",consumer electronics,CSV,434 MB,ODbL,"7,367 views",665 downloads,,2 topics,https://www.kaggle.com/sasanj/human-activity-smart-devices,"Context
The study of human mobility and activities has opened up to an incredible number of studies in the past, most of which included the use of sensors distributed on the body of the subject. More recently, the use of smart devices has been particularly relevant because they are already everywhere and they come with accurate miniaturized sensors. Whether it is smartphones, smartwatches or smartglasses, each device can be used to describe complementary information such as emotions, precise movements, or environmental conditions.
Content
First of all, a smartphone is used to capture mainly contextual data. Two applications are used: a simple data collection application based on the SWIPE open-source sensing system (SWIPE), and a logbook application for obtaining real data on user activity (TimeLogger). SWIPE is a platform for sensing, recording and processing human dynamics using smartwatches and smartphones.
Then, a smartwatch is used primarily to capture the user's heart rate. Motion data is also collected, without being at the heart of the dataset due to its need to be configured with a low sampling frequency, which would drastically increase the dataset and drain the battery as well. An application based on SWIPE is used.
Finally, JINS MEME smartglasses are used. This model has the advantage of being compact and simple to carry. It does not have a camera or a screen; it simply has three types of sensors: an accelerometer (for detecting steps or activities), a gyroscope (for head movements) and an occulographic sensor (eye blinking, eye orientation). The official DataLogger application from JINS MEME is used.
For more information on the dataset please refer to the corresponding publication, available at An Open Dataset for Human Activity Analysis using Smart Devices.
The current dataset on Kaggle contains smartglasses data with 20ms interval (due to storage limitations), same data with 10ms interval is also available on demand. Contact sasan.jafarnejad [at] uni [dot] lu to receive the 10ms version.
Acknowledgements
This work was performed within the eGLASSES project, which is partially funded by NCBiR, FWF, SNSF, ANR and FNR under the ERA-NET CHIST-ERAII framework."
(LoL) League of Legends Ranked Games,"Details from over 50,000 ranked games of LoL",Mitchell J,35,"Version 9,2017-09-23|Version 8,2017-09-22|Version 7,2017-09-22|Version 6,2017-09-07|Version 5,2017-09-07|Version 4,2017-09-07|Version 3,2017-09-07|Version 2,2017-09-07|Version 1,2017-09-06","video games
internet",{}JSON,9 MB,CC0,"8,996 views","1,042 downloads",3 kernels,6 topics,https://www.kaggle.com/datasnaek/league-of-legends,"General Info
This is a collection of over 50,000 ranked EUW games from the game League of Legends, as well as json files containing a way to convert between champion and summoner spell IDs and their names. For each game, there are fields for:
Game ID
Creation Time (in Epoch format)
Game Duration (in seconds)
Season ID
Winner (1 = team1, 2 = team2)
First Baron, dragon, tower, blood, inhibitor and Rift Herald (1 = team1, 2 = team2, 0 = none)
Champions and summoner spells for each team (Stored as Riot's champion and summoner spell IDs)
The number of tower, inhibitor, Baron, dragon and Rift Herald kills each team has
The 5 bans of each team (Again, champion IDs are used)
This dataset was collected using the Riot Games API, which makes it easy to lookup and collect information on a users ranked history and collect their games. However finding a list of usernames is the hard part, in this case I am using a list of usernames scraped from 3rd party LoL sites.
Possible Uses
There is a vast amount of data in just a single LoL game. This dataset takes the most relevant information and makes it available easily for use in things such as attempting to predict the outcome of a LoL game, analysing which in-game events are most likely to lead to victory, understanding how big of an effect bans of a specific champion have, and more."
Meetups data from meetup.com,Organized data for various meetup.com entities along with relational schema,Sumit Kumar,35,"Version 1,2017-12-15","hobbies
social groups
internet",CSV,197 MB,CC0,"2,974 views",399 downloads,,0 topics,https://www.kaggle.com/sirpunch/meetups-data-from-meetupcom,"Summary
“Meetup is a social networking website that aims to brings people together to do, explore, teach and learn the things that help them come alive.”
Meetup allows members to find and join groups unified by a common interest. As of 2017, there are 32 million users with 280 thousand groups available across 182 countries.
A member needs to be able to identify groups and activities which interest them the most to be able to use this platform to network effectively.
The aim of our team was to use this dataset to build a recommender system which will identify and suggest groups and activities to a member based on their interest and additional interests of similar members. Furthermore, a social network analysis was done to identify the relationship between groups and people.
Database EER diagram
Data Collection Method
Data was collected using Meetup API.
Python script was used to ping meetup API and collect responses as JSON objects.
Logical chunks of data were exported and saved as csv files.
Data Cleaning
Data is filtered to include only 3 cities' information (New York, San Francisco, Chicago).
Character encoding is normalized to ASCII characters across tables.
Example Visualizations"
Global Commodity Trade Statistics,Three decades of global trade flows,United Nations,35,"Version 3,2017-11-15|Version 2,2017-11-15|Version 1,2017-11-15","supply chain
economics
shipping",CSV,121 MB,Other,"3,732 views",588 downloads,4 kernels,0 topics,https://www.kaggle.com/unitednations/global-commodity-trade-statistics,"Are you curious about fertilizer use in developing economies? The growth of Chinese steel exports? American chocolate consumption? Which parts of the world still use typewriters? You'll find all of that and more here. This dataset covers import and export volumes for 5,000 commodities across most countries on Earth over the last 30 years.
Acknowledgements
This dataset was kindly published by the United Nations Statistics Division on the UNData site. You can find the original dataset here.
Inspiration
Some of these numbers are more trustworthy than others. I'd expect that British tea imports are fairly accurate, but doubt that Afghanistan exported exactly 51 sheep in 2016. Can you identify which nations appear to have the most trustworthy data? Which industries?
License
Per the UNData terms of use: all data and metadata provided on UNdata’s website are available free of charge and may be copied freely, duplicated and further distributed provided that UNdata is cited as the reference."
Online Job Postings,"Dataset of 19,000 online job posts from 2004 to 2015",Mad Hab,34,"Version 1,2017-04-22","employment
linguistics",CSV,92 MB,Other,"11,063 views","1,795 downloads",5 kernels,0 topics,https://www.kaggle.com/madhab/jobposts,"Job Posts dataset
The dataset consists of 19,000 job postings that were posted through the Armenian human resource portal CareerCenter. The data was extracted from the Yahoo! mailing group https://groups.yahoo.com/neo/groups/careercenter-am. This was the only online human resource portal in the early 2000s. A job posting usually has some structure, although some fields of the posting are not necessarily filled out by the client (poster). The data was cleaned by removing posts that were not job related or had no structure. The data consists of job posts from 2004-2015
Content
jobpost – The original job post
date – Date it was posted in the group
Title – Job title
Company - employer
AnnouncementCode – Announcement code (some internal code, is usually missing)
Term – Full-Time, Part-time, etc
Eligibility -- Eligibility of the candidates
Audience --- Who can apply?
StartDate – Start date of work
Duration - Duration of the employment
Location – Employment location
JobDescription – Job Description
JobRequirment - Job requirements
RequiredQual -Required Qualification
Salary - Salary
ApplicationP – Application Procedure
OpeningDate – Opening date of the job announcement
Deadline – Deadline for the job announcement
Notes - Additional Notes
AboutC - About the company
Attach - Attachments
Year - Year of the announcement (derived from the field date)
Month - Month of the announcement (derived from the field date)
IT – TRUE if the job is an IT job. This variable is created by a simple search of IT job titles within column “Title”
Acknowledgements
The data collection and initial research was funded by the American University of Armenia’s research grant (2015).
Inspiration
The online job market is a good indicator of overall demand for labor in the local economy. In addition, online job postings data are easier and quicker to collect, and they can be a richer source of information than more traditional job postings, such as those found in printed newspapers. The data can be used in the following ways: -Understand the demand for certain professions, job titles, or industries -Help universities with curriculum development -Identify skills that are most frequently required by employers, and how the distribution of necessary skills changes over time -Make recommendations to job seekers and employers
Past research
We have used association rules mining and simple text mining techniques to analyze the data. Some results can be found here (https://www.slideshare.net/HabetMadoyan/it-skills-analysis-63686238)."
PUBG Match Deaths and Statistics,Over 65 million death entries for PlayerUnknown Battleground's matches,KP,34,"Version 3,2018-01-12|Version 2,2018-01-12|Version 1,2018-01-11","video games
geography
demographics",Other,4 GB,CC0,"8,572 views",604 downloads,2 kernels,4 topics,https://www.kaggle.com/skihikingkevin/pubg-match-deaths,"Introduction
Video games are a rich area for data extraction due to its digital nature. Notable examples such as the complex EVE Online economy, World of Warcraft corrupted blood incident and even Grand Theft Auto self-driving cars tells us that fiction is closer to reality than we really think. Data scientists can gain insight on the logic and decision-making that the players face when put in hypothetical and virtual scenarios.
In this Kaggle Dataset, I provide over 720,000 competitive matches from the popular game PlayerUnknown's Battlegrounds. The data was extracted from pubg.op.gg, a game tracker website. I intend for this data-set to be purely exploratory, however users are free to create their own predictive models they see fit.
PlayerUnknown's Battlegrounds
PUBG is a first/third-person shooter battle royale style game that matches over 90 players on a large island where teams and players fight to the death until one remains. Players are airdropped from an airplane onto the island where they are to scavenge towns and buildings for weapons, ammo, armor and first-aid. Players will then decide to either fight or hide with the ultimate goal of being the last one standing. A bluezone (see below) will appear a few minutes into the game to corral players closer and closer together by dealing damage to anyone that stands within the bluezone and sparing whoever is within the safe zone.
Read more about PUBG here
The Dataset
This dataset provides two zips: aggregate and deaths.
In deaths, the files record every death that occurred within the 720k matches. That is, each row documents an event where a player has died in the match.
In aggregate, each match's meta information and player statistics are summarized (as provided by pubg). It includes various aggregate statistics such as player kills, damage, distance walked, etc as well as metadata on the match itself such as queue size, fpp/tpp, date, etc.
The uncompressed data is divided into 5 chunks of approximately 2gb each. For details on columns, please see the file descriptions.
Interpreting Positional Data
The X,Y coordinates are all in in-game coordinates and need to be linearly scaled to be plotted on square erangel and miramar maps. The coordinate min,max are 0,800000 respectively.
Potential Bias in the Data
The scraping methodology first starts with an initial seed player, I chose this to be my own account (a rather low rank individual). I then use the seed player to scrape for all players that it has encountered in its historical matches. I then take a random subset of 5000 players from this and then scrape for their historical games for the final dataset. What this could produce is an unrepresentative sample of all games played as it is more likely that I queued and matched with lower rated players and those players more than likely also got matched against lower rated players as well. Thus, the matches and deaths are more representative of lower tier gameplay but given the simplicity of the dataset, this shouldn't be an issue.
Acknowledgements
Pubg.op.gg, if this is against the TOS, please let me know and I will take it down"
Audio Cats and Dogs,Classify raw sound events,marc moreaux,34,"Version 5,2017-10-05|Version 4,2017-10-04|Version 3,2017-10-03|Version 2,2017-09-29|Version 1,2017-09-29","animals
acoustics",Other,59 MB,CC3,"6,389 views",486 downloads,9 kernels,,https://www.kaggle.com/mmoreaux/audio-cats-and-dogs,"Context
With this dataset we hope to do a nice cheeky wink to the ""cats and dogs"" image dataset. In fact, this dataset is aimed to be the audio counterpart of the famous ""cats and dogs"" image classification task, here available on Kaggle.
Content
The dataset consists in many ""wav"" files for both the cat and dog classes :
cat has 164 WAV files to which corresponds 1323 sec of audio
dog has 113 WAV files to which corresponds 598 sec of audio
You can have an visual description of the Wav here : Visualizing woofs & meows 🐱. In Accessing the Dataset 2 we propose a train / test split which can be used.
All the WAV files contains 16KHz audio and have variable length.
Acknowledgements
We have not much credit in proposing the dataset here. Much of the work have been done by the AE-Dataset creator (From which we extracted the two classes) and by the humans behind FreeSound From which was extracted the AE-Dataset.
Inspiration
You might use this dataset to test raw audio classification challenge ;)
A more challenging dataset is available here"
Religious and philosophical texts,"5 texts from Project Gutenberg to encourage text-mining, sentiment analysis, etc",Rob Harrand,34,"Version 1,2016-09-07","languages
faith and traditions
linguistics",Other,8 MB,Other,"7,162 views",910 downloads,23 kernels,,https://www.kaggle.com/tentotheminus9/religious-and-philosophical-texts,"These are 5 texts taken from Project Gutenberg, uploaded to Kaggle to encourage things like text-mining and sentiment analysis. These are fun skills to develop and many existing datasets on Kaggle don't lend themselves to these sorts of analyses.
The 5 books are,
The King James Bible
The Quran
The Book Of Mormon
The Gospel of Buddha
Meditations, by Marcus Aurelius
Project Gutenberg is an online archive of books that are free to download and distribute. These files are taken without alteration (filenames or contents) from the Project Gutenberg website"
Blood Cell Images,"12,500 images: 4 different cell types",paultimothymooney,34,"Version 5,2018-01-20|Version 4,2018-01-18|Version 3,2018-01-11|Version 2,2018-01-11|Version 1,2018-01-10",medicine,Other,293 MB,CC0,"2,482 views",349 downloads,,,https://www.kaggle.com/paultimothymooney/blood-cells,"Context
The diagnosis of blood-based diseases often involves identifying and characterizing patient blood samples. Automated methods to detect and classify blood cell subtypes have important medical applications.
Content
This dataset contains 12,500 augmented images of blood cells (JPEG) with accompanying cell type labels (CSV). There are approximately 3,000 images for each of 4 different cell types grouped into 4 different folders (according to cell type). The cell types are Eosinophil, Lymphocyte, Monocyte, and Neutrophil. This dataset is accompanied by an additional dataset containing the original 410 images (pre-augmentation) as well as two additional subtype labels (WBC vs WBC) and also bounding boxes for each cell in each of these 410 images (JPEG + XML metadata). More specifically, the folder 'dataset-master' contains 410 images of blood cells with subtype labels and bounding boxes (JPEG + XML), while the folder 'dataset2-master' contains 2,500 augmented images as well as 4 additional subtype labels (JPEG + CSV). There are approximately 3,000 augmented images for each class of the 4 classes as compared to 88, 33, 21, and 207 images of each in folder 'dataset-master'.
Acknowledgements
https://github.com/Shenggan/BCCD_Dataset MIT License
Inspiration
The diagnosis of blood-based diseases often involves identifying and characterizing patient blood samples. Automated methods to detect and classify blood cell subtypes have important medical applications."
Restaurant Data with Consumer Ratings,Data from a restaurant recommender prototype,UCI Machine Learning,34,"Version 1,2017-09-28",business,CSV,203 KB,CC0,"11,132 views","1,709 downloads",5 kernels,,https://www.kaggle.com/uciml/restaurant-data-with-consumer-ratings,"Context
This dataset was used for a study where the task was to generate a top-n list of restaurants according to the consumer preferences and finding the significant features. Two approaches were tested: a collaborative filter technique and a contextual approach: (i) The collaborative filter technique used only one file i.e., rating_final.csv that comprises the user, item and rating attributes. (ii) The contextual approach generated the recommendations using the remaining eight data files.
Content
There are 9 data files and a README, and are grouped like this:
Restaurants
1 chefmozaccepts.csv
2 chefmozcuisine.csv
3 chefmozhours4.csv
4 chefmozparking.csv
5 geoplaces2.csv
Consumers
6 usercuisine.csv
7 userpayment.csv
8 userprofile.csv
User-Item-Rating
9 rating_final.csv
More detailed file descriptions can also be found in the README:
1 chefmozaccepts.csv
Instances: 1314
Attributes: 2
placeID: Nominal
Rpayment: Nominal, 12
2 chefmozcuisine.csv
Instances: 916
Attributes: 2
placeID: Nominal
Rcuisine: Nominal, 59
3 chefmozhours4.csv
Instances: 2339
Attributes: 3
placeID: Nominal
hours: Nominal, Range:00:00-23:30
days: Nominal, 7
4 chefmozparking.csv
Instances: 702
Attributes: 2
placeID: Nominal
parking_lot: Nominal, 7
5 geoplaces2.csv
Instances: 130
Attributes: 21
placeID: Nominal
latitude: Numeric
longitude: Numeric
the_geom_meter: Nominal (Geospatial)
name: Nominal
address: Nominal,Missing: 27
city: Nominal, Missing: 18
state: Nominal, Missing: 18
country: Nominal, Missing: 28
fax: Numeric, Missing: 130
zip: Nominal,Missing: 74
alcohol: Nominal, Values: 3
smoking_area: Nominal, 5
dress_code: Nominal, 3
accessibility: Nominal, 3
price: Nominal, 3
url: Nominal, Missing: 116
Rambience: Nominal, 2
franchise: Nominal, 2
area: Nominal, 2
other_services: Nominal, 3
6 rating_final.csv
Instances: 1161
Attributes: 5
userID: Nominal
placeID: Nominal
rating: Numeric, 3
food_rating: Numeric, 3
service_rating: Numeric, 3
7 usercuisine.csv
Instances: 330
Attributes: 2
userID: Nominal
Rcuisine: Nominal, 103
8 userpayment.csv
Instances: 177
Attributes: 2
userID: Nominal
Upayment: Nominal, 5
9 userprofile
Instances: 138
Attributes: 19
userID: Nominal
latitude: Numeric
longitude: Numeric
the_geom_meter: Nominal (Geospatial)
smoker: Nominal
drink_level: Nominal, 3
dress_preference:Nominal, 4
ambience: Nominal, 3
transport: Nominal, 3
marital_status: Nominal, 3
hijos: Nominal, 3
birth_year: Nominal
interest: Nominal, 5
personality: Nominal, 4
religion: Nominal, 5
activity: Nominal, 4
color: Nominal, 8
weight: Numeric
budget: Nominal, 3
height: Numeric
Acknowledgements
This dataset was originally downloaded from the UCI ML Repository: UCI ML
Creators: Rafael Ponce Medellín and Juan Gabriel González Serna rafaponce@cenidet.edu.mx, gabriel@cenidet.edu.mx Department of Computer Science. National Center for Research and Technological Development CENIDET, México
Donors of database: Blanca Vargas-Govea and Juan Gabriel González Serna blanca.vargas@cenidet.edu.mx/blanca.vg@gmail.com, gabriel@cenidet.edu.mx Department of Computer Science. National Center for Research and Technological Development CENIDET, México
Inspiration
Use this data to create a restaurant recommender or determine which restaurants a person is most likely to visit."
International Datasets,International health and population metrics,US Census Bureau,34,"Version 1,2017-06-28","demographics
international relations",CSV,2 GB,Other,"7,151 views","1,184 downloads",4 kernels,0 topics,https://www.kaggle.com/census/international-data,"Content
The United States Census Bureau’s International Dataset provides estimates of country populations since 1950 and projections through 2050. Specifically, the data set includes midyear population figures broken down by age and gender assignment at birth. Additionally, they provide time-series data for attributes including fertility rates, birth rates, death rates, and migration rates.
The full documentation is available here. For basic field details, please see the data dictionary.
Note: The U.S. Census Bureau provides estimates and projections for countries and areas that are recognized by the U.S. Department of State that have a population of at least 5,000.
Acknowledgements
This dataset was created by the United States Census Bureau.
Inspiration
Which countries have made the largest improvements in life expectancy? Based on current trends, how long will it take each country to catch up to today’s best performers?
Use this dataset with BigQuery
You can use Kernels to analyze, share, and discuss this data on Kaggle, but if you’re looking for real-time updates and bigger data, check out the data on BigQuery, too: https://cloud.google.com/bigquery/public-data/international-census."
"Hurricanes and Typhoons, 1851-2014","Location, wind, and pressure of tropical cyclones in Atlantic and Pacific Oceans",NOAA,34,"Version 1,2017-01-21",weather,CSV,9 MB,CC0,"7,344 views","1,309 downloads",7 kernels,0 topics,https://www.kaggle.com/noaa/hurricane-database,"Context
The National Hurricane Center (NHC) conducts a post-storm analysis of each tropical cyclone in the Atlantic basin (i.e., North Atlantic Ocean, Gulf of Mexico, and Caribbean Sea) and and the North Pacific Ocean to determine the official assessment of the cyclone's history. This analysis makes use of all available observations, including those that may not have been available in real time. In addition, NHC conducts ongoing reviews of any retrospective tropical cyclone analyses brought to its attention and on a regular basis updates the historical record to reflect changes introduced.
Content
The NHC publishes the tropical cyclone historical database in a format known as HURDAT, short for HURricane DATabase. These databases (Atlantic HURDAT2 and NE/NC Pacific HURDAT2) contain six-hourly information on the location, maximum winds, central pressure, and (starting in 2004) size of all known tropical cyclones and subtropical cyclones."
Employee Attrition,Can you forecast employee attrition?,People HR Analytics Repository,34,"Version 2,2017-04-27|Version 1,2017-01-14","employment
business",CSV,7 MB,CC0,"22,008 views","2,429 downloads",20 kernels,3 topics,https://www.kaggle.com/HRAnalyticRepository/employee-attrition-data,"Context
This data was originally posted on my personal oneDrive account.
It represent fictitious/fake data on terminations. For each of 10 years it show employees that are active and those that terminated.
The intent is to see if individual terminations can be predicted from the data provided.
The thing to be predicted is status of active or terminated
Content
The data contains
employee id employee record date ( year of data) birth date hire date termination date age length of service city department job title store number gender termination reason termination type status year status business unit
These might be typical types of data in hris
Acknowledgements
None- its fake data
Inspiration
A lot of turnover analyses occur at an aggregate level-such as turnover rates. But few analyses concentrate on trying to identify exactly which individuals might leave based on patterns that might be present in existing data.
Machine learning algorithms often showcase customer churn examples for telcos or product marketing. Those algorithms equally apply to employee churn."
IMDB data from 2006 to 2016,"A data set of 1,000 popular movies on IMDB in the last 10 years",PromptCloud,34,"Version 1,2017-06-26",film,CSV,303 KB,Other,"15,077 views","2,108 downloads",8 kernels,,https://www.kaggle.com/PromptCloudHQ/imdb-data,"Here's a data set of 1,000 most popular movies on IMDB in the last 10 years. The data points included are:
Title, Genre, Description, Director, Actors, Year, Runtime, Rating, Votes, Revenue, Metascrore
Feel free to tinker with it and derive interesting insights."
US Gross Rent ACS Statistics,"+40,000 Samples: Real Estate Application (Mean, Median, Standard Deviation)",Golden Oak Research Group,34,"Version 3,2017-08-23|Version 2,2017-08-21|Version 1,2017-08-21","finance
real estate
demographics",Other,5 MB,Other,"2,632 views",444 downloads,3 kernels,,https://www.kaggle.com/goldenoakresearch/acs-gross-rent-us-statistics,"What you get:
Upvote! The database contains +40,000 records on US Gross Rent & Geo Locations. The field description of the database is documented in the attached pdf file. To access, all 325,272 records on a scale roughly equivalent to a neighborhood (census tract) see link below and make sure to upvote. Upvote right now, please. Enjoy!
Get the full free database with coupon code: FreeDatabase, See directions at the bottom of the description... And make sure to upvote :) coupon ends at 2:00 pm 8-23-2017
Gross Rent & Geographic Statistics:
Mean Gross Rent (double)
Median Gross Rent (double)
Standard Deviation of Gross Rent (double)
Number of Samples (double)
Square area of land at location (double)
Square area of water at location (double)
Geographic Location:
Longitude (double)
Latitude (double)
State Name (character)
State abbreviated (character)
State_Code (character)
County Name (character)
City Name (character)
Name of city, town, village or CPD (character)
Primary, Defines if the location is a track and block group.
Zip Code (character)
Area Code (character)
Abstract
The data set originally developed for real estate and business investment research. Income is a vital element when determining both quality and socioeconomic features of a given geographic location. The following data was derived from over +36,000 files and covers 348,893 location records.
License
Only proper citing is required please see the documentation for details. Have Fun!!!
Golden Oak Research Group, LLC. “U.S. Income Database Kaggle”. Publication: 5, August 2017. Accessed, day, month year.
For any questions, you may reach us at research_development@goldenoakresearch.com. For immediate assistance, you may reach me on at 585-626-2965
please note: it is my personal number and email is preferred
Check our data's accuracy: Census Fact Checker
Access all 325,272 location for Free Database Coupon Code:
Don't settle. Go big and win big. Optimize your potential**. Access all gross rent records and more on a scale roughly equivalent to a neighborhood, see link below:
Website: Golden Oak Research make sure to upvote
A small startup with big dreams, giving the every day, up and coming data scientist professional grade data at affordable prices It's what we do."
Tweets Targeting Isis,General tweets about Isis & related words,ActiveGalaXy,33,"Version 3,2016-07-30|Version 2,2016-07-29|Version 1,2016-07-27","crime
internet",CSV,29 MB,CC0,"11,563 views","1,016 downloads",26 kernels,3 topics,https://www.kaggle.com/activegalaxy/isis-related-tweets,"Context
The image at the top of the page is a frame from today's (7/26/2016) Isis #TweetMovie from twitter, a ""normal"" day when two Isis operatives murdered a priest saying mass in a French church. (You can see this in the center left). A selection of data from this site is being made available here to Kaggle users.
UPDATE: An excellent study by Audrey Alexander titled Digital Decay? is now available which traces the ""change over time among English-language Islamic State sympathizers on Twitter.
Intent
This data set is intended to be a counterpoise to the How Isis Uses Twitter data set. That data set contains 17k tweets alleged to originate with ""100+ pro-ISIS fanboys"". This new set contains 122k tweets collected on two separate days, 7/4/2016 and 7/11/2016, which contained any of the following terms, with no further editing or selection:
isis
isil
daesh
islamicstate
raqqa
Mosul
""islamic state""
This is not a perfect counterpoise as it almost surely contains a small number of pro-Isis fanboy tweets. However, unless some entity, such as Kaggle, is willing to expend significant resources on a service something like an expert level Mechanical Turk or Zooniverse, a high quality counterpoise is out of reach.
A counterpoise provides a balance or backdrop against which to measure a primary object, in this case the original pro-Isis data. So if anyone wants to discriminate between pro-Isis tweets and other tweets concerning Isis you will need to model the original pro-Isis data or signal against the counterpoise which is signal + noise. Further background and some analysis can be found in this forum thread.
This data comes from postmodernnews.com/token-tv.aspx which daily collects about 25MB of Isis tweets for the purposes of graphical display. PLEASE NOTE: This server is not currently active.
Data Details
There are several differences between the format of this data set and the pro-ISIS fanboy dataset. 1. All the twitter t.co tags have been expanded where possible 2. There are no ""description, location, followers, numberstatuses"" data columns.
I have also included my version of the original pro-ISIS fanboy set. This version has all the t.co links expanded where possible."
Datascience Universities across US,Contains datascience programs offered and location data by university.,SrihariRao,33,"Version 1,2016-09-25","education
information technology",CSV,342 KB,ODbL,"9,630 views","1,466 downloads",8 kernels,,https://www.kaggle.com/sriharirao/datascience-universities-across-us,"Contains DataScience programs offered by universities along with program details, world ranking and a lot lot more. Happy exploring !!!"
TechCrunch Posts Compilation,40k compiled posts with a rich set of features will boost your visualizations,Thiago Balbo,33,"Version 1,2016-10-19","linguistics
internet",CSV,136 MB,CC0,"5,740 views",505 downloads,34 kernels,,https://www.kaggle.com/thibalbo/techcrunch-posts-compilation,"Inspiration
I'm a big fan of TechCrunch for a while now. Kind of because I get to know about new startups that's coming up or maybe just because I find Tito Hamze videos fun. But TechCrunch got plenty of good content. And where we find good content we produce great exploratory analysis.
This dataset is a great opportunity for you to boost your skills as an EDA expert! It provides several features that make you able to create different analyses such as time series, clustering, predictive, segmenting, classification and tons of others. Let's not forget about word2vec for that. It would be awesome to see that in action here!
I've made the scraper available on github, if you want to check it out, here is the link: techcrunch scraper repo
Content
This dataset comes with a rich set of features. You will have:
authors: authors of the post - can be one or multiple authors
category: post category
content: post content - each paragraph can be extracted by splitting on the \n
date: post date
id: post id - the same id used on techcrunch website
img_src: post main image url
section: post section - each section is one of the options on the main page dropdown menu
tags: post tags - can be zero or multiple tags
title: post title
topics: post topics
url: post url
Acknowledgements
All posts were scraped from the TechCrunch website on mid oct-16. Each line contains information about one post and each post appear in no more than one line."
Short Jokes,"Collection of over 200,000 short jokes for humour research",Abhinav Moudgil,33,"Version 1,2017-02-07","humor
linguistics",CSV,23 MB,ODbL,"9,366 views",916 downloads,5 kernels,0 topics,https://www.kaggle.com/abhinavmoudgil95/short-jokes,"Context
Generating humor is a complex task in the domain of machine learning, and it requires the models to understand the deep semantic meaning of a joke in order to generate new ones. Such problems, however, are difficult to solve due to a number of reasons, one of which is the lack of a database that gives an elaborate list of jokes. Thus, a large corpus of over 0.2 million jokes has been collected by scraping several websites containing funny and short jokes.
Visit my Github repository for more information regarding collection of data and the scripts used.
Content
This dataset is in the form of a csv file containing 231,657 jokes. Length of jokes ranges from 10 to 200 characters. Each line in the file contains a unique ID and joke.
Disclaimer
It has been attempted to keep the jokes as clean as possible. Since the data has been collected by scraping websites, it is possible that there may be a few jokes that are inappropriate or offensive to some people."
Spotify Song Attributes,An attempt to build a classifier that can predict whether or not I like a song,GeorgeMcIntire,33,"Version 1,2017-08-05",,CSV,217 KB,Other,"13,177 views","1,517 downloads",7 kernels,,https://www.kaggle.com/geomack/spotifyclassification,"Context
A dataset of 2017 songs with attributes from Spotify's API. Each song is labeled ""1"" meaning I like it and ""0"" for songs I don't like. I used this to data to see if I could build a classifier that could predict whether or not I would like a song.
I wrote an article about the project I used this data for. It includes code on how to grab this data from the Spotipy API wrapper and the methods behind my modeling. https://opendatascience.com/blog/a-machine-learning-deep-dive-into-my-spotify-data/
Content
Each row represents a song.
There are 16 columns. 13 of which are song attributes, one column for song name, one for artist, and a column called ""target"" which is the label for the song.
Here are the 13 track attributes: acousticness, danceability, duration_ms, energy, instrumentalness, key, liveness, loudness, mode, speechiness, tempo, time_signature, valence.
Information on what those traits mean can be found here: https://developer.spotify.com/web-api/get-audio-features/
Acknowledgements
I would like to thank Spotify for providing this readily accessible data.
Inspiration
I'm a music lover who's curious about why I love the music that I love."
PLAYERUNKNOWN'S BATTLEGROUNDS Player Statistics,Player statistics for PUBG. 150 features per player.,JustinMoore,33,"Version 1,2017-08-01","games and toys
video games",CSV,62 MB,CC0,"7,833 views",711 downloads,8 kernels,2 topics,https://www.kaggle.com/lazyjustin/pubgplayerstats,"Context
Grab your pans...
Player statistics for approximately 85,000 of the top PUBG players (as tracked by https://pubgtracker.com/). All statistics were gathered using aggregate region filters (all regions) and feature labels are subdivided by server type: solo, duo, and squad.
87,898 players with 150 numerical game-play features per player (+2 for player name and PUBG Tracker ID).
Content
Features include KD ratios, wins, losses, damage, wins, top 10's, and movement characteristics (walking/riding distance etc...)
Acknowledgements
Special thanks to pubgtracker.com for their support and aid with gathering this data. More information can be found here: https://pubgtracker.com/
PLAYERUNKNOWN'S BATTLEGROUNDS is a registered trademark, trademark or service mark of Bluehole, Inc. and its affiliates https://www.playbattlegrounds.com/main.pu
Inspiration
As a gamer addicted to PUBG, it was a blast putting this data set together. Some great project ideas include:
a. Visualizations of player skill vs. specific strategies
b. Unsupervised clustering of players based on strategy (for matchmaking or team building)
c. Prediction of features based on player skill and/or strategies"
NBA Enhanced Box Score and Standings Stats,Box Scores and Standings with advanced calculations applied,Paul Rossotti,33,"Version 19,2018-02-26|Version 18,2018-02-18|Version 17,2018-02-11|Version 16,2018-02-03|Version 15,2018-01-28|Version 14,2018-01-21|Version 13,2018-01-13|Version 12,2018-01-06|Version 11,2018-01-06|Version 10,2018-01-06|Version 9,2017-12-31|Version 8,2017-12-24|Version 7,2017-12-16|Version 6,2017-12-14|Version 5,2017-11-18|Version 4,2017-11-13|Version 3,2017-11-12|Version 2,2017-11-11|Version 1,2017-11-11","basketball
sports",CSV,6 MB,CC4,"3,648 views",556 downloads,6 kernels,0 topics,https://www.kaggle.com/pablote/nba-enhanced-stats,"Context
Dataset is based on box score and standing statistics from the NBA.
Calculations such as number of possessions, floor impact counter, strength of schedule, and simple rating system are performed.
Finally, extracts are created based on a perspective:
teamBoxScore.csv communicates game data from each teams perspective
officialBoxScore.csv communicates game data from each officials perspective
playerBoxScore.csv communicates game data from each players perspective
standing.csv communicates standings data for each team every day during the season
Content
Data Sources
Box score and standing statistics were obtained by a Java application using RESTful APIs provided by xmlstats.
Calculation Sources
Another Java application performs advanced calculations on the box score and standing data.
Formulas for these calculations were primarily obtained from these sources:
https://basketball.realgm.com/info/glossary
https://www.nbastuffer.com/team-evaluation-metrics/
https://www.basketball-reference.com/about/glossary.html
Inspiration
Favoritism
Does a referee impact the number of fouls made against a player or the pace of a game?
Forcasting
Can the aggregated points scored by and against a team along with their strength of schedule be used to determine their projected winning percentage for the season?
Predicting the Past
For a given game, can games played earlier in the season help determine how a team will perform?
Lots of data elements and possibilities. Let your imagination roam!"
350 000+ movies from themoviedb.org,More than 350k movies and main casting/crew up to Aug17,Stephanerappeneau,33,"Version 8,2017-10-13|Version 7,2017-10-13|Version 6,2017-10-13|Version 5,2017-10-13|Version 4,2017-10-05|Version 3,2017-09-23|Version 2,2017-09-20|Version 1,2017-09-17","arts and entertainment
film
actors",CSV,192 MB,Other,"6,906 views",942 downloads,5 kernels,6 topics,https://www.kaggle.com/stephanerappeneau/350-000-movies-from-themoviedborg,"Context
I love movies. 
I tend to avoid marvel-transformers-standardized products, and prefer a mix of classic hollywood-golden-age and obscure polish artsy movies. Throw in an occasional japanese-zombie-slasher-giallo as an alibi. Good movies don't exist without bad movies. 
On average I watch 200+ movies each year, with peaks at more than 500 movies. Nine years ago I started to log my movies to avoid watching the same movie twice, and also assign scores. Over the years, it gave me a couple insights on my viewing habits but nothing more than what a tenth-grader would learn at school. 
I've recently suscribed to Netflix and it pains me to see the global inefficiency of recommendation systems for people like me, who mostly swear by ""La politique des auteurs"". It's a term coined by famous new-wave french movie critic André Bazin, meaning that the quality of a movie is essentially linked to the director and it's capacity to execute his vision with his crew. We could debate it depends on movie production pipeline, but let's not for now. Practically, what it means, is that I essentially watch movies from directors who made films I've liked.
I suspect Neflix calibrate their recommandation models taking into account the way the ""average-joe"" chooses a movie. A few months ago I had read a study based on a survey, showing that people chose a movie mostly based on genre (55%), then by leading actors (45%). Director or Release Date were far behind around 10% each. It is not surprising, since most people I know don't care who the director is. Lots of US blockbusters don't even mention it on the movie poster. I am aware that collaborative filtering is based on user proximity , which I believe decreases (or even eliminates) the need to characterize a movie. So here I'm more interested in content based filtering which is based on product proximity for several reasons :
Users tastes are not easily accessible. It is, after all, Netflix treasure chest
Movie offer on Netflix is so bad for someone who likes author's movies that it wouldn't help
Modeling a movie intrinsic qualities is a nice challenge
Enough.
""The secret of getting ahead is getting started"" (Mark Twain)
Content
The primary source is www.themoviedb.org. If you watch obscure artsy romanian homemade movies you may find only 95% of your movies referenced...but for anyone else it should be in the 98%+ range.
movies details are from www.themoviedb.org API : movies/details
movies crew & casting are from www.themoviedb.org API : movies/credits
both can be joined by id
they contain all 350k movies up, from end of 19th century to august 2017. If you remove short movies from imdb you get similar amounts of movies.
I uploaded the program to retrieve incremental movie details on github : https://github.com/stephanerappeneau/scienceofmovies/tree/master/PycharmProjects/GetAllMovies (need a dev API key from themoviedb.org though)
I have tried various supervised (decision tree) / unsupervised (clustering, NLP) approaches described in the discussions, source code is on github : https://github.com/stephanerappeneau/scienceofmovies
As a bonus I've uploaded the bio summary from top 500 critically-acclaimed directors from wikipedia, for some interesting NLTK analysis
Here is overview of the available sources that I've tried :
• Imdb.com free csv dumps (ftp://ftp.funet.fi/pub/mirrors/ftp.imdb.com/pub/temporaryaccess/) are badly documented, incomplete, loosely structured and impossible to join/merge. There's an API hosted by Amazon Web Service : 1€ every 100 000 requests. With around 1 million movies, it could become expensive also features are bare. So I've searched for other sources. 
• www.themoviedb.org is based on crowdsourcing and has an excellent API, limited to 40 requests every 10 seconds. It is quite generous, well documented, and enough to sweep the 450 000 movies in a few days. For my purpose, data quality is not significantly worse than imdb, and as imdb key is also included there's always the possibility to complete my dataset later (I actually did it)
• www.Boxofficemojo.com has some interesting budget/revenue figures (which are sorely lacking in both imdb & tmdb), but it actually tracks only a few thousand movies, mainly blockbusters. There are other professional sources that are used by film industry to get better predictive / marketing insights but that's beyond my reach for this experiment.   • www.wikipedia.com is an interesting source with no real cap on API calls, however it requires a bit of webscraping and for movies or directors the layout and quality varies a lot, so I suspected it'd get a lot of work to get insights so I put this source in lower priority.
• www.google.com will ban you after a few minutes of web scraping because their job is to scrap data from others, than sell it, duh.   • It's worth mentionning that there are a few dumps of Netflix anonymized user tastes on kaggle, because they've organised a few competitions to improve their recommendation models. https://www.kaggle.com/netflix-inc/netflix-prize-data
• Online databases are largely white anglo-saxon centric, meaning bollywood (India is the 2nd bigger producer of movies) offer is mostly absent from datasets. I'm fine with that, as it's not my cup of tea plus I lack domain knowledge. The sheer amount of indian movies would probably skew my results anyway (I don't want to have too many martial-arts-musicals in my recommendations ;-)). I have, however, tremendous respect for indian movie industry so I'd love to collaborate with an indian cinephile !
Inspiration
Starting from there, I had multiple problem statements for both supervised / unsupervised machine learning
Can I program a tailored-recommendation system based on my own criteria ?
What are the characteristics of movies/directors I like the most ?
What is the probability that I will like my next movie ?
Can I find the data ?
One of the objectives of sharing my work here is to find cinephile data-scientists who might be interested and, hopefully, contribute or share insights :) Other interesting leads : use tagline for NLP/Clustering/Genre guessing, leverage on budget/revenue, link with other data sources using the imdb normalized title, etc.
Motivation, Disclaimer and Acknowledgements
I've graduated from an french engineering school, majoring in artificial intelligence, but that was 17 years ago right in the middle of A.I-winter. Like a lot of white male rocket scientists, I've ended up in one of the leading european investment bank, quickly abandonning IT development to specialize in trading/risk project management and internal politics. My recent appointment in the Data Office made me aware of recent breakthroughts in datascience, and I thought that developing a side project would be an excellent occasion to learn something new. Plus it'd give me a well-needed credibility which too often lack decision makers when it comes to datascience.
I've worked on some of the features with Cédric Paternotte, a fellow friend of mine who is a professor of philosophy of sciences in La Sorbonne. Working with someone with a different background seem a good idea for motivation, creativity and rigor.
Kudos to www.themoviedb.org or www.wikipedia.com sites, who really have a great attitude towards open data. This is typically NOT the case of modern-bigdata companies who mostly keep data to themselves to try to monetize it. Such a huge contrast with imdb or instagram API, which generously let you grab your last 3 comments at a miserable rate. Even if 15 years ago this seemed a mandatory path to get services for free, I predict one day governments will need to break this data monopoly.
[Disclaimer : I apologize in advance for my engrish (I'm french ^-^), any bad-code I've written (there are probably hundreds of way to do it better and faster), any pseudo-scientific assumption I've made, I'm slowly getting back in statistics and lack senior guidance, one day I regress a non-stationary time series and the day after I'll discover I shouldn't have, and any incorrect use of machine-learning models]"
Crime in Los Angeles,Crime data from 2010 through September 2017,City of Los Angeles,33,"Version 7,2017-09-18|Version 6,2017-09-13|Version 5,2017-09-13|Version 4,2017-09-13|Version 3,2017-09-12|Version 2,2017-09-12|Version 1,2017-09-07",crime,Other,360 MB,CC0,"10,038 views","1,343 downloads",9 kernels,4 topics,https://www.kaggle.com/cityofLA/crime-in-los-angeles,"This dataset reflects incidents of crime in the City of Los Angeles dating back to 2010. This data is transcribed from original crime reports that are typed on paper and therefore there may be some inaccuracies within the data. Some location fields with missing data are noted as (0°, 0°). Address fields are only provided to the nearest hundred block in order to maintain privacy.
Reporting District Shapefile Attributes
REPDIST, Number, min: 101 max: 2,199 avg: 1,162 count: 1,135
PREC, Number, min: 1 max: 21 avg: 11 count: 1,135
APREC, Text, PACIFIC (74), DEVONSHIRE (70), WEST LOS ANGELES (69), NORTHEAST (64), HOLLENBECK (63), MISSION (62)... (15 more)
BUREAU, Text, VALLEY BUREAU (399), WEST BUREAU (288), CENTRAL BUREAU (267), SOUTH BUREAU (181)
BASICCAR, Text, 8A29 (17), 17A35 (17), 1A1 (15), 17A49 (14), 16A35 (14), 14A73 (14), 19A43 (13), 8A95 (12), 19A7 (12)... (160 more)
TOOLTIP, Text, Bureau: SOUTH BUREAU\nDistrict: 562\nDivision: HARBOR (1)... (1134 more)
OBJECTID
Unique ID
Acknowledgements
This dataset was kindly released by the City of Los Angeles. You can find the original dataset, updated weekly, here.
Inspiration
Some of the MO codes seem unlikely or unrelated to crime. Can you find out what would lead to the use of code 0107 God or 1021 Repair?"
Y Combinator Companies,Publicly launched YC companies funded from summer 2005 to summer 2016,Ben Hamner,32,"Version 2,2016-09-08|Version 1,2016-09-08","business
finance",CSV,122 KB,Other,"9,461 views",833 downloads,27 kernels,0 topics,https://www.kaggle.com/benhamner/y-combinator-companies,"Data scraped from www.ycombinator.com/companies on September 8, 2016."
My Settlers of Catan Games,"Starting positions, roll distributions, postgame statistics of 50 4-player games",Lumin,32,"Version 1,2016-08-30",board games,CSV,16 KB,CC4,"9,045 views",555 downloads,24 kernels,,https://www.kaggle.com/lumins/settlers-of-catan-games,"The strategic board game The Settlers of Catan is a modern classic. Introduced in 1995, it has sold over 22 million copies worldwide. Learning how to play the game well requires an inherent understanding of probability, economics, game theory, and social interactions.
This is my personal dataset of 50 4-player games I played on playcatan.com in 2014. Using the ingame statistics page and a spreadsheet, I logged starting position choices, the distribution of dice rolls, and how each player spent the resources they acquired by the end of the game. Note, of course, because this dataset only consists of my games, any analysis done is most relevant for games involving me...
My personal analysis of this dataset consisted of a best subsets regression, and resulted a 4-variable model that likely overfitted, but managed to ascertain the winner correctly, in 40 of 50 games.
Questions to possibly consider:
How much luck is involved in winning a Catan game?
Does starting position matter? If so, what starting settlements lead to success from each position?
How much information on the eventual winner can be gained from starting position/settlements alone?
By looking at postgame stats, what leads to a win? Can these statistics be a guide for ingame strategy?
Data details/guide:
gameNum - each game I played has 4 corresponding rows, 1 per player.
player - the starting position corresponding to each row
points - how many points the player ended the game with (the game is won with 10 or more)
me - the position I played during the game
2, 3, ..., 12 - how many rolls of each value occurred during the game (game is played with 2 dice)
settlement1, settlement2 - each starting settlement is logged as 3 pairs of [number, resource]:
L = lumber
C = clay
S = sheep
W = wheat
O = ore
3G = 3:1 general port
2(X) = 2:1 port for resource X
D = desert
EX: in game 1, player 1's first settlement was on a 6-lumber, 3-clay, and 11-clay.
production - total cards gained from settlements and cities during game
tradeGain - total cards gained from peer AND bank trades during game
robberCardsGain - total cards gained from stealing with the robber, plus cards gained with non-knight development cards. A road building card is +4 resources.
totalGain - sum of previous 3 columns.
tradeLoss - total cards lost from peer AND bank trades during game
robberCardsLoss - total cards lost from robbers, knights, and other players' monopoly cards
tribute - total cards lost when player had to discard on a 7 roll (separate from previous column.)
totalLoss - sum of previous 3 columns.
totalAvailable - totalGain minus totalLoss.
I only ask that if you produce a good model, you share it with me! Please don't hesitate to ask any clarifying questions."
Twitter Friends,40k full Twitter user profile data (including who they follow!),Hubert Wassner,32,"Version 4,2016-09-02|Version 3,2016-09-02|Version 2,2016-09-02|Version 1,2016-08-31","twitter
internet",CSV,428 MB,CC4,"5,924 views",578 downloads,5 kernels,,https://www.kaggle.com/hwassner/TwitterFriends,"Twitter Friends and hashtags
Context
This datasets is an extract of a wider database aimed at collecting Twitter user's friends (other accound one follows). The global goal is to study user's interest thru who they follow and connection to the hashtag they've used.
Content
It's a list of Twitter user's informations. In the JSON format one twitter user is stored in one object of this more that 40.000 objects list. Each object holds :
avatar : URL to the profile picture
followerCount : the number of followers of this user
friendsCount : the number of people following this user.
friendName : stores the @name (without the '@') of the user (beware this name can be changed by the user)
id : user ID, this number can not change (you can retrieve screen name with this service : https://tweeterid.com/)
friends : the list of IDs the user follows (data stored is IDs of users followed by this user)
lang : the language declared by the user (in this dataset there is only ""en"" (english))
lastSeen : the time stamp of the date when this user have post his last tweet.
tags : the hashtags (whith or without #) used by the user. It's the ""trending topic"" the user tweeted about.
tweetID : Id of the last tweet posted by this user.
You also have the CSV format which uses the same naming convention.
These users are selected because they tweeted on Twitter trending topics, I've selected users that have at least 100 followers and following at least 100 other account (in order to filter out spam and non-informative/empty accounts).
Acknowledgements
This data set is build by Hubert Wassner (me) using the Twitter public API. More data can be obtained on request (hubert.wassner AT gmail.com), at this time I've collected over 5 milions in different languages. Some more information can be found here (in french only) : http://wassner.blogspot.fr/2016/06/recuperer-des-profils-twitter-par.html
Past Research
No public research have been done (until now) on this dataset. I made a private application which is described here : http://wassner.blogspot.fr/2016/09/twitter-profiling.html (in French) which uses the full dataset (Millions of full profiles).
Inspiration
On can analyse a lot of stuff with this datasets :
stats about followers & followings
manyfold learning or unsupervised learning from friend list
hashtag prediction from friend list
Contact
Feel free to ask any question (or help request) via Twitter : @hwassner
Enjoy! ;)"
What is a note?,A collection of notes played on a guitar,Melvyn Drag,32,"Version 4,2016-11-23|Version 3,2016-11-20|Version 2,2016-11-19|Version 1,2016-11-17",music,Other,14 MB,CC0,"5,866 views",382 downloads,15 kernels,,https://www.kaggle.com/juliancienfuegos/what-is-a-note,"Overview
We could take a music theory class to understand what a note is, but why don't we just find out for ourselves? In this data set we have the notes on a guitar on open strings, and on the 1st-8th frets on every string. The notes were recorded on a nice but low-end guitar called the Yamaha C-40. The guitar is in standard tuning (from the top string to the bottom on we have E low, A, D, G, B, E high).
What to do with this dataset
I didn't label any notes (except the open ones - an open A string is an A). If you want a challenge you can cluster the notes and see if your clustering lumps all the same notes together. If you want labels so you can do some inspection of notes that are the same you can look on google for a guitar fretboard diagram. I have done both of these experiments and learned a bit about music which I 'm hoping to verify soon when I find a good music theory book.
This data set might be interesting to use to be able to write sheet music from an audio sample of some finger-picked music. Identifying chords is a difficult computational task, but finger-style guitar, with clear, individual notes, might be easier. If this is the case, a simple script could be written to write sheet music from an audio sample.
One draw back about the data set is that some non-plucked strings were vibrating when I played a note. I tried various techniques to muffle them, but there is still some noise in the background. I don't know if this is because of my technique or something that happens to all players on all guitars. In any event, this noise didn't hurt my analysis.
About the data
They were recorded by me, Melvyn, using a program called audacity.
There is a directory with the name of the string. Inside the directory you will find .wav files named either open, 1, 2, ....8 for the fingering of the string. There is also a directory called ""scale"" I recorded some notes that make a ""do-re-mi..."" scale. You can use these for a number of things.
I use the GuitarTuner app to tune the Guitar - I'm just learning so I don't have an ear for notes yet. After some initial analysis it looks like the guitar might be a bit out of tune, so the resonant frequencies are a bit off from what they should be. Another thing that is interesting to think about it is how far a frequency must be from the proper on until it becomes distinguishable as a different note."
"Emotion, Aging, and Sentiment Over Time",Data from the Chronist project,Chris Roth,32,"Version 1,2017-02-13","gerontology
linguistics",CSV,39 MB,Other,"7,811 views",757 downloads,13 kernels,0 topics,https://www.kaggle.com/cjroth/chronist,"Chronist is a project to quantitatively monitor the emotional and physical changes of an individual over periods of time. My thesis is that if you can accurately show emotional or physical change over time, you can objectively pinpoint how an environmental change such as a career change, moving to a new city, starting or ending a relationship, or starting a new habit like going to the gym affected your physical and emotional health. This can lead to important insights on an individual level and for a population as a whole.
If you are interested in hearing more about this project, contributing your data, or collaborating, contact me at chris@cjroth.com.
See the GitHub repository to read more about the tools that were used to generate the dataset."
Solar Radiation Prediction,Task from NASA Hackathon,Andrey,32,"Version 1,2017-05-22","space
energy",CSV,3 MB,ODbL,"7,082 views",881 downloads,38 kernels,3 topics,https://www.kaggle.com/dronio/SolarEnergy,"Context
Space Apps Moscow was held on April 29th & 30th. Thank you to the 175 people who joined the International Space Apps Challenge at this location!
Content
The dataset contains such columns as: ""wind direction"", ""wind speed"", ""humidity"" and temperature. The response parameter that is to be predicted is: ""Solar_radiation"". It contains measurements for the past 4 months and you have to predict the level of solar radiation. Just imagine that you've got solar energy batteries and you want to know will it be reasonable to use them in future?
Acknowledgements
Thanks NASA for the dataset.
Inspiration
Predict the level of solar radiation. Here are some intersecting dependences that i have figured out: 1. Humidity & Solar_radiation. 2.Temeperature & Solar_radiation.
The best result of accuracy I could get using cross-validation was only 55%."
"Structural MRI Datasets (T1, T2, FLAIR etc.)",Coursera NeuroHacking in R course datasets,IlknurIcke,32,"Version 2,2017-01-05|Version 1,2017-01-04",healthcare,Other,212 MB,Other,"11,885 views","1,552 downloads",14 kernels,2 topics,https://www.kaggle.com/ilknuricke/neurohackinginrimages,"Context
Data from the Coursera Course: Neurohacking In R taught by Dr. Elizabeth Sweeney , Rice Academy Postdoctoral Fellow, Ciprian M. Crainiceanu, Professor and John Muschelli III , Assistant Scientist
Please see https://www.coursera.org/learn/neurohacking for the lecture notes and example code from the instructors.
Content
Structural MRI images for visualization and image processing
From the instructors:
About this course: Neurohacking describes how to use the R programming language (https://cran.r-project.org/) and its associated packages to perform manipulation, processing, and analysis of neuroimaging data. We focus on publicly-available structural magnetic resonance imaging (MRI). We discuss concepts such as inhomogeneity correction, image registration, and image visualization. By the end of this course, you will be able to: Read/write images of the brain in the NIfTI (Neuroimaging Informatics Technology Initiative) format Visualize and explore these images, perform inhomogeneity correction, brain extraction, and image registration (within a subject and to a template).
Acknowledgements
Dataset is public domain and was originally posted for the Coursera online course NeuroHacking in R.
Notes
A. When you download the zip archive, double clicking might try to compress the file instead of extracting it. Unzipping on terminal (mac) correctly decompresses the archive.
B. The zip file contains a directory structure:
       BRAINIX
            -----DICOM
                ----FLAIR
                ----ROI
                ----T1
                ----T2
            -----NIFTI
      kirby21
            -----visit 1
                ----113
            -----visit 2
                ----113
      Template
However, when it is unzipped here on Kaggle environment, somehow the directory structure is not maintained, therefore files with the same names are being overwritten. As a workaround, I added the directory names to the files ie. BRAINIX_DICOM_T1_IM_0001_0011.dcm instead of just IM_0001_0011.dcm.
Check out script https://www.kaggle.com/ilknuricke/d/ilknuricke/neurohackinginrimages/structural-mri-visualization/code for example use."
World Tennis Odds Database,"+139K tennis matches, 15 bookies, worldwide.",DMPierre,32,"Version 1,2017-09-11","tennis
sports",CSV,49 MB,CC4,"4,259 views",368 downloads,3 kernels,0 topics,https://www.kaggle.com/dmpierre/world-tennis-odds-database,"Context
How good of an arbitrageur would you be?
Find it out in the World Tennis Database which gathers more than 139K matches with odds from 15 different bookies (49 MB).
If you are looking to predict the outcome of a tennis match, to find arbitrage opportunities, inspecting variations in a particular player odds or simply searching to improve your Machine Learning or visualisation skills, then this dataset might be looking for you too.
Content
Data is packed in CSV format, ready to spit out some interesting statistics. It is composed of the following 72 columns:
Url, string
Country, string
Date, (yyyy-mm-dd hh:mm), to ease date-time transformations.
Day, string
Tournament name, string
Doubles, either 0 or 1, when it is not a single player match
Player 1(2) name, string
Player 1(2) score, int, number of sets won
Player 1(2) set 0 score, int, up until set 4 - indexing of sets starts at 0, ask why to python ;)
No set info, either 0 or 1, when there is no informations about the final set scores
Missing bookies, either 0 or 1, when there is no informations about any bookies odds
Retired player, either 0 or 1, when one player retired
Cancelled game, either 0 or 1, when the game got cancelled
Comments, string used to insert any comments during the scraping process
Walkover, either 0 or 1, when one player chose to walkover
Awarded player, either 0 or 1, when one player got awarded
Fifteen bookies were then taken into account, each having three type of infos: Player 1 odd, Player 2 odd, Payout. This result in adding to the preceding 27 columns, 45 others.
Bookies were sorted alphabetically:
10Bet
18Bet
5Dimes
Bet At Home
Bet365
BetHard
BetOlimp
BetRally
BWin
JetBull
MarathonBet
Pinnacle
TempoBet
TonyBet
Unibet
Acknowledgements
Huge kudos to the OddsPortal Website for their wonderful archiving job.
Cover photo by Jeremy Galliani on Unsplash.
Inspiration
Various interesting infos and predictions can be made out of this dataset.
Individual players trajectories and their respective odds movements.
Bookies respective strategies. Who sets the pace?
Detecting patterns in arbitrage situations (arbitrageur perspective).
And of course, predicting the winner of a game, as draws are not allowed.
Of course, I got inspired by the European Soccer Database.
Finally, for details about the scraping process, visit https://dmpierre.github.io/."
German Credit Risk,Credit classification,UCI Machine Learning,32,"Version 1,2016-12-15",,CSV,49 KB,ODbL,"24,058 views","2,456 downloads",16 kernels,3 topics,https://www.kaggle.com/uciml/german-credit,"Context
The original dataset contains 1000 entries with 20 categorial/symbolic attributes prepared by Prof. Hofmann. In this dataset, each entry represents a person who takes a credit by a bank. Each person is classified as good or bad credit risks according to the set of attributes. The link to the original dataset can be found below.
Content
It is almost impossible to understand the original dataset due to its complicated system of categories and symbols. Thus, I wrote a small Python script to convert it into a readable CSV file. Several columns are simply ignored, because in my opinion either they are not important or their descriptions are obscure. The selected attributes are:
Age (numeric)
Sex (text: male, female)
Job (numeric: 0 - unskilled and non-resident, 1 - unskilled and resident, 2 - skilled, 3 - highly skilled)
Housing (text: own, rent, or free)
Saving accounts (text - little, moderate, quite rich, rich)
Checking account (numeric, in DM - Deutsch Mark)
Credit amount (numeric, in DM)
Duration (numeric, in month)
Purpose (text: car, furniture/equipment, radio/TV, domestic appliances, repairs, education, business, vacation/others)
Acknowledgements
Source: UCI"
Hospital General Information,General information & quality ratings for almost all US hospitals,Centers for Medicare & Medicaid Services,32,"Version 2,2017-08-10|Version 1,2017-01-07","healthcare
hospitals
public health",CSV,3 MB,Other,"11,210 views","1,372 downloads",36 kernels,4 topics,https://www.kaggle.com/cms/hospital-general-information,"Context
There are all sorts of reasons why you'd want to know a hospital's quality rating.
Your mom is having her second hip replacement. Her first one went terribly and you're nervous about how she'll do. Which hospital would you suggest she have her surgery?
You're selecting a health plan on your state's Exchange, but your top two choices partner with different hospitals. How will you decide which plan to pick?
Your brother has Cystic Fibrosis and has to go to the ER frequently. He hates waiting. Which hospitals/states provide care in the timeliest manner?
Your in-laws moved to Florida recently to retire, and have been trying to convince you to move there too. You're looking for any way possible to show them that your state is better. Does your state have better hospitals?
Every hospital in the United States of America that accepts publicly insured patients (Medicaid or MediCare) is required to submit quality data, quarterly, to the Centers for Medicare & Medicaid Services (CMS). There are very few hospitals that do not accept publicly insured patients, so this is quite a comprehensive list.
Content
This file contains general information about all hospitals that have been registered with Medicare, including their addresses, type of hospital, and ownership structure. It also contains information about the quality of each hospital, in the form of an overall rating (1-5, where 5 is the best possible rating & 1 is the worst), and whether the hospital scored above, same as, or below the national average for a variety of measures.
This data was updated by CMS on July 25, 2017. CMS' overall rating includes 60 of the 100 measures for which data is collected & reported on Hospital Compare website (https://www.medicare.gov/hospitalcompare/search.html). Each of the measures have different collection/reporting dates, so it is impossible to specify exactly which time period this dataset covers. For more information about the timeframes for each measure, see: https://www.medicare.gov/hospitalcompare/Data/Data-Updated.html# For more information about the data itself, APIs and a variety of formats, see: https://data.medicare.gov/Hospital-Compare
Acknowledgements
Attention: Works of the U.S. Government are in the public domain and permission is not required to reuse them. An attribution to the agency as the source is appreciated. Your materials, however, should not give the false impression of government endorsement of your commercial products or services. See 42 U.S.C. 1320b-10.
Inspiration
Which hospital types & hospital ownerships are most common?
Which hospital types & ownerships are associated with better than average ratings/mortality/readmission/etc?
What is the average hospital rating, by state?
Which hospital types & hospital ownerships are more likely to have not submitted proper data (""Not Available"" & ""Results are not available for this reporting period"")?
Which parts of the country have the highest & lowest density of religious hospitals?"
Women's Shoe Prices,"A list of 10,000 women's shoes and the prices at which they are sold.",Datafiniti,32,"Version 2,2017-05-15|Version 1,2017-04-25","clothing
business",CSV,102 MB,CC4,"13,544 views","2,002 downloads",59 kernels,3 topics,https://www.kaggle.com/datafiniti/womens-shoes-prices,"About This Data
This is a list of 10,000 women's shoes and their product information provided by Datafiniti's Product Database.
The dataset includes shoe name, brand, price, and more. Note that each shoe will have an entry for each price found for it and some shoes may have multiple entries.
What You Can Do with This Data
You can use this data to determine brand markups, pricing strategies, and trends for luxury shoes. E.g.:
What is the average price of each distinct brand listed?
Which brands have the highest prices?
Which ones have the widest distribution of prices?
Is there a typical price distribution (e.g., normal) across brands or within specific brands?
Further processing data would also let you:
Correlate specific product features with changes in price.
You can cross-reference this data with a sample of our Men's Shoe Prices to see if there are any differences between women's brands and men's brands.
Data Schema
A full schema for the data is available in our support documentation.
About Datafiniti
Datafiniti provides instant access to web data. We compile data from thousands of websites to create standardized databases of business, product, and property information. Learn more.
Want More?
You can get more data like this by joining Datafiniti or requesting a demo."
"SpaceX Missions, 2006-Present","Location, date/time, and outcome for every rocket launch",SpaceX,32,"Version 1,2017-03-01","space
business",CSV,7 KB,Other,"6,597 views",751 downloads,19 kernels,2 topics,https://www.kaggle.com/spacex/spacex-missions,"Context
SpaceX has gained worldwide attention for a series of historic milestones. It is the only private company ever to return a spacecraft from low-Earth orbit, which it first accomplished in December 2010. The company made history again in May 2012 when its Dragon spacecraft attached to the International Space Station, exchanged cargo payloads, and returned safely to Earth — a technically challenging feat previously accomplished only by governments. Since then Dragon has delivered cargo to and from the space station multiple times, providing regular cargo resupply missions for NASA.
Under a $1.6 billion contract with NASA, SpaceX is flying numerous cargo resupply missions to the International Space Station, for a total of at least 20 flights under the Commercial Resupply Services contract. In 2016, NASA awarded SpaceX a second version of that contract that will cover a minimum of 6 additional flights from 2019 onward. In the near future, SpaceX will carry crew as part of NASA’s Commercial Crew Program as well. Dragon was designed from the outset to carry astronauts and SpaceX is in the process of upgrading Dragon to make it crew-ready. SpaceX is the world’s fastest-growing provider of launch services and has over 70 future missions on its manifest, representing over $10 billion in contracts. These include commercial satellite launches as well as NASA and other US Government missions.
Content
This dataset includes a record for each payload carried during a SpaceX mission into outer space.
Acknowledgements
The data was scraped from the SpaceX and NASA website.
Inspiration
Has the rate of SpaceX rocket launches increased in the past decade? How many missions do you predict will be launched in 2018?"
Handwritten math symbols dataset,Over 100 000 image samples.,Xai Nano,31,"Version 2,2017-01-16|Version 1,2017-01-15","writing
mathematics",Other,410 MB,CC0,"9,919 views",938 downloads,8 kernels,3 topics,https://www.kaggle.com/xainano/handwrittenmathsymbols,"Contact
Email me at: itsawesome17@gmail.com
My blog: http://blog.mathocr.com/
Content
Dataset consists of jpg files(45x45) DISCLAIMER: dataset does not contain Hebrew alphabet at all. It includes basic Greek alphabet symbols like: alpha, beta, gamma, mu, sigma, phi and theta. English alphanumeric symbols are included. All math operators, set operators. Basic pre-defined math functions like: log, lim, cos, sin, tan. Math symbols like: \int, \sum, \sqrt, \delta and more.
Acknowledgements
Original source, that was parsed, extracted and modified is CROHME dataset.
Visit CROHME at http://www.isical.ac.in/~crohme/index.html.
Inspiration
Due to the technological advances in recent years, paper scientific documents are used less and less. Thus, the trend in the scientific community to use digital documents has increased considerably. Among these documents, there are scientific documents and more specifically mathematics documents. So I give a tool, to research recognizing handwritten math language in variety of applications.
Source code
You can find source code responsible for parsing original CROHME dataset here:
https://github.com/XaiNano/CROHME_extractor
This parser allows you not only to extract math symbols into square images of desired size, but also lets you specify categories of classes to be extracted like: digits, greek_letters, lowercase_letters, operators, and more. It also contains visualization tools and histograms showing appearances of each class in the dataset.
Commercial use
Rights for commercial usage cannot be granted."
Food Images (Food-101),Labeled food images in 101 categories from apple pies to waffles,Kevin Mader,31,"Version 4,2017-08-09|Version 3,2017-08-08|Version 2,2017-08-08|Version 1,2017-08-08","food and drink
popular culture
image data
multiclass classification",Other,663 MB,Other,"10,093 views","1,551 downloads",10 kernels,0 topics,https://www.kaggle.com/kmader/food41,"Overview
The dataset contains a number of different subsets of the full food-101 data. The idea is to make a more exciting simple training set for image analysis than CIFAR10 or MNIST. For this reason the data includes massively downscaled versions of the images to enable quick tests. The data has been reformatted as HDF5 and specifically Keras HDF5Matrix which allows them to be easily read in. The file names indicate the contents of the file. For example
food_c101_n1000_r384x384x3.h5 means there are 101 categories represented, with n=1000 images, that have a resolution of 384x384x3 (RGB, uint8)
food_test_c101_n1000_r32x32x1.h5 means the data is part of the validation set, has 101 categories represented, with n=1000 images, that have a resolution of 32x32x1 (float32 from -1 to 1)
Challenge
The first goal is to be able to automatically classify an unknown image using the dataset, but beyond this there are a number of possibilities for looking at what regions / image components are important for making classifications, identify new types of food as combinations of existing tags, build object detectors which can find similar objects in a full scene.
Data Acknowledgement
The data was repackaged from the original source (gzip) available at https://www.vision.ee.ethz.ch/datasets_extra/food-101/
License
The Food-101 data set consists of images from Foodspotting [1]. Any use beyond scientific fair use must be negotiated with the respective picture owners according to the Foodspotting terms of use [2].
[1] http://www.foodspotting.com/ [2] http://www.foodspotting.com/terms/"
Historical Hourly Weather Data 2012-2017,Hourly weather data for 30 US & Canadian Cities + 6 Israeli Cities,Selfish Gene,31,"Version 2,2017-12-28|Version 1,2017-12-23","united states
time series
geography
+ 2 more...",CSV,12 MB,ODbL,"3,585 views",593 downloads,,2 topics,https://www.kaggle.com/selfishgene/historical-hourly-weather-data,"Historical Hourly Weather Data
Who amongst us doesn't small talk about the weather every once in a while?
The goal of this dataset is to elevate this small talk to medium talk.
Just kidding, I actually originally decided to collect this dataset in order to demonstrate basic signal processing concepts, such as filtering, Fourier transform, auto-correlation, cross-correlation, etc..., (for a data analysis course I'm currently preparing).
I wanted to demonstrate these concepts on signals that we all have intimate familiarity with and hope that this way these concepts will be better understood than with just made up signals.
The weather is excellent for demonstrating these kinds of concepts as it contains periodic temporal structure with two very different periods (daily and yearly).
Content
The dataset contains ~5 years of high temporal resolution (hourly measurements) data of various weather attributes, such as temperature, humidity, air pressure, etc.
This data is available for 30 US and Canadian Cities, as well as 6 Israeli cities.
I've organized the data according to a common time axis for easy use.
Each attribute has it's own file and is organized such that the rows are the time axis (it's the same time axis for all files), and the columns are the different cities (it's the same city ordering for all files as well).
Additionally, for each city we also have the country, latitude and longitude information in a separate file.
Acknowledgements
The dataset was aquired using Weather API on the OpenWeatherMap website, and is available under the ODbL License.
Inspiration
Weather data is both intrinsically interesting, and also potentially useful when correlated with other types of data.
For example, Wildfire spread is potentially related to weather conditions, demand for cabs is famously known to be correlated with weather conditions (here, here and here you can find NYC cab ride data), and use of city bikes is probably also correlated with weather in interesting ways (check out this Austin dataset, this SF dataset, this Montreal dataset, and this NYC dataset).
Traffic is also probably related to weather.
Another potentially interesting source of correlation is between weather and crime. Here are a few crime datasets on kaggle of cities present in this weather dataset: Chicago, Philadelphia, Los Angeles, Vancouver, Austin, NYC
There are many other potentially interesting connections between everyday life and the weather that we can explore together with the help of this dataset. Have fun!"
Rainfall in India,Sub-division wise monthly data for 115 years from 1901-2015.,Rajanand Ilangovan / இராஜ்ஆனந்த் இளங்கோவன்,31,"Version 2,2017-08-05|Version 1,2017-07-23","india
weather",CSV,583 KB,CC4,"6,058 views","1,308 downloads",5 kernels,0 topics,https://www.kaggle.com/rajanand/rainfall-in-india,"Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks.
Context
This data set contains monthly rainfall detail of 36 meteorological sub-divisions of India.
Content
Time Period: 1901 - 2015
Granularity: Monthly
Location: 36 meteorological sub-divisions in India
Rainfall unit: mm
Acknowledgements
India Meteorological Department(IMD) Govt. of India has shared this dataset under Govt. Open Data License - India."
"English Premier League Players Dataset, 2017/18","A unique dataset containing FPL data, popularity and market values",ShubhamMaurya,31,"Version 1,2017-08-03","popular culture
association football
sports",CSV,34 KB,CC0,"8,498 views","1,156 downloads",7 kernels,0 topics,https://www.kaggle.com/mauryashubham/english-premier-league-players-dataset,"Context
For most football fans, May - July represents a lull period due to the lack of club football. What makes up for it, is the intense transfer speculation that surrounds all major player transfers today. Their market valuations also lead to a few raised eyebrows, lately more than ever. I was curious to see how good a proxy popularity could be for ability, and the predictive power it would have in a model estimating a player's market value.
Content
name: Name of the player
club: Club of the player
age : Age of the player
position : The usual position on the pitch
position_cat :
1 for attackers
2 for midfielders
3 for defenders
4 for goalkeepers
market_value : As on transfermrkt.com on July 20th, 2017
page_views : Average daily Wikipedia page views from September 1, 2016 to May 1, 2017
fpl_value : Value in Fantasy Premier League as on July 20th, 2017
fpl_sel : % of FPL players who have selected that player in their team
fpl_points : FPL points accumulated over the previous season
region:
1 for England
2 for EU
3 for Americas
4 for Rest of World
nationality
new_foreign : Whether a new signing from a different league, for 2017/18 (till 20th July)
age_cat
club_id
big_club: Whether one of the Top 6 clubs
new_signing: Whether a new signing for 2017/18 (till 20th July)
Inspiration
To statistically analyse the beautiful game."
My Uber Drives,Complete Details of My Uber Drives in 2016,Zeeshan-ul-hassan Usmani,31,"Version 1,2017-03-23",road transport,CSV,84 KB,ODbL,"18,369 views","1,877 downloads",26 kernels,4 topics,https://www.kaggle.com/zusmani/uberdrives,"Context
My Uber Drives (2016)
Here are the details of my Uber Drives of 2016. I am sharing this dataset for data science community to learn from the behavior of an ordinary Uber customer.
Content
Geography: USA, Sri Lanka and Pakistan
Time period: January - December 2016
Unit of analysis: Drives
Total Drives: 1,155
Total Miles: 12,204
Dataset: The dataset contains Start Date, End Date, Start Location, End Location, Miles Driven and Purpose of drive (Business, Personal, Meals, Errands, Meetings, Customer Support etc.)
Acknowledgements & References
Users are allowed to use, download, copy, distribute and cite the dataset for their pet projects and training. Please cite it as follows: “Zeeshan-ul-hassan Usmani, My Uber Drives Dataset, Kaggle Dataset Repository, March 23, 2017.”
Past Research
Uber TLC FOIL Response - The dataset contains over 4.5 million Uber pickups in New York City from April to September 2014, and 14.3 million more Uber pickups from January to June 2015 https://github.com/fivethirtyeight/uber-tlc-foil-response
1.1 Billion Taxi Pickups from New York - http://toddwschneider.com/posts/analyzing-1-1-billion-nyc-taxi-and-uber-trips-with-a-vengeance/
What you can do with this data - a good example by Yao-Jen Kuo - https://yaojenkuo.github.io/uber.html
Inspiration
Some ideas worth exploring:
• What is the average length of the trip?
• Average number of rides per week or per month?
• Total tax savings based on traveled business miles?
• Percentage of business miles vs personal vs. Meals
• How much money can be saved by a typical customer using Uber, Careem, or Lyft versus regular cab service?"
SEPTA - Regional Rail,Predict Arrival Times of Philadelphia's Regional Trains.,SEPTA,31,"Version 26,2016-11-07|Version 25,2016-11-03|Version 24,2016-11-02|Version 23,2016-11-01|Version 22,2016-10-11|Version 21,2016-10-08|Version 20,2016-10-02|Version 19,2016-09-26|Version 18,2016-09-16|Version 17,2016-09-01|Version 16,2016-08-12|Version 15,2016-08-02|Version 14,2016-07-25|Version 13,2016-07-20|Version 12,2016-07-15|Version 11,2016-07-07|Version 10,2016-06-11|Version 9,2016-06-02|Version 8,2016-05-30|Version 7,2016-05-28|Version 6,2016-05-26|Version 5,2016-05-25|Version 4,2016-05-25|Version 3,2016-05-25|Version 2,2016-05-25|Version 1,2016-05-24",rail transport,CSV,775 MB,CC0,"25,033 views","1,803 downloads",89 kernels,11 topics,https://www.kaggle.com/septa/on-time-performance,"SEPTA - Southeastern Pennsylvania Transportation Authority
The SEPTA Regional Rail system consists of commuter rail service on 13 branches to more than 150 active stations in Philadelphia, Pennsylvania, and its suburbs and satellite cities.
SEPTA uses On-Time Performance (OTP) to measure service reliability. OTP identifies the number of trains for all rail lines that arrive at their scheduled destination at the scheduled time. However, by industry standard, a train may arrive up to 5 minutes and 59 seconds after its scheduled time and still be considered on-time.
SEPTA has established an annual goal of 91% for Regional Rail On-Time Performance. How well are they doing? Is it even a meaningful measure?
Data Description
otp.csv
train_id
direction ('N' or 'S' direction is demarcated as either Northbound or Southbound)^1
origin (See map below - you'll see 'Warminster', 'Glenside',...'Airport Terminal..')
next_station (Think of this as the station stop, at timeStamp)
date
status ('On Time', '5 min', '10 min'. This is a status on train lateness. 999 is a suspended train)
timeStamp
trainView.csv - GPS Train data (early release)
Most GPS coordinates are based on track telemetry; however, cars are being equipped with GPS units.
train_id
status
next_station
service
dest
lon
lat
source
track_change
track
date
timeStamp0 First timeStamp at coordinates.
timeStamp1 Last timeStamp at coordinates.
You can look at the example here on how to track a single train.
What To Look For...
Ah, as a commuter you care more about the performance of the train(s) you plan to take. OTP maybe 91% or above; but, if the train you take runs late, you'll spend extra time on the tracks. If it consistently runs late, maybe the schedule should be changed.
Look for patterns. For example, during the Tuesday morning rush do some trains run consistently late? How long does it take to get to from point A to point B in the system? Performance is VERY important to SEPTA.
Below is a map of the system and station stops. This dataset contains data on the Regional Rail Lines.
SEPTA train schedules can be found here. Note different Weekday, Saturday, and Sunday schedules."
French presidential election,Extract from Twitter about the French election (with a taste of Google Trends),Jean-Michel D.,31,"Version 21,2017-07-05|Version 20,2017-05-12|Version 19,2017-05-06|Version 18,2017-04-29|Version 17,2017-04-25|Version 16,2017-04-21|Version 15,2017-04-17|Version 14,2017-04-14|Version 13,2017-04-11|Version 12,2017-04-07|Version 11,2017-04-05|Version 10,2017-04-03|Version 9,2017-04-03|Version 8,2017-03-30|Version 7,2017-03-27|Version 6,2017-03-24|Version 5,2017-03-23|Version 4,2017-03-22|Version 3,2017-02-28|Version 2,2017-02-28|Version 1,2017-02-28",politics,SQLite,3 GB,ODbL,"11,027 views",909 downloads,39 kernels,2 topics,https://www.kaggle.com/jeanmidev/french-presidential-election,"Context
This dataset is born from a test with the twitter streaming api to filter and collect data from this flow on a specific topic, in this case the French election.The script used to make this data collection is available on this Github repository.
Since the 18th of March, the French election enter in the final straight line until the first poll the 23 April 2017 , the candidates for the position are:
M. Nicolas DUPONT-AIGNAN
Mme Marine LE PEN
M. Emmanuel MACRON
M. Benoît HAMON
Mme Nathalie ARTHAUD
M. Philippe POUTOU
M. Jacques CHEMINADE
M. Jean LASSALLE
M. Jean-Luc MÉLENCHON
M. François ASSELINEAU
M. François FILLON
The idea was to collect the data from the Twitter API periodically. The acquisition process evolved as follows:
Versions 1, 2 and 3 Every hour a python script listens to the twitter api stream for 10 minutes during 3 weeks.
Version 4+ The new versions will be based on a new data structure, and start after the validation by the French constitutional council on 18 March 2017 of the candidates.
The data will be stored in a dbsqlite files(database_number of the week_number_block_weekday.sqlite format) and will be updated as often as I can (at least every week).
After the first round (version 18+) i had to readjust the number of files per week and the 20 files kaggle limitation push me to reduce the number of files to upload (but you can join for your local analytics the version 17 + version 18+)
Example : Illustration of the number of mentions of the different candidates
I add to these databases a sqlite database that contains the informations from the google trends about the top 5 candidates.In thid database there is :
A table that contains the overall interests by region
A table that contains the interests by region for each candidate
A table with the top25 associated queries for each candidate in top and rising ranking
Content
In this dbsqlite file, you will find a data table that contains for every row:
===============Common===============
the index of the line
the language of the tweet
for each candidate :mention_candidatename, if the candidate or his associated account has been called (0 or 1)
the tweet
the timestamp in milliseconds
===============Version 4+===============
the day
the hour (London timezone)
the username of the user that made the tweet
the username location (that he gives with his profile)
if the tweet is a retweet or a quote (0 or 1)
the username that has been retweeted
the original tweet (the one retweeted or quoted)
Acknowledgements
This election is gonna be intense.
Inspiration
The first version of the dataset was just a test to collect the data and see the first pieces of work that the community can do with this dataset.The new versions are (I think and hope) adapted to do deep text analytics."
"Oil Pipeline Accidents, 2010-Present","Causes, injuries/fatalities, and costs of pipeline leaks and spills",Department of Transportation,31,"Version 1,2017-02-09","environment
energy",CSV,887 KB,CC0,"8,446 views","1,218 downloads",40 kernels,0 topics,https://www.kaggle.com/usdot/pipeline-accidents,"Content
This database includes a record for each oil pipeline leak or spill reported to the Pipeline and Hazardous Materials Safety Administration since 2010. These records include the incident date and time, operator and pipeline, cause of incident, type of hazardous liquid and quantity lost, injuries and fatalities, and associated costs.
Acknowledgements
The oil pipeline accident reports were collected and published by the DOT's Pipeline and Hazardous Materials Safety Administration."
Gene expression dataset (Golub et al.),Molecular Classification of Cancer by Gene Expression Monitoring,Chris Crawford,30,"Version 3,2017-08-09|Version 2,2017-08-09|Version 1,2017-08-09","human genetics
biology
health sciences
biotechnology",CSV,4 MB,CC0,"7,855 views",971 downloads,28 kernels,0 topics,https://www.kaggle.com/crawford/gene-expression,"Context
This dataset comes from a proof-of-concept study published in 1999 by Golub et al. It showed how new cases of cancer could be classified by gene expression monitoring (via DNA microarray) and thereby provided a general approach for identifying new cancer classes and assigning tumors to known classes. These data were used to classify patients with acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL).
Content
Golub et al ""Molecular Classification of Cancer: Class Discovery and Class Prediction by Gene Expression Monitoring""
There are two datasets containing the initial (training, 38 samples) and independent (test, 34 samples) datasets used in the paper. These datasets contain measurements corresponding to ALL and AML samples from Bone Marrow and Peripheral Blood. Intensity values have been re-scaled such that overall intensities for each chip are equivalent.
Acknowledgements
Molecular Classification of Cancer: Class Discovery and Class Prediction by Gene Expression
Science 286:531-537. (1999). Published: 1999.10.14
T.R. Golub, D.K. Slonim, P. Tamayo, C. Huard, M. Gaasenbeek, J.P. Mesirov, H. Coller, M. Loh, J.R. Downing, M.A. Caligiuri, C.D. Bloomfield, and E.S. Lander
These datasets have been converted to a comma separated value files (CSV).
Inspiration
These datasets are great for classification problems. The original authors used the data to classify the type of cancer in each patient by their gene expressions."
Brazil's House of Deputies Reimbursements,Refunds Spendings from 2009 to 2017,epattaro,30,"Version 4,2017-11-04|Version 3,2017-11-04|Version 2,2017-01-27|Version 1,2017-01-22","brazil
finance
politics",CSV,394 MB,CC0,"5,539 views",465 downloads,23 kernels,6 topics,https://www.kaggle.com/epattaro/brazils-house-of-deputies-reimbursements,"Context
Brazilian politicians are entitled to refunds if they spend any of their money on an activity that is enabling them to ""better serve the people"".
Those expenses are public data, however, there is little monitoring/data analysis of it. A quick look at it shows a deputy with over 800 flights in one year. Another deputy has over 140.000R$ expenses on mailing (old fashion mail) in one year.
There are a lot of very suspicious data regarding the deputies expending behavior. Can you help spot outliers and companies charging unusual amounts of money for a service?
Content
Data is public. It was collected from the official government website:
http://www2.camara.leg.br/transparencia/cota-para-exercicio-da-atividade-parlamentar/dados-abertos-cota-parlamentar
It was converted from xml to csv, filtered out irrelevant columns, and translated a few of the features to English.
The uploaded data contains:
u'deputy_name', u'political_party', u'deputy_state', u'company_name' u'company_id' u'refund_date'
Inspiration
Brazil is currently passing through thriving times.
Its political group has always been public involved in many scandals, but it is just now that a few brave men and women have started doing something about it. In 2016 we have had senators, former ministers and many others formally charged and arrested for their crimes."
NEWS SUMMARY,Generating short length descriptions of news articles.,Kondalarao Vonteru,30,"Version 1,2017-08-10","journalism
india
linguistics",CSV,11 MB,GPL,"6,163 views",562 downloads,,2 topics,https://www.kaggle.com/sunnysai12345/news-summary,"Context
I am currently working on summarizing chat context where it helps an agent in understanding previous context quickly. It interests me to apply the deep learning models to existing datasets and how they perform on them. I believe news articles are rich in grammar and vocabulary which allows us to gain greater insights.
Content
The dataset consists of 4515 examples and contains Author_name, Headlines, Url of Article, Short text, Complete Article. I gathered the summarized news from Inshorts and only scraped the news articles from Hindu, Indian times and Guardian. Time period ranges from febrauary to august 2017.
Acknowledgements
I would like to thank the authors of Inshorts for their amazing work
Inspiration
Generating short length descriptions(headlines) from text(news articles).
Summarizing large amount of information which can be represented in compressed space
Purpose
When I was working on the summarization task I didn't find any open source data-sets to work on, I believe there are people just like me who are working on these tasks and I hope it helps them.
Contributions
It will be really helpful if anyone found nice insights from this data and can share their work. Thankyou...!!!
For those who are interested here is the link for the github code which includes the scripts for scraping. https://github.com/sunnysai12345/News_Summary"
Douban Movie Short Comments Dataset,DMSCD producted by utmhikari,utmhikari,30,"Version 7,2017-03-06|Version 6,2017-01-16|Version 5,2017-01-16|Version 4,2017-01-16|Version 3,2017-01-01|Version 2,2016-12-29|Version 1,2016-12-29","film
internet",CSV,387 MB,CC0,"7,193 views",517 downloads,3 kernels,2 topics,https://www.kaggle.com/utmhikari/doubanmovieshortcomments,"Douban Movie Short Comments Dataset V2
Introduction
Douban Movie is a Chinese website that allows Internet users to share their comments and viewpoints about movies. Users are able to post short or long comments on movies and give them marks. This dataset contains more than 2 million short comments of 28 movies in Douban Movie website. It can be used on text classification, text clustering, sentiment analysis, semantic web construction and some other fields that relate to web mining or NLP (of Chinese lol).
Metadata
ID the ID of the comment (start from 0)
Movie_Name_EN the English name of the movie
Movie_Name_CN the Chinese name of the movie
Crawl_Date the date that the data are crawled
Number the number of the comment
Username the username of the account
Date the date that the comment posted
Star the star that users give to the movie (from 1 to 5, 5 grades)
Comment the content of the comment
Like the count of ""like"" on the comment"
GoodReads Book reviews,Dataset of book Reviews,Gnanesh,30,"Version 2,2017-10-21|Version 1,2017-10-21",,CSV,305 MB,CC4,"9,980 views",374 downloads,,,https://www.kaggle.com/gnanesh/goodreads-book-reviews,"Context
Goodreads Book Reviews dataset.
Content
This data was collected duing oct (12-21) '17
Schema: root |-- bookID: string (nullable = false) |-- title: string (nullable = false) |-- author: string (nullable = false) |-- rating: string (nullable = false) |-- ratingsCount: string (nullable = false) |-- reviewsCount: string (nullable = false) |-- reviewerName: string (nullable = true) |-- reviewerRatings: string (nullable = true) |-- review: string (nullable = true)
Acknowledgements
Thank you, Goodreads for the data. All the data here belongs to Goodreads.
Inspiration
This data was actually scrapped for WebMining Project."
Recommendation System for Angers Smart City,Global Smart City Survey for Recommendation Systems,ASSO PAVIC - Angers Smart City,30,"Version 1,2017-11-29","learning
artificial intelligence
mobile web",Other,757 KB,ODbL,313 views,14 downloads,,0 topics,https://www.kaggle.com/assopavic/recommendation-system-for-angers-smart-city,"Context
In January 2017, PAVIC submitted a survey focused on Smart City and collected data from 1076 people. This survey was fully anonymous and was aimed at improving the citizens' lives in the future Smart City
Content
The idea of the survey is to obtain a precise insight concerning the citizens' reactions to different recommendations in two different contexts. In clear, respondents were asked to choose among a set of 18 recommendations those that they would be most interested in if it were proposed in two different contexts: on a sunny and warm (20°C) Saturday afternoon in Spring (referred to as the ""Sun"" context) and on a rainy and cold (8°C) Saturday afternoon in Winter (referred to as the ""Rain"" context). The recommendations concerned various subjects : social or cultural events, discounts in restaurants, useful city information, etc. and people were asked in each context which they would like to receive as push notifications on their phones. For each context, respondents could give several or no responses.
The following are the precise text of the questions submitted to the respondents : - for the ""Sun"" dataset : A Saturday in spring around 4pm with a comfy temperature of 20°C or 68°F. You are downtown in the city for the afternoon and your mobile application can send you personalized services/activities notifications in real-time. Which of the following activities would you want to receive in your notifications ? (Several may be chosen). - for the ""Rain"" dataset : A rainy Saturday in the winter around 4pm with a cool temperature of 8°C or 46°F. You are downtown in the city for the afternoon and your mobile application can send you personalized services/activities notifications in real-time. Which of the following activities would you want to receive in your notifications ? (Several may be chosen).
Inspiration
These dataset could allow future applications both to simulate recommendation system algorithms, and to deduce clusters from the collected profiles."
Hacker News Posts,Hacker News posts from the past 12 months (including # of votes and comments),Hacker News,30,"Version 1,2016-09-27","news agencies
internet",CSV,45 MB,CC0,"6,415 views",650 downloads,34 kernels,,https://www.kaggle.com/hacker-news/hacker-news-posts,"This data set is Hacker News posts from the last 12 months (up to September 26 2016).
It includes the following columns:
title: title of the post (self explanatory)
url: the url of the item being linked to
num_points: the number of upvotes the post received
num_comments: the number of comments the post received
author: the name of the account that made the post
created_at: the date and time the post was made (the time zone is Eastern Time in the US)
One fun project suggestion is a model to predict the number of votes a post will attract.
The scraper is written, so I can keep this up-to-date and add more historical data. I can also scrape the comments. Just make the request in this dataset's forum.
The is a fork of minimaxir's HN scraper (thanks minimaxir): https://github.com/minimaxir/get-all-hacker-news-submissions-comments"
Education Statistics,Studies of education status indicators around the world,World Bank,30,"Version 1,2016-11-18","countries
education",CSV,296 MB,Other,"16,223 views","2,151 downloads",10 kernels,0 topics,https://www.kaggle.com/theworldbank/education-statistics,"Context
The World Bank EdStats All Indicator Query holds over 4,000 internationally comparable indicators that describe education access, progression, completion, literacy, teachers, population, and expenditures. The indicators cover the education cycle from pre-primary to vocational and tertiary education.
Content
In addition to the above mentioned indicators, this dataset also holds learning outcome data from international and regional learning assessments (e.g. PISA, TIMSS, PIRLS), equity data from household surveys, and projection/attainment data to 2050. For further information, please visit the EdStats website.
Inspiration
In your opinion, what are some of the more surprising indicators? Are there any you would consider adding?
Which countries have the highest adult illiteracy rates? Have they changed over time, and do rates vary based on age bracket?
Do school enrollment rates have an impact on adult illiteracy rates? If so, can you determine approximately how long a change in enrollment takes in order to impact illiteracy? Does this change vary among countries, and if so, can you point to changes in policies or NGO efforts that might play a role?
Acknowledgements
Data was acquired from the World Bank, and can be accessed in multiple formats here."
Illegal Immigrants Arrested by US Border Patrol,Has the number of Mexican citizens crossing the border increased or decreased?,US Customs and Border Protection,30,"Version 1,2017-01-28","crime
international relations",CSV,6 KB,CC0,"7,911 views",940 downloads,55 kernels,,https://www.kaggle.com/cbp/illegal-immigrants,"Content
This report provides statistics for the number of illegal immigrants arrested or apprehended by the border patrol in each division (or sector) of the United States borders with Canada, Mexico, and Caribbean islands; this data is a partial measure of the flow of people illegally entering the United States.
Acknowledgements
Data was compiled and published by the US Border Patrol on the Customs and Border Protection webpage."
Road Accidents Incidence,Road Accidents Data Great Britain 1979-2015,Akshay Babbar,29,"Version 1,2017-01-23",road transport,CSV,68 MB,Other,"9,609 views","1,611 downloads",10 kernels,3 topics,https://www.kaggle.com/akshay4/road-accidents-incidence,"Context
Road Accidents
Content
Dataset has been fetched from here and the files have been merged and cleaned to reach the final data attached. Primarily Captures Road Accidents in UK between 1979 and 2015 and has 70 features/columns and about 250K rows. Also attached with it is an excel file with Multiple Tabs that can help one to understand the Data.
Acknowledgements
Data has been fetched from Open Data Platform UK and is being shared under Open Government Licence. For more details refer to Open Data UK"
Tourists Visiting Brazil,Who visited Brazil between 1989 and 2015?,Luiza Fontana,29,"Version 1,2016-11-27","brazil
leisure",CSV,33 MB,Other,"6,142 views",750 downloads,18 kernels,,https://www.kaggle.com/zafontana/touristsinbrazil,"Context:
This dataset contains the number of international tourists arriving in Brazil each month from 1989 to 2015.
Content:
Continent
Country
State of arrival
Way in (by land, sea, river or air)
Year
Month
Count
Acknowledgements:
I've downloaded this dataset from dados.gov.br, the Brazilian open data portal, and tried to tidy it up a bit."
International Debt Statistics,Major Financial Indicators from Developing and Advanced Economies,World Bank,29,"Version 1,2016-11-25",finance,CSV,14 MB,Other,"6,979 views",999 downloads,10 kernels,0 topics,https://www.kaggle.com/theworldbank/international-debt-statistics,"Context
Focuses on financial flows, trends in external debt, and other major financial indicators for developing and advanced economies (data from Quarterly External Debt Statistics and Quarterly Public Sector Debt databases). Includes over 200 time series indicators from 1970 to 2014, for most reporting countries, and pipeline data for scheduled debt service payments on existing commitments to 2022.
Content
This dataset contains country names and indicator variables from 1970 until 2024. Additional materials and detailed descriptions of the datasets can be downloaded from here.
Acknowledgement
The original datasets and data dictionaries can be found here.
Inspiration
Few ideas for exploring the dataset:
Compare the current account balance across countries. Is there a pattern associated with developing vs. advanced economies?
How have the debt-related indicators changed over time? Are these strongly associated with other financial indicators?"
Storm Prediction Center,"Historic tornado data, 1950 to 2015",JeffTennis,29,"Version 3,2016-11-22|Version 2,2016-11-06|Version 1,2016-11-06",climate,CSV,5 MB,Other,"10,219 views","1,212 downloads",11 kernels,5 topics,https://www.kaggle.com/jtennis/spctornado,"Database of tornado activity from 1950 to 2015
Created by National Weather service and available at http://www.spc.noaa.gov/gis/svrgis/
Enhance understanding of where tornados happen, indicators of damage, and weather conditions associated with tornados (temp/El Nino, La Nina)
Metadata available at http://www.spc.noaa.gov/wcm/data/SPC_severe_database_description.pdf"
Suicides in India,Sucides in each state is classified according to various parameters from 2001-12,Rajanand Ilangovan / இராஜ்ஆனந்த் இளங்கோவன்,29,"Version 1,2017-07-23","india
mental health
death
health",CSV,15 MB,CC4,"11,599 views","1,940 downloads",26 kernels,5 topics,https://www.kaggle.com/rajanand/suicides-in-india,"Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks.
Context
This data set contains yearly suicide detail of all the states/u.t of India by various parameters from 2001 to 2012.
Content
Time Period: 2001 - 2012 Granularity: Yearly Location: States and U.T's of India
Parameters:
a) Suicide causes b) Education status c) By means adopted d) Professional profile e) Social status
Acknowledgements
National Crime Records Bureau (NCRB), Govt of India has shared this dataset under Govt. Open Data License - India. NCRB has also shared the historical data on their website"
One Week of Global News Feeds,7 days of tracking 20k news feeds worldwide,Rohk,29,"Version 2,2017-09-30|Version 1,2017-09-25","news agencies
historiography
linguistics
internet",CSV,280 MB,CC4,"6,201 views",377 downloads,6 kernels,2 topics,https://www.kaggle.com/therohk/global-news-week,"Context
This dataset is a snapshot of most of the new news content published online over one week (August 24, 2017 through August 30, 2017).
Prepared by Rohit Kulkarni
It includes approximately 1.4 million articles, with 20,000 news sources and 20+ languages.
This dataset has just four fields (as per the column metadata):
publish_time - earliest known time of the url appearing online in yyyyMMddHHmm format, IST timezone
feed_code - unique identifier for the publisher or domain
source_url - url of the article
headline_text - Headline of the article (UTF8, 20 possible languages)
See the ""Basic Data Exploration"" notebook for a quick look at the dataset contents.
Inspiration
The sources include news feeds, news websites, government agencies, tech journals, company websites, blogs and wikipedia updates. The data has been collected by polling RSS feeds and by crawling other large news aggregators.
This 7 day slice was selected as there wasn't any downtime or internet outage during the interval. New news content is produced at this rate by publishers everyday, throughout the year.
Several other News datasets exploring other attributes, countries and topics can be seen on my profile.
Acknowledgements
This dataset is free to use with the following citation:
Rohit Kulkarni (2017), One Week of Global Feeds [News CSV Dataset], doi:10.7910/DVN/ILAT5B, Retrieved from: [this url]"
Tsunami Causes and Waves,"Cause, magnitude, and intensity of every tsunami since 2000 BC",NOAA,29,"Version 1,2017-02-03",oceans,CSV,3 MB,CC0,"6,660 views",898 downloads,13 kernels,2 topics,https://www.kaggle.com/noaa/seismic-waves,"Context
Tsunami is a Japanese word that translates to ""harbor wave"". It is a wave or a series of waves generated by an impulsive vertical displacement of the surface of the ocean or other body of water. Tsunamis have been responsible for over 500,000 fatalities throughout the world — almost half from the 2004 Indian Ocean earthquake and tsunami!
Content
The NOAA/WDS tsunami database is a listing of historical tsunami source events and runup locations throughout the world from 2000 B.C. to the present. The events were gathered from scientific and scholarly sources, regional and worldwide catalogs, tide gauge data, deep ocean sensor data, individual event reports, and unpublished works. There are currently over 2,000 source events in the database with event validities greater than one and over 13,000 runup locations where tsunami effects were observed.
Acknowledgements
NOAA's National Centers for Environmental Information (NCEI) and the World Data Service for Geophysics compiled and published this tsunami database for tsunami warning centers, engineers, oceanographers, seismologists, and the general public."
"NASA Astronauts, 1959-Present",Which American astronaut has spent the most time in space?,NASA,29,"Version 1,2017-03-08","space
astronauts
employment",CSV,80 KB,CC0,"4,751 views",560 downloads,17 kernels,0 topics,https://www.kaggle.com/nasa/astronaut-yearbook,"Context
The term ""astronaut"" derives from the Greek words meaning ""space sailor"" and refers to all who have been launched as crew members aboard NASA spacecraft bound for orbit and beyond.
Content
The National Aeronautics and Space Administration (NASA) selected the first group of astronauts in 1959. From 500 candidates with the required jet aircraft flight experience and engineering training in addition to a height below 5 feet 11 inches, seven military men became the nation's first astronauts. The second and third groups chosen included civilians with extensive flying experience. By 1964, requirements had changed, and emphasis was placed on academic qualifications; in 1965, six scientist astronauts were selected from a group of 400 applicants who had a doctorate or equivalent experience in the natural sciences, medicine, or engineering. The group named in 1978 was the first of space shuttle flight crews and fourteen groups have been selected since then with a mix of pilots and mission specialists.
There are currently 50 active astronauts and 35 management astronauts in the program; 196 astronauts have retired or resigned and 49 are deceased (as of April 2013).
Acknowledgements
This dataset was published by the National Aeronautics and Space Administration as the ""Astronaut Fact Book"" (April 2013 edition). Active astronauts' mission names and flight statistics were updated from the NASA website.
Inspiration
Which American astronaut has spent the most time in space? What university has produced the most astronauts? What subject did the most astronauts major in at college? Have most astronauts served in the military? Which branch? What rank did they achieve?"
Toy Products on Amazon,"10,000 toy products on Amazon.com",PromptCloud,29,"Version 1,2017-09-16",internet,CSV,34 MB,CC4,"8,759 views","1,253 downloads",3 kernels,0 topics,https://www.kaggle.com/PromptCloudHQ/toy-products-on-amazon,"Context
This is a pre-crawled dataset, taken as subset of a bigger dataset (more than 7 million products) that was created by extracting data from Amazon.com.
Content
This dataset has following fields:
product_name
manufacturer - The item manufacturer, as reported on Amazon. Some common ""manufacturers"", like Disney, actually outsource their assembly line.
price
number_available_in_stock
number_of_reviews
number_of_answered_questions - Amazon includes a Question and Answer service on all or most of its products. This field is a count of how many questions that were asked actually got answered.
average_review_rating
amazon_category_and_sub_category - A tree-based, >>-delimited categorization for the item in question.
customers_who_bought_this_item_also_bought - References to other items that similar users bought. This is a recommendation engine component that played a big role in making Amazon popular initially.
description
product_information
product_description
items_customers_buy_after_viewing_this_item
customer_questions_and_answers - A string entry with all of the product's JSON question and answer pairs.
customer_reviews - A string entry with all of the product's JSON reviews.
sellers - A string entry with all of the product's JSON seller information (many products on Amazon are sold by third parties).
Acknowledgements
This dataset was created by PromptCloud's in-house web-crawling service.
Inspiration
This detailed dataset can be used to answer questions like:
What types of toys are most popular on Amazon?
How dominant are brands in the Amazon toy market?
Can you break down reviews to analyze their sentiment and contents?"
NYC Property Sales,A year's worth of properties sold on the NYC real estate market,City of New York,29,"Version 1,2017-09-23","cities
real estate",CSV,13 MB,CC0,"9,350 views","1,362 downloads",8 kernels,3 topics,https://www.kaggle.com/new-york-city/nyc-property-sales,"Context
This dataset is a record of every building or building unit (apartment, etc.) sold in the New York City property market over a 12-month period.
Content
This dataset contains the location, address, type, sale price, and sale date of building units sold. A reference on the trickier fields:
BOROUGH: A digit code for the borough the property is located in; in order these are Manhattan (1), Bronx (2), Brooklyn (3), Queens (4), and Staten Island (5).
BLOCK; LOT: The combination of borough, block, and lot forms a unique key for property in New York City. Commonly called a BBL.
BUILDING CLASS AT PRESENT and BUILDING CLASS AT TIME OF SALE: The type of building at various points in time. See the glossary linked to below.
For further reference on individual fields see the Glossary of Terms. For the building classification codes see the Building Classifications Glossary.
Note that because this is a financial transaction dataset, there are some points that need to be kept in mind:
Many sales occur with a nonsensically small dollar amount: $0 most commonly. These sales are actually transfers of deeds between parties: for example, parents transferring ownership to their home to a child after moving out for retirement.
This dataset uses the financial definition of a building/building unit, for tax purposes. In case a single entity owns the building in question, a sale covers the value of the entire building. In case a building is owned piecemeal by its residents (a condominium), a sale refers to a single apartment (or group of apartments) owned by some individual.
Acknowledgements
This dataset is a concatenated and slightly cleaned-up version of the New York City Department of Finance's Rolling Sales dataset.
Inspiration
What can you discover about New York City real estate by looking at a year's worth of raw transaction records? Can you spot trends in the market, or build a model that predicts sale value in the future?"
Arabic Handwritten Characters Dataset,Arabic Handwritten Characters Data-set,Mohamed Loey,28,"Version 1,2017-06-23",,CSV,73 MB,ODbL,"4,195 views",488 downloads,3 kernels,,https://www.kaggle.com/mloey1/ahcd1,"Arabic Handwritten Characters Dataset
Astract
Handwritten Arabic character recognition systems face several challenges, including the unlimited variation in human handwriting and large public databases. In this work, we model a deep learning architecture that can be effectively apply to recognizing Arabic handwritten characters. A Convolutional Neural Network (CNN) is a special type of feed-forward multilayer trained in supervised mode. The CNN trained and tested our database that contain 16800 of handwritten Arabic characters. In this paper, the optimization methods implemented to increase the performance of CNN. Common machine learning methods usually apply a combination of feature extractor and trainable classifier. The use of CNN leads to significant improvements across different machine-learning classification algorithms. Our proposed CNN is giving an average 5.1% misclassification error on testing data.
Context
The motivation of this study is to use cross knowledge learned from multiple works to enhancement the performance of Arabic handwritten character recognition. In recent years, Arabic handwritten characters recognition with different handwriting styles as well, making it important to find and work on a new and advanced solution for handwriting recognition. A deep learning systems needs a huge number of data (images) to be able to make a good decisions.
Content
The data-set is composed of 16,800 characters written by 60 participants, the age range is between 19 to 40 years, and 90% of participants are right-hand. Each participant wrote each character (from ’alef’ to ’yeh’) ten times on two forms as shown in Fig. 7(a) & 7(b). The forms were scanned at the resolution of 300 dpi. Each block is segmented automatically using Matlab 2016a to determining the coordinates for each block. The database is partitioned into two sets: a training set (13,440 characters to 480 images per class) and a test set (3,360 characters to 120 images per class). Writers of training set and test set are exclusive. Ordering of including writers to test set are randomized to make sure that writers of test set are not from a single institution (to ensure variability of the test set).
In an experimental section we showed that the results were promising with 94.9% classification accuracy rate on testing images. In future work, we plan to work on improving the performance of handwritten Arabic character recognition.
Acknowledgements
Ahmed El-Sawy, Mohamed Loey, Hazem EL-Bakry, Arabic Handwritten Characters Recognition using Convolutional Neural Network, WSEAS, 2017 Our proposed CNN is giving an average 5.1% misclassification error on testing data.
Inspiration
Creating the proposed database presents more challenges because it deals with many issues such as style of writing, thickness, dots number and position. Some characters have different shapes while written in the same position. For example the teh character has different shapes in isolated position.
Benha University
http://bu.edu.eg/staff/mloey
https://mloey.github.io/"
Delhi Weather Data,Delhi Weather Data from 1997 to 2016 december,MahirKukreja,28,"Version 2,2017-04-25|Version 1,2017-04-14",climate,CSV,6 MB,Other,"7,647 views","1,284 downloads",26 kernels,,https://www.kaggle.com/mahirkukreja/delhi-weather-data,"Context
This dataset contains weather data for New Delhi, India.
Content
This data was taken out from wunderground with the help of their easy to use api. It contains various features such as temperature, pressure, humidity, rain, precipitation,etc.
Acknowledgements
This data is owned by wunderground and although I ended up using noaa's data for my research, i thought that i'd share this data here as I haven't worked on hourly data yet and this might be of huge importance.
Inspiration
The main target is to develop a prediction model accurate enough for predicting the weather. We can try something like predicting the weather in the next 24 hours like microsoft tried some time back.
https://blogs.microsoft.com/next/2015/08/10/hows-the-weather-using-artificial-intelligence-for-better-answers/#sm.018l60051a9neka10is1m5qpi6u5y"
Indian Premier League CSV dataset,577 matches up to season 9,HarshaVardhan,28,"Version 2,2016-12-30|Version 1,2016-12-27",cricket,CSV,6 MB,Other,"15,661 views","3,225 downloads",12 kernels,,https://www.kaggle.com/harsha547/indian-premier-league-csv-dataset,"Upon request from some users, I am uploading CSV Version.
Yes, There is already a dataset from manas
However, I thought this dataset is different than that one.which includes player metadata, information about all the 11 players who participated in the match.
Thoughts :
■ who are the valuable players for respective teams.
■ who are more effective bowlers to bowl in the slog overs , is it spinners ?
■ Dhoni's strike rate against left-arm spinners in last five overs
Have fun with this dataset. Files in the dataset include:
1. Ball_by_Ball : Includes ball by ball details of all the 577 matches.
2. Match : Match metadata
3. Player : Player metadata
4. Player_Match : to know , who is the captain and keeper of the match , Includes every player who take part in match even If player haven't get a chance to either bat or bowl.
5. Season : Season wise details , Orange cap , Purple cap , Man_Of_The_Series
6. Team : Team Name
Diagram"
Air quality in northern Taiwan,"Air quality monitoring data from northern Taiwan, 2015",Nelson Chu,28,"Version 5,2016-07-29|Version 4,2016-07-27|Version 3,2016-07-27|Version 2,2016-07-27|Version 1,2016-07-27",environment,CSV,18 MB,Other,"7,300 views",840 downloads,13 kernels,2 topics,https://www.kaggle.com/nelsonchu/air-quality-in-northern-taiwan,"Context
This data is from Environmental Protection Administration, Executive Yuan, R.O.C. (Taiwan).
There is air quality data and meteorological monitoring data for research and analysis (only include northern Taiwan 2015).
Content
25 observation stations data in the 2015_Air_quality_in_northern_Taiwan.csv
The columns in csv file are:
time - The first column is observation time of 2015
station - The second column is station name, there is 25 observation stations
[Banqiao, Cailiao, Datong, Dayuan, Guanyin, Guting, Keelung, Linkou, Longtan, Pingzhen, Sanchong, Shilin, Songshan, Tamsui, Taoyuan, Tucheng, Wanhua, Wanli, Xindian, Xinzhuang, Xizhi, Yangming, Yonghe, Zhongli, Zhongshan]
items - From the third column to the last one
item - unit - description
SO2 - ppb - Sulfur dioxide
CO - ppm - Carbon monoxide
O3 - ppb - ozone
PM10 - μg/m3 - Particulate matter
PM2.5 - μg/m3 - Particulate matter
NOx - ppb - Nitrogen oxides
NO - ppb - Nitric oxide
NO2 - ppb - Nitrogen dioxide
THC - ppm - Total Hydrocarbons
NMHC - ppm - Non-Methane Hydrocarbon
CH4 - ppm - Methane
UVB - UVI - Ultraviolet index
AMB_TEMP - Celsius - Ambient air temperature
RAINFALL - mm
RH - % - Relative humidity
WIND_SPEED - m/sec - The average of last ten minutes per hour
WIND_DIREC - degress - The average of last ten minutes per hour
WS_HR - m/sec - The average of hour
WD_HR - degress - The average of hour
PH_RAIN - PH - Acid rain
RAIN_COND - μS/cm - Conductivity of acid rain
Data mark
# indicates invalid value by equipment inspection
* indicates invalid value by program inspection
x indicates invalid value by human inspection
NR indicates no rainfall
blank indicates no data
License
Open Government Data License, version 1.0 http://data.gov.tw/license"
Billboard 1964-2015 Songs + Lyrics,50 years of pop music lyrics,RakanNimer,28,"Version 1,2017-04-16","music
linguistics",CSV,8 MB,Other,"7,244 views",856 downloads,10 kernels,2 topics,https://www.kaggle.com/rakannimer/billboard-lyrics,"Original Dataset Author : https://github.com/walkerkq
From https://github.com/walkerkq/musiclyrics :
50 Years of Pop Music Lyrics
Billboard has published a Year-End Hot 100 every December since 1958. The chart measures the performance of singles in the U.S. throughout the year. Using R, I’ve combined the lyrics from 50 years of Billboard Year-End Hot 100 (1965-2015) into one dataset for analysis. You can download that dataset here.
The songs used for analysis were scraped from Wikipedia’s entry for each Billboard Year-End Hot 100 Songs (e.g., 2014). This is the year-end chart, not weekly rankings. Many artists have made the weekly chart but not the final year end chart. The final chart is calculated using an inverse point system based on the weekly Billboard charts (100 points for a week at number one, 1 point for a week at number 100, etc).
I used the xml and RCurl packages to scrape song and artist names from each Wikipedia entry. I then used that list to scrape lyrics from sites that had predictable URL strings (for example, metrolyrics.com uses metrolyrics.com/SONG-NAME-lyrics-ARTIST-NAME.html). If the first site scrape failed, I moved onto the second, and so on. About 78.9% of the lyrics were scraped from metrolyics.com, 15.7% from songlyrics.com, 1.8% from lyricsmode.com. About 3.6% (187/5100) were unavailable.
The dataset features 5100 observations with the features rank (1-100), song, artist, year, lyrics, and source. The artist feature is fairly standardized thanks to Wikipedia, but there is still quite a bit of noise when it comes to artist collaborations (Justin Timberlake featuring Timbaland, for example). If there were any errors in the lyrics that were scraped, such as spelling errors or derivatives like ""nite"" instead of ""night,"" they haven't been corrected.
Full analysis can be found here.
walkerkq
Acknowledgements
Dataset is a mirror of : https://github.com/walkerkq/musiclyrics All credits to gathering it goes to https://github.com/walkerkq
Inspiration
What makes a song's lyrics popular ?"
Silicon Valley Diversity Data,What’s diversity like for 23 top tech companies?,Rachael Tatman,28,"Version 1,2017-11-09","united states
employment
demographics",CSV,217 KB,ODbL,"4,654 views",553 downloads,7 kernels,2 topics,https://www.kaggle.com/rtatman/silicon-valley-diversity-data,"Context
There has been a lot of discussion of the ways in which the workforce for Silicon Valley tech companies differs from that of the United States as a whole. In particular, a lot of evidence suggests that tech workers (who tend to be more highly paid than workers in many other professions) are more likely to be white and male. This dataset will allow you to investigate the demographics for 23 Silicon Valley tech companies for yourself.
Contents
This database contains EEO-1 reports filed by Silicon Valley tech companies. It was compiled by Reveal from The Center for Investigative Reporting.
There are six columns in this dataset:
company: Company name
year: For now, 2016 only
race: Possible values: ""American_Indian_Alaskan_Native"", ""Asian"", ""Black_or_African_American"", ""Latino"", ""Native_Hawaiian_or_Pacific_Islander"", ""Two_or_more_races"", ""White"", ""Overall_totals""
gender: Possible values: ""male"", ""female"". Non-binary gender is not counted in EEO-1 reports.
job_category: Possible values: ""Administrative support"", ""Craft workers"", ""Executive/Senior officials & Mgrs"", ""First/Mid officials & Mgrs"", ""laborers and helpers"", ""operatives"", ""Professionals"", ""Sales workers"", ""Service workers"", ""Technicians"", ""Previous_totals"", ""Totals""
count: Mostly integer values, but contains ""na"" for a no-data variable.
Acknowledgements:
The EEO-1 database is licensed under the Open Database License (ODbL) by Reveal from The Center for Investigative Reporting.
You are free to copy, distribute, transmit and adapt the spreadsheet, so long as you:
credit Reveal (including this link if it’s distributed online);
inform Reveal that you are using the data in your work by emailing Sinduja Rangarajan at srangarajan@revealnews.org; and
offer any new work under the same license.
Inspiration:
How does each company’s workforce compare to the United States population as a whole? You can find county level diversity information here.
Which company is the most diverse? Least diverse?"
Global Food Prices,743k Rows of Monthly Market Food Prices Across Developing Countries,Jacob Boysen,28,"Version 1,2017-08-04","food and drink
economics",CSV,83 MB,Other,"7,664 views","1,261 downloads",3 kernels,0 topics,https://www.kaggle.com/jboysen/global-food-prices,"Context:
Global food price fluctuations can cause famine and large population shifts. Price changes are increasingly critical to policymakers as global warming threatens to destabilize the food supply.
Content:
Over 740k rows of prices obtained in developing world markets for various goods. Data includes information on country, market, price of good in local currency, quantity of good, and month recorded.
Acknowledgements:
Compiled by the World Food Program and distributed by HDX.
Inspiration:
This data would be particularly interesting to pair with currency fluctuations, weather patterns, and/or refugee movements--do any price changes in certain staples predict population upheaval? Do certain weather conditions influence market prices?
License:
Released under CC BY-IGO."
Fatal Police Shootings in the US,Fatal police shootings in the US since 2015 with additional US census data,Karolina Wullum,28,"Version 1,2017-09-23","united states
death
crime
violence",CSV,3 MB,CC4,"4,738 views","1,560 downloads",7 kernels,2 topics,https://www.kaggle.com/kwullum/fatal-police-shootings-in-the-us,"The 2014 killing of Michael Brown in Ferguson, Missouri, began the protest movement culminating in Black Lives Matter and an increased focus on police accountability nationwide.
Since Jan. 1, 2015, The Washington Post has been compiling a database of every fatal shooting in the US by a police officer in the line of duty. It's difficult to find reliable data from before this period, as police killings haven't been comprehensively documented, and the statistics on police brutality are much less available. As a result, a vast number of cases go unreported.
The Washington Post is tracking more than a dozen details about each killing - including the race, age and gender of the deceased, whether the person was armed, and whether the victim was experiencing a mental-health crisis. They have gathered this information from law enforcement websites, local new reports, social media, and by monitoring independent databases such as ""Killed by police"" and ""Fatal Encounters"". The Post has also conducted additional reporting in many cases.
There are four additional datasets. These are US census data on poverty rate, high school graduation rate, median household income, and racial demographics.
Source of census data: https://factfinder.census.gov/faces/nav/jsf/pages/community_facts.xhtml"
Biomechanical features of orthopedic patients,Classifying patients based on six features,UCI Machine Learning,28,"Version 1,2017-09-07",,CSV,50 KB,CC0,"5,074 views","1,588 downloads",9 kernels,0 topics,https://www.kaggle.com/uciml/biomechanical-features-of-orthopedic-patients,"Context
The data have been organized in two different but related classification tasks.
column_3C_weka.csv (file with three class labels)
The first task consists in classifying patients as belonging to one out of three categories: Normal (100 patients), Disk Hernia (60 patients) or Spondylolisthesis (150 patients).
column_2C_weka.csv (file with two class labels)
For the second task, the categories Disk Hernia and Spondylolisthesis were merged into a single category labelled as 'abnormal'. Thus, the second task consists in classifying patients as belonging to one out of two categories: Normal (100 patients) or Abnormal (210 patients).
Content
Field Descriptions:
Each patient is represented in the data set by six biomechanical attributes derived from the shape and orientation of the pelvis and lumbar spine (each one is a column):
pelvic incidence
pelvic tilt
lumbar lordosis angle
sacral slope
pelvic radius
grade of spondylolisthesis
Acknowledgements
The original dataset was downloaded from UCI ML repository:
Lichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science
Files were converted to CSV
Inspiration
Use these biomechanical features to classify patients according to their labels"
Global Historical Climatology Network,Land and ocean temperature anomalies,NOAA,28,"Version 1,2016-10-24","climate
history",CSV,20 MB,Other,"9,696 views",871 downloads,28 kernels,,https://www.kaggle.com/noaa/global-historical-climatology-network,"Global Historical Climatology Network-Monthly (GHCN-M)
Context
The Global Historical Climatology Network (GHCN) is an integrated database of climate summaries from land surface stations across the globe. This data set contains gridded mean temperature anomalies, or departures from a reference value or long-term average, from the Global Historical Climatology Network-Monthly (GHCN-M) version 3.2.1 temperature data set. The gridded anomalies were produced from GHCN-M bias corrected data. Each month of data consists of 2,592 gridded data points produced on a 5° by 5° basis for the entire globe (72 longitude by 36 latitude grid boxes).
Frequency: Monthly
Period: 1880 to 2016
Content
Gridded data for every month from January 1880 to the most recent month is available. The data are temperature anomalies in degrees Celsius. Each gridded value was multiplied by 100 and written to file as an integer. Missing values are represented by the value -9999. The data are formatted by year, month, latitude and longitude. There are 72 longitude grid values per line -- each grid is labeled as a concatenation of ""lon"", ""w"" or ""e"", then the degree. The latitude is captured in the ""lat"" field where the value indicates the lower bound of a grid cell (e.g. 85 indicates 85-90N whereas -90 indicates 85-90S). Longitude values are written from 180°W to 180°E, and latitude values from 90°N to 90°S.
This dataset permits the quantification of changes in the mean monthly temperature and precipitation for the earth's surface. Changes in the observing system itself have been carefully removed to allow for the true climate variability at the earth's surface to be represented in the data.
Many surface weather stations undergo minor relocations through their history of observation. Stations may also be subject to changes in instrumentation as measurement technology evolves. Further, the land use/land cover in the vicinity of an observing site may also change with time. Such modifications to an observing site have the potential to alter a thermometer's microclimate exposure characteristics and/or change the bias of measurements, the impact of which can be a systematic shift in the mean level of temperature readings that is unrelated to true climate variations. The process of removing such ""non-climatic"" artifacts in a climate time series is called homogenization. In version 3 of the GHCN-Monthly temperature data, the apparent impacts of documented and undocumented inhomogeneities are detected and removed through automated pairwise comparisons of mean monthly temperature series as detailed in Menne and Williams [2009].
Inspiration
This granular dataset permits extensive historical analysis of the earth’s climate to answer questions about climate change, including how different regions of the planet have been affected by changes in temperature over time. Get started by forking the kernel Mapping Historical Temperature Anomalies with R.
Acknowledgements
This data is a product of NOAA's National Centers for Environmental Information (NCEI). It was compiled through the aggregation and analysis of many thousands of weather station records. The compete description of the processes and methods used may be found at https://www.ncdc.noaa.gov/ghcnm/v3.php.
The Global Historical Climatology Network-Monthly (GHCN-M) temperature dataset was first developed in the early 1990s (Vose et al. 1992). A second version was released in 1997 following extensive efforts to increase the number of stations and length of the data record (Peterson and Vose, 1997). Methods for removing inhomogeneities from the data record associated with non-climatic influences such as changes in instrumentation, station environment, and observing practices that occur over time were also included in the version 2 release (Peterson and Easterling, 1994; Easterling and Peterson 1995). Since that time efforts have focused on continued improvements in dataset development methods including new quality control processes and advanced techniques for removing data inhomogeneities (Menne and Williams, 2009).
License
Public Domain License"
Restaurants on TripAdvisor,"Data on 18,000 restaurants",PromptCloud,28,"Version 1,2017-09-15",internet,CSV,7 MB,CC4,"8,571 views","1,302 downloads",2 kernels,0 topics,https://www.kaggle.com/PromptCloudHQ/restaurants-on-tripadvisor,"Context
This is a pre-crawled dataset, taken as subset of a bigger dataset (more than 14,92,992 restaurants) that was created by extracting data from TripAdvisor.com.
Content
This dataset has following fields:
Restaurant URL
Name
Address
Phone
City
State
Country
Neighbourhood
Email ID
Menu
Website
Latitude
Longitude
About Restaurant
Cuisine
Good for(suitable)
Price
Currency
Rating
Ranking
Deal(Promotion)
Total Review
Last Reviewed
Recommended
Dining Option
Award
Acknowledgements
This dataset was created by PromptCloud's in-house web-crawling service.
Inspiration
The country-wise analyses of cuisine, rating, ranking, etc. can be performed."
NYC Parking Tickets,"42.3M Rows of Parking Ticket Data, Aug 2013-June 2017",City of New York,28,"Version 2,2017-10-27|Version 1,2017-10-26","cities
government
law
automobiles",CSV,8 GB,CC0,"6,894 views","1,210 downloads",3 kernels,0 topics,https://www.kaggle.com/new-york-city/nyc-parking-tickets,"Context
The NYC Department of Finance collects data on every parking ticket issued in NYC (~10M per year!). This data is made publicly available to aid in ticket resolution and to guide policymakers.
Content
There are four files, covering Aug 2013-June 2017. The files are roughly organized by fiscal year (July 1 - June 30) with the exception of the initial dataset. The initial dataset also lacks 8 columns that are included in the other three datasets (although be warned that these additional data columns are used sparingly). See the dataset descriptions for exact details. Columns include information about the vehicle ticketed, the ticket issued, location, and time.
Acknowledgements
Data was produced by NYC Department of Finance. FY2018 data is found here with updates every third week of the month.
Inspiration
When are tickets most likely to be issued? Any seasonality?
Where are tickets most commonly issued?
What are the most common years and types of cars to be ticketed?"
Electoral Donations in Brazil,Statistical analysis of Brazil in 2014,FelipeLeiteAntunes,27,"Version 5,2017-11-19|Version 4,2017-10-12|Version 3,2017-10-10|Version 2,2017-10-09|Version 1,2017-10-09","crime
finance
politics",CSV,69 MB,ODbL,"2,711 views",196 downloads,4 kernels,,https://www.kaggle.com/felipeleiteantunes/electoral-donations-brazil2014,"Context
Brazil has elections every two years, but alternating between two different types of elections, each type occurring every four years. There are the municipal elections, where mayors and city council members are elected (the last one occurred in 2016) and general elections where president, governors, senators and congressmen (regional and national) are elected (the last one occurred in 2014). Brazil has 26 federal units plus the federal district. Each one of these units (regions) elects its senators, congressmen and governors.
For each federal unit, Brazil's TSE provides information on the donations declared by the three entities: candidates, parties and committees. The data comprises information describing every donation received. The donations can be divided in two categories with respect to the donor: they can come from legal persons (private citizens, identified by the CPF (CPF is an identification number used by the Brazilian tax revenue office. It is roughly the Brazilian analogue to a social security number. With the same purpose, companies are identified with a similar number called CNPJ number) or from legal entities (i.e. companies, identified by the CNPJ number). Also, some entities can make donations among them (the party can give part of the money from a given donation to a candidate). In this type of transaction, the information on the original donor is also specified in the declarations. From now on, these type of donations will be referred to as non-original donations. Apart from information concerning each Brazilian federal unit separately, one can also obtain the information declared by the parties and committees at national level and for the presidential campaign (which has national and not regional scope).
Related paper:
https://arxiv.org/pdf/1707.08826.pdf"
Moscow Ring Roads,Shapefiles for use in Sberbank,Chippy,27,"Version 3,2017-05-27|Version 2,2017-05-22|Version 1,2017-05-22","russia
geography",CSV,3 MB,ODbL,"3,255 views","1,136 downloads",11 kernels,0 topics,https://www.kaggle.com/nigelcarpenter/sberbankmoscowroads,"Context
This data set was created for use in the Sberbank Kaggle competition.
Content
The data consists of three GIS shapefiles one for each of the 3 major Moscow ring roads; the MKAD, TTK (or third ring) and Sadovoe (or garden ring).
Acknowledgements
The road shapefiles have been extracted from OpenStreetMap data, processed in QGiS to extract only the roads of interest.
OpenStreetMap License: https://www.OpenStreetMap.org/copyright
Inspiration
With these files and the distances given in the Sberbank dataset it should be possible to better understand the location of properties. With a better understanding of location it may be possible to improve the quality of the overall dataset which contains material amounts of missing or poorly coded data."
Circadian Rhythm in the Brain,Fluorescence signal from the circadian regulation region of the brain,Kevin Mader,27,"Version 3,2017-03-21|Version 2,2017-03-21|Version 1,2017-03-21",neuroscience,Other,1 GB,Other,"4,722 views",293 downloads,8 kernels,,https://www.kaggle.com/kmader/circadian-rhythm-in-the-brain,"Overview
The data are a time-series of fluorescence images measured of at OHSU Lab of Charles Allen (https://www.ohsu.edu/xd/research/centers-institutes/oregon-institute-occupational-health-sciences/research/allen/).
Introduction
We use a fluorescent protein as a reporter for the circadian clock gene Period1. We are able to follow the expression of this gene in many neurons for several days to understand how the neural network in the suprachiasmatic nucleus synchronizes the circadian clock of individual neurons to produce a precise circadian rhythm. We analyze each image to determine the fluorescence intensity of each neuron over multiple circadian cycles.
FAQ
How where the images obtained: which animal and what staining?
The images were taken from a transgenic mouse in which expression of the fluorescent protein Venus is driven by the promoter for the circadian clock gene Period 1.
What is the anatomy of the images, and how are they oriented?
The bright line is the third ventricle, which resides on the midline of the brain. The two bright regions on either side of the ventricle are the two portions of the Suprachiasmatic nucleus (SCN). Below the ventricle and the SCN is a dark horizontal band that represents the optic chasm.
What is the bright vertical line in the top middle?
The bright line is the third ventricle. Pericytes that line the ventricle express the Venus at very high levels. We don't know the function of the circadian clock in these cells.
Challenge
Currently we have to analyze each experiment by hand to follow an individual through a couple hundred images. This takes several days. This problem is going to get worse because we have just purchased a new automated microscope stage that will allow us to simultaneously image from four suprachiasmatic nuclei.
Preview
Ideas for Analysis
Wavelets (pywavelets) following https://www.ncbi.nlm.nih.gov/pubmed/18931366
Questions
Do the cells move during the experiment?
How regular is their signal?
Is the period 24 hours?
Do nearby cells oscillate together?
Do they form chunks or groups, over what range do they work?
Are there networks formed from time-precedence?"
Cheltenham's Facebook Groups,Discover How a Community Leverages Facebook,Mike Chirico,27,"Version 44,2017-12-23|Version 43,2017-11-20|Version 42,2017-11-14|Version 41,2017-11-06|Version 40,2017-08-19|Version 39,2017-07-15|Version 38,2017-07-02|Version 37,2017-06-25|Version 36,2017-06-10|Version 35,2017-05-28|Version 34,2017-05-14|Version 33,2017-03-23|Version 32,2017-03-11|Version 31,2017-03-11|Version 30,2017-03-03|Version 29,2017-02-23|Version 28,2016-12-02|Version 27,2016-10-23|Version 26,2016-10-18|Version 25,2016-10-11|Version 24,2016-10-03|Version 23,2016-09-22|Version 22,2016-09-16|Version 21,2016-09-05|Version 20,2016-08-19|Version 19,2016-08-08|Version 18,2016-08-03|Version 17,2016-07-30|Version 16,2016-07-22|Version 15,2016-07-20|Version 14,2016-07-15|Version 13,2016-06-25|Version 12,2016-06-25|Version 11,2016-06-19|Version 10,2016-06-15|Version 9,2016-06-10|Version 8,2016-06-09|Version 7,2016-06-07|Version 6,2016-06-07|Version 5,2016-06-07|Version 4,2016-06-05|Version 3,2016-06-04|Version 2,2016-06-03|Version 1,2016-06-03",internet,CSV,59 MB,GPL,"12,868 views",862 downloads,50 kernels,,https://www.kaggle.com/mchirico/cheltenham-s-facebook-group,"Facebook is becoming an essential tool for more than just family and friends. Discover how Cheltenham Township (USA), a diverse community just outside of Philadelphia, deals with major issues such as the Bill Cosby trial, everyday traffic issues, sewer I/I problems and lost cats and dogs. And yes, theft.
Communities work when they're connected and exchanging information. What and who are the essential forces making a positive impact, and when and how do conversational threads get directed or misdirected?
Use Any Facebook Public Group
You can leverage the examples here for any public Facebook group. For an example of the source code used to collect this data, and a quick start docker image, take a look at the following project: facebook-group-scrape.
Data Sources
There are 4 csv files in the dataset, with data from the following 5 public Facebook groups:
Unofficial Cheltenham Township
Elkins Park Happenings!
Free Speech Zone
Cheltenham Lateral Solutions
Cheltenham Township Residents
post.csv
These are the main posts you will see on the page. It might help to take a quick look at the page. Commas in the msg field have been replaced with {COMMA}, and apostrophes have been replaced with {APOST}.
gid Group id (5 different Facebook groups)
pid Main Post id
id Id of the user posting
name User's name
timeStamp
shares
url
msg Text of the message posted.
likes Number of likes
comment.csv
These are comments to the main post. Note, Facebook postings have comments, and comments on comments.
gid Group id
pid Matches Main Post identifier in post.csv
cid Comment Id.
timeStamp
id Id of user commenting
name Name of user commenting
rid Id of user responding to first comment
msg Message
like.csv
These are likes and responses. The two keys in this file (pid,cid) will join to post and comment respectively.
gid Group id
pid Matches Main Post identifier in post.csv
cid Matches Comments id.
response Response such as LIKE, ANGRY etc.
id The id of user responding
name Name of the user responding
member.csv
These are all the members in the group. Some members never, or rarely, post or comment. You may find multiple entries in this table for the same person. The name of the individual never changes, but they change their profile picture. Each profile picture change is captured in this table. Facebook gives users a new id in this table when they change their profile picture.
gid Group id
id Id of the member
name Name of the member
url URL of the member"
Forecasts for Product Demand,Make Accurate Forecasts for Thousands of Different Products,FelixZhao,27,"Version 1,2017-08-25","business
supply chain
product",CSV,49 MB,GPL,"6,631 views",857 downloads,2 kernels,0 topics,https://www.kaggle.com/felixzhao/productdemandforecasting,"Context
The dataset contains historical product demand for a manufacturing company with footprints globally. The company provides thousands of products within dozens of product categories. There are four central warehouses to ship products within the region it is responsible for. Since the products are manufactured in different locations all over the world, it normally takes more than one month to ship products via ocean to different central warehouses. If forecasts for each product in different central with reasonable accuracy for the monthly demand for month after next can be achieved, it would be beneficial to the company in multiple ways. This dataset is all real-life data and products/warehouse and category information encoded.
Content
Product_Code: The product name encoded. Warehouse: Warehouse name encoded. Product_Category: Product Category for each Product_Code encoded. Date: The date customer needs the product. Order_Demand: single order qty.
Inspiration
Is it possible to make forecasts for thousands of products (some of them are highly variable in terms of monthly demand) for the the month after next?"
"Bad teeth, sugar and government health spending",Thanks to Gapminder Data,churandy,27,"Version 3,2016-08-19|Version 2,2016-08-19|Version 1,2016-08-19","public health
dentistry
health",CSV,304 KB,CC0,"12,681 views","2,035 downloads",16 kernels,,https://www.kaggle.com/angelmm/healthteethsugar,"If you get richer your teeth could get worse (if you eat more sugar foods) or better (because of better health assistance or, even, more education and health-conciousness). These variables can be analysed with these data, downloaded from Gapminder Data:
Bad teeth per child (12 yr, WHO)
GDP/capita (US$, inflation-adjusted, World Bank)
Government health spending per person (US$, WHO)
Sugar comsumption per person (g per day, FAO)
Literacy rate, adult total (% of people ages 15 and above, UNESCO)"
65 World Indexes,Why are some countries so different?,JoniHoppen,27,"Version 2,2017-02-27|Version 1,2017-02-18",international relations,CSV,120 KB,ODbL,"5,836 views",934 downloads,16 kernels,,https://www.kaggle.com/joniarroba/65-world-indexes-gathered,"Context
Why some countries are so different from the others?
Feel free to upvote :) Autor: Joni Hoppen - linkedin - https://www.linkedin.com/in/joniarroba/
Content
I have gathered manually most of the information at World Bank, Unicef and so on. Some data were not there so I used K-nn to guess some values and have a full dataset that can be used of our data science community.
Information of each of the 65 variables were made available here http://bit.ly/2l2Hjh3
Acknowledgements
Thanks www.aquare.la Advanced Analytics that came up with the idea of creating this dataset to test their VORTX tool. Also Thanks to professionals involved in creating Indexes and collecting them, this is such a great valuable work to help better see the world.
Inspiration
What would be the best, way to equalize the world?"
Historical Sales and Active Inventory,Records of sold and unsold products and their characteristics,JamesS,27,"Version 3,2016-12-08|Version 2,2016-12-08|Version 1,2016-12-08","business
product",CSV,13 MB,Other,"18,882 views","2,483 downloads",34 kernels,4 topics,https://www.kaggle.com/flenderson/sales-analysis,"Context
Attached is a set of products in which we are trying to determine which products we should continue to sell, and which products to remove from our inventory. The file contains BOTH historical sales data AND active inventory, which can be discerned with the column titled ""File Type"".
We suspect that data science applied to the set--such as a decision tree analysis or logistic regression, or some other machine learning model---can help us generate a value (i.e., probability score) for each product, that can be used as the main determinant evaluating the inventory. Each row in the file represents one product.
It is important to note that we have MANY products in our inventory, and very few of them tend to sell (only about 10% sell each year) and many of the products only have a single sale in the course of a year.
Content
The file contains historical sales data (identified with the column titled File_Type) along with current active inventory that is in need of evaluation (i.e., File Type = ""Active""). The historical data shows sales for the past 6 months. The binary target (1 = sale, 0 = no sale in past six months) is likely the primary target that should drive the analysis.
The other columns contain numeric and categorical attributes that we deem relevant to sales.
Note that some of the historical sales SKUs are ALSO included in the active inventory.
A few comments about the attributes included, as we realize we may have some attributes that are unnecessary or may need to be explained.
SKU_number: This is the unique identifier for each product.
Order: Just a sequential counter. Can be ignored.
SoldFlag: 1 = sold in past 6 mos. 0 = Not sold
MarketingType = Two categories of how we market the product. This should probably be ignored, or better yet, each type should be considered independently.
New_Release_Flag = Any product that has had a future release (i.e., Release Number > 1)
Inspiration
(1) What is the best model to use that will provide us with a probability estimate of a sale for each SKU? We are mainly interested in a relative unit that we can continuously update based on these attributes (and others that we add, as we are able).
(2) Is it possible to provide a scored file (i.e., a probability score for each SKU in the file), and to provide an evaluation of the accuracy of the selected model?
(3) What are the next steps we should take?
Thanks very much for any suggestions you may provide."
Devanagari Character Set,Over 92 thousand images of characters from devanagari script,Rishi Anand,27,"Version 4,2017-11-21|Version 3,2017-08-25|Version 2,2017-06-02|Version 1,2017-04-25","languages
linguistics
image data
multiclass classification",Other,121 MB,CC0,"5,590 views",433 downloads,14 kernels,2 topics,https://www.kaggle.com/rishianand/devanagari-character-set,"Context
This is a dataset of Devanagari Script Characters. It comprises of 92000 images [32x32 px] corresponding to 46 characters, consonants ""ka"" to ""gya"", and the digits 0 to 9. The vowels are missing.
Content
The CSV file is of the dimension 92000 * 1025. There are 1024 input features of pixel values in grayscale (0 to 255). The column ""character"" represents the Devanagari Character Name corresponding to each image.
Acknowledgements
This dataset was originally created by Computer Vision Research Group, Nepal. [website archive] (https://web.archive.org/web/20160105230017/http://cvresearchnepal.com/wordpress/dhcd/)
Example Script
https://nbviewer.jupyter.org/github/rishianand9/devanagari-character-recognition/blob/master/DCRS.ipynb"
American Time Use Survey,Multi-Year Survey Microdata Files from 2003-2015,US Bureau of Labor Statistics,27,"Version 2,2017-06-15|Version 1,2016-11-10","time series
demographics",CSV,2 GB,Other,"22,868 views","1,159 downloads",10 kernels,0 topics,https://www.kaggle.com/bls/american-time-use-survey,"Context
The American Time Use Survey (ATUS) is the Nation’s first federally administered, continuous survey on time use in the United States. The goal of the survey is to measure how people divide their time among life’s activities.
In ATUS, individuals are randomly selected from a subset of households that have completed their eighth and final month of interviews for the Current Population Survey (CPS). ATUS respondents are interviewed only one time about how they spent their time on the previous day, where they were, and whom they were with. The survey is sponsored by the Bureau of Labor Statistics and is conducted by the U.S. Census Bureau.
The major purpose of ATUS is to develop nationally representative estimates of how people spend their time. Many ATUS users are interested in the amount of time Americans spend doing unpaid, nonmarket work, which could include unpaid childcare, eldercare, housework, and volunteering. The survey also provides information on the amount of time people spend in many other activities, such as religious activities, socializing, exercising, and relaxing. In addition to collecting data about what people did on the day before the interview, ATUS collects information about where and with whom each activity occurred, and whether the activities were done for one’s job or business. Demographic information—including sex, race, age, educational attainment, occupation, income, marital status, and the presence of children in the household—also is available for each respondent. Although some of these variables are updated during the ATUS interview, most of this information comes from earlier CPS interviews, as the ATUS sample is drawn from a subset of households that have completed month 8 of the CPS.
The user guide can be found here.
Content
There are 8 datasets containing microdata from 2003-2015:
Respondent file: The Respondent file contains information about ATUS respondents, including their labor force status and earnings.
Roster file: The Roster file contains information about household members and nonhousehold children (under 18) of ATUS respondents. It includes information such as age and sex.
Activity file: The Activity file contains information about how ATUS respondents spent their diary day. It includes information such as activity codes, activity start and stop times, and locations. Because Activity codes have changed somewhat between 2003 and 2015, this file uses activity codes that appear in the 2003-2015 ATUS Coding Lexicon (PDF).
Activity summary file: The Activity summary file contains information about the total time each ATUS respondent spent doing each activity on the diary day. Because Activity codes have changed somewhat between 2003 and 2015, this file uses activity codes that appear in the 2003-2015 ATUS Coding Lexicon (PDF).
Who file: The Who file includes codes that indicate who was present during each activity.
CPS 2003-2015 file: The ATUS-CPS file contains information about each household member of all individuals selected to participate in ATUS. The information on the ATUS-CPS file was collected 2 to 5 months before the ATUS interview.
Eldercare Roster file: The ATUS Eldercare Roster file contains information about people for whom the respondent provided care. Eldercare data have been collected since 2011.
Replicate weights file: The Replicate weights file contains miscellaneous ATUS weights.
The ATUS interview data dictionary can be found here.
The ATUS Current Population Survey (CPS) data dictionary can be found here.
The ATUS occupation and industry codes can be found here.
The ATUS activity lexicon can be found here.
Acknowledgements
The original datasets can be found here.
Inspiration
How do daily activities differ by:
labor force status
income
household composition
geographical region
disability status"
Journalists Killed Worldwide Since 1992,Journalist Deaths from 1992-2016,Committee to Protect Journalists,27,"Version 1,2016-11-13","news agencies
journalism
death
crime",CSV,313 KB,Other,"6,165 views",910 downloads,17 kernels,,https://www.kaggle.com/cpjournalists/journalists-killed-worldwide-since-1992,"Context
CPJ began compiling detailed records on journalist deaths in 1992. CPJ applies strict journalistic standards when investigating a death. One important aspect of their research is determining whether a death was work-related. As a result, they classify deaths as ""motive confirmed"" or ""motive unconfirmed.""
Content
The dataset contains 18 variables:
Type: CPJ classified deaths as motive confirmed or motive confirmed, as well as Media Workers
Date
Name
Sex
Country_killed
Organization
Nationality
Medium
Job
Coverage
Freelance
Local_Foreign
Source_fire
Type_death
Impunity_for_murder
Taken_captive
Threatened
Tortured
Acknowledgements
The original dataset can be found here.
Inspiration
Some ideas for exploring the dataset:
What is the trend in journalist deaths over time and how does this differ by type of death, job, coverage, and country?
Are there differences by sex and/or nationality?"
Chronic Disease Indicators,"Disease Data Across the US, 2001-2016",Centers for Disease Control and Prevention,27,"Version 1,2017-08-18",healthcare,CSV,117 MB,CC0,"7,215 views",859 downloads,2 kernels,0 topics,https://www.kaggle.com/cdc/chronic-disease,"Context:
CDC's Division of Population Health provides cross-cutting set of 124 indicators that were developed by consensus and that allows states and territories and large metropolitan areas to uniformly define, collect, and report chronic disease data that are important to public health practice and available for states, territories and large metropolitan areas. In addition to providing access to state-specific indicator data, the CDI web site serves as a gateway to additional information and data resources.
Content:
A variety of health-related questions were assessed at various times and places across the US over the past 15 years. Data is provided with confidence intervals and demographic stratification.
Acknowledgements:
Data was compiled by the CDC.
Inspiration:
Any interesting trends in certain groups?
Any correlation between disease indicators and locality hospital spending?"
Correlates of War: World Religions,"World, regional, and national populations by religious beliefs",University of Michigan,27,"Version 1,2017-01-28","faith and traditions
politics
war",CSV,627 KB,Other,"7,744 views","1,208 downloads",66 kernels,0 topics,https://www.kaggle.com/umichigan/world-religions,"Content
The World Religion Project aims to provide detailed information about religious adherence worldwide since 1945. It contains data about the number of adherents by religion in each of the states in the international system for every half-decade period. Some of the religions are divided into religious families, and the breakdown of adherents within a given religion into religious families is provided to the extent data are available.
The project was developed in three stages. The first stage consisted of the formation of a religions tree. A religion tree is a systematic classification of major religions and of religious families within those major religions. To develop the religion tree we prepared a comprehensive literature review, the aim of which was to define a religion, to find tangible indicators of a given religion of religious families within a major religion, and to identify existing efforts at classifying world religions. The second stage consisted of the identification of major data sources of religious adherence and the collection of data from these sources according to the religion tree classification. This created a dataset that included multiple records for some states for a given point in time, yet contained multiple missing data for specific states, specific time periods, and specific religions. The third stage consisted of cleaning the data, reconciling discrepancies of information from different sources, and imputing data for the missing cases.
Acknowledgements
The dataset was created by Zeev Maoz, University of California-Davis, and Errol Henderson, Pennsylvania State University, and published by the Correlates of War Project."
"Chase Bank Branch Deposits, 2010-2016",Where did Chase Bank customers deposit the most money last year?,Chase Bank,27,"Version 1,2017-03-15",finance,CSV,953 KB,CC0,"8,385 views",980 downloads,8 kernels,,https://www.kaggle.com/chasebank/bank-deposits,"Content
This dataset includes a record for every branch of Chase Bank in the United States, including the branch's name and number, date established as a bank office and (if applicable) acquired by JP Morgan Chase, physical location as street address, city, state, zip, and latitude and longitude coordinates, and the amount deposited at the branch (or the institution, for the bank's main office) between July 1 and June 30, 2016, in US dollars.
Acknowledgements
The location data was scraped from the Chase Bank website. The deposit data was compiled from the Federal Deposit Insurance Corporation's annual Summary of Deposits reports.
Inspiration
Where did Chase Bank customers deposit the most money last year? Which bank branch has seen the most growth in deposits? How did the bank network of branch locations grow over the past century? What city has the most bank branches per capita?"
Common Voice,"500 hours of speech recordings, with speaker demographics",Mozilla,27,"Version 2,2017-12-12|Version 1,2017-12-01","languages
acoustics
linguistics",Other,12 GB,CC0,"2,281 views",231 downloads,,2 topics,https://www.kaggle.com/mozillaorg/common-voice,"General Information
Common Voice is a corpus of speech data read by users on the Common Voice website (http://voice.mozilla.org/), and based upon text from a number of public domain sources like user submitted blog posts, old books, movies, and other public speech corpora. Its primary purpose is to enable the training and testing of automatic speech recognition (ASR) systems.
Structure
The corpus is split into several parts for your convenience. The subsets with “valid” in their name are audio clips that have had at least 2 people listen to them, and the majority of those listeners say the audio matches the text. The subsets with “invalid” in their name are clips that have had at least 2 listeners, and the majority say the audio does not match the clip. All other clips, ie. those with fewer than 2 votes, or those that have equal valid and invalid votes, have “other” in their name.
The “valid” and “other” subsets are further divided into 3 groups:
dev - for development and experimentation
train - for use in speech recognition training
test - for testing word error rate
Organization and Conventions
Each subset of data has a corresponding csv file with the following naming convention:
“cv-{type}-{group}.csv”
Here “type” can be one of {valid, invalid, other}, and “group” can be one of {dev, train, test}. Note, the invalid set is not divided into groups.
Each row of a csv file represents a single audio clip, and contains the following information:
filename - relative path of the audio file
text - supposed transcription of the audio
up_votes - number of people who said audio matches the text
down_votes - number of people who said audio does not match text
age - age of the speaker, if the speaker reported it
teens: '< 19'
twenties: '19 - 29'
thirties: '30 - 39'
fourties: '40 - 49'
fifties: '50 - 59'
sixties: '60 - 69'
seventies: '70 - 79'
eighties: '80 - 89'
nineties: '> 89'
gender - gender of the speaker, if the speaker reported it
male
female
other
accent - accent of the speaker, if the speaker reported it
us: 'United States English'
australia: 'Australian English'
england: 'England English'
canada: 'Canadian English'
philippines: 'Filipino'
hongkong: 'Hong Kong English'
indian: 'India and South Asia (India, Pakistan, Sri Lanka)'
ireland: 'Irish English'
malaysia: 'Malaysian English'
newzealand: 'New Zealand English'
scotland: 'Scottish English'
singapore: 'Singaporean English'
southatlandtic: 'South Atlantic (Falkland Islands, Saint Helena)'
african: 'Southern African (South Africa, Zimbabwe, Namibia)'
wales: 'Welsh English'
bermuda: 'West Indies and Bermuda (Bahamas, Bermuda, Jamaica, Trinidad)'
The audio clips for each subset are stored as mp3 files in folders with the same naming conventions as it’s corresponding csv file. So, for instance, all audio data from the valid train set will be kept in the folder “cv-valid-train” alongside the “cv-valid-train.csv” metadata file.
Acknowledgments
This dataset was compiled by Michael Henretty, Tilman Kamp, Kelly Davis & The Common Voice Team, who included the following acknowledgments:
We sincerely thank all of the people who donated their voice on the Common Voice website and app. You are the backbone of this project, and we thank you for making this possible!
We also thank our community on Discourse (https://discourse.mozilla-community.org/c/voice) and Github (https://github.com/mozilla/voice-web), you have made this project better every step of the way.
And special thanks to Mycroft, SNIPS.ai, Mythic, Tatoeba.org, Bangor University, and SAP for joining us on this journey. We look forward to working more with each of you."
Google Job Skills,Find what you need to get a job at Google,Niyamat Ullah,26,"Version 1,2018-01-07","databases
learning
internet",CSV,407 KB,CC4,"5,112 views",605 downloads,5 kernels,0 topics,https://www.kaggle.com/niyamatalmass/google-job-skills,"Context
There is a question in our mind that which language, skills, and experience should we add to our toolbox for getting a job in Google. Well, I think why not we find out the answer by analyzing the Google Jobs Site. Google published all of their jobs at https://careers.google.com/. So I scraped all of the job data from that site by going every job page using Selenium. I only take Job Title, Job Location, Job responsibilities, minimum and preferred qualifications for this dataset.
Content
This dataset is collected using Selenium by scraping all of the jobs text for Google Career site. About the column
Title: The title of the job
Category: Category of the job
Location: Location of the job
Responsibilities: Responsibilities for the job
Minimum Qualifications: Minimum Qualifications for the job
Preferred Qualifications: Preferred Qualifications for the job
Acknowledgements
This dataset is collected using Selenium. This product uses the Google Career site but is not endorsed or certified by Google Career site.
Inspiration
You can find most popular skills for Google Jobs
Create identical job posts
Most popular languages
etc"
US Energy Statistics,All data published by the US Energy Information Administration,Sohier Dane,26,"Version 2,2017-08-22|Version 1,2017-08-22","government agencies
energy",{}JSON,911 MB,Other,"3,855 views",389 downloads,,0 topics,https://www.kaggle.com/sohier/us-energy-statistics,"This dataset contains the entire contents of each major API data set published by the US Energy Information Administration. That's everything from the hourly electricity consumption in the United States to natural gas futures contracts.
This data has been lightly reprocessed from the EIA's bulk download facility by converting each file from a zip of jsons into a single json with the series name as the keys to the specific time series. Please note that there are thousands of time series in here, and many of them may still require additional cleaning to deal with odd date formats and so on. The file preview is unable to show a complete listing. You can usually find full details of a given time series in the 'description' field."
Sentiment Lexicons for 81 Languages,Sentiment Polarity Lexicons (Positive vs. Negative),Rachael Tatman,26,"Version 1,2017-09-14","languages
linguistics",CSV,2 MB,Other,"4,513 views",553 downloads,,3 topics,https://www.kaggle.com/rtatman/sentiment-lexicons-for-81-languages,"Context:
Sentiment analysis, the task of automatically detecting whether a piece of text is positive or negative, generally relies on a hand-curated list of words with positive sentiment (good, great, awesome) and negative sentiment (bad, gross, awful). This dataset contains both positive and negative sentiment lexicons for 81 languages.
Content:
The sentiment lexicons in this dataset were generated via graph propagation based on a knowledge graph--a graphical representation of real-world entities and the links between them. The general intuition is that words which are closely linked on a knowledge graph probably have similar sentiment polarities. For this project, sentiments were generated based on English sentiment lexicons.
This dataset contains sentiment lexicons for the following languages:
Afrikaans
Albanian
Arabic
Aragonese
Armenian
Azerbaijani
Basque
Belarusian
Bengali
Bosnian
Breton
Bulgarian
Catalan
Chinese
Croatian
Czech
Danish
Dutch
Esperanto
Estonian
Faroese
Finnish
French
Galician
Georgian
German
Greek
Gujarati
Haitian Creole
Hebrew
Hindi
Hungarian
Icelandic
Ido
Indonesian
Interlingua
Irish
Italian
Kannada
Khmer
Kirghiz
Korean
Kurdish
Latin
Latvian
Lithuanian
Luxembourgish
Macedonian
Malay
Maltese
Marathi
Norwegian
Norwegian
Persian
Polish
Portuguese
Romanian
Romansh
Russian
Scottish
Serbian
Slovak
Slovene
Spanish
Swahili
Swedish
Tagalog
Tamil
Telugu
Thai
Turkish
Turkmen
Ukrainian
Urdu
Uzbek
Vietnamese
Volapük
Walloon
Welsh
Western Frisian
Yiddish
For more information and additional sentiment lexicons, please visit the project’s website.
Acknowledgements:
This dataset was collected by Yanqing Chen and Steven Skiena. If you use it in your work, please cite the following paper:
Chen, Y., & Skiena, S. (2014). Building Sentiment Lexicons for All Major Languages. In ACL (2) (pp. 383-389).
It is distributed here under the GNU General Public License. Note that this is the full GPL, which allows many free uses, but does not allow its incorporation into any type of distributed proprietary software, even in part or in translation. For commercial applications please contact the dataset creators.
Inspiration:
These word lists contain many words with similar meanings. Can you automatically detect which words are cognates?
Can you use these sentiment lexicons to reverse-engineer the knowledge graphs that generated them?"
Top Spotify Tracks of 2017,Audio features of top Spotify songs,Nadin Tamer,26,"Version 1,2017-12-20","popular culture
music",CSV,13 KB,Other,"3,490 views",567 downloads,,2 topics,https://www.kaggle.com/nadintamer/top-tracks-of-2017,"Top Spotify Tracks of 2017
At the end of each year, Spotify compiles a playlist of the songs streamed most often over the course of that year. This year's playlist (Top Tracks of 2017) included 100 songs. The question is: What do these top songs have in common? Why do people like them?
Original Data Source: The audio features for each song were extracted using the Spotify Web API and the spotipy Python library. Credit goes to Spotify for calculating the audio feature values.
Data Description: There is one .csv file in the dataset. (featuresdf.csv) This file includes:
Spotify URI for the song
Name of the song
Artist(s) of the song
Audio features for the song (such as danceability, tempo, key etc.)
A more detailed explanation of the audio features can be found in the Metadata tab.
Exploring the Data: Some suggestions for what to do with the data:
Look for patterns in the audio features of the songs. Why do people stream these songs the most?
Try to predict one audio feature based on the others
See which features correlate the most"
Dark Net Marketplace Data (Agora 2014-2015),"Includes over 100,000 unique listings of drugs, weapons and more",philipjames11,26,"Version 1,2017-12-06","crime
illegal drugs
internet",CSV,8 MB,CC0,"3,688 views",374 downloads,,0 topics,https://www.kaggle.com/philipjames11/dark-net-marketplace-drug-data-agora-20142015,"Context
This data set was made from an html rip made by reddit user ""usheep"" who threatened to expose all the vendors on Agora to the police if they did not meet his demands (sending him a small monetary amount ~few hundred dollars in exchange for him not leaking their info). Most information about what happened to ""usheep"" and his threats is nonexistent. He posted the html rip and was never heard from again. Agora shut down a few months after. It is unknown if this was related to ""usheep"" or not, but the raw html data remained.
Content
This is a data parse of marketplace data ripped from Agora (a dark/deep web) marketplace from the years 2014 to 2015. It contains drugs, weapons, books, services, and more. Duplicate listings have been removed and prices have been averaged of any duplicates. All of the data is in a csv file and has over 100,000 unique listings.
It is organized by:
Vendor: The seller
Category: Where in the marketplace the item falls under
Item: The title of the listing
Description: The description of the listing
Price: Cost of the item (averaged across any duplicate listings between 2014 and 2015)
Origin: Where the item is listed to have shipped from
Destination: Where the item is listed to be shipped to (blank means no information was provided, but mostly likely worldwide. I did not enter worldwide for any blanks however as to not make assumptions)
Rating: The rating of the seller (a rating of [0 deals] or anything else with ""deals"" in it means there is not concrete rating as the amount of deals is too small for a rating to be displayed)
Remarks: Only remark options are blank, or ""Average price may be skewed outliar > .5 BTC found"" which is pretty self explanatory.
Acknowledgements
Though I got this data from a 3rd party, it seems as though it originally came from here: https://www.gwern.net/DNM-archives Gwern Branwen seems to have complied all of his dark net marketplace leaks and html rips and has a multitude of possible uses for the data at the link above. It is free for anyone to use as long as proper credit is given to the creator. I would be happy to parse more data if anyone would like to request a specific website and/or format.
Inspiration
This data could be used to track drug dealers across different platforms. Potentially find correlations between different drugs and from where/to they ship in the world to show correlations between types of drugs and where drug dealers that supply them are located. Prices can estimate drug economies in certain regions of the world. Similar listings from 2 different vendors can perhaps point to competition to corner a market, or even show that some vendors may work together to corner a market. There are quite a few opportunities to do some really great stuff to find correlations between illegal drugs, weapons, and more in order to curb the flow of dark net drug trade by identifying high risk regions or vendors. I can potentially do a new parse of other websites so you can find correlations across websites rather than just within Agora."
World Development Indicators,The most accurate global development data available,World Bank,26,"Version 1,2017-08-14","countries
globalization
economics",CSV,234 MB,Other,"3,703 views",490 downloads,,,https://www.kaggle.com/theworldbank/world-development-indicators,"World Development Indicators provides a compilation of relevant, high-quality, and internationally comparable statistics about global development and the fight against poverty. It is intended to help policymakers, students, analysts, professors, program managers, and citizens find and use data related to all aspects of development, including those that help monitor progress toward the World Bank Group’s two goals of ending poverty and promoting shared prosperity.
Content
This dataset includes indicators at both national and regional levels for: -Agriculture & Rural Development -Aid Effectiveness -Climate Change -Economy & Growth -Education -Energy & Mining -Environment -External Debt -Financial Sector -Gender -Health -Infrastructure -Labor & Social Protection -Poverty -Private Sector -Public Sector -Science & Technology -Social Development -Trade, Urban Development
Acknowledgements
This dataset was kindly made available by the World Bank. Please check their instance at http://data.worldbank.org/data-catalog/world-development-indicators for updates and related information."
Match Statistics from top 5 European Leagues,"Italy, Spain, England, Germany, France 2012-2017",Jemilu Mohammed,26,"Version 3,2017-07-07|Version 2,2017-07-07|Version 1,2017-05-16",association football,CSV,6 MB,CC0,"4,376 views",601 downloads,2 kernels,4 topics,https://www.kaggle.com/jangot/ligue1-match-statistics,"Context
I am a student exploring the possibility of making money in football betting. I am currently doing a literature review on modelling association football scores and trying to put together a machine learning system to use for my first betting campaign next season. What I have learned thus far is that outcomes of football events are partly deterministic and partly random. I do not know exactly how to go about implementing this in a machine learning system yet. I am also hoping to find useful features from this dataset.
Content
The data here contains match statistics collected from whoscored.com europes top five leagues from 2012-2013 to 2016-2017 season. It contains just about all match statistics that anyone can ever hope for including but not limited to Goals, Corners, Possession, Ratings, Coaches, LineUps, and other relevent match statistics
The features are simply just self explanatory and have been given long but meaningful names
Acknowledgement
I collected the data from the whoscored.com website. I scraped it using beautifulSoup in python and just extracted the features I thought could have some use.
Inspiration
This is just something I hope could become something but hey, it may be nothing. I am just interested to know the kind of insights that could be generated from this."
"Finishers Boston Marathon 2015, 2016 & 2017","This data has the names, times and general demographics of the finishers",rojour,26,"Version 4,2017-04-30|Version 3,2017-04-30|Version 2,2017-04-30|Version 1,2017-03-25","running
walking",CSV,12 MB,Other,"8,689 views","1,238 downloads",34 kernels,2 topics,https://www.kaggle.com/rojour/boston-results,"Context
This is a list of the finishers of the Boston Marathon of 2015, 2016 and 2017.
It's important to highlight that the Boston Marathon is the oldest marathon run in the US as it is the only marathon (other than olympic trails) that most of the participants have to qualify to participate.
For the professional runners, it's a big accomplishment to win the marathon. For most of the other participants, it's an honor to be part of it.
Content
It contains the name, age, gender, country, city and state (where available), times at 9 different stages of the race, expected time, finish time and pace, overall place, gender place and division place.
Decided to keep every year as a separate file, making it more manageable and easier to deal with it.
Acknowledgements
Data was scrapped from the official marathon website - http://registration.baa.org/2017/cf/Public/iframe_ResultsSearch.cfm
I have found that other people have done this kind of scraping, so, some of those ideas together with things I have learned in my quest to become a data scientist created the set.
You can actually find the scraping notebooks at - https://github.com/rojour/boston_results . Notebook it's not very clean yet, but I will get to it soon...
Inspiration
I was a participant in the marathon 2016 and 2017 edition, as well as a data science student, so it was a natural curiosity.
I have done a preliminary study of some fun facts. You can see the kernel here as well as in the github page listed above.
Already some people have created some fun analysis of the results (mostly of the first part - 2016) that was the first upload, but I am curious of what people may come up with... now that three years are available, it may spark the creative juices of some.
I believe it's a simple, fun dataset that can be used by the new to play with, and by some veterans to get creative."
DC Metro Crime Data,Consolidated set of all registered crimes from crimemaps.dc.gov,LucasVinze,26,"Version 5,2017-11-03|Version 4,2017-04-18|Version 3,2016-08-19|Version 2,2016-08-19|Version 1,2016-08-18",crime,CSV,109 MB,Other,"13,501 views","1,695 downloads",47 kernels,2 topics,https://www.kaggle.com/vinchinzu/dc-metro-crime-data,"Dataset of all of the crimes in the DC metro police system ranging from Theft, Arson, Assault, Homicide, Sex Abuse, Robbery, and Burglary.
Data can be easily geocoded and mapped, trends can be extracted, and predictions can be made.
Would be interesting to combine with other datasets, i.e. changes in housing prices, history of construction sites etc. j An informal hypothesis would be: If the local government invests in fixing the sidewalks in a neighborhood, how much would the investment decrease crime levels on a block by block basis.
Raw Data can be accessed from: http://crimemap.dc.gov/CrimeMapSearch.aspx#tabs-GeoOther
The data is most easily accessed by downloading 1 ward at a time for the specific data range."
UFO Sightings around the world,"80,000+ documented close encounters from the past 70 years",Cam Nugent,26,"Version 1,2017-08-15","history
linguistics",CSV,13 MB,CC0,"5,323 views",677 downloads,5 kernels,0 topics,https://www.kaggle.com/camnugent/ufo-sightings-around-the-world,"Context
Extraterrestrials, visitors, little green men, UFOs, swap gas. What do they want? Where do they come from? Do they like cheeseburgers? This dataset will likely not help you answer these questions. It does contain over 80,000 records of UFO sightings dating back as far as 1949. With the latitude and longitude data it is possible to assess the global distribution of UFO sightings (patterns could aid in planetary defence if invasion proves to be imminent). The dates and times, along with the duration of the UFO's stay and description of the craft also lend themselves to predictions. Can we find patterns in their arrival times and durations? Do aliens work on weekends? Help defend the planet and learn about your fellow earthlings (and when they are most likely to see ET).
Content
Date_time - standardized date and time of sighting
city - location of UFO sighting
state/province - the US state or Canadian province, appears blank for other locations
country - Country of UFO sighting
UFO_shape - a one word description of the ""spacecraft""
length_of_encounter_seconds - standardized to seconds, length of the observation of the UFO
described_duration _of_encounter - raw description of the length of the encounter (shows uncertainty to previous column)
description - text description of the UFO encounter. Warning column is messy, with some curation it could lend itself to some natural language processing and sentiment analysis.
date_documented - when was the UFO sighting reported
latitude - latitude
longitude - longitude
Note there are missing data in the columns. I've left it as is because depending on what the user is interested in the missing values in any one column may or may not matter.
Acknowledgements
I found these data here: https://github.com/planetsig/ufo-reports Full credit to them for the curation, I added some column headers and just described what I've seen
Inspiration
Some great ways to use these data would be:
A global plot of the locations of recorded UFO sightings.
Can the duration of the UFO visit be predicted from the other data?
Is there a pattern to the appearances? At certain times of day, on certain days of the week or days of the year? (i.e. are people on their way home from the pub more likely to see little green men?)
Are certain shapes of UFO more likely to be seen in different geographical regions."
Association of Tennis Professionals Matches,ATP tournament results from 2000 to 2017,GMAdevs,26,"Version 2,2017-02-07|Version 1,2017-02-05","tennis
sports",CSV,11 MB,CC4,"5,598 views","1,250 downloads",11 kernels,0 topics,https://www.kaggle.com/gmadevs/atp-matches-dataset,"Context
A dataset of ATP matches including individual statistics.
Content
In these datasets there are individual csv files for ATP tournament from 2000 to 2017.
The numbers in the last columns are absolute values, using them you can calculate percentages.
Dataset legend
All the match statistics are in absolute number format, you can convert to percentages using the total point number
ace = absolute number of aces
df = number of double faults
svpt = total serve points
1stin = 1st serve in
1st won = points won on 1st serve
2ndwon = points won on 2nd serve
SvGms = serve games
bpSaved = break point saved
bpFaced = break point faced
Acknowledgement
Thanks to Jeff Sackmann for the excellent work. Be sure to visit his github profile
https://github.com/JeffSackmann/tennis_atp
Inspiration
This dataset would be likely used to develop predictive modeling of tennis matches and to do statistic research. I'm planning to add historical odds and injuries data as soon as I have the time to get them."
U.S. Educational Finances,"Revenues and expenditures for U.S. grade schools, by year and state",Roy Garrard,26,"Version 4,2018-02-04|Version 3,2018-01-16|Version 2,2017-12-01|Version 1,2017-11-29","united states
education
finance",Other,81 MB,CC0,"4,670 views",671 downloads,8 kernels,0 topics,https://www.kaggle.com/noriuk/us-educational-finances,"Context
The United States Census Bureau conducts annual surveys to assess the finances of elementary and high schools. The attached CSV file contains a summary of revenue and expenditure for the years 1992-2015, organized by state.
Content
[elsec_main.csv] A comma-separated spreadsheet containing revenues and expenditures for all U.S. school districts, 1993-2015.
STATE,ENROLL,NAME,YRDATA,TOTALREV,TFEDREV,TSTREV,TLOCREV,TOTALEXP,TCURINST,TCURSSVC,TCURONON,TCAPOUT
Alabama,7568,AUTAUGA CO SCH DIST,1995,31827,2821,21389,7617,27457,15228,7123,2575,2176
Alabama,19961,BALDWIN CO SCH DIST,1995,93379,6655,55108,31616,87973,48750,22961,6927,6795
[elsec_summary.csv] A comma-separated spreadsheet containing state summaries of revenues and expenditures, organized by year.
STATE,YEAR,ENROLL,TOTAL_REVENUE,FEDERAL_REVENUE,STATE_REVENUE,LOCAL_REVENUE,TOTAL_EXPENDITURE,INSTRUCTION_EXPENDITURE,SUPPORT_SERVICES_EXPENDITURE,OTHER_EXPENDITURE,CAPITAL_OUTLAY_EXPENDITURE
Alabama,1992,,2678885,304177,1659028,715680,2653798,1481703,735036,,174053
Alaska,1992,,1049591,106780,720711,222100,972488,498362,350902,,37451
Be warned, some data will be NaN's (most notably, the 1992 records contain no data for enrollment).
Data was created from the spreadsheets in [elsec.zip] (taken from the U.S. Census Bureau site) using [chew_data.py] and [state_summary.py]. Column names are documented in [school15doc.pdf].
Sources
https://www.census.gov/programs-surveys/school-finances/data/tables.html
Changelog
[v 0.2] Added data from 1993-2001. Data is now harvested from the main spreadsheets instead of the summary spreadsheets. Data by school district is now available.
[v 0.3] Added 1992 data. Added enrollment data for all years except 1992 (unavailable).
[v 0.4] Straightening a few things out as I play with the data in my own kernel. Changed ""program_other_expenditure"" to ""other_expenditure"" and fixed chew_data.py to properly pull that information. Removed ""non-elsec"" funding and ""program_current_expenditure"" columns."
StarCraft II Replay Analysis,In-Depth Look at StarCraft II Replays,Simon Fraser University - Summit,26,"Version 1,2016-11-05",video games,CSV,532 KB,CC4,"7,535 views",447 downloads,27 kernels,,https://www.kaggle.com/sfu-summit/starcraft-ii-replay-analysis,"Context
This dataset is an aggregate of the screen-fixations from screen movements of StarCraft 2 replay files.
Content
This dataset contains 21 variables:
GameID: Unique ID for each game
LeagueIndex: 1-8 for Bronze, Silver, Gold, Diamond, Master, GrandMaster, Professional leagues
Age: Age of each player
HoursPerWeek: Hours spent playing per week
TotalHours: Total hours spent playing
APM: Action per minute
SelectByHotkeys: Number of unit selections made using hotkeys per timestamp
AssignToHotkeys: Number of units assigned to hotkeys per timestamp
UniqueHotkeys: Number of unique hotkeys used per timestamp
MinimapAttacks: Number of attack actions on minimal per timestamp
MinimapRightClicks: Number of right-clicks on minimal per timestamp
NumberOfPACs: Number of PACs per timestamp
GapBetweenPACs: Mean duration between PACs (milliseconds)
ActionLatency: Mean latency from the onset of PACs to their first action (milliseconds)
ActionsInPAC: Mean number of actions within each PAC
TotalMapExplored: Number of 24x24 game coordinate grids viewed by player per timestamp
WorkersMade: Number of SCVs, drones, probes trained per timestamp
UniqueUnitsMade: Unique units made per timestamp
ComplexUnitsMade: Number of ghosts, investors, and high templars trained per timestamp
ComplexAbilityUsed: Abilities requiring specific targeting instructions used per timestamp
MaxTimeStamp: Time stamp of game's last recorded event
Inspiration
Questions worth exploring:
How do the replay attributes differ by level of player expertise?
What are significant predictors of a player's league?
Acknowledgements
This dataset is from Simon Fraser University - Summit and can be found here. You must give attribution to the work; You may not use this work for commercial purposes; You may not alter, transform, or build upon this work. Any further uses require the permission of the rights holder."
"Crime in Context, 1975-2015",Are violent crime rates rising or falling in American cities?,The Marshall Project,26,"Version 1,2017-02-10","history
crime",CSV,258 KB,Other,"7,097 views","1,295 downloads",13 kernels,,https://www.kaggle.com/marshallproject/crime-rates,"Context
Is crime in America rising or falling? The answer is not as simple as politicians make it out to be because of how the FBI collects crime data from the country’s more than 18,000 police agencies. National estimates can be inconsistent and out of date, as the FBI takes months or years to piece together reports from those agencies that choose to participate in the voluntary program.
To try to fill this gap, The Marshall Project collected and analyzed more than 40 years of data on the four major crimes the FBI classifies as violent — homicide, rape, robbery and assault — in 68 police jurisdictions with populations of 250,000 or greater. We obtained 2015 reports, which have yet to be released by the FBI, directly from 61 of them. We calculated the rate of crime in each category and for all violent crime, per 100,000 residents in the jurisdiction, based on the FBI’s estimated population for that year. We used the 2014 estimated population to calculate 2015 crime rates per capita.
Acknowledgements
The crime data was acquired from the FBI Uniform Crime Reporting program's ""Offenses Known and Clearances by Arrest"" database for the year in question, held at the National Archives of Criminal Justice Data. The data was compiled and analyzed by Gabriel Dance, Tom Meagher, and Emily Hopkins of The Marshall Project; the analysis was published as Crime in Context on 18 August 2016."
UNHCR Refugee Data,Data on Uprooted Populations and Asylum Processing,United Nations,26,"Version 1,2017-08-01","politics
demographics",CSV,40 MB,CC4,"3,751 views",713 downloads,3 kernels,0 topics,https://www.kaggle.com/unitednations/refugee-data,"Context:
The mass movement of uprooted people is a highly charged geopolitical issue. This data, gathered by the UN High Commissioner for Refugees (UNHCR), covers movement of displaced persons (asylum seekers, refugees, internally displaced persons (IDP), stateless). Also included are destination country responses to asylum petitions.
Content:
This dataset includes 6 csv files covering:
Asylum monthly applications opened (asylum_seekers_monthly.csv)
Yearly progress through the refugee system (asylum_seekers.csv)
Refugee demographics (demographics.csv)
Yearly time series data on UNHCR’s populations of concern (time_series.csv)
Yearly population statistics on refugees by residence and destination (persons_of_concern.csv)
Yearly data on resettlement arrivals, with or without UNHCR assistance (resettlement.csv)
Acknowledgements:
This dataset was gathered from UNHCR. Photo by Ali Tareq.
Inspiration:
What are the most frequent destination countries for refugees? How has refugee flow changed? Any trends that could predict future refugee patterns?"
All Lending Club loan data,2007 through current Lending Club accepted and rejected loan data,NathanGeorge,25,"Version 6,2018-02-01|Version 5,2018-01-31|Version 4,2017-08-11|Version 3,2017-03-03|Version 2,2017-03-02|Version 1,2017-03-02","business
finance
lending",CSV,409 MB,CC0,"5,616 views",881 downloads,8 kernels,0 topics,https://www.kaggle.com/wordsforthewise/lending-club,"Context
I wanted an easy way to share all the lending club data with others. Unfortunately, the data on their site is fragmented into many smaller files. There is another lending club dataset on Kaggle, but it hasn't been updated in years. It also doesn't include the rejected loans, which I put in here.
I created a git repo for the code to create this data: https://github.com/nateGeorge/preprocess_lending_club_data
Content
The definitions for the fields are here, at the bottom of the page.
Unfortunately, there is a limit of 500MB for dataset files (so lame!), so I had to compress the files with gzip in the Python pandas package.
I cleaned the data a tiny bit: I removed %s from int_rate and revol_util, and deleted the url column.
To load the data in Python:
import pandas as pd

accept_df = pd.read_csv('../input/accepted_2007_to_2016.csv.gz', compression='gzip')
reject_df = pd.read_csv('../input/rejected_2007_to_2016.csv.gz', compression='gzip')

# too many columns to print the info summary out, so we need to force it
print(accept_df.info(verbose=True, null_counts=True))
In R:
library(data.table)

accepted_def &lt;- read.csv(gzfile('rejected_2007_to_2016.csv.gz'), na.strings='')
acc_dt &lt;- as.data.table(accepted_def)
rejected_def &lt;- read.csv(gzfile('accepted_2007_to_2016.csv.gz'), na.strings='')
rej_dt &lt;- as.data.table(accepted_def)
There are also separate csvs in the main input folder, but the only advantage over the lending club site is that the 2016 year is joined into one file instead of 4.
Inspiration
I wanted to make this dataset easily available for others to use."
101 Innovations - Research Tools Survey,Explore global research practices and opinions on scholarly communication,Bianca Kramer,25,"Version 2,2016-05-18|Version 1,2016-05-17",research,CSV,27 MB,CC0,"16,619 views","1,057 downloads",19 kernels,0 topics,https://www.kaggle.com/bmkramer/101-innovations-research-tools-survey,"Many new websites and online tools have come into existence to support scholarly communication in all phases of the research workflow. To what extent researchers are using these and more traditional tools has been largely unknown. This 2015-2016 survey aimed to fill that gap.
The survey captured information on tool usage for 17 research activities, stance towards open access and open science, and expectations of the most important development in scholarly communication. Respondents’ demographics included research roles, country of affiliation, research discipline and year of first publication. The online survey employed an open, non-probability sample. A largely self-selected group of 20,663 researchers, librarians, editors, publishers and other groups involved in research took the survey, which was available in seven languages. The survey was open from May 10, 2015 to February 10, 2016.
This data set contains:
Full raw (anonymized) and cleaned data files (csv, each file containing 20,663 records and 178 variables)
Variable lists for raw and cleaned data files (csv)
Readme file (txt)
The dataset is also deposited in Zenodo: http://dx.doi.org/10.5281/zenodo.49583
The full description of survey methodology is in a data publication in F1000 Research: http://dx.doi.org/10.12688/f1000research.8414.1
More information on the project this survey is part of can be found here: http://101innovations.wordpress.com
[edited to add] For quick visual exploration of the data, check out the interactive dashboard on Silk: http://dashboard101innovations.silk.co/
Contact:
Jeroen Bosman: http://orcid.org/0000-0001-5796-2727 / j.bosman@uu.nl
Bianca Kramer: http://orcid.org/0000-0002-5965-6560 / b.m.r.kramer@uu.nl"
Gun Deaths in the US: 2012-2014,"Information about gun-deaths from the CDC: Ages, gender, intent and more",Zurda,25,"Version 1,2017-01-25","death
crime
violence
demographics",CSV,6 MB,Other,"10,227 views","1,828 downloads",58 kernels,0 topics,https://www.kaggle.com/hakabuk/gun-deaths-in-the-us,"Context
This dataset includes information about gun-death in the US in the years 2012-2014.
Content
The data includes data regarding the victim's age, sex, race, education, intent, time (month and year) and place of death, and whether or not police was at the place of death.
Acknowledgements
I came across this thanks to FiveThirtyEight's Gun Deaths in America project. The data originated from the CDC, and can be found here."
Bible Corpus,English Bible Translations Dataset for Text Mining and NLP,Oswin Rahadiyan Hartono,25,"Version 3,2017-06-16|Version 2,2017-06-15|Version 1,2017-06-07","faith and traditions
linguistics",CSV,427 MB,CC0,"4,131 views",331 downloads,4 kernels,,https://www.kaggle.com/oswinrh/bible,"Context
Bible (or Biblia in Greek) is a collection of sacred texts or scriptures that Jews and Christians consider to be a product of divine inspiration and a record of the relationship between God and humans (Wiki). And for data mining purpose, we could do many things using Bible scriptures as for NLP, Classification, Sentiment Analysis and other particular topics between Data Science and Theology perspective.
Content
Here you will find the following bible versions in sql, sqlite, xml, csv, and json format:
American Standard-ASV1901 (ASV)
Bible in Basic English (BBE)
Darby English Bible (DARBY)
King James Version (KJV)
Webster's Bible (WBT)
World English Bible (WEB)
Young's Literal Translation (YLT)
Each verse is accessed by a unique key, the combination of the BOOK+CHAPTER+VERSE id.
Example:
Genesis 1:1 (Genesis chapter 1, verse 1) = 01001001 (01 001 001)
Exodus 2:3 (Exodus chapter 2, verse 3) = 02002003 (02 002 003)
The verse-id system is used for faster, simplified queries.
For instance: 01001001 - 02001005 would capture all verses between Genesis 1:1 through Exodus 1:5.
Written simply:
SELECT * FROM bible.t_asv WHERE id BETWEEN 01001001 AND 02001005
Coordinating Tables
There is also a number-to-book key (key_english table), a cross-reference list (cross_reference table), and a bible key containing meta information about the included translations (bible_version_key table). See below SQL table layout. These tables work together providing you a great basis for a bible-reading and cross-referencing app. In addition, each book is marked with a particular genre, mapping in the number-to-genre key (key_genre_english table) and common abbreviations for each book can be looked up in the abbreviations list (key_abbreviations_english table). While its expected that your programs would use the verse-id system, book #, chapter #, and verse # columns have been included in the bible versions tables.
A Valuable Cross-Reference Table
A very special and valuable addition to these databases is the extensive cross-reference table. It was created from the project at http://www.openbible.info/labs/cross-references/. See .txt version included from http://www.openbible.info website. Its extremely useful in bible study for discovering related scriptures. For any given verse, you simply query vid (verse id), and a list of rows will be returned. Each of those rows has a rank (r) for relevance, start-verse (sv), and end verse (ev) if there is one.
Basic Web Interaction
The web folder contains two php files. Edit the first few lines of index.php to match your server's settings. Place these in a folder on your webserver. The references search box can be multiple comma separated values. (i.e. John 3:16, Rom 3:23, 1 Jn 1:9, Romans 10:9-10) You can also directly link to a verse by altering the URI: [http://localhost/index.php?b=John 3:16, Rom 3:23, 1 Jn 1:9, Romans 10:9-10](http://localhost/index.php?b=John 3:16, Rom 3:23, 1 Jn 1:9, Romans 10:9-10)
bible-mysql.sql (MySQL) is the main database and most feature-oriented due to contributions from developers. It is suggested you use that for most things, or at least convert the information from it.
cross_references-mysql.sql (MySQL) is the cross-reference table. It has been separated to become an optional feature. This is converted from the project at http://www.openbible.info/labs/cross-references/.
bible-sqlite.db (SQLite) is a basic simplified database for simpler applications (includes cross-references too).
cross_references.txt is the source cross-reference file obtained from http://www.openbible.info/labs/cross-references/
In CSV folder, you will find (same list order with the other formats):
bible_version_key.csv
key_abbreviations_english.csv
key_english.csv
key_genre_english.csv
t_asv.csv, t_bbe.csv, t_dby.csv, t_wbt.csv, t_web.csv, t_ylt.csv
Acknowledgements
In behalf of the original contributors (Github)
Inspirations
WordNet as an additional semantic resource for NLP"
Caravan Insurance Challenge,Identify potential purchasers of caravan insurance policies,UCI Machine Learning,25,"Version 1,2016-11-28","finance
automobiles",CSV,2 MB,Other,"12,266 views","1,169 downloads",17 kernels,,https://www.kaggle.com/uciml/caravan-insurance-challenge,"This data set used in the CoIL 2000 Challenge contains information on customers of an insurance company. The data consists of 86 variables and includes product usage data and socio-demographic data derived from zip area codes. The data was collected to answer the following question: Can you predict who would be interested in buying a caravan insurance policy and give an explanation why?
Acknowledgements
DISCLAIMER
This dataset is owned and supplied by the Dutch datamining company Sentient Machine Research, and is based on real world business data. You are allowed to use this dataset and accompanying information for non commercial research and education purposes only. It is explicitly not allowed to use this dataset for commercial education or demonstration purposes. For any other use, please contact Peter van der Putten, info@smr.nl.
This dataset has been used in the CoIL Challenge 2000 datamining competition. For papers describing results on this dataset, see the TIC 2000 homepage: http://www.wi.leidenuniv.nl/~putten/library/cc2000/
Please cite/acknowledge:
P. van der Putten and M. van Someren (eds) . CoIL Challenge 2000: The Insurance Company Case. Published by Sentient Machine Research, Amsterdam. Also a Leiden Institute of Advanced Computer Science Technical Report 2000-09. June 22, 2000.
The Data
Originally, this dataset was broken into two parts: the training set and the evaluation set. As this was a competition, the responses to the evaluation set were not given as part of the original release; they were, however, released after the end of the competition in a separate file. This dataset contains all three of these files, combined into one.
The field ORIGIN in the caravan-insurance-challenge.csv file has the values train and test, corresponding to the training and evaluation sets, respectively. To simulate the original challenge, you can ignore the test rows, and test your model's prediction on those observations once you've trained only on the training set.
Each observation corresponds to a postal code. Variables beginning with M refer to demographic statistics of the postal code, while variables beginning with P and A (as well as CARAVAN, the target variable) refer to product ownership and insurance statistics in the postal code.
The data file contains the following fields:
ORIGIN: train or test, as described above
MOSTYPE: Customer Subtype; see L0
MAANTHUI: Number of houses 1 - 10
MGEMOMV: Avg size household 1 - 6
MGEMLEEF: Avg age; see L1
MOSHOOFD: Customer main type; see L2
** Percentages in each group, per postal code (see L3)**:
MGODRK: Roman catholic
MGODPR: Protestant ...
MGODOV: Other religion
MGODGE: No religion
MRELGE: Married
MRELSA: Living together
MRELOV: Other relation
MFALLEEN: Singles
MFGEKIND: Household without children
MFWEKIND: Household with children
MOPLHOOG: High level education
MOPLMIDD: Medium level education
MOPLLAAG: Lower level education
MBERHOOG: High status
MBERZELF: Entrepreneur
MBERBOER: Farmer
MBERMIDD: Middle management
MBERARBG: Skilled labourers
MBERARBO: Unskilled labourers
MSKA: Social class A
MSKB1: Social class B1
MSKB2: Social class B2
MSKC: Social class C
MSKD: Social class D
MHHUUR: Rented house
MHKOOP: Home owners
MAUT1: 1 car
MAUT2: 2 cars
MAUT0: No car
MZFONDS: National Health Service
MZPART: Private health insurance
MINKM30: Income < 30.000
MINK3045: Income 30-45.000
MINK4575: Income 45-75.000
MINK7512: Income 75-122.000
MINK123M: Income >123.000
MINKGEM: Average income
MKOOPKLA: Purchasing power class
** Total number of variable in postal code (see L4)**:
PWAPART: Contribution private third party insurance
PWABEDR: Contribution third party insurance (firms) ...
PWALAND: Contribution third party insurane (agriculture)
PPERSAUT: Contribution car policies
PBESAUT: Contribution delivery van policies
PMOTSCO: Contribution motorcycle/scooter policies
PVRAAUT: Contribution lorry policies
PAANHANG: Contribution trailer policies
PTRACTOR: Contribution tractor policies
PWERKT: Contribution agricultural machines policies
PBROM: Contribution moped policies
PLEVEN: Contribution life insurances
PPERSONG: Contribution private accident insurance policies
PGEZONG: Contribution family accidents insurance policies
PWAOREG: Contribution disability insurance policies
PBRAND: Contribution fire policies
PZEILPL: Contribution surfboard policies
PPLEZIER: Contribution boat policies
PFIETS: Contribution bicycle policies
PINBOED: Contribution property insurance policies
PBYSTAND: Contribution social security insurance policies
AWAPART: Number of private third party insurance 1 - 12
AWABEDR: Number of third party insurance (firms) ...
AWALAND: Number of third party insurance (agriculture)
APERSAUT: Number of car policies
ABESAUT: Number of delivery van policies
AMOTSCO: Number of motorcycle/scooter policies
AVRAAUT: Number of lorry policies
AAANHANG: Number of trailer policies
ATRACTOR: Number of tractor policies
AWERKT: Number of agricultural machines policies
ABROM: Number of moped policies
ALEVEN: Number of life insurances
APERSONG: Number of private accident insurance policies
AGEZONG: Number of family accidents insurance policies
AWAOREG: Number of disability insurance policies
ABRAND: Number of fire policies
AZEILPL: Number of surfboard policies
APLEZIER: Number of boat policies
AFIETS: Number of bicycle policies
AINBOED: Number of property insurance policies
ABYSTAND: Number of social security insurance policies
CARAVAN: Number of mobile home policies 0 - 1
Keys (L1 - L4)
L0: Customer subtype
1: High Income, expensive child
2: Very Important Provincials
3: High status seniors
4: Affluent senior apartments
5: Mixed seniors
6: Career and childcare
7: Dinki's (double income no kids)
8: Middle class families
9: Modern, complete families
10: Stable family
11: Family starters
12: Affluent young families
13: Young all american family
14: Junior cosmopolitan
15: Senior cosmopolitans
16: Students in apartments
17: Fresh masters in the city
18: Single youth
19: Suburban youth
20: Etnically diverse
21: Young urban have-nots
22: Mixed apartment dwellers
23: Young and rising
24: Young, low educated
25: Young seniors in the city
26: Own home elderly
27: Seniors in apartments
28: Residential elderly
29: Porchless seniors: no front yard
30: Religious elderly singles
31: Low income catholics
32: Mixed seniors
33: Lower class large families
34: Large family, employed child
35: Village families
36: Couples with teens 'Married with children'
37: Mixed small town dwellers
38: Traditional families
39: Large religous families
40: Large family farms
41: Mixed rurals
L1: average age keys:
1: 20-30 years 2: 30-40 years 3: 40-50 years 4: 50-60 years 5: 60-70 years 6: 70-80 years
L2: customer main type keys:
1: Successful hedonists
2: Driven Growers
3: Average Family
4: Career Loners
5: Living well
6: Cruising Seniors
7: Retired and Religeous
8: Family with grown ups
9: Conservative families
10: Farmers
L3: percentage keys:
0: 0%
1: 1 - 10%
2: 11 - 23%
3: 24 - 36%
4: 37 - 49%
5: 50 - 62%
6: 63 - 75%
7: 76 - 88%
8: 89 - 99%
9: 100%
L4: total number keys:
0: 0
1: 1 - 49
2: 50 - 99
3: 100 - 199
4: 200 - 499
5: 500 - 999
6: 1000 - 4999
7: 5000 - 9999
8: 10,000 - 19,999
9: >= 20,000"
StackSample: 10% of Stack Overflow Q&A,Text from 10% of Stack Overflow questions and answers on programming topics,Stack Overflow,25,"Version 1,2016-10-21","internet
programming languages",CSV,3 GB,Other,"10,271 views","1,146 downloads",17 kernels,0 topics,https://www.kaggle.com/stackoverflow/stacksample,"Dataset with the text of 10% of questions and answers from the Stack Overflow programming Q&A website.
This is organized as three tables:
Questions contains the title, body, creation date, closed date (if applicable), score, and owner ID for all non-deleted Stack Overflow questions whose Id is a multiple of 10.
Answers contains the body, creation date, score, and owner ID for each of the answers to these questions. The ParentId column links back to the Questions table.
Tags contains the tags on each of these questions
Datasets of all R questions and all Python questions are also available on Kaggle, but this dataset is especially useful for analyses that span many languages.
Example projects include:
Identifying tags from question text
Predicting whether questions will be upvoted, downvoted, or closed based on their text
Predicting how long questions will take to answer
License
All Stack Overflow user contributions are licensed under CC-BY-SA 3.0 with attribution required."
Hotel Reviews,"A list of 1,000 hotels and their online reviews.",Datafiniti,25,"Version 1,2017-05-29","databases
hotels
linguistics
internet",CSV,16 MB,CC4,"10,556 views","1,476 downloads",24 kernels,3 topics,https://www.kaggle.com/datafiniti/hotel-reviews,"About This Data
This is a list of 1,000 hotels and their reviews provided by Datafiniti's Business Database. The dataset includes hotel location, name, rating, review data, title, username, and more.
What You Can Do With This Data
You can use this data to compare hotel reviews on a state-by-state basis; experiment with sentiment scoring and other natural language processing techniques. The review data lets you correlate keywords in the review text with ratings. E.g.:
What are the bottom and top states for hotel reviews by average rating?
What is the correlation between a state’s population and their number of hotel reviews?
What is the correlation between a state’s tourism budget and their number of hotel reviews?
Data Schema
A full schema for the data is available in our support documentation.
About Datafiniti
Datafiniti provides instant access to web data. We compile data from thousands of websites to create standardized databases of business, product, and property information. Learn more.
Want More?
You can get more data like this by joining Datafiniti or requesting a demo."
Human Development Report 2015,"Countries ranked by human development, gender inequality, and poverty",United Nations Development Program,25,"Version 1,2017-01-25","demographics
international relations",CSV,270 KB,CC4,"7,245 views","1,587 downloads",15 kernels,0 topics,https://www.kaggle.com/undp/human-development,"Content
The Human Development Index (HDI) is a summary measure of achievements in key dimensions of human development: a long and healthy life, access to knowledge, and a decent standard of living. The HDI is the geometric mean of normalized indices for each of the three dimensions. The health dimension is assessed by life expectancy at birth, the education dimension is measured by mean of years of education for adults aged 25 years and more and expected years of education for children, and the standard of living dimension is measured by gross national income per capita. The Inequality-Adjusted Human Development Index (IHDI) adjusts the HDI for inequality in the distribution of each dimension across the population.
The Gender Development Index (GDI) measures gender inequalities in achievement in three basic dimensions of human development: health, measured by female and male life expectancy at birth; education, measured by female and male expected years of education for children and female and male mean years of education for adults ages 25 and older; and command over economic resources, measured by female and male estimated earned income. The Gender Inequality Index (GII) reflects gender-based disadvantage in three dimensions—reproductive health, empowerment, and the labour market—for as many countries as data of reasonable quality allow. It shows the loss in potential human development due to inequality between female and male achievements in these dimensions.
The Multidimensional Poverty Index (MPI) identifies multiple deprivations at the household level in education, health, and standard of living as indicators of poverty. It uses micro data from household surveys, and — unlike the IHDI — all the indicators needed to construct the measure must come from the same survey."
2016 Global Ecological Footprint,Does your country consume more resources than it produces in a year?,Global Footprint Network,25,"Version 1,2017-03-02",ecology,CSV,22 KB,CC4,"5,878 views",704 downloads,8 kernels,0 topics,https://www.kaggle.com/footprintnetwork/ecological-footprint,"Context
The ecological footprint measures the ecological assets that a given population requires to produce the natural resources it consumes (including plant-based food and fiber products, livestock and fish products, timber and other forest products, space for urban infrastructure) and to absorb its waste, especially carbon emissions. The footprint tracks the use of six categories of productive surface areas: cropland, grazing land, fishing grounds, built-up (or urban) land, forest area, and carbon demand on land.
A nation’s biocapacity represents the productivity of its ecological assets, including cropland, grazing land, forest land, fishing grounds, and built-up land. These areas, especially if left unharvested, can also absorb much of the waste we generate, especially our carbon emissions.
Both the ecological footprint and biocapacity are expressed in global hectares — globally comparable, standardized hectares with world average productivity.
If a population’s ecological footprint exceeds the region’s biocapacity, that region runs an ecological deficit. Its demand for the goods and services that its land and seas can provide — fruits and vegetables, meat, fish, wood, cotton for clothing, and carbon dioxide absorption — exceeds what the region’s ecosystems can renew. A region in ecological deficit meets demand by importing, liquidating its own ecological assets (such as overfishing), and/or emitting carbon dioxide into the atmosphere. If a region’s biocapacity exceeds its ecological footprint, it has an ecological reserve.
Acknowledgements
The ecological footprint measure was conceived by Mathis Wackernagel and William Rees at the University of British Columbia. Ecological footprint data was provided by the Global Footprint Network.
Inspiration
Is your country running an ecological deficit, consuming more resources than it can produce per year? Which countries have the greatest ecological deficits or reserves? Do they consume less or produce more than the average country? When will Earth Overshoot Day, the day on the calendar when humanity has used one year of natural resources, occur in 2017?"
Bitcoin Price Prediction (LightWeight CSV),Build Model from Market Data,Team AI,25,"Version 2,2017-08-13|Version 1,2017-08-13","time series
finance",CSV,109 KB,CC0,"11,437 views","1,197 downloads",18 kernels,4 topics,https://www.kaggle.com/team-ai/bitcoin-price-prediction,"Context
Coming Soon
Content
Coming Soon
Acknowledgements
This data is taken from coinmarketcap and it is free to use the data. https://coinmarketcap.com/
Warning
実際の取引にこの情報を使うときは十分ご注意ください。弊社およびコミュニティメンバーは損失の責任を取ることができません。"
Individual Income Tax Statistics,Summaries of individual income tax returns by zip code,Internal Revenue Service,25,"Version 2,2017-09-06|Version 1,2017-08-16","finance
government",CSV,838 MB,Other,"3,861 views",583 downloads,,,https://www.kaggle.com/irs/individual-income-tax-statistics,"ZIP Code data show selected income and tax items classified by State, ZIP Code, and size of adjusted gross income. Data are based on individual income tax returns filed with the IRS. The data include items, such as:
Number of returns, which approximates the number of households
Number of personal exemptions, which approximates the population
Adjusted gross income
Wages and salaries
Dividends before exclusion
Interest received
Content
For details of the exact fields available, please see the field_definitions.csv. Please note that the exact fields available can change from year to year, this definitions file was generated by retaining only the most recent year's entry from the years which had pdf manuals. The associated IRS form numbers are the most likely to change over time.
Acknowledgements
This data was generated by the Internal Revenue Service."
GitHub Issues,GitHub issue titles and descriptions for NLP analysis.,David Shinn,25,"Version 1,2018-01-18","linguistics
computers",CSV,980 MB,Other,"1,747 views",192 downloads,4 kernels,0 topics,https://www.kaggle.com/davidshinn/github-issues,"Description
Over 8 million GitHub issue titles and descriptions from 2017. Prepared from instructions at How To Create Data Products That Are Magical Using Sequence-to-Sequence Models.
Original Source
The data was adapted from GitHub data accessible from GitHub Archive. The constructocat image is from https://octodex.github.com/constructocat-v2.
License
MIT License
Copyright (c) 2018 David Shinn
Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:
The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
Favicons,"Image data and metadata for 360,000 favicons scraped from popular websites",ColinMorris,25,"Version 6,2017-11-15|Version 5,2017-08-23|Version 4,2017-08-23|Version 3,2017-08-23|Version 2,2017-08-23|Version 1,2017-08-23","internet
image data",Other,837 MB,ODbL,"5,940 views",448 downloads,3 kernels,2 topics,https://www.kaggle.com/colinmorris/favicons,"Context
Favicons are the (usually tiny) image files that browsers may use to represent websites in tabs, in the URL bar, or for bookmarks. Kaggle, for example, uses an image of a blue lowercase ""k"" as its favicon. This dataset contains about 360,000 favicons from popular websites.
Content and Acknowledgements
These favicons were scraped in July 2016. I wrote a crawler that went through Alexa's top 1 million sites, and made a request for 'favicon.ico' at the site root. If I got a 200 response code, I saved the result as ${site_url}.ico. For domains that were identical but for the TLD (e.g. google.com, google.ca, google.jp...), I scraped only one favicon. My scraping/cleaning code is on GitHub here.
Of 1m sites crawled, 540k responded with a 200 code. The dataset has 360k images, which were the remains after filtering out:
empty files (-140k)
non-image files, according to the file command (-40k). These mostly had type HTML, ASCII, or UTF-*.
corrupt/malformed image files - i.e. those that were sufficiently messed up that ImageMagick failed to parse them. (-1k)
The remaining files are exactly as I received them from the site. They are mostly ICO files, with the most common sizes being 16x16, 32x32, and 48x48. But there's a long tail of more exotic formats and sizes (there is at least one person living among us who thought that 88x31 was a fine size for a favicon).
The favicon files are divided among 6 zip files, full-0.zip, full-1.zip... full-5.zip. (If you wish to download the full dataset as a single tarball, you can do so from the Internet Archive)
favicon_metadata.csv is a csv file with one row per favicon in the dataset. The split_index says which of the zip files the image landed in. For an example of loading and interacting with particular favicons in a kernel context, check out the Favicon helper functions kernel.
As mentioned above, the full dataset is a dog's breakfast of different file formats and dimensions. I've created 'standardized' subsets of the data that may be easier to work with (particularly for machine learning applications, where it's necessary to have fixed dimensions).
16_16.tar.gz is a tarball containing all 16x16 favicons in the dataset, converted to PNG. It has 290k images. ICO is a container format, and many of the ico files in the raw dataset contain several versions of the same favicon at different resolutions. 16x16 favicons that were stuffed together in an ICO file with images of other sizes are included in this set. But I did no resizing - if a favicon has no 'native' 16x16 version, it isn't in this set.
16_16_distinct.tar.gz is identical to the above, but with 70k duplicate or near-duplicate images removed. There are a small number of commonly repeated favicons like the Blogger ""B"" that occur thousands of times, which could be an annoyance depending on the use case - e.g. a generative model might get stuck in a local maximum of spitting out Blogger Bs.
Alexa's top 1-million list includes 'adult' sites, so some URLs and favicons may be NSFW or offensive. (It's pretty hard to make a credible depiction of nudity in 256 pixels, but there are some occasional attempts.)
Inspiration
I hope this dataset might be especially useful for small-scale deep learning experiments. Scaling photographs down to 16x16 would render many of them unintelligible, but these favicons were born tiny. The 16_16 fold has more instances than MNIST, and the images are even smaller! (Though, unlike MNIST, most of the images in this dataset are not grayscale.)
If you liked this, you should also check out the recently released Large Logo Dataset. They've currently made available 550k favicons resized to 32x32. Their data was collected more recently, and their scraping process was more robust, so their dataset should probably be preferred (though you might still want to use this one if you need the raw favicon files, or if you prefer to use 16x16 non-resized images)."
1000 Cameras Dataset,Data describing 1000 cameras in 13 properties,Chris Crawford,25,"Version 2,2017-10-25|Version 1,2017-08-19",electronics,CSV,85 KB,CC3,"3,657 views",648 downloads,103 kernels,0 topics,https://www.kaggle.com/crawford/1000-cameras-dataset,"Context
Some camera enthusiast went and described 1,000 cameras based on 13 properties!
Content
Row one describes the datatype for each column and can probably be removed.
The 13 properties of each camera:
Model
Release date
Max resolution
Low resolution
Effective pixels
Zoom wide (W)
Zoom tele (T)
Normal focus range
Macro focus range
Storage included
Weight (inc. batteries)
Dimensions
Price
Acknowledgements
These datasets have been gathered and cleaned up by Petra Isenberg, Pierre Dragicevic and Yvonne Jansen. The original source can be found here.
This dataset has been converted to CSV."
Google Project Sunroof,Solar Panel Power Consumption Offset Estimates,Jacob Boysen,25,"Version 4,2017-09-12|Version 3,2017-09-11|Version 2,2017-09-09|Version 1,2017-09-06",,CSV,44 MB,Other,"4,838 views",523 downloads,,3 topics,https://www.kaggle.com/jboysen/google-project-sunroof,"Context:
As the price of installing solar has gotten less expensive, more homeowners are turning to it as a possible option for decreasing their energy bill. We want to make installing solar panels easy and understandable for anyone. Project Sunroof puts Google's expansive data in mapping and computing resources to use, helping calculate the best solar plan for you.
Content:
See metadata for indepth description. Data is at census-tract level. Project Sunroof computes how much sunlight hits your roof in a year. It takes into account: Google's database of imagery and maps 3D modeling of your roof Shadows cast by nearby structures and trees All possible sun positions over the course of a year Historical cloud and temperature patterns that might affect solar energy production
Acknowledgements:
Data was compiled by Google Project Sunroof. You can use Kernels to analyze, share, and discuss this data on Kaggle, but if you’re looking for real-time updates and bigger data, check out the data on BigQuery, too.
Inspiration:
Which tracts have the highest potential possible coverage? Carbon offsets?
Which tracts have the highest estimated solar panel utilization? As a percent of carbon offsets?
If you want more energy data, check out 30 Years of European Wind Generation and 30 Years of European Solar Generation."
New Car Sales in Norway,Monthly car sales for 2007-2017 by make and most popular models,dmi3kno,24,"Version 2,2017-02-19|Version 1,2017-02-14","business
automobiles",CSV,229 KB,Other,"12,553 views","2,301 downloads",22 kernels,,https://www.kaggle.com/dmi3kno/newcarsalesnorway,"Context
On the morning of 10 January 2017, Opplysningsrådet for Veitrafikken (OFV), Norwegian road association, held a business breakfast for its member organizations, where they presented the annual presentation under the title ""Car Year 2016. Status and trends"" (Bilåret 2016 – status og trender). Among the highlights for the year, OFV reported all-time-high sales of electric cars, with fully electric and plug-in hybrid cars accounting for 40,2% of all new car sales (compare to 7.4% for Sweden and 3.6% for Denmark). No other country in the world has this level of popularity of battery-equipped vehicles! In November 2016, 12 out of 15 most popular cars sold in Norway were either hybrids of fully electric vehicles with BWM-i3 snapping the title as the most popular car in Norway, ahead of undisputed leader of the last decade VW Golf (including eGolf), according to bilnorge.no. Among 10 most popular cars for the year, OFV reported, there was only one(!) fossil fuel vehicle.
OFV makes annual forecast of new passenger car sales. Short summary of their methodology:
Based on OFV statistics over several years
Taking into account the actual monthly figures for the last four years
Actual same-month sales for the previous year is combined with the average for the eight previous months, weighed by the month's proportion in a year, adjusted by year's actual sales compared with those of the last year.
OFV forecast for 2016 was 157 500 new passenger cars. Actual sales were 154 603 cars. Applying the same model for 2017, OFV forecasts 152 400 new passenger cars to be sold in Norway.
Content
Dataset includes two tables:
1) Monthly sales of new passenger cars by make (manufacturer brand) - norway_new_car_sales_by_make.csv
Year - year of sales
Month - month of sales
Make - car make (e.g. Volkswagen, Toyota, Tesla)
Quantity - number of units sold
Pct - percent share in monthly total
2) Monthly summary of top-20 most popular models (by make and model) - norway_new_car_sales_by_model.csv
Year - year of sales
Month - month of sales
Make - car make (e.g. Volkswagen, Toyota, Tesla)
Model - car model (e.g. BMW-i3, Volkswagen Golf, Tesla S75)
Quantity - number of units sold
Pct - percent share in monthly total
3) Summary stats for car sales in Norway by month - norway_new_car_sales_by_month.csv
Year - year of sales
Month - month of sales
Quantity - total number of units sold
Quantity_YoY - change YoY in units
Import - total number of units imported (used cars)
Import_YoY - change YoY in units
Used - total number of units owner changes inside the country (data available from 2012)
Used_YoY - change YoY in units
Avg_CO2 - average CO2 emission of all cars sold in a given month (in g/km)
Bensin_CO2 - average CO2 emission of bensin-fueled cars sold in a given month (in g/km)
Diesel_CO2 - average CO2 emission of diesel-fueled cars sold in a given month (in g/km)
Quantity_Diesel - number of diesel-fueled cars sold in the country in a given month
Diesel_Share - share of diesel cars in total sales (Quantity_Diesel / Quantity)
Diesel_Share_LY - share of diesel cars in total sales a year ago
Quantity_Hybrid - number of new hybrid cars sold in the country (both PHEV and BV)
Quantity_Electric - number of new electric cars sold in the country (zero emission vehicles)
Import_Electric - number of used electric cars imported to the country (zero emission vehicles)
Note: The numbers on sales of hybrid and electric cars is unavailable prior to 2011.
Data is complied from monthly tables published on OFV website (example here). Additional datapoints added from summary tables published on dinside.no
Acknowledgements
Opplysningsrådet for Veitrafikken (OFV) is a politically independent membership organization that works to get politicians and authorities to build safer and more efficient roads in Norway. The organization has about 60 members, representing different types of road users. Members are leading players in road safety, car owner associations, public transportation companies, shippers, car dealers, oil companies, banking, finance and insurance, road builders and general contractors.
Site: http://www.ofvas.no and http://www.ofv.no
Monthly summary statistics and market news: http://www.dinside.no/emne/bilsalget and http://statistikk.ofv.no/ofv_bilsalg_small.asp
Detailed sales per model: http://www.ofvas.no/co2-utslippet/category406.html (using http://www.newocr.com/)
Inspiration
1) How did Norway get here? When did they start on the journey towards electric-powered vehicles and what might have contributed? 2) Did you now that until recently (September 2016), Norway has been second most important market for Tesla Motors (after US)? 3) Can you beat the forecast accuracy of OFV for 2016 and produce a better estimate for 2017?"
Import and Export by India from 2014 to 2017,Commodity & country wise annual import and export data.,Rajanand Ilangovan / இராஜ்ஆனந்த் இளங்கோவன்,24,"Version 1,2017-08-05","india
business
industry",CSV,5 MB,CC4,"4,874 views",711 downloads,8 kernels,3 topics,https://www.kaggle.com/rajanand/import-and-export-by-india,"Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks.
Context
To better understand the imports and exports by India and how it changed in 3 years.
Content
Import and export data available by principle commodity and country wise for 3 years from Apr'2014 to Mar'2017.
Column Descriptions
pc_code: Integer, Principal Commodity Code
pc: String, Principal Commodity Name
unit: String, measurement of quantity
country_code: Integer, country code
country_name: String, country name
quantity: Integer, quantify of export or import
value: Integer, monetary valeu of the quantity (in million USD)
Acknowledgements
Ministry of Commerce and Industry, Govt of India has published these datasets in Open Govt Data Platform India portal under Govt. Open Data License - India.
Inspiration
Some of questions I would like to be answered are
Top countries by growth percentage.
Top commodity by quantity or value.
YoY growth of export and import."
Toxic Armories,Data from The Oregonian's investigation,Anton Prokopyev,24,"Version 1,2017-02-21","environment
military",CSV,206 KB,CC0,"1,824 views",127 downloads,8 kernels,0 topics,https://www.kaggle.com/prokopyev/armories,"Context
According to The Oregonian hundreds of National Guard armories across the U.S. may have been contaminated with lead from indoor firing ranges. It was reported that areas populated by children under 7 years of age should have less than 40 micrograms of lead per square foot.
Content
The Oregonian collected over 23,000 pages of public records following a Freedom Of Information Act request. The dataset covers armory inspections conducted since 2012 and may facilitate investigation of lead contamination in the U.S.
Acknowledgements
The data assembly process is described by Melissa Lewis here.
Inspiration
This dataset can be used to conduct research in the realm of public health. It will be especially useful if 1) you know about health effects of exposure to lead in relatively short terms periods; 2) you are able to find relevant health data to conduct a study on lead poisoning."
CartolaFC,Data from the popular Brazilian Fantasy Football (2014 to 2017) ⚽️,Luiz Gustavo Schiller,24,"Version 5,2017-08-09|Version 4,2017-08-01|Version 3,2017-07-26|Version 2,2017-07-22|Version 1,2017-07-18","association football
brazil
sports",CSV,8 MB,CC0,"2,895 views",440 downloads,3 kernels,,https://www.kaggle.com/schiller/cartolafc,"Context
CartolaFC is the most popular fantasy football in Brazil. Before each round of the Brazilian Football League, players choose which athletes they want for their teams, and they score points based on their real-life performances.
Content
Data is divided in 7 kinds of files:
Athletes (atletas)
""atleta_id"": id,
""nome"": athlete's full name,
""apelido"": athlete's nickname
Clubs (clubes)
""id"": id,
""nome"": club's name,
""abreviacao"": name abbreviation,
""slug"": used for some API calls
Matches (partidas)
""rodada_id"": current round,
""clube_casa_id"": home team id,
""clube_visitante_id"": away team id,
""clube_casa_posicao"": home team's position on the league,
""clube_visitante_posicao"": away team's position on the league,
""aproveitamento_mandante"": home team's outcome on the last five matches (d: loss, e: draw, v: victory),
""aproveitamento_visitante"": away team's outcome on the last five matches (d: loss, e: draw, v: victory),
""placar_oficial_mandante"": home team's score,
""placar_oficial_visitante"": away team's score,
""partida_data"": match date,
""local"": stadium,
""valida"": match valid for scoring
Scouts
""atleta_id"": reference to athlete,
""rodada_id"": current round,
""clube_id"": reference to club,
""posicao_id"": reference to position,
""status_id"": reference to status,
""pontos_num"": points scored on current round,
""preco_num"": current price,
""variacao_num"": price variation from previous round,
""media_num"": average points per played round,
""jogos_num"": number of matches played,
""FS"": suffered fouls,
""PE"": missed passes,
""A"": assistances,
""FT"": shots on the post,
""FD"": defended shots,
""FF"": shots off target,
""G"": goals,
""I"": offsides,
""PP"": missed penalties,
""RB"": successful tackes,
""FC"": fouls commited,
""GC"": own goals,
""CA"": yellow cards,
""CV"": red cards,
""SG"": clean sheets (only defenders),
""DD"": difficult defenses (only goalies),
""DP"": defended penalties (only goalies),
""GS"": suffered goals (only goalies)
Positions (posicoes)
""id"": id,
""nome"": name,
""abreviacao"": abbreviation
Status
""id"": id,
""nome"": name
Points (pontuacao)
""abreviacao"": abbreviation,
""nome"": name,
""pontuacao"": points earned for respective scout
Acknowledgements
The datasets from 2014 to 2016 were taken from here: https://github.com/thevtm/CartolaFCDados.
Data from 2017 until round 11 was taken from this repo: https://github.com/henriquepgomide/caRtola.
From 2017 round 12 and on, I've been extracting the data from CartolaFC's API (which is not officially public).
Inspiration
It would be interesting to see analyses on which factors make an athlete or team more likely to score points, and also predictive models for future scores."
Fall Detection Data from China,Activity of elderly patients along with their medical information,MACHINE LEARNING DATASETS,24,"Version 1,2017-04-20","gerontology
health",CSV,611 KB,Other,"8,518 views",843 downloads,12 kernels,2 topics,https://www.kaggle.com/pitasr/falldata,"Falls among the elderly is an important health issue. Fall detection and movement tracking are therefore instrumental in addressing this issue. This paper responds to the challenge of classifying different movements as a part of a system designed to fulfill the need for a wearable device to collect data for fall and near-fall analysis. Four different fall trajectories (forward, backward, left and right), three normal activities (standing, walking and lying down) and near-fall situations are identified and detected.
Falls are a serious public health problem and possibly life threatening for people in fall risk groups. We develop an automated fall detection system with wearable motion sensor units fitted to the subjects’ body at six different positions. Each unit comprises three tri-axial devices (accelerometer, gyroscope, and magnetometer/compass). Fourteen volunteers perform a standardized set of movements including 20 voluntary falls and 16 activities of daily living (ADLs), resulting in a large dataset with 2520 trials. To reduce the computational complexity of training and testing the classifiers, we focus on the raw data for each sensor in a 4 s time window around the point of peak total acceleration of the waist sensor, and then perform feature extraction and reduction.
We successfully distinguish falls from ADLs using six machine learning techniques (classifiers): the k-nearest neighbor (k-NN) classifier, least squares method (LSM), support vector machines (SVM), Bayesian decision making (BDM), dynamic time warping (DTW), and artificial neural networks (ANNs). We compare the performance and the computational complexity of the classifiers and achieve the best results with the k-NN classifier and LSM, with sensitivity, specificity, and accuracy all above 95%. These classifiers also have acceptable computational requirements for training and testing. Our approach would be applicable in real-world scenarios where data records of indeterminate length, containing multiple activities in sequence, are recorded.
If you are using this dataset don't forget to cite
Özdemir, Ahmet Turan, and Billur Barshan. “Detecting Falls with Wearable Sensors Using Machine Learning Techniques.” Sensors (Basel, Switzerland) 14.6 (2014): 10691–10708. PMC. Web. 23 Apr. 2017."
MRI and Alzheimers,Magnetic Resonance Imaging Comparisons of Demented and Nondemented Adults,Jacob Boysen,24,"Version 1,2017-08-17","healthcare
neurology
health sciences
+ 2 more...",CSV,49 KB,CC0,"6,550 views",744 downloads,4 kernels,0 topics,https://www.kaggle.com/jboysen/mri-and-alzheimers,"Context:
The Open Access Series of Imaging Studies (OASIS) is a project aimed at making MRI data sets of the brain freely available to the scientific community. By compiling and freely distributing MRI data sets, we hope to facilitate future discoveries in basic and clinical neuroscience. OASIS is made available by the Washington University Alzheimer’s Disease Research Center, Dr. Randy Buckner at the Howard Hughes Medical Institute (HHMI)( at Harvard University, the Neuroinformatics Research Group (NRG) at Washington University School of Medicine, and the Biomedical Informatics Research Network (BIRN).
Content:
Cross-sectional MRI Data in Young, Middle Aged, Nondemented and Demented Older Adults: This set consists of a cross-sectional collection of 416 subjects aged 18 to 96. For each subject, 3 or 4 individual T1-weighted MRI scans obtained in single scan sessions are included. The subjects are all right-handed and include both men and women. 100 of the included subjects over the age of 60 have been clinically diagnosed with very mild to moderate Alzheimer’s disease (AD). Additionally, a reliability data set is included containing 20 nondemented subjects imaged on a subsequent visit within 90 days of their initial session.
Longitudinal MRI Data in Nondemented and Demented Older Adults: This set consists of a longitudinal collection of 150 subjects aged 60 to 96. Each subject was scanned on two or more visits, separated by at least one year for a total of 373 imaging sessions. For each subject, 3 or 4 individual T1-weighted MRI scans obtained in single scan sessions are included. The subjects are all right-handed and include both men and women. 72 of the subjects were characterized as nondemented throughout the study. 64 of the included subjects were characterized as demented at the time of their initial visits and remained so for subsequent scans, including 51 individuals with mild to moderate Alzheimer’s disease. Another 14 subjects were characterized as nondemented at the time of their initial visit and were subsequently characterized as demented at a later visit.
Acknowledgements:
When publishing findings that benefit from OASIS data, please include the following grant numbers in the acknowledgements section and in the associated Pubmed Central submission: P50 AG05681, P01 AG03991, R01 AG021910, P20 MH071616, U24 RR0213
Inspiration:
Can you predict dementia? Alzheimer’s?"
H1B Disclosure Dataset,H1B Disclosure Dataset - Predicting the Case Status,Charmi,24,"Version 1,2018-01-01","united states
mining
artificial intelligence
machine learning",Other,42 MB,CC4,"2,616 views",408 downloads,,0 topics,https://www.kaggle.com/trivedicharmi/h1b-disclosure-dataset,"Project Description:
1) Data Background
In the Data Mining class, we had the opportunity to analyze data by performing data mining algorithms to a dataset. Our dataset is from Office of Foreign Labor Certification (OFLC). OFLC is a division of the U.S. Department of Labor. The main duty of OFLC is to assist the Secretary of Labor to enforce part of the Immigration and Nationality Act (INA), which requires certain labor conditions exist before employers can hire foreign workers. H-1B is a visa category in the United States of America under the INA, section 101(a)(15)(H) which allows U.S. employers to employ foreign workers. The first step employer must take to hire a foreign worker is to file the Labor Condition Application. In this project, we will analyze the data from the Labor Condition Application.
1.1) Introduction to H1B Dataset
The H-1B Dataset selected for this project contains data from employer’s Labor Condition Application and the case certification determinations processed by the Office of Foreign Labor Certification (OFLC) where the date of the determination was issues on or after October 1, 2016 and on or before June 30, 2017.
The Labor Condition Application (LCA) is a document that a perspective H-1B employer files with U.S. Department of Labor Employment and Training Administration (DOLETA) when it seeks to employ non-immigrant workers at a specific job occupation in an area of intended employment for not more than three years.
1.2) Goal of the Project
Our goal for this project is to predict the case status of an application submitted by the employer to hire non-immigrant workers under the H-1B visa program. Employer can hire non-immigrant workers only after their LCA petition is approved. The approved LCA petition is then submitted as part of the Petition for a Non-immigrant Worker application for work authorizations for H-1B visa status.
We want to uncover insights that can help employers understand the process of getting their LCA approved. We will use WEKA software to run data mining algorithms to understand the relationship between attributes and the target variable.
2)Dataset Information:
a) Source: Office of Foreign Labor Certification, U.S. Department of Labor Employment and Training Administration
b) List Link: https://www.foreignlaborcert.doleta.gov/performancedata.cfm
c) Dataset Type: Record – Transaction Data
d) Number of Attributes: 40
e) Number of Instances: 528,147
f) Date Created: July 2017
3) Attribute List:
The detailed description of each attribute below is given in the Record Layout file available in the zip folder H1B Disclosure Dataset Files.
The H-1B dataset from OFLC contained 40 attributes and 528,147 instances. The attributes are in the table below. The attributes highlighted bold were removed during the data cleaning process.
1) CASE_NUMBER
2)CASE_SUBMITTED
3)DECISION_DATE
4)VISA_CLASS
5)EMPLOYMENT_START_DATE
6)EMPLOYMENT_END_DATE
7)EMPLOYER_NAME
8)EMPLOYER_ADDRESS
9)EMPLOYER_CITY
10)EMPLOYER_STATE
11)EMPLOYER_POSTAL_CODE
12)EMPLOYER_COUNTRY
13)EMPLOYER_PROVINCE
14)EMPLOYER_PHONE
15)EMPLOYER_PHONE_EXT
16)AGENT_ATTORNEY_NAME
17)AGENT_ATTORNEY_CITY
18)AGENT_ATTORNEY_STATE
19)JOB_TITLE
20)SOC_CODE
21)SOC_NAME
22)NAICS_CODE
23)TOTAL_WORKERS
24)FULL_TIME_POSITION
25)PREVAILING_WAGE
26)PW_UNIT_OF_PAY
27)PW_SOURCE
28)PW_SOURCE_YEAR
29)PW_SOURCE_OTHER
30)WAGE_RATE_OF_PAY_FROM
31)WAGE_RATE_OF_PAY_TO
32)WAGE_UNIT_OF_PAY
33)H-1B_DEPENDENT
34) WILLFUL_VIOLATOR
35) WORKSITE_CITY
36)WORKSITE_COUNTY
37)WORKSITE_STATE
38)WORKSITE_POSTAL_CODE
39)ORIGINAL_CERT_DATE
40)CASE_STATUS* - __Class Attribute - To be predicted
3.1) Class Attribute
For the H-1B Dataset our class attribute is ‘CASE_STATUS’. There are 4 categories of Case Status. The values of Case_Status attributes are:
1) Certified
2) Certified_Withdrawn
3) Withdrawn
4) Denied
Certified means the LCA of an employer was approved. Certified Withdrawn means the case was withdrawn after it was certified by OFLC. Withdrawn means the case was withdrawn by the employer. Denied means the case was denied OFLC."
Breast Histopathology Images,IDC vs non-IDC classification,paultimothymooney,24,"Version 1,2017-12-19","medicine
machine learning
image data
binary classification",Other,2 GB,CC0,"3,731 views",351 downloads,,,https://www.kaggle.com/paultimothymooney/breast-histopathology-images,"Context
Invasive Ductal Carcinoma (IDC) is the most common subtype of all breast cancers. To assign an aggressiveness grade to a whole mount sample, pathologists typically focus on the regions which contain the IDC. As a result, one of the common pre-processing steps for automatic aggressiveness grading is to delineate the exact regions of IDC inside of a whole mount slide.
Content
The original dataset consisted of 162 whole mount slide images of Breast Cancer (BCa) specimens scanned at 40x. From that, 277,524 patches of size 50 x 50 were extracted (198,738 IDC negative and 78,786 IDC positive). Each patch’s file name is of the format: u_xX_yY_classC.png — > example 10253_idx5_x1351_y1101_class0.png . Where u is the patient ID (10253_idx5), X is the x-coordinate of where this patch was cropped from, Y is the y-coordinate of where this patch was cropped from, and C indicates the class where 0 is non-IDC and 1 is IDC.
Acknowledgements
The original files are located here: http://gleason.case.edu/webdata/jpi-dl-tutorial/IDC_regular_ps50_idx5.zip Citation: https://www.ncbi.nlm.nih.gov/pubmed/27563488 and http://spie.org/Publications/Proceedings/Paper/10.1117/12.2043872
Inspiration
Breast cancer is the most common form of cancer in women, and invasive ductal carcinoma (IDC) is the most common form of breast cancer. Accurately identifying and categorizing breast cancer subtypes is an important clinical task, and automated methods can be used to save time and reduce error."
Indian Liver Patient Records,"Patient records collected from North East of Andhra Pradesh, India",UCI Machine Learning,24,"Version 1,2017-09-20","healthcare
health sciences
health
medicine",CSV,23 KB,CC0,"5,633 views",724 downloads,7 kernels,2 topics,https://www.kaggle.com/uciml/indian-liver-patient-records,"Context
Patients with Liver disease have been continuously increasing because of excessive consumption of alcohol, inhale of harmful gases, intake of contaminated food, pickles and drugs. This dataset was used to evaluate prediction algorithms in an effort to reduce burden on doctors.
Content
This data set contains 416 liver patient records and 167 non liver patient records collected from North East of Andhra Pradesh, India. The ""Dataset"" column is a class label used to divide groups into liver patient (liver disease) or not (no disease). This data set contains 441 male patient records and 142 female patient records.
Any patient whose age exceeded 89 is listed as being of age ""90"".
Columns:
Age of the patient
Gender of the patient
Total Bilirubin
Direct Bilirubin
Alkaline Phosphotase
Alamine Aminotransferase
Aspartate Aminotransferase
Total Protiens
Albumin
Albumin and Globulin Ratio
Dataset: field used to split the data into two sets (patient with liver disease, or no disease)
Acknowledgements
This dataset was downloaded from the UCI ML Repository:
Lichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.
Inspiration
Use these patient records to determine which patients have liver disease and which ones do not."
Health Insurance Coverage,Coverage rates before and after the Affordable Care Act,US Department of Health and Human Services,24,"Version 1,2017-03-03",health,CSV,5 KB,CC0,"10,174 views","1,273 downloads",13 kernels,2 topics,https://www.kaggle.com/hhs/health-insurance,"Context
The Affordable Care Act (ACA) is the name for the comprehensive health care reform law and its amendments which addresses health insurance coverage, health care costs, and preventive care. The law was enacted in two parts: The Patient Protection and Affordable Care Act was signed into law on March 23, 2010 by President Barack Obama and was amended by the Health Care and Education Reconciliation Act on March 30, 2010.
Content
This dataset provides health insurance coverage data for each state and the nation as a whole, including variables such as the uninsured rates before and after Obamacare, estimates of individuals covered by employer and marketplace healthcare plans, and enrollment in Medicare and Medicaid programs.
Acknowledgements
The health insurance coverage data was compiled from the US Department of Health and Human Services and US Census Bureau.
Inspiration
How has the Affordable Care Act changed the rate of citizens with health insurance coverage? Which states observed the greatest decline in their uninsured rate? Did those states expand Medicaid program coverage and/or implement a health insurance marketplace? What do you predict will happen to the nationwide uninsured rate in the next five years?"
US Population By Zip Code,For both 2000 and 2010,US Census Bureau,24,"Version 1,2017-06-28",demographics,CSV,112 MB,Other,"7,880 views",843 downloads,,2 topics,https://www.kaggle.com/census/us-population-by-zip-code,"Content
The United States census count (also known as the Decennial Census of Population and Housing) is a count of every resident of the US. The census occurs every 10 years and is conducted by the United States Census Bureau. Census data is publicly available through the census website, but much of the data is available in summarized data and graphs. The raw data is often difficult to obtain, is typically divided by region, and it must be processed and combined to provide information about the nation as a whole. The United States census dataset includes nationwide population counts from the 2000 and 2010 censuses. Data is broken out by gender, age and location using zip code tabular areas (ZCTAs) and GEOIDs. ZCTAs are generalized representations of zip codes, and often, though not always, are the same as the zip code for an area. GEOIDs are numeric codes that uniquely identify all administrative, legal, and statistical geographic areas for which the Census Bureau tabulates data. GEOIDs are useful for correlating census data with other censuses and surveys.
Dataset Description
| geo_id | STRING | Geo code | |-------------|---------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| | minimum_age | INTEGER | The minimum age in the age range. If null, this indicates the row as a total for male, female, or overall population. | | maximum_age | INTEGER | The maximum age in the age range. If null, this indicates the row as having no maximum (such as 85 and over) or the row is a total of the male, female, or overall population. | | gender | STRING | male or female. If empty, the row is a total population summary. | | population | INTEGER | The total count of the population for this segment. |
Acknowledgements
This dataset was created by the United States Census Bureau.
Use this dataset with BigQuery
You can use Kernels to analyze, share, and discuss this data on Kaggle, but if you’re looking for real-time updates and bigger data, check out the data on BigQuery, too: https://cloud.google.com/bigquery/public-data/international-census."
Possible Asteroid Impacts with Earth,"Name, orbit, and year range for impacts predicted by Sentry system",NASA,24,"Version 1,2017-02-11","astronomy
space",CSV,2 MB,CC0,"7,140 views",786 downloads,18 kernels,,https://www.kaggle.com/nasa/asteroid-impacts,"Context
An asteroid's orbit is computed by finding the elliptical path about the sun that best fits the available observations of the object. That is, the object's computed path about the sun is adjusted until the predictions of where the asteroid should have appeared in the sky at several observed times match the positions where the object was actually observed to be at those same times. As more and more observations are used to further improve an object's orbit, we become more and more confident in our knowledge of where the object will be in the future.
When the discovery of a new near Earth asteroid is announced by the Minor Planet Center, Sentry automatically prioritizes the object for an impact risk analysis. If the prioritization analysis indicates that the asteroid cannot pass near the Earth or that its orbit is very well determined, the computationally intensive nonlinear search for potential impacts is not pursued. If, on the other hand, a search is deemed necessary then the object is added to a queue of objects awaiting analysis. Its position in the queue is determined by the estimated likelihood that potential impacts may be found.
Content
Sentry is a highly automated collision monitoring system that continually scans the most current asteroid catalog for possibilities of future impact with Earth over the next 100 years. This dataset includes the Sentry system's list of possible asteroid impacts with Earth and their probability, in addition to a list of all known near Earth asteroids and their characteristics.
Acknowledgements
The asteroid orbit and impact risk data was collected by NASA's Near Earth Object Program at the Jet Propulsion Laboratory (California Institute of Technology).
Inspiration
During which year is Earth at the highest risk of an asteroid impact? How do asteroid impact predictions change over time? Which possible asteroid impact would be the most devastating, given the asteroid's size and speed?"
Air Quality Annual Summary,A summary of air quality from 1987 to 2017,US Environmental Protection Agency,24,"Version 1,2017-06-30","environment
climate",CSV,948 MB,CC0,"4,891 views",600 downloads,,0 topics,https://www.kaggle.com/epa/air-quality,"Context:
The Environmental Protection Agency (EPA) creates air quality trends using measurements from monitors located across the country. All of this data comes from EPA’s Air Quality System (AQS). Data collection agencies report their data to the EPA via this system and it calculates several types of aggregate (summary) data for EPA internal use.
Content:
Field descriptions:
State Code: The FIPS code of the state in which the monitor resides.
County Code: The FIPS code of the county in which the monitor resides.
Site Num:A unique number within the county identifying the site.
Parameter Code: The AQS code corresponding to the parameter measured by the monitor.
POC: This is the “Parameter Occurrence Code” used to distinguish different instruments that measure the same parameter at the same site.
Latitude: The monitoring site’s angular distance north of the equator measured in decimal degrees.
Longitude: The monitoring site’s angular distance east of the prime meridian measured in decimal degrees.
Datum: The Datum associated with the Latitude and Longitude measures.
Parameter Name: The name or description assigned in AQS to the parameter measured by the monitor. Parameters may be pollutants or non-pollutants.
Sample Duration: The length of time that air passes through the monitoring device before it is analyzed (measured). So, it represents an averaging period in the atmosphere (for example, a 24-hour sample duration draws ambient air over a collection filter for 24 straight hours). For continuous monitors, it can represent an averaging time of many samples (for example, a 1-hour value may be the average of four one-minute samples collected during each quarter of the hour).
Pollutant Standard:A description of the ambient air quality standard rules used to aggregate statistics. (See description at beginning of document.)
Metric Used: The base metric used in the calculation of the aggregate statistics presented in the remainder of the row. For example, if this is Daily Maximum, then the value in the Mean column is the mean of the daily maximums.
Method Name: A short description of the processes, equipment, and protocols used in gathering and measuring the sample.
Year: The year the annual summary data represents.
Units of Measure: The unit of measure for the parameter. QAD always returns data in the standard units for the parameter. Submitters are allowed to report data in any unit and EPA converts to a standard unit so that we may use the data in calculations.
Event Type: Indicates whether data measured during exceptional events are included in the summary. A wildfire is an example of an exceptional event; it is something that affects air quality, but the local agency has no control over. No Events means no events occurred. Events Included means events occurred and the data from them is included in the summary. Events Excluded means that events occurred but data form them is excluded from the summary. Concurred Events Excluded means that events occurred but only EPA concurred exclusions are removed from the summary. If an event occurred for the parameter in question, the data will have multiple records for each monitor.
Observation Count: The number of observations (samples) taken during the year.
Observation Percent: The percent representing the number of observations taken with respect to the number scheduled to be taken during the year. This is only calculated for monitors where measurements are required (e.g., only certain parameters).
Completeness Indicator: An indication of whether the regulatory data completeness criteria for valid summary data have been met by the monitor for the year. Y means yes, N means no or that there are no regulatory completeness criteria for the parameter.
Valid Day Count: The number of days during the year where the daily monitoring criteria were met, if the calculation of the summaries is based on valid days.
Required Day Count: The number of days during the year which the monitor was scheduled to take samples if measurements are required.
Exceptional Data Count: The number of data points in the annual data set affected by exceptional air quality events (things outside the norm that affect air quality).
Null Data Count: The count of scheduled samples when no data was collected and the reason for no data was reported.
Primary Exceedance Count: The number of samples during the year that exceeded the primary air quality standard.
Secondary Exceedance Count: The number of samples during the year that exceeded the secondary air quality standard.
Certification Indicator: An indication whether the completeness and accuracy of the information on the annual summary record has been certified by the submitter. Certified means the submitter has certified the data (due May 01 the year after collection). Certification not required means that the parameter does not require certification or the deadline has not yet passed. Uncertified (past due) means that certification is required but is overdue. Requested but not yet concurred means the submitter has completed the process, but EPA has not yet acted to certify the data. Requested but denied means the submitter has completed the process, but EPA has denied the request for cause. Was certified but data changed means the data was certified but data was replaced and the process has not been repeated.
Num Obs Below MDL: The number of samples reported during the year that were below the method detection limit (MDL) for the monitoring instrument. Sometimes these values are replaced by 1/2 the MDL in summary calculations.
Arithmetic Mean: The average (arithmetic mean) value for the year.
Arithmetic Standard Dev: The standard deviation about the mean of the values for the year.
1st Max Value: The highest value for the year.
1st Max DateTime: The date and time (on a 24-hour clock) when the highest value for the year (the previous field) was taken.
2nd Max Value: The second highest value for the year.
2nd Max DateTime: The date and time (on a 24-hour clock) when the second highest value for the year (the previous field) was taken.
3rd Max Value: The third highest value for the year.
3rd Max DateTime: The date and time (on a 24-hour clock) when the third highest value for the year (the previous field) was taken.
4th Max Value: The fourth highest value for the year.
4th Max DateTime: The date and time (on a 24-hour clock) when the fourth highest value for the year (the previous field) was taken.
1st Max Non Overlapping Value: For 8-hour CO averages, the highest value of the year.
1st NO Max DateTime: The date and time (on a 24-hour clock) when the first maximum non overlapping value for the year (the previous field) was taken.
2nd Max Non Overlapping Value: For 8-hour CO averages, the second highest value of the year that does not share any hours with the 8-hour period of the first max non overlapping value.
2nd NO Max DateTime: The date and time (on a 24-hour clock) when the second maximum non overlapping value for the year (the previous field) was taken.
99th Percentile: The value from this monitor for which 99 per cent of the rest of the measured values for the year are equal to or less than.
98th Percentile: The value from this monitor for which 98 per cent of the rest of the measured values for the year are equal to or less than.
95th Percentile: The value from this monitor for which 95 per cent of the rest of the measured values for the year are equal to or less than.
90th Percentile: The value from this monitor for which 90 per cent of the rest of the measured values for the year are equal to or less than.
75th Percentile: The value from this monitor for which 75 per cent of the rest of the measured values for the year are equal to or less than.
50th Percentile: The value from this monitor for which 50 per cent of the rest of the measured values for the year are equal to or less than (i.e., the median).
10th Percentile: The value from this monitor for which 10 per cent of the rest of the measured values for the year are equal to or less than.
Local Site Name: The name of the site (if any) given by the State, local, or tribal air pollution control agency that operates it.
Address: The approximate street address of the monitoring site.
State Name: The name of the state where the monitoring site is located.
County Name: The name of the county where the monitoring site is located.
City Name: The name of the city where the monitoring site is located. This represents the legal incorporated boundaries of cities and not urban areas.
CBSA Name: The name of the core bases statistical area (metropolitan area) where the monitoring site is located.
Date of Last Change: The date the last time any numeric values in this record were updated in the AQS data system.
Acknowledgements:
These data come from the EPA. You can use Kernels to analyze, share, and discuss this data on Kaggle, but if you’re looking for real-time updates and bigger data, check out the data on Google BigQuery, too: https://cloud.google.com/bigquery/public-data/epa
Inspiration:
Within these data are tons of ways for you to learn about air pollution and how it can affect our health and environment. You can also compare key air emissions to gross domestic product, vehicle miles traveled, population, and energy consumption back to 1970. Best of all, you can check out air trends where you live!"
"Commercial Bank Failures, 1934-Present",Every bank failure in the United States since the Great Depression,Federal Deposit Insurance Corporation,24,"Version 1,2017-03-09","history
finance",CSV,396 KB,CC0,"7,024 views",888 downloads,11 kernels,,https://www.kaggle.com/fdic/bank-failures,"Content
This report lists each failure of a commercial bank, savings association, and savings bank since the establishment of the FDIC in 1933. Each record includes the institution name and FIN number, institution and charter types, location of headquarters (city and state), effective date, insurance fund and certificate number, failure transaction type, total deposits and total assets last reported prior to failure (in thousands of dollars), and the estimated cost of resolution. Data on estimated losses are not available for FDIC insured failures prior to 1986 or for FSLIC insured failures from 1934-88.
Acknowledgements
The bank failure report was downloaded from the FDIC website.
Inspiration
What type of banking institution is the most likely to fail? How have bank failure rates changed over time? What commercial bank failure cost the federal government the most to resolve?"
NIPS 2017: Adversarial Learning Development Set,Development images used in the NIPS 2017 Adversarial Learning challenges,Google Brain,24,"Version 2,2017-07-02|Version 1,2017-06-30",artificial intelligence,CSV,146 MB,Other,"5,478 views",884 downloads,8 kernels,,https://www.kaggle.com/google-brain/nips-2017-adversarial-learning-development-set,"Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake.
Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model.
To accelerate research on adversarial examples, Google Brain is organizing Competition on Adversarial Examples and Defenses within the NIPS 2017 competition track. This dataset contains the development images for this competition.
The competition on Adversarial Examples and Defenses consist of three sub-competitions:
Non-targeted Adversarial Attack. The goal of the non-targeted attack is to slightly modify source image in a way that image will be classified incorrectly by generally unknown machine learning classifier.
Targeted Adversarial Attack. The goal of the targeted attack is to slightly modify source image in a way that image will be classified as specified target class by generally unknown machine learning classifier.
Defense Against Adversarial Attack. The goal of the defense is to build machine learning classifier which is robust to adversarial example, i.e. can classify adversarial images correctly.
In each of the sub-competitions you're invited to make and submit a program which solves the corresponding task. In the end of the competition we will run all attacks against all defenses to evaluate how each of the attacks performs against each of the defenses."
UK Housing Prices Paid,Records of all individual transactions in England and Wales since 1995,HM Land Registry,24,"Version 2,2017-08-16|Version 1,2017-08-15","housing
finance
government",CSV,2 GB,Other,"5,779 views",705 downloads,2 kernels,3 topics,https://www.kaggle.com/hm-land-registry/uk-housing-prices-paid,"The Price Paid Data includes information on all registered property sales in England and Wales that are sold for full market value. Address details have been truncated to the town/city level.
You might also find the HM Land Registry transaction records to be a useful supplement to this dataset: https://www.kaggle.com/hm-land-registry/uk-land-registry-transactions
The available fields are as follows:
Transaction unique identifier A reference number which is generated automatically recording each published sale. The number is unique and will change each time a sale is recorded.
Price Sale price stated on the transfer deed.
Date of Transfer Date when the sale was completed, as stated on the transfer deed.
Property Type D = Detached, S = Semi-Detached, T = Terraced, F = Flats/Maisonettes, O = Other Note that: - we only record the above categories to describe property type, we do not separately identify bungalows. - end-of-terrace properties are included in the Terraced category above. - ‘Other’ is only valid where the transaction relates to a property type that is not covered by existing values.
Old/New Indicates the age of the property and applies to all price paid transactions, residential and non-residential. Y = a newly built property, N = an established residential building
Duration Relates to the tenure: F = Freehold, L= Leasehold etc. Note that HM Land Registry does not record leases of 7 years or less in the Price Paid Dataset.
Town/City
District
County
PPD Category Type Indicates the type of Price Paid transaction. A = Standard Price Paid entry, includes single residential property sold for full market value. B = Additional Price Paid entry including transfers under a power of sale/repossessions, buy-to-lets (where they can be identified by a Mortgage) and transfers to non-private individuals. Note that category B does not separately identify the transaction types stated. HM Land Registry has been collecting information on Category A transactions from January 1995. Category B transactions were identified from October 2013.
Record Status - monthly file only Indicates additions, changes and deletions to the records.(see guide below). A = Addition C = Change D = Delete.
Note that where a transaction changes category type due to misallocation (as above) it will be deleted from the original category type and added to the correct category with a new transaction unique identifier.
This data was kindly released by HM Land Registry under the Open Government License 3.0. You can find their current release here.
Data produced by HM Land Registry © Crown copyright 2017."
EEG data from basic sensory task in Schizophrenia,Button press and auditory tone event related potentials from 81 human subjects,Brian Roach,24,"Version 3,2017-11-17|Version 2,2017-11-11|Version 1,2017-11-11","mental health
time series
neuroscience",CSV,2 GB,CC4,"3,385 views",237 downloads,2 kernels,0 topics,https://www.kaggle.com/broach/button-tone-sz,"Context
Humans (and many other animals) have the ability to reduce or suppress their brains' responses to sensory consequences that are a result of their own actions. The nervous system accomplishes this with a corollary discharge forward model system in which an ""efference copy"" of an impending motor plan is transmitted from motor to sensory cortex where it generates a ""corollary discharge"" representation of the expected sensory consequences of the imminent motor act. For example, when you move your eyes from left to right, your brain knows the environment is not shifting. When you speak, your auditory cortex has a reduced response to the expected sound of your voice.
Schizophrenia is a chronic mental illness that affects about 1% of people across the globe. One possible explanation for some of the symptoms of schizophrenia is that one or more problems with the corollary discharge process in the nervous system makes it difficult for patients to differentiate between internally and externally generated stimuli. Therefore, studying this process and its relationship to symptoms in the illness might allow us to better understand abnormal brain processes in patients with this diagnosis.
In a previously published EEG experiment (full report), we used a simple button pressing task in which subjects either (1) pressed a button to immediately generated a tone, (2) passively listened to the same tone, or (3) pressed a button without generating a tone to study the corollary discharge in people with schizophrenia and comparison controls. We found that comparison controls suppressed the N100, a negative deflection in EEG brain wave 100 milliseconds after the onset of a sound, when they pressed a button to generate a tone compared to passive playback, but patients with schizophrenia did not. This data set is a larger sample replication of that previous study. Specifically, EEG data from 22 controls and 36 patients with schizophrenia have been combined with 10 controls and 13 patients from the previous report.
Methods
Due to the size of the raw EEG data, some pre-processing was done prior to upload. EEG data acquisition parameters and the experimental task was identical to that described in our paper. However, pre-processing differed. All individual subject data had at least the following data processing steps applied, in this order:
Re-reference to averaged ear lobes
0.1 Hz high-pass filter
Interpolation of outlier channels in the continuous EEG data (outliers defined as in this paper)
Chop continous data into single trial epochs 1.5 seconds before and after task events (3s total)
Baseline correction -100 to 0ms
Canonical correlation analysis to remove muscle and high-frequency white noise artifacts
Rejection of outlier single trials (outliers defined as in this paper)
Removal of outlier components from a spatial independent components analysis (outliers defined as in this paper)
Interpolation of outlier channels within single trials (outliers defined as in this paper)
Derived data includes event-related potential (ERP) averages for 9 electrode sites analyzed in our previous report, including Fz, FCz, Cz, FC3, FC4, C3, C4, CP3, CP4 (pictured below):
The ERPs are calculated by averaging across trials for every sample in the time series, separately for each subject, electrode, and condition.
Content
The single trial data from all 64 channels are too large to be uploaded for all 81 subjects, but those interested in that type of data will find one subject (subject 21 in 21.csv) among the data files. This includes all his data after the pre-processing step 9 listed above.
For those interested in comparing patients with schizophrenia to control subjects, the ERPdata.csv file contains the averaged, ERP time series for all subjects, conditions, and the 9 electrodes mentioned above. These data, along with the subject information in demographic.csv could be used to replicate the analyses in our prior report.
For those interested in single trial categorization/prediction like the grasp-and-lift challenge or the face decoding challenge, the mergedTrialData.csv contains summary measurements from nearly 24,000 individual trials (all subjects and conditions are included).
Acknowledgements
Funding for the study procedures, initial analyses and publications came from the National Institute of Mental Health. Please see grant info for additional details, and cite this NIMH project number (R01MH058262) in any work related to these data. All study participants gave written, informed consent to participate in this study, which received Institutional Review Board approval."
Burritos in San Diego,Mexican food enthusiasts rate 10 dimensions of hundreds of burritos in San Diego,Scott Cole,24,"Version 2,2018-01-02|Version 1,2016-09-26",food and drink,CSV,67 KB,CC4,"7,355 views","1,028 downloads",13 kernels,0 topics,https://www.kaggle.com/srcole/burritos-in-san-diego,"Mexican cuisine is often the best food option is southern California. And the burrito is the hallmark of delicious taco shop food: tasty, cheap, and filling. Appropriately, an effort was launched to critique burritos across the county and make this data open to the lay burrito consumer. At this time, the data set contains ratings from over 200 burritos from around 50 restaurants.
There are 10 core dimensions of the San Diego burrito. * Volume * Tortilla quality *Temperature * Meat quality * Non-meat filling quality * Meat-to-filling ratio * Uniformity * Salsa quality * Flavor synergy * Wrap integrity
All of these measures (except for Volume) are rated on a scale from 0 to 5, 0 being terrible, and 5 being optimal. Other information available for each burrito includes an overall rating, cost, Yelp rating of the restaurant, and more.
More information about the data set, as well as a link to the continuously updated dataset, can be found here."
Death Metal,Death metal bands and albums,zjf,24,"Version 2,2017-01-13|Version 1,2017-01-12",music,CSV,71 MB,Other,"6,064 views",455 downloads,35 kernels,2 topics,https://www.kaggle.com/zhangjuefei/death-metal,"Context
Metal-Archives.com (MA for short) is an encyclopedia website which includes information of nearly all heavy bands and albums on the earth. This information is collected and submitted by metalheads from all around the world. This dataset includes all ""death metal"" bands and albums on MA (by Nov. 2016). It's the search result by genre key word ""death metal"" which includes all bands which contain phrase ""death metal"" in their genre. (e.g. ""technical death metal"", ""brutal death metal"", ""melodic death metal"" ... )
The banner of dataset is the cover art of New Jersey-based death metal band Disma's debut full-length album ""Towards the Megalith"" (2011, Profound Lore Records). It's beautiful but not quite typical for this dataset's theme. But 1900+ resolution picture about death metal is rare, so I've chosen this one.
Content
There are three csv files included in the dataset:
bands.csv contains 37,723 bands. Each record is consisted of 8 fields:
id: sequential integer id.
name: the band's name which can contains non-english character, punctuations, numbers and other weird characters.
country: country the band is from. ""International"" means the members of the band are from multiple countries.
status: band's current activity-status: 'Unknown', 'Split-up', 'Active', 'Changed name', 'On hold' and 'Disputed'.
from_in: the year in which the band formed.
genre: the description of the band's genre. It's irregular, so you'd better not deem it as category but short text.
theme: the description of the band's lyric theme.
active: the time-span in which the band is active.
albums.csv contains 28,069 albums. Each record is consisted of 4 fields:
id: sequential integer id.
band: foreign key to band's id in bands.csv.
title: album title.
year: the album's release year.
reviews.csv contains 21,510 reviews. Each record is consisted of 5 fields:
id: sequential integer id.
album: foreign key to album's id in albums.csv.
title: the review's title.
score: the score for that album. Float number from 0.0 to 1.0 (from negative to positive).
content: the review's text.
Notice
This dataset only contains full-length studio albums (excluding EPs, singles, live albums, split albums and others).
All commas in dataset are replaced by ""|"" to make comma available for fields separator.
NA value is ""N/A"".
All text is in utf-8 encoding.
Acknowledgements
Metal-Archives! \m/
Inspiration
Statistical analysis
Emotional analysis (reviews)
Genre classification (by NLP of title and/or theme)
Score prediction (by NLP of review's content)"
Brazilian Coins,Classification and regression with Brazilian coin images,Luis Moneda,24,"Version 3,2017-04-23|Version 2,2017-04-17|Version 1,2017-04-17","money
image data
multiclass classification",Other,391 MB,CC4,"5,972 views",568 downloads,7 kernels,6 topics,https://www.kaggle.com/lgmoneda/br-coins,"Context
These datasets were created for a college course work. It was an opportunity to test Deep Learning capabilities for computer vision in a very restricted problem.
The idea is to explore a classification problem for a single coin and a regression problem for a group of coins, trying to count how much money they sum. You can see my initial approach here.
Content
There are two datasets. One for classification and another for regression. The first contains 3000 images with just a single coin in it. The second contains the first one and another 3000 images with two or more coins present in each example.
In the classification problem there are five classes: 5, 10, 25, 50 and 100 cents. For regression, there are examples from 5 to 175 cents.
Every file contains its value in money as its filename. For example: 5_1477146780.jpg contains a single 5 cents coin. If there's only one coin, it's the coin value itself. In the 80_1477851090.jpg you're going to find enough coins to sum 80 cents. An example (90_1477854720.jpg):
Different coins from each type were used to make it more interesting. My fingers appear in some images!
I tried to keep the distance, illumination and background constant for all of them, but some differences will be noticed, specially in the illumination. Changing the coin position has an great impact in how the light reflects over it. The structure used to take the pictures (in fact, a second light source was added):
Inspiration
A model that can sum coins and tell how much money we have in a group of coins could be used for people with vision disabilities. Can Deep Learning count, classify and sum in a single model? Should we split the problem into segmentation, classification and then sum it? What can be done with this amount of data? Can we achieve a good generalization and predict sums beyond the dataset greatest value?
Citation
If you want to use this dataset for any purpose contemplated by its license, add the reference:
MONEDA, L. (2016) Brazilian Coins Dataset. Retrieved from: http://lgmoneda.github.io/
Acknowledgment
I'd like to thanks Luciana Harada and Rafael de Souza, my group in the college course that generated these datasets."
Current Population Survey,The primary source of labor force statistics for the US population,US Census Bureau,24,"Version 1,2016-10-24","employment
demographics",CSV,300 MB,CC0,"8,354 views",909 downloads,18 kernels,,https://www.kaggle.com/census/current-population-survey,"Current Population Survey - August 2016
Context
The Current Population Survey (CPS) is one of the oldest, largest, and most well-recognized surveys in the United States. It is immensely important, providing information on many of the things that define us as individuals and as a society – our work, our earnings, and our education.
Frequency: Monthly
Period: August 2016
Content
In addition to being the primary source of monthly labor force statistics, the CPS is used to collect data for a variety of other studies that keep the nation informed of the economic and social well-being of its people. This is done by adding a set of supplemental questions to the monthly basic CPS questions. Supplemental inquiries vary month to month and cover a wide variety of topics such as child support, volunteerism, health insurance coverage, and school enrollment. Supplements are usually conducted annually or biannually, but the frequency and recurrence of a supplement depend completely on what best meets the needs of the supplement’s sponsor.
Data Dictionary: http://thedataweb.rm.census.gov/pub/cps/basic/201501-/January_2015_Record_Layout.txt
Acknowledgements
The Current Population Survey (CPS) is administered, processed, researched and disseminated by the U.S. Census Bureau on behalf of the Bureau of Labor Statistics (BLS)."
Severe Weather Data Inventory,Detections of hail storm cells based on NEXRAD radar data during 2015,NOAA,24,"Version 1,2016-10-24",climate,CSV,667 MB,Other,"8,401 views",653 downloads,14 kernels,,https://www.kaggle.com/noaa/severe-weather-data-inventory,"Severe Weather Data Inventory
Context
The Severe Weather Data Inventory (SWDI) is an integrated database of severe weather records for the United States. Severe weather is a phenomenon that risks the physical well-being of people and property. In fact, the frozen precipitation resulting from fast updrafts during strong thunderstorms can lead to serious damage and harm. Each year, the U.S. sees approximately $1 billion in property and crop damage due to severe weather incidents.
Frequency: Event-level
Period: 2015
Content
The records in SWDI come from a variety of sources in the National Climatic Data Center archive and cover a number of weather phenomena. This extract from 2015 covers hail detections including the probability of a weather event as well as the size and severity of hail -- all of which help understand potential damage to property and injury to people. Records are event-level records. Individual storm cells with a high probability of yielding hail are included in this dataset -- a total of n = 10,824,080.
Inspiration
Think about the geospatial and spatial statistical techniques that can be applied to this data to uncover patterns in storms.
How often does serious severe weather happen?
Where do these severe weather events normally occur?
What correlations exist between severe weather and other environmental phenomena?
Acknowledgements
This data is a product of NOAA's National Centers for Environmental Information (NCEI). The dataset is generated by a variety of products that have been submitted to NOAA's weather and climate archives at NCEI. The datasets and methods are described at http://www.ncdc.noaa.gov/swdi/.
SWDI provides a uniform way to access data from a variety of sources, but it does not provide any additional quality control beyond the processing which took place when the data were archived. The data sources in SWDI will not provide complete severe weather coverage of a geographic region or time period, due to a number of factors (eg, reports for a location or time period not provided to NOAA). The absence of SWDI data for a particular location and time should not be interpreted as an indication that no severe weather occurred at that time and location. Furthermore, much of the data in SWDI is automatically derived from radar data and represents probable conditions for an event, rather than a confirmed occurrence.
License
Public Domain License"
StackLite: Stack Overflow questions and tags,"Stack Overflow questions and tags, without text included",Stack Overflow,24,"Version 4,2017-02-07|Version 3,2016-10-21|Version 2,2016-10-20|Version 1,2016-10-20","linguistics
internet
programming languages",CSV,2 GB,ODbL,"13,547 views",633 downloads,15 kernels,,https://www.kaggle.com/stackoverflow/stacklite,"A dataset of Stack Overflow programming questions. For each question, it includes:
Question ID
Creation date
Closed date, if applicable
Score
Owner user ID
Number of answers
Tags
This dataset is ideal for answering questions such as:
The increase or decrease in questions in each tag over time
Correlations among tags on questions
Which tags tend to get higher or lower scores
Which tags tend to be asked on weekends vs weekdays
This dataset was extracted from the Stack Overflow database at 2016-10-13 18:09:48 UTC and contains questions up to 2016-10-12. This includes 12583347 non-deleted questions, and 3654954 deleted ones.
This is all public data within the Stack Exchange Data Dump, which is much more comprehensive (including question and answer text), but also requires much more computational overhead to download and process. This dataset is designed to be easy to read in and start analyzing. Similarly, this data can be examined within the Stack Exchange Data Explorer, but this offers analysts the chance to work with it locally using their tool of choice.
Note that for space reasons only non-deleted questions are included in the sqllite dataset, but the csv.gz files include deleted questions as well (with an additional DeletionDate file).
See the GitHub repo for more."
Hard Drive Test Data,Daily Snapshot of Each Operational Hard Drive in 2016,Backblaze,24,"Version 1,2016-11-05","computer science
electrical components",CSV,1 GB,Other,"8,405 views",797 downloads,93 kernels,,https://www.kaggle.com/backblaze/hard-drive-test-data,"Context
Each day, Backblaze takes a snapshot of each operational hard drive that includes basic hard drive information (e.g., capacity, failure) and S.M.A.R.T. statistics reported by each drive. This dataset contains data from the first two quarters in 2016.
Content
This dataset contains basic hard drive information and 90 columns or raw and normalized values of 45 different S.M.A.R.T. statistics. Each row represents a daily snapshot of one hard drive.
date: Date in yyyy-mm-dd format
serial_number: Manufacturer-assigned serial number of the drive
model: Manufacturer-assigned model number of the drive
capacity_bytes: Drive capacity in bytes
failure: Contains a “0” if the drive is OK. Contains a “1” if this is the last day the drive was operational before failing.
90 variables that begin with 'smart': Raw and Normalized values for 45 different SMART stats as reported by the given drive
Inspiration
Some items to keep in mind as you process the data:
S.M.A.R.T. statistic can vary in meaning based on the manufacturer and model. It may be more informative to compare drives that are similar in model and manufacturer
Some S.M.A.R.T. columns can have out-of-bound values
When a drive fails, the 'failure' column is set to 1 on the day of failure, and starting the day after, the drive will be removed from the dataset. Each day, new drives are also added. This means that total number of drives each day may vary.
S.M.A.R.T. 9 is the number of hours a drive has been in service. To calculate a drive's age in days, divide this number by 24.
Given the hints above, below are a couple of questions to help you explore the dataset:
What is the median survival time of a hard drive? How does this differ by model/manufacturer?
Can you calculate the probability that a hard drive will fail given the hard drive information and statistics in the dataset?
Acknowledgement
The original collection of data can be found here. When using this data, Backblaze asks that you cite Backblaze as the source; you accept that you are solely responsible for how you use the data; and you do not sell this data to anyone."
Seattle Airbnb Open Data,"A sneak peek into the Airbnb activity in Seattle, WA, USA",Airbnb,24,"Version 1,2016-11-17","united states
home
hotels",CSV,86 MB,CC0,"11,660 views","1,504 downloads",14 kernels,,https://www.kaggle.com/airbnb/seattle,"Context
Since 2008, guests and hosts have used Airbnb to travel in a more unique, personalized way. As part of the Airbnb Inside initiative, this dataset describes the listing activity of homestays in Seattle, WA.
Content
The following Airbnb activity is included in this Seattle dataset: * Listings, including full descriptions and average review score * Reviews, including unique id for each reviewer and detailed comments * Calendar, including listing id and the price and availability for that day
Inspiration
Can you describe the vibe of each Seattle neighborhood using listing descriptions?
What are the busiest times of the year to visit Seattle? By how much do prices spike?
Is there a general upward trend of both new Airbnb listings and total Airbnb visitors to Seattle?
For more ideas, visualizations of all Seattle datasets can be found here.
Acknowledgement
This dataset is part of Airbnb Inside, and the original source can be found here."
"Vehicle Collisions in NYC, 2015-Present",Where are the most pedestrians struck by vehicles in New York City?,NYPD,24,"Version 2,2017-03-09|Version 1,2017-01-19","walking
road transport",CSV,85 MB,CC0,"12,096 views","1,361 downloads",32 kernels,2 topics,https://www.kaggle.com/nypd/vehicle-collisions,"Content
The motor vehicle collision database includes the date and time, location (as borough, street names, zip code and latitude and longitude coordinates), injuries and fatalities, vehicle number and types, and related factors for all 65,500 collisions in New York City during 2015 and 2016.
Acknowledgements
The vehicle collision data was collected by the NYPD and published by NYC OpenData."
Water Consumption in a Median Size City,This dataset contains water consumption per capita from late 2000s to 2016.,Marco Molina,23,"Version 1,2016-09-15",water technology,CSV,45 MB,ODbL,"9,351 views","1,326 downloads",45 kernels,,https://www.kaggle.com/marcomolina/water-consumption-in-a-median-size-city,"The dataset contains the following variables: water consumption per user (cubic meters) from 2009 to 2016, land use, type of user (e.g. industrial, housing, public infrastructure, etc.), zip code, and others. The challenge is to treat NAs in a way that do not distort the overall dataset. You should also check whether there are any missing values. If so, can you ﬁll them in, and do you understand why they are missing? This dataset is property of a local water provider called AguaH and its part of a research developed between 2014 and 2016."
The Marvel Universe Social Network,An artificial social network of heroes,Claudio Sanhueza,23,"Version 1,2017-01-28","popular culture
comics",CSV,24 MB,CC4,"10,201 views",937 downloads,5 kernels,,https://www.kaggle.com/csanhueza/the-marvel-universe-social-network,"The Marvel Universe
Marvel Comics, originally called Timely Comics Inc., has been publishing comic books for several decades. ""The Golden Age of Comics"" name that was given due to the popularity of the books during the first years, was later followed by a period of decline of interest in superhero stories due to World War ref. In 1961, Marvel relaunched its superhero comic books publishing line. This new era started what has been known as the Marvel Age of Comics. Characters created during this period such as Spider-Man, the Hulk, the Fantastic Four, and the X-Men, together with those created during the Golden Age such as Captain America, are known worldwide and have become cultural icons during the last decades. Later, Marvel's characters popularity has been revitalized even more due to the release of several recent movies which recreate the comic books using spectacular modern special effects. Nowadays, it is possible to access the content of the comic books via a digital platform created by Marvel, where it is possible to subscribe monthly or yearly to get access to the comics. More information about the Marvel Universe can be found here.
Content
The dataset contains heroes and comics, and the relationship between them. The dataset is divided into three files:
nodes.csv: Contains two columns (node, type), indicating the name and the type (comic, hero) of the nodes.
edges.csv: Contains two columns (hero, comic), indicating in which comics the heroes appear.
hero-edge.csv: Contains the network of heroes which appear together in the comics. This file was originally taken from http://syntagmatic.github.io/exposedata/marvel/
Past Research (Acknowledgements)
The Marvel Comics character collaboration graph was originally constructed by Cesc Rosselló, Ricardo Alberich, and Joe Miro from the University of the Balearic Islands. They compare the characteristics of this universe to real-world collaboration networks, such as the Hollywood network, or the one created by scientists who work together in producing research papers. Their original sources can be found here. With this dataset, the authors published the paper titled: ""Marvel Universe looks almost like a real social network""."
Sign Language MNIST,Drop-In Replacement for MNIST for Hand Gesture Recognition Tasks,tecperson,23,"Version 1,2017-10-20","languages
healthcare
linguistics",Other,101 MB,CC0,"2,853 views",430 downloads,5 kernels,2 topics,https://www.kaggle.com/datamunge/sign-language-mnist,"The original MNIST image dataset of handwritten digits is a popular benchmark for image-based machine learning methods but researchers have renewed efforts to update it and develop drop-in replacements that are more challenging for computer vision and original for real-world applications. As noted in one recent replacement called the Fashion-MNIST dataset, the Zalando researchers quoted the startling claim that ""Most pairs of MNIST digits (784 total pixels per sample) can be distinguished pretty well by just one pixel"". To stimulate the community to develop more drop-in replacements, the Sign Language MNIST is presented here and follows the same CSV format with labels and pixel values in single rows. The American Sign Language letter database of hand gestures represent a multi-class problem with 24 classes of letters (excluding J and Z which require motion).
The dataset format is patterned to match closely with the classic MNIST. Each training and test case represents a label (0-25) as a one-to-one map for each alphabetic letter A-Z (and no cases for 9=J or 25=Z because of gesture motions). The training data (27,455 cases) and test data (7172 cases) are approximately half the size of the standard MNIST but otherwise similar with a header row of label, pixel1,pixel2....pixel784 which represent a single 28x28 pixel image with grayscale values between 0-255. The original hand gesture image data represented multiple users repeating the gesture against different backgrounds. The Sign Language MNIST data came from greatly extending the small number (1704) of the color images included as not cropped around the hand region of interest. To create new data, an image pipeline was used based on ImageMagick and included cropping to hands-only, gray-scaling, resizing, and then creating at least 50+ variations to enlarge the quantity. The modification and expansion strategy was filters ('Mitchell', 'Robidoux', 'Catrom', 'Spline', 'Hermite'), along with 5% random pixelation, +/- 15% brightness/contrast, and finally 3 degrees rotation. Because of the tiny size of the images, these modifications effectively alter the resolution and class separation in interesting, controllable ways.
This dataset was inspired by the Fashion-MNIST 2 and the machine learning pipeline for gestures by Sreehari 4.
A robust visual recognition algorithm could provide not only new benchmarks that challenge modern machine learning methods such as Convolutional Neural Nets but also could pragmatically help the deaf and hard-of-hearing better communicate using computer vision applications. The National Institute on Deafness and other Communications Disorders (NIDCD) indicates that the 200-year-old American Sign Language is a complete, complex language (of which letter gestures are only part) but is the primary language for many deaf North Americans. ASL is the leading minority language in the U.S. after the ""big four"": Spanish, Italian, German, and French. One could implement computer vision in an inexpensive board computer like Raspberry Pi with OpenCV, and some Text-to-Speech to enabling improved and automated translation applications."
India Water Quality Data,Government data related to the water quality of India,Venkat Ramakrishnan,23,"Version 2,2016-12-31|Version 1,2016-12-31",water technology,CSV,41 MB,Other,"11,509 views","1,452 downloads",33 kernels,4 topics,https://www.kaggle.com/venkatramakrishnan/india-water-quality-data,"Context:
There has been increased interest among the public about the Environment and living conditions in India. Especially, after since many manufacturing units are being planned, people are worried about how it will affect the underground water quality and the environment. Government of India, under the Ministry of Drinking Water and Sanitation has released the Water Quality Affected Data for 2009, 2010, 2011 and 2012. The objective here is to analyze this data alongside with Forest, Industries, Habitation, and development projects data in the same area (panchayat) to figure out whether there is any connection between the development effort and the quality of water getting affected. This effort will identify such associations and create awareness such that people and the Govt. can act in time to avoid further deterioration of the water resources.
Content:
Currently, there is this data set of areas with affected water quality for the years 2009, 2010, 2011 and 2012. Further datasets are expected for subsequent years. These datasets identify the state, district and specific localities in which water quality degradation has been reported in that particular year. Focus should be on the area (Panchayat/Village) rather than the district or the state as a whole, and observations should be made if there are any associations between the other sets of data available for the same area (from industrial, habitation, manufacturing and other sources, which I intend to add here also).
Acknowledgements:
My deep gratitude to the Government of India for making this data available through the Open Data initiative.
Inspiration:
Let's explore if there are any repetitive patterns of water quality degradation in the same area for multiple years.
As a whole, which areas in India has a lot of water quality degradation issues over the years (heat maps)
Which chemical is predominantly present in most of the water quality issues (heat maps). And why (from the associations with other developmental data like industry, manufacturing, development initiatives, housing, habitation, etc.)
As a whole, for the country, is the water quality degrading or upgrading (number of instances reported of water quality getting affected)?
Let's explore if there are any associations between the water quality data and the other developmental data. If there is, then what is the extent (visualisation) and how can we address it (prescriptive).
Let's predict if there are GOING TO BE water quality issues in areas that are not affected right now based on the developmental and water quality data that is available right now. Prevention is better than cure!
It would be great if we could have water quality and industrial/development experts in this analysis, so that they can contribute their valuable inputs!"
Real Time Bidding,Predict clicks and handle imbalanced data,Ricky,23,"Version 2,2017-02-28|Version 1,2017-02-27","business
artificial intelligence",CSV,455 MB,Other,"6,687 views",526 downloads,12 kernels,0 topics,https://www.kaggle.com/zurfer/rtb,"Context
This is real real-time bidding data that is used to predict if an advertiser should bid for a marketing slot e.g. a banner on a webpage. Explanatory variables are things like browser, operation system or time of the day the user is online, marketplace his identifiers were traded on earlier, etc. The column 'convert' is 1, when the person clicked on the ad, and 0 if this is not the case.
Content
Unfortunately, the data had to be anonymized, so you basically can't do a lot of feature engineering. I just applied PCA and kept 0.99 of the linear explanatory power. However, I think it's still really interesting data to just test your general algorithms on imbalanced data. ;)
Inspiration
Since it's heavily imbalanced data, it doesn't make sense to train for accuracy, but rather try to get obtain a good AUC, F1Score, MCC or recall rate, by cross-validating your data. It's interesting to compare different models (logistic regression, decision trees, svms, ...) over these metrics and see the impact that your split in train:test data has on the data.
It might be good strategy to follow these Tactics to combat imbalanced classes."
Starcraft: Scouting The Enemy,Limited reconnaissance in a real-time strategy game,Ed King,23,"Version 1,2016-11-07",video games,CSV,11 MB,CC0,"3,899 views",217 downloads,4 kernels,0 topics,https://www.kaggle.com/kinguistics/starcraft-scouting-the-enemy,"This dataset contains information on player reconnaissance in over 500 professional-level Starcraft games. From the perspective of one player (the Terran), it contains information on how many enemy (Protoss) units the player has observed, can observe, has seen destroyed, etc., along with an overall measure of how much enemy territory the player can see.
Acknowledgements
This dataset was downloaded from this webpage. It was the basis for the following paper:
Hostetler, J., Dereszynski, E., Dietterich, T., and Fern, A. (2012). Inferring strategies from limited reconnaissance in real-time strategy games. Proc. 28th Conference on Uncertainty in Artificial Intelligence (UAI 2012) (to appear).
The Data
Games are divided into 30 second chunks, with the first 7 minutes of each game being represented in this dataset. Values of variables at any given time cycle represent their values over the entire chunk that ends at that time.
This dataset contains the following fields:
game: a unique identifier for the game being played
cycle: the cycle (in game frames, which are typically 24 fps)
unit: the enemy unit that this row gives info for
losses: how many of this enemy unit were lost during this time chunk?
observable-units: how many of this enemy unit could the player see during this time chunk?
observed-losses: how many of this enemy did the player observe being lost during this time chunk?
production: how many of this enemy unit became observable (i.e., was produced, or -- in the case of buildings -- was under construction) during this time chunk?
scouting: how many of this enemy unit did the player scout during this time chunk?
vision: what proportion of the total enemy territory could the player observe during this time chunk? -- NOTE that vision appears once per unit; however, the vision variable is not linked to any one unit. Its value spans the time chunk, and is identical in every row that represents a given time chunk"
rDany Chat,157 chats & 6300+ messages with a (fake) virtual companion,Eibriel,23,"Version 11,2017-03-01|Version 10,2017-02-13|Version 9,2017-02-06|Version 8,2017-01-31|Version 7,2017-01-27|Version 6,2017-01-27|Version 5,2017-01-25|Version 4,2017-01-22|Version 3,2017-01-22|Version 2,2017-01-22|Version 1,2017-01-21","linguistics
artificial intelligence",Other,3 MB,CC4,"8,446 views",857 downloads,15 kernels,3 topics,https://www.kaggle.com/eibriel/rdany-conversations,"Context
I have the idea to build a virtual companion, capable of holding long and interesting conversations. But the lack of a good technique, and good datasets apparently is holding the advances of AI in that sense. Chatbots don't have personality, nor context awareness, and datasets used to train them are just pair of question/answers, or IT conversations. This dataset is being built using rDany bot for Telegram, Kik and Messenger.
If you want to see this dataset grow, please use and share it.
Telegram: https://t.me/rDanyBot
Kik: https://kik.me/rDanyBot
Messenger: https://m.me/rDanyBot
You can also support the development on Patreon: https://www.patreon.com/rDanyBot
This bot have a personality:
Candid
True
Fun
Optimistic
Empathic
Gender neutral
Likes art
And knows a very limited word, its room, Wikipedia, and a schematic view of the world. And speaks Spanish (native), English (with some errors), and other languages (using automatic translation). You can learn more about it on rDany's Telegram channel: https://t.me/rDany
Content
The dataset consists on 157 anonymized (modified personal information) conversations between a human and other human acting as a companion bot. Each conversation and messages are labeled with hashed IDs.
{
    ""2059a7bf16436f39b3e713f7b5fe756776cd5e5a601186a1ba17c017027781d9"": [
        {
            ""date"": 0,
            ""hashed_message_id"": ""0fd2e5cf87ae0f148db113f06ab746e0c76a55de04819e1a45c9454a34ba8a97"",
            ""source"": ""human"",
            ""text"": ""[START]""
        },
        {
            ""date"": 108,
            ""hashed_message_id"": ""a8c5a80334c8177f07913a192f048d05cd5ad5cc77752eb0abb8d0705eccedfb"",
            ""source"": ""human"",
            ""text"": ""hello""
        },
        {
            ""date"": 15097,
            ""hashed_message_id"": ""73e9765c0d0eab4dfd6f9be2d665e32cc97c5fc3e0fd9c2d12ef920d18ecf349"",
            ""source"": ""robot"",
            ""text"": ""Hi! How are you?!""
        }
    ]
}
date: Seconds since first message
hashed_message_id: Message ID
source: human if the message is from the user, and robot if is from rDany
text: text of the message, or ""[START]"" for the start command, or ""[photo]"", ""[document]"", ""[audio]"", ""[voice]"", ""[unknown]"" for other types of messages.
Acknowledgements
Thanks to all the amazing people that spent time speaking to a crazy human pretending to be a robot :)
Inspiration
Can you train your own virtual companion that says hello using this dataset?"
Car Sale Advertisements,Data collected from private car sale advertisements in Ukraine,Anton Bobanev,23,"Version 1,2017-05-04",automobiles,CSV,526 KB,CC0,"6,204 views",990 downloads,23 kernels,0 topics,https://www.kaggle.com/antfarol/car-sale-advertisements,"Context
This dataset was collected by me from car sale advertisements for study/practice purposes in 2016. Though there is couple well known car features datasets they seems quite simple and outdated. Car topic is really interesting. But I wanted to practice with real raw data which has all inconvenient moments (as NA’s for example).
This dataset contains data for more than 9.5K cars sale in Ukraine. Most of them are used cars so it opens the possibility to analyze features related to car operation. At the end of the day I look at this data as a subset from all Ukrainian car fleet.
Content
Dataset contains 9576 rows and 10 variables with essential meanings:
car: manufacturer brand
price: seller’s price in advertisement (in USD)
body: car body type
mileage: as mentioned in advertisement (‘000 Km)
engV: rounded engine volume (‘000 cubic cm)
engType: type of fuel (“Other” in this case should be treated as NA)
registration: whether car registered in Ukraine or not
year: year of production
model: specific model name
drive: drive type
Data has gaps, so be careful and check for NA’s. I tried to check and drop repeated offers, but theoretically duplications are possible.
Inspiration
Data will be handy to study and practice different models and approaches. As a further step you can compare patters in Ukrainian market to your own domestic car market characteristics."
"Aircraft Wildlife Strikes, 1990-2015",What bird species has caused the most damage to airplanes?,Federal Aviation Administration,23,"Version 1,2017-02-08","animals
aviation",CSV,35 MB,CC0,"3,828 views",570 downloads,31 kernels,,https://www.kaggle.com/faa/wildlife-strikes,"Content
The dataset contains a record of each reported wildlife strike of a military, commercial, or civil aircraft between 1990 and 2015. Each row contains the incident date, aircraft operator, aircraft make and model, engine make and model, airport name and location, species name and quantity, and aircraft damage.
Acknowledgements
The wildlife strike database was compiled from reports received from airports, airlines, and pilots and published by the Federal Aviation Association."
Museum of Modern Art Collection,"Title, artist, date, and medium of every artwork in the MoMA collection",The Museum of Modern Art,23,"Version 1,2017-02-15","museums
visual arts",CSV,33 MB,CC0,"4,161 views",513 downloads,11 kernels,0 topics,https://www.kaggle.com/momanyc/museum-collection,"Context
The Museum of Modern Art (MoMA) acquired its first artworks in 1929, the year it was established. Today, the Museum’s evolving collection contains almost 200,000 works from around the world spanning the last 150 years. The collection includes an ever-expanding range of visual expression, including painting, sculpture, printmaking, drawing, photography, architecture, design, film, and media and performance art.
Content
MoMA is committed to helping everyone understand, enjoy, and use our collection. The Museum’s website features 72,706 artworks from 20,956 artists. The artworks dataset contains 130,262 records, representing all of the works that have been accessioned into MoMA’s collection and cataloged in our database. It includes basic metadata for each work, including title, artist, date, medium, dimensions, and date acquired by the Museum. Some of these records have incomplete information and are noted as “not curator approved.” The artists dataset contains 15,091 records, representing all the artists who have work in MoMA's collection and have been cataloged in our database. It includes basic metadata for each artist, including name, nationality, gender, birth year, and death year.
Inspiration
Which artist has the most works in the museum collection or on display? What is the largest work of art in the collection? How many pieces in the collection were made during your birth year? What gift or donation is responsible for the most artwork in the collection?"
30 Years of European Solar Generation,Hourly energy potential for 1986-2015,Sohier Dane,23,"Version 2,2017-09-15|Version 1,2017-09-15",energy,CSV,564 MB,CC0,"3,003 views",378 downloads,,,https://www.kaggle.com/sohier/30-years-of-european-solar-generation,"This dataset contains hourly estimates of an area's energy potential for 1986-2015 as a percentage of a power plant's maximum output.
The overall scope of EMHIRES is to allow users to assess the impact of meteorological and climate variability on the generation of solar power in Europe and not to mime the actual evolution of solar power production in the latest decades. For this reason, the hourly solar power generation time series are released for meteorological conditions of the years 1986-2015 (30 years) without considering any changes in the solar installed capacity. Thus, the installed capacity considered is fixed as the one installed at the end of 2015. For this reason, data from EMHIRES should not be compared with actual power generation data other than referring to the reference year 2015.
Content
The data is available at both the national level and the NUTS 2 level. The NUTS 2 system divides the EU into 276 statistical units.
Please see the manual for the technical details of how these estimates were generated.
This product is intended for policy analysis over a wide area and is not the best for estimating the output from a single system. Please don't use it commercially.
Acknowledgements
This dataset was kindly made available by the European Commission's STETIS program. You can find the original dataset here.
Inspiration
How clean is the dataset? Older solar estimates used to contain impossible values around sunset (ie more energy than the sun releases) or negative sunlight.
What does a typical year look like? One common approach is to stitch together 12 months of raw data, using the 12 most typical months per this ISO standard.
If you like
If you like this dataset, you might also enjoy: - 30 years of European wind - Google's Project Sunroof"
Stopword Lists for 19 Languages,Lists of high-frequency words usually removed during NLP analysis,Rachael Tatman,23,"Version 1,2017-07-28","languages
india
europe
linguistics",Other,53 KB,Other,"1,999 views",342 downloads,,0 topics,https://www.kaggle.com/rtatman/stopword-lists-for-19-languages,"Context:
Some words, like “the” or “and” in English, are used a lot in speech and writing. For most Natural Language Processing applications, you will want to remove these very frequent words. This is usually done using a list of “stopwords” which has been complied by hand.
Content:
This dataset contains a list of stopwords for the following languages (Languages which are not from the Indo-European language family have been starred):
English
French
German
Italian
Spanish
Portuguese
Finnish*
Swedish
Arabic*
Russian
Hungarian
Bulgarian
Romanian
Czech
Polish
Persian/Farsi
Hindi
Marathi
Bengali
Acknowledgements:
This dataset is Copyright (c) 2005, Jacques Savoy and distributed under the BSD License. More information can be found here.
Inspiration:
This dataset is mainly helpful for use during NLP analysis, however there may some interesting insights to be found in the data.
What qualities do stopwords share across languages? Given a novel language, could you predict what its stopwords should be?
What stopwords are shared across languages?
Often, related languages will have words with the same meaning and similar spellings. Can you automatically identify any of these pairs of words?
You may also like:
Stopword Lists for 9 African Languages"
Financial Distress Prediction,Bankruptcy Prediction,Ebrahimi,23,"Version 1,2017-12-15","finance
machine learning",CSV,815 KB,Other,"4,724 views",495 downloads,4 kernels,0 topics,https://www.kaggle.com/shebrahimi/financial-distress,"Context
This data set deals with the financial distress prediction for a sample of companies.
Content
First column: Company represents sample companies.
Second column: Time shows different time periods that data belongs to. Time series length varies between 1 to 14 for each company.
Third column: The target variable is denoted by ""Financial Distress"" if it is greater than -0.50 the company should be considered as healthy (0). Otherwise, it would be regarded as financially distressed (1).
Fourth column to the last column: The features denoted by x1 to x83, are some financial and non-financial characteristics of the sampled companies. These features belong to the previous time period, which should be used to predict whether the company will be financially distressed or not (classification). Feature x80 is categorical variable.
For example, company 1 is financially distressed at time 4 but company 2 is still healthy at time 14.
This data set is imbalanced (there are 136 financially distressed companies against 286 healthy ones i.e., 136 firm-year observations are financially distressed while 3546 firm-year observations are healthy) and skewed, so f-score should be employed as the performance evaluation criterion.
It should be noted that 30% of this data set should be randomly assigned as hold-out test set so the remaining 70% is used for feature selection and model selection i.e., train set.
Note: 1- This data could be viewed as a classification problem. 2- This data could also be considered as a regression problem and then the result will be converted into a classification. 3- This data could be regarded as a multivariate time series classification.
Inspiration
Which features are most indicative of financial distress?
What types of machine learning models perform best on this dataset?"
Chicago Taxi Rides 2016,Details of taxi rides in Chicago,City of Chicago,23,"Version 2,2017-07-07|Version 1,2017-07-06",road transport,CSV,2 GB,Other,"8,417 views","2,555 downloads",2 kernels,4 topics,https://www.kaggle.com/chicago/chicago-taxi-rides-2016,"Context
This dataset includes taxi trips for 2016, reported to the City of Chicago in its role as a regulatory agency. To protect privacy but allow for aggregate analyses, the Taxi ID is consistent for any given taxi medallion number but does not show the number, Census Tracts are suppressed in some cases, and times are rounded to the nearest 15 minutes. Due to the data reporting process, not all trips are reported but the City believes that most are. See http://digital.cityofchicago.org/index.php/chicago-taxi-data-released for more information about this dataset and how it was created.
Content
Please see the data dictionary for details of specific fields. We also shrunk the original files by roughly two thirds by dropping redundant columns and remapping several others to use shorter IDs. For example, the taxi_id column used to be a 128 character string. We’ve replaced it with an integer containing at most four digits.
The redundant columns were unique_key, pickup_location, and dropoff_location. The remapped columns were taxi_id, company, pickup_census_tract, dropoff_census_tract, pickup_latitude, pickup_longitude, dropoff_latitude, and dropoff_longitude. The original versions of those columns can be unpacked using the column_remapping.json.
Acknowledgements
This dataset was kindly made publically available by the City of Chicago at: https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew
Please note that this site provides applications using data that has been modified for use from its original source, www.cityofchicago.org, the official website of the City of Chicago. The City of Chicago makes no claims as to the content, accuracy, timeliness, or completeness of any of the data provided at this site. The data provided at this site is subject to change at any time. It is understood that the data provided at this site is being used at one’s own risk.
Inspiration
How centralized is Chicago? In other words, what portion of trips are to or from downtown?
Chicago has an extensive metro system. Are taxis competing with the trains by covering similar routes or supplementing public transit by getting people to and from train stations?
Use this dataset with BigQuery
You can use Kernels to analyze, share, and discuss this data on Kaggle, but if you’re looking for real-time updates and bigger data, check out the data on BigQuery, too: https://cloud.google.com/bigquery/public-data/chicago-taxi. BigQuery hosts the full version of this dataset, which extends from 2013 through the present."
Hotels on Makemytrip,"Details of 20,000 hotels on MakeMyTrip.com",PromptCloud,23,"Version 1,2017-09-16","hotels
internet",CSV,36 MB,CC4,"5,034 views",828 downloads,,0 topics,https://www.kaggle.com/PromptCloudHQ/hotels-on-makemytrip,"Context
This is a pre-crawled dataset, taken as subset of a bigger dataset (more than 615,000 hotels) that was created by extracting data from MakeMyTrip.com, a travel portal in India. The complete dataset is available on DataStock, a web data repository with historical records from several industries.
Content
This dataset has following fields:
area
city
country
crawl_date
highlight_value
hotel_overview
hotel_star_rating
image_urls
in_your_room
is_value_plus
latitude
longitude
mmt_holidayiq_review_count
mmt_location_rating
mmt_review_count
mmt_review_rating
mmt_review_score
mmt_traveller_type_review_count
mmt_tripadvisor_count
pageurl
property_address
property_id
property_name
property_type
qts
query_time_stamp
room_types
site_review_count
site_review_rating
sitename
state
traveller_rating
Acknowledgements
This dataset was created by PromptCloud's in-house web-crawling service.
Inspiration
Analyses of the reviews, ratings and property description can be performed."
General Practice Prescribing Data,One year of British National Health Service Prescription data,National Health Service,23,"Version 1,2017-08-10","healthcare
pharmaceutical industry
pharmaceuticals policy
+ 2 more...",CSV,4 GB,Other,"2,767 views",256 downloads,,6 topics,https://www.kaggle.com/nhs/general-practice-prescribing-data,"Context
The British National Health Service releases data covering every public sector prescription made in the country. This covers a single year of that data.
Content
Covering all general practices in England, the data includes figures on the number of prescription items that are dispensed each month and information relating to costs.
For each GP practice, the total number of items that were prescribed and then dispensed is shown. The total Net Ingredient Cost and the total Actual Cost of these items is shown.
Chemical level
All prescribed and dispensed medicines (by chemical name), dressings and appliances (at section level) are listed for each GP practice.
Presentation level
All prescribed and dispensed medicines, dressings and appliances are listed at presentation level, for each GP practice. (Presentation level gives the individual drug name, the form, and strength or size accordingly). The total quantity of drugs dispensed (in terms of number of tablets or millilitres, for example) is shown. This data does not list each individual prescription and does not contain any patient identifiable data.
The data have been edited from their original version. During the data preparation process I:
Dropped obsolete and redundant columns.
Normalized the BNF (British National Formulary) codes, BNF names, and practice codes. These steps reduced the total file size by roughly 75%, at the cost of requiring one table join to access some of the data.
For further details, please see:
FAQ
Glossary of Terms
Acknowledgements
This dataset was kindly released by the United Kingdom's National Health Service under their government open data license v3. You can find this and other datasets at their open data site.
Inspiration
What trends can you see in the data? For example, can you identify the onset of winter based on the types of drugs being prescribed?
The BNF Name entries contain dosage data that I haven't yet cleaned and extracted. Can you unpack that field into item dispensed, units, and dosage? If so, let me know in the forums and I'll add it to the dataset!
Per this blog from Oxford, the raw BNF codes contain quite a bit of information about a drug's function. Can you find a source of open data for translating these codes? It's probable that one exists somewhere at https://www.nhsbsa.nhs.uk/nhs-prescription-services."
UN General Debates,Transcriptions of general debates at the UN from 1970 to 2016,United Nations,23,"Version 2,2017-09-06|Version 1,2017-08-29","international relations
linguistics",CSV,129 MB,CC0,"2,782 views",247 downloads,4 kernels,,https://www.kaggle.com/unitednations/un-general-debates,"Context:
Every year since 1947, representatives of UN member states gather at the annual sessions of the United Nations General Assembly. The centrepiece of each session is the General Debate. This is a forum at which leaders and other senior officials deliver statements that present their government’s perspective on the major issues in world politics. These statements are akin to the annual legislative state-of-the-union addresses in domestic politics. This dataset, the UN General Debate Corpus (UNGDC), includes the corpus of texts of General Debate statements from 1970 (Session 25) to 2016 (Session 71).
Content:
This dataset includes the text of each country’s statement from the general debate, separated by country, session and year and tagged for each. The text was scanned from PDFs of transcripts of the UN general sessions. As a result, the original scans included page numbers in the text from OCR (Optical character recognition) scans, which have been removed. This dataset only includes English.
Acknowledgements:
This dataset was prepared by Alexander Baturo, Niheer Dasandi, and Slava Mikhaylov, and is presented in the paper ""Understanding State Preferences With Text As Data: Introducing the UN General Debate Corpus"" Research & Politics, 2017.
Inspiration:
This dataset includes over forty years of data from different countries, which allows for the exploration of differences between countries and over time. This allows you to ask both country-specific and longitudinal questions. Some questions that might be interesting:
How has the sentiment of each country’s general debate changed over time?
What topics have been more or less popular over time and by region?
Can you build a classifier which identifies which country a given text is from?
Are there lexical or syntactic changes over time or differences between region?
How does the latitude of a country affect lexical complexity?"
World Cities,"All of the world's major cities above 15,000 inhabitants",Open Knowledge International,23,"Version 1,2017-06-14",cities,CSV,852 KB,Other,"4,070 views",517 downloads,3 kernels,0 topics,https://www.kaggle.com/okfn/world-cities,"Utility Data
The data is extracted from geonames, a very exhaustive list of worldwide toponyms. It can be joined with datasets containing geographic fields to facilitate geospatial analysis including mapping.
This datapackage only lists cities above 15,000 inhabitants. Each city is associated with its country and subcountry to reduce the number of ambiguities. Subcountry can be the name of a state (e.g., in United Kingdom or the United States of America) or the major administrative section (e.g., ''region'' in France''). See admin1 field on geonames website for further info about subcountry.
Notice that:
Some cities like Vatican City or Singapore are a whole state so they don't belong to any subcountry. Therefore subcountry is N/A.
There is no guaranty that a city has a unique name in a country and subcountry (At the time of writing, there are about 60 ambiguities). But for each city, the source data primary key geonameid is provided.
Preparation
You can run the script yourself to update the data and publish them to GitHub/Kaggle: see scripts README
Acknowledgments and License
All data is licensed under the Creative Common Attribution License as is the original data from geonames. This means you have to credit geonames when using the data. And while no credit is formally required a link back or credit to Lexman and the Open Knowledge Foundation is much appreciated. This dataset description is reproduced here from its original source with slight modifications."
Flight Route Database,"A database of 59,036 flight routes",OpenFlights,23,"Version 1,2017-08-30",,CSV,2 MB,ODbL,"5,591 views",637 downloads,2 kernels,0 topics,https://www.kaggle.com/open-flights/flight-route-database,"Routes database
As of January 2012, the OpenFlights/Airline Route Mapper Route Database contains 59036 routes between 3209 airports on 531 airlines spanning the globe.
Content
The data is ISO 8859-1 (Latin-1) encoded.
Each entry contains the following information:
Airline 2-letter (IATA) or 3-letter (ICAO) code of the airline.
Airline ID Unique OpenFlights identifier for airline (see Airline).
Source airport 3-letter (IATA) or 4-letter (ICAO) code of the source airport.
Source airport ID Unique OpenFlights identifier for source airport (see Airport)
Destination airport 3-letter (IATA) or 4-letter (ICAO) code of the destination airport.
Destination airport ID Unique OpenFlights identifier for destination airport (see Airport)
Codeshare ""Y"" if this flight is a codeshare (that is, not operated by Airline, but another carrier), empty otherwise.
Stops Number of stops on this flight (""0"" for direct)
Equipment 3-letter codes for plane type(s) generally used on this flight, separated by spaces
The special value \N is used for ""NULL"" to indicate that no value is available.
Notes:
Routes are directional: if an airline operates services from A to B and from B to A, both A-B and B-A are listed separately.
Routes where one carrier operates both its own and codeshare flights are listed only once.
Acknowledgements
This dataset was downloaded from Openflights.org under the Open Database license. This is an excellent resource and there is a lot more on their website, so check them out!"
NBA Players Stats - 2014-2015,"Points, Assists, Height, Weight and other personal details and stats",DrGuillermo,22,"Version 1,2017-05-04",basketball,CSV,78 KB,Other,"6,516 views","1,159 downloads",17 kernels,,https://www.kaggle.com/drgilermo/nba-players-stats-20142015,"Context
This data set can be paired with the shot logs data set from the same season.
Content
Full players stats from the 2014-2015 season + personal details such as height. weight, etc.
The data was scraped and copied from: http://www.basketball-reference.com/teams/ and http://stats.nba.com/leaders#!?Season=2014-15&SeasonType=Regular%20Season&StatCategory=MIN&CF=MIN*G*2&PerMode=Totals"
EMNIST (Extended MNIST),An extended variant of the full NIST dataset,Chris Crawford,22,"Version 3,2017-12-21|Version 2,2017-12-20|Version 1,2017-12-16",machine learning,CSV,1 GB,CC0,"3,040 views",516 downloads,2 kernels,,https://www.kaggle.com/crawford/emnist,"EMNIST
The EMNIST dataset is a set of handwritten character digits derived from the NIST Special Database 19 and converted to a 28x28 pixel image format and dataset structure that directly matches the MNIST dataset. Further information on the dataset contents and conversion process can be found in the paper available at https://arxiv.org/abs/1702.05373v1.
Format
There are six different splits provided in this dataset and each are provided in two formats:
Binary (see emnist_source_files.zip)
CSV (combined labels and images)
Each row is a separate image
785 columns
First column = class_label (see mappings.txt for class label definitions)
Each column after represents one pixel value (784 total for a 28 x 28 image)
ByClass and ByMerge datsets
The full complement of the NIST Special Database 19 is available in the ByClass and ByMerge splits. These two datasets have the same image information but differ in the number of images in each class. Both datasets have an uneven number of images per class and there are more digits than letters. The number of letters roughly equate to the frequency of use in the English language.
train: 697,932
test: 116,323
total: 814,255
classes: ByClass 62 (unbalanced) / ByMerge 47 (unbalanced)
Balanced dataset
The EMNIST Balanced dataset is meant to address the balance issues in the ByClass and ByMerge datasets. It is derived from the ByMerge dataset to reduce mis-classification errors due to capital and lower case letters and also has an equal number of samples per class. This dataset is meant to be the most applicable.
train: 112,800
test: 18,800
total: 131,600
classes: 47 (balanced)
Letters datasets
The EMNIST Letters dataset merges a balanced set of the uppercase and lowercase letters into a single 26-class task.
train: 88,800
test: 14,800
total: 103,600
classes: 37 (balanced)
Digits and MNIST datsets
The EMNIST Digits and EMNIST MNIST dataset provide balanced handwritten digit datasets directly compatible with the original MNIST dataset.
train: Digits 240,000 / MNIST 60,000
test: Digits 40,000 / MNIST 10,000
total: Digits 280,000 / MNIST 70,000
classes: 47 (balanced)
Visual breakdown of EMNIST datasets
Please refer to the EMNIST paper for details on the structure of the dataset https://arxiv.org/abs/1702.05373v1.
Acknowldgements
Cohen, G., Afshar, S., Tapson, J., & van Schaik, A. (2017). EMNIST: an extension of MNIST to handwritten letters.
Dataset retrieved from https://www.nist.gov/itl/iad/image-group/emnist-dataset
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre van Schaik
The MARCS Institute for Brain, Behaviour and Development
Western Sydney University
Penrith, Australia 2751"
Linear Regression,Randomly created dataset for linear regression,Vahe Andonians,22,"Version 2,2017-05-12|Version 1,2017-05-12",,CSV,14 KB,Other,"19,354 views","2,586 downloads",52 kernels,,https://www.kaggle.com/andonians/random-linear-regression,"Context
This is probably the dumbest dataset on Kaggle. The whole point is, however, to provide a common dataset for linear regression. Although such a dataset can easily be generated in Excel with random numbers, results would not be comparable.
Content
The training dataset is a CSV file with 700 data pairs (x,y). The x-values are numbers between 0 and 100. The corresponding y-values have been generated using the Excel function NORMINV(RAND(), x, 3). Consequently, the best estimate for y should be x. The test dataset is a CSV file with 300 data pairs.
Acknowledgements
Thank you, Dan Bricklin and Bob Frankston for inventing the first spreadsheet.
Inspiration
I hope this dataset will encourage all newbies to enter the world of machine learning, possibly starting with a simple linear regression.
Data license
Obviously, data is free."
Scientific Researcher Migrations,Movement of ~742k Scientists,Jacob Boysen,22,"Version 1,2017-08-31",demographics,CSV,34 MB,CC0,"3,123 views",246 downloads,7 kernels,4 topics,https://www.kaggle.com/jboysen/scientist-migrations,"Context:
ORCID provides a persistent digital identifier that distinguishes you from every other researcher and, through integration in key research workflows such as manuscript and grant submission, supports automated linkages between you and your professional activities ensuring that your work is recognized. Find out more.
Content:
This data is a subset of the entire ORCID collection. The subset here was produced by John Bohannon. You can see his excellent Ipython notebook and the entire (300GB!) ORCID archives here.
The data covers ~742k unique researchers and includes:
orcid_id
phd_year
country_2016
earliest_year
earliest_country
has_phd
phd_country
has_migrated
Acknowledgements:
Bohannon J, Doran K (2017) Introducing ORCID. Science 356(6339) 691-692. http://dx.doi.org/10.1126/science.356.6339.691
Additionally, please cite the Dryad data package:
Bohannon J, Doran K (2017) Data from: Introducing ORCID. Dryad Digital Repository. http://dx.doi.org/10.5061/dryad.48s16
Inspiration:
Where do most researchers move to?
What countries experience the largest ‘brain drain’? As a % of population?
Can you predict researcher migration?"
Weather data in New York City - 2016,"Added for the ""New York City Taxi Trip Duration"" challenge",Mathijs Waegemakers,22,"Version 3,2017-09-25|Version 2,2017-07-23|Version 1,2017-07-21",weather,CSV,11 KB,Other,"7,931 views","1,272 downloads",24 kernels,5 topics,https://www.kaggle.com/mathijs/weather-data-in-new-york-city-2016,"Context
As a former transportation student I know how the weather can influence traffic. Both the increase of traffic, as well as the decrease of road conditions increases the travel time.
Content
Weather data collected from the National Weather Service. It contains the first six months of 2016, for a weather station in central park. It contains for each day the minimum temperature, maximum temperature, average temperature, precipitation, new snow fall, and current snow depth. The temperature is measured in Fahrenheit and the depth is measured in inches. T means that there is a trace of precipitation.
Acknowledgements
The data was retrieved on 20th of July, 2017 on the website http://w2.weather.gov/climate/xmacis.php?wfo=okx."
Hacker News Corpus,A subset of all Hacker News articles,Hacker News,22,"Version 2,2017-06-30|Version 1,2017-06-29","news agencies
internet",CSV,1 GB,Other,"10,683 views",325 downloads,4 kernels,,https://www.kaggle.com/hacker-news/hacker-news-corpus,"Context
This dataset contains a randomized sample of roughly one quarter of all stories and comments from Hacker News from its launch in 2006. Hacker News is a social news website focusing on computer science and entrepreneurship. It is run by Paul Graham's investment fund and startup incubator, Y Combinator. In general, content that can be submitted is defined as ""anything that gratifies one's intellectual curiosity"".
Content
Each story contains a story ID, the author that made the post, when it was written, and the number of points the story received.
Please note that the text field includes profanity. All texts are the author’s own, do not necessarily reflect the positions of Kaggle or Hacker News, and are presented without endorsement.
Acknowledgements
This dataset was kindly made publicly available by Hacker News under the MIT license.
Inspiration
Recent studies have found that many forums tend to be dominated by a very small fraction of users. Is this true of Hacker News?
Hacker News has received complaints that the site is biased towards Y Combinator startups. Do the data support this?
Is the amount of coverage by Hacker News predictive of a startup’s success?
Use this dataset with BigQuery
You can use Kernels to analyze, share, and discuss this data on Kaggle, but if you’re looking for real-time updates and bigger data, check out the data in BigQuery, too: https://cloud.google.com/bigquery/public-data/hacker-news
The BigQuery version of this dataset has roughly four times as many articles."
Pantheon Project: Historical Popularity Index,Record of every historical figure with Wikipedia biography in 25+ languages,Massachusetts Institute of Technology,22,"Version 1,2017-03-02","biography
history
internet",CSV,1 MB,CC4,"2,828 views",225 downloads,5 kernels,2 topics,https://www.kaggle.com/mit/pantheon-project,"Context
Pantheon is a project celebrating the cultural information that endows our species with these fantastic capacities. To celebrate our global cultural heritage we are compiling, analyzing and visualizing datasets that can help us understand the process of global cultural development. Dive in, visualize, and enjoy.
Content
The Pantheon 1.0 data measures the global popularity of historical characters using two measures. The simpler of the two measures, which we denote as L, is the number of different Wikipedia language editions that have an article about a historical character. The more sophisticated measure, which we name the Historical Popularity Index (HPI) corrects L by adding information on the age of the historical character, the concentration of page views among different languages, the coefficient of variation in page views, and the number of page views in languages other than English.
For annotations of specific values visit the column metadata in the /Data tab. A more comprehensive breakdown is available on the Parthenon website.
Acknowledgements
Pantheon is a project developed by the Macro Connections group at the Massachusetts Institute of Technology Media Lab. For more on the dataset and to see visualizations using it, visit its landing page on the MIT website.
Inspiration
Which historical figures have a biography in the most languages? Who received the most Wikipedia page views? Which occupations or industries are the most popular? What country has the most individuals with a historical popularity index over twenty?"
Inception V3 Model,Inception V3 Tensorflow Model,Google Brain,22,"Version 1,2017-06-30",artificial intelligence,Other,104 MB,Other,"19,067 views",668 downloads,6 kernels,0 topics,https://www.kaggle.com/google-brain/inception-v3,"Inception-v3 is trained for the ImageNet Large Visual Recognition Challenge using the data from 2012. This is a standard task in computer vision, where models try to classify entire images into 1000 classes, like ""Zebra"", ""Dalmatian"", and ""Dishwasher"".
Here's code on GitHub to train Inception-v3"
Movie Dialog Corpus,A metadata-rich collection of fictional conversations from raw movie scripts,Cornell University,22,"Version 1,2017-07-12","film
linguistics",Other,29 MB,Other,"4,862 views",731 downloads,3 kernels,,https://www.kaggle.com/Cornell-University/movie-dialog-corpus,"Context
This corpus contains a metadata-rich collection of fictional conversations extracted from raw movie scripts:
220,579 conversational exchanges between 10,292 pairs of movie characters
involves 9,035 characters from 617 movies
in total 304,713 utterances
movie metadata included:
genres
release year
IMDB rating
number of IMDB votes
IMDB rating
character metadata included:
gender (for 3,774 characters)
position on movie credits (3,321 characters)
Content
In all files the original field separator was "" +++$+++ "" and have been converted to tabs (\t). Additionally, the original file encoding was ISO-8859-2. It's possible that the field separator conversion and decoding may have left some artifacts.
movie_titles_metadata.txt
contains information about each movie title
fields:
movieID,
movie title,
movie year,
IMDB rating,
no. IMDB votes,
genres in the format ['genre1','genre2',É,'genreN']
movie_characters_metadata.txt
contains information about each movie character
fields:
characterID
character name
movieID
movie title
gender (""?"" for unlabeled cases)
position in credits (""?"" for unlabeled cases)
movie_lines.txt
contains the actual text of each utterance
fields:
lineID
characterID (who uttered this phrase)
movieID
character name
text of the utterance
movie_conversations.txt
the structure of the conversations
fields
characterID of the first character involved in the conversation
characterID of the second character involved in the conversation
movieID of the movie in which the conversation occurred
list of the utterances that make the conversation, in chronological order: ['lineID1','lineID2',É,'lineIDN'] has to be matched with movie_lines.txt to reconstruct the actual content
raw_script_urls.txt
the urls from which the raw sources were retrieved
Acknowledgements
This corpus comes from the paper, ""Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs"" by Cristian Danescu-Niculescu-Mizil and Lillian Lee.
The paper and up-to-date data can be found here: http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html
Please see the README for more information on the authors' collection procedures.
The file formats were converted to TSV and may contain a few errors
Inspiration
What are all of these imaginary people talking about? Are they representative of how real people communicate?
Can you identify themes in movies from certain writers or directors?
How does the dialog change between characters?"
Airline Database,A database of over 5000 airlines,OpenFlights,22,"Version 1,2017-08-29",aviation,CSV,314 KB,ODbL,"6,316 views",634 downloads,2 kernels,0 topics,https://www.kaggle.com/open-flights/airline-database,"Airline database
As of January 2012, the OpenFlights Airlines Database contains 5888 airlines. Some of the information is public data and some is contributed by users.
Content
The data is ISO 8859-1 (Latin-1) encoded.
Each entry contains the following information: - Airline ID Unique OpenFlights identifier for this airline. - Name Name of the airline. - Alias Alias of the airline. For example, All Nippon Airways is commonly known as ""ANA"". - IATA 2-letter IATA code, if available. - ICAO 3-letter ICAO code, if available. - Callsign Airline callsign. - Country Country or territory where airline is incorporated. - Active ""Y"" if the airline is or has until recently been operational, ""N"" if it is defunct. This field is not reliable: in particular, major airlines that stopped flying long ago, but have not had their IATA code reassigned (eg. Ansett/AN), will incorrectly show as ""Y"".
The special value \N is used for ""NULL"" to indicate that no value is available. This is from a MySQL database where \N is used for NULL.
Notes: Airlines with null codes/callsigns/countries generally represent user-added airlines. Since the data is intended primarily for current flights, defunct IATA codes are generally not included. For example, ""Sabena"" is not listed with a SN IATA code, since ""SN"" is presently used by its successor Brussels Airlines.
Acknowledgements
This dataset was downloaded from Openflights.org under the Open Database license. This is an excellent resource and there is a lot more on their website, so check them out!"
A millennium of macroeconomic data,Economic Data for the UK from 1086-2016,Bank of England,22,"Version 4,2017-09-20|Version 3,2017-09-20|Version 2,2017-09-19|Version 1,2017-09-19","history
finance
banking
economics",CSV,25 MB,CC0,"3,358 views",378 downloads,14 kernels,0 topics,https://www.kaggle.com/bank-of-england/a-millennium-of-macroeconomic-data,"The dataset contains a broad set of macroeconomic and financial data for the UK stretching back in some cases to the C13th and with one or two benchmark estimates available for 1086, the year of the Domesday Book. The dataset was originally called the 'Three centuries of macroeconomic data' spreadsheet but has now been renamed given its broader coverage. Version 3 of the dataset has now been updated to 2016.
Content
The Excel file contains the original data. It contains hundreds of time series, while the csv is an extract of several dozen headline time series.
If you would like to see more of the data made available in CSV format; please let me know what you would like extracted and I'll be happy to add it. Please see excel_sheet_names.csv for details of what other data has yet to be unpacked.
Acknowledgements
This dataset was kindly made available by the Bank of England. You can find the original dataset here.
Inspiration
Which metrics give similar answers about when the industrial revolution began? How clear is the cutoff point?
If you like
If you enjoyed this dataset, you might also like the Allen-Unger Global Commodity Prices dataset, which provides historic commodity prices from locations around the world."
Bias Media CAT,Sentiment Bias of News on Catalonia Independence Crisis,Jose Berengueres,22,"Version 14,2017-10-28|Version 13,2017-10-28|Version 12,2017-10-28|Version 11,2017-10-28|Version 10,2017-10-28|Version 9,2017-10-28|Version 8,2017-10-28|Version 7,2017-10-28|Version 6,2017-10-27|Version 5,2017-10-27|Version 4,2017-10-26|Version 3,2017-10-15|Version 2,2017-10-15|Version 1,2017-10-15","journalism
cognitive biases
biases
+ 2 more...",CSV,72 MB,CC0,"2,227 views",123 downloads,7 kernels,,https://www.kaggle.com/harriken/bias-media-cat,"What I talk about when I talk about Catalonia
This is my grain of sand to help in the Catalonia Independence crisis. The Iberian media has been a key driver to incubate disaffection between Catalonia and Spain. For example, a leading newspaper tweeted boycott and Catalonia 9 times in the last month. In this dataset we analyze:
tweets
topics of tweets and
sentiment differentials in news
Context
The dramatic Catalonia independence crisis offers a unique opportunity to analyze bias in news reporting as opinions on the issue are quite polarized (See #catalanReferendum on twitter). In this dataset, we compare how different newspapers (NYT, Washington-Post, Bloomberg...) have reported one singular specific event in the saga: The reply of the Spanish Government to M.H. Puigdemont speech of October 11th of 2017. For each of the 30 newspapers considered, the most popular news article that reported this news is represented as a row in the dataset. Why this news? The Spanish government, published a pdf called (""requerimiento"") which was faxed to Puigdemont. The document requires that Puigdemont reply in five days a clarification of the meaning of his speech. This ""clean"" news offers a rare example where the news is about a written document rather than a speech or an action (usually subjected to more interpretations and biases)
Content
news_...csv each row contains the news article and its translation to English.
all3.csv contains 100k tweets.
Acknowledgements
All the journalists who made this dataset possible. Thanks to @DataCanary for helping make the visualizations better!
Inspiration
I always thought that sentiment analysis was a useless topic, but here there is a chance to use an objective measure to show how polarized reporting has become, (even if sentiment does not account for fakenews, nuances or sarcasm). The linear regressions shows that news written in Spanish language are less positive about the event than the global mean. In other words, sentiment seems strongly biased by language. Bias by location of the newspapers is also analyzed.
Disclaimer
Note that the 'bing' scale is used. Other scales such AFINN might yield different results."
2015 US Traffic Fatalities,Raw Traffic Fatality data for 2015 from the NHTSA,anokas,22,"Version 1,2016-09-18",,CSV,9 MB,Other,"12,990 views","1,913 downloads",49 kernels,7 topics,https://www.kaggle.com/anokas/2015-us-traffic-fatalities,"In 2015, 30,092 people were killed in the US in traffic accidents. This presents a 7.2% rise in fatalities over 2014, a startling change to a 50 year trend of declining fatality rates.
Because of this, the US government has publicly released this data and posted a call to action to try and investigate the data, to try and gain insights into why this is happening.
I am posting the data here in raw format. There are very many files, so I will try to create a more cohesive dataset and add it here in the future. For now I am just uploading the *_AUX.csv files, which contain the most data. I look forward to seeing what visualisations you guys can make from this!
The data is compiled by the National Highway Traffic Safety Administration, and was released in this blog post."
2016 US Presidential Election Vote By County,County-level data on presidential voting,Steve Palley,22,"Version 1,2016-11-20","geography
politics",CSV,2 MB,Other,"6,616 views",790 downloads,13 kernels,,https://www.kaggle.com/stevepalley/2016uspresidentialvotebycounty,"This dataset includes county-level data from the 2016 US Presidential Election.
Data are from Michael W. Kearney's GitHub page, by way of Max Galka's County-Level Results Map on metrocosm.com."
Transcriptomics in yeast,A computational bioinformatics dataset with 92 sets of yeast data on 6000 genes,CostalAether,22,"Version 3,2017-01-24|Version 2,2016-11-28|Version 1,2016-11-25",biology,CSV,10 MB,CC4,"5,151 views",370 downloads,5 kernels,2 topics,https://www.kaggle.com/costalaether/yeast-transcriptomics,"Disclaimer
This is a data set of mine that I though might be enjoyable to the community. It's concerning Next generation sequencing and Transcriptomics. I used several raw datasets, that are public, but the processing to get to this dataset is extensive. This is my first contribution to kaggle, so be nice, and let me know how I can improve the experience. NGS machines are combined the biggest data producer worldwide. So why not add some (more? ) to kaggle.
A look into Yeast transcriptomics
Background
Yeasts ( in this case saccharomyces cerevisiae) are used in the production of beer, wine, bread and a whole lot of Biotech applications such as creating complex pharmaceuticals. They are living eukaryotic organisms (meaning quite complex). All living organisms store information in their DNA, but action within a cell is carried out by specific Proteins. The path from DNA to Protein (from data to action) is simple. a specific region on the DNA gets transcribed to mRNA, that gets translated to proteins. Common assumption says that the translation step is linear, more mRNA means more protein. Cells actively regulate the amount of protein by the amount of mRNA it creates. The expression of each gene depends on the condition the cell is in (starving, stressed etc..) Modern methods in Biology show us all mRNA that is currently inside a cell. Assuming the linearity of the process, we can get more protein the more specific mRNA is available to a cell. Making mRNA an excellent marker for what is actually happening inside a cell. It is important to consider that mRNA is fragile. It is actively replenished only when it is needed. Both mRNA and proteins are expensive for a cell to produce .
Yeasts are good model organisms for this, since they only have about 6000 genes. They are also single cells which is more homogeneous, and contain few advanced features (splice junctions etc.)
( all of this is heavily simplified, let me know if I should go into more details )
The data
files
The following files are provided SC_expression.csv expression values for each gene over the available conditions **labels_CC.csv ** labels for the individual genes , their status and where known intracellular localization ( see below)
Maybe this would be nice as a little competition, I'll see how this one is going before I'll upload the other label files. Please provide some feedback on the presentation, and whatever else you would want me to share.
background
I used 92 samples from various openly available raw datasets, and ran them through a modern RNAseq pipeline. Spanning a range of different conditions (I hid the raw names). The conditions covered stress conditions, temperature and heavy metals, as well as growth media changes and the deletion of specific genes. Originally I had 150 sets, 92 are of good enough quality. Evaluation was done on gene level. Each gene got it's own row, Samples are columns (some are in replicates over several columns) . Expression levels were normalized with by TPM (transcripts per million), a default normalization procedure. Raw counts would have been integers, normalized they are floats.
Analysis and labels
Genes
The function of individual genes is a matter of dispute. Clearly living cells are complex. The inner machinations of cells are not visible. Gene functionality is commonly inferred indirectly by removing a gene, and test the cells behavior. This is time consuming and not very precise. As you can see in the dataset, there is still much to be done to fully understand even single cell yeasts.
The provided dataset is allows for a different approach to functional classification of genes. The label files contained in the set correspond a gene to a specific label. The classification is based on the official Gene Onthology associations classification. I simplified the nomenclature. Gene functionality is usually given in a hierarchical structure. [inside cell --> cytoplasma --> associated to complex A ... ] I'm only keeping high level associations, and using readable terms instead of GO terms. I'll extend if people are interested.
Labels
CC labels concern Cellular Component.
Where the gene is within a cell. goes into details of found associations. the term 'cellular_component' should be seen as E.g the label 'cellular_component' is synonymous with 'unknown location' . CC is the easiest label to attach to a gene. It is the one that can be studied the easiest. Still there are many genes missing.
MF labels concern Molecular Function. What is the gene doing. [upcoming] BP labels concern Biological Processes. What is the genes involvement. [upcoming]
The core interest here is whether it is possible to improve the genes classification by modeling the data. A common assumption says that genes that are expressed in the same conditions have functional relations. There are a bunch of possible applications out there, many of which are limited by our current state of knowledge on the complex systems we observe, or fail to do so. Bringing biology into the realm of data science is an ongoing effort. Having a better insight into the data might very well help.
Note
The dataset is real, and therefore noisy the labels are incomplete even though I'm using the current state of the art. That is how much is known. Using expression levels for classification was already attempted by softwares like SPELL (Serial Pattern of Expression Levels Locator).
Acknowledgements
I guess I own the dataset. It is a by product of another project of mine. If someone is interested in publishing this, contact me.
Inspiration
Unraveling genetic mechanisms is a complex but rewarding task. Humans and yeast are quite similar in many ways. So apart from the fact that we use it for food and medicine, we might actually use knowledge gained from yeast eventually for studying diseases.
Again, any feedback is welcome, Enjoy, CE"
T20 cricket matches,Details of over 6000 T20 matches since 2003,cricketsavant,22,"Version 3,2017-04-12|Version 2,2017-04-12|Version 1,2016-12-20",cricket,CSV,2 MB,Other,"8,740 views","1,305 downloads",11 kernels,,https://www.kaggle.com/imrankhan17/t20matches,"Context
Match details of over 6000 T20 matches, including innings scores and results.
Content
The first 3 columns show the original data that was scraped. The remaining columns are individual data points extracted from these columns.
• match_details: summary of match including stage of tournament (if applicable), home/away teams, venue and date of match.
• result: summary of final result. Includes ties (and any winners as a result of bowl-outs/super overs etc.), no results and abandoned matches.
• scores: summary of scores of both innings. Includes scores even if match ends in a no result. Blank indicates that match was abandoned without a ball bowled.
• date: date of match in standard date format, dd/mm/yyyy. If match goes to reserve day, this date is used.
• venue: city of match. Can be assumed to be main stadium within city. If more than one stadium in a city, it is usually labelled.
• round: stage within tournament, e.g. final, semi-final, group stage etc. Also, includes 1st, 2nd T20I etc. for bilateral series.
• home: home or designated home team.
• away: away or designated away team.
• winner: winner of match including any winners by any method to determine a winner after a tie.
• win_by_runs: number of runs team batting first wins by.
• win_by_wickets: number of wickets team batting second wins by.
• balls_remaining: number of balls remaining for team batting second after win.
• innings1: team batting first
• innings1_runs: first innings score
• innings1_wickets: first innings wickets
• innings1_overs_batted: actual length of first innings
• innings1_overs: maximum length of first innings
• innings2: team batting second
• innings2_runs: second innings score
• innings2_wickets: second innings wickets
• innings2_overs_batted: actual length of second innings
• innings2_overs: maximum length of second innings
• D/L method: 1 means that the D/L method (or VJB method) was used to determine winner.
• target: rain-adjusted target. If blank, target is first innings score plus 1, as normal.
NEW: all T20 series added.
Please let me know if you spot any mistakes!"
Arrests by Baltimore Police Department,Data of 131k arrests made by the Baltimore Police Department,Amandeep Rathee,22,"Version 1,2016-11-23",crime,CSV,19 MB,CC0,"5,553 views",605 downloads,39 kernels,0 topics,https://www.kaggle.com/arathee2/arrests-by-baltimore-police-department,"Context
This data represents the top arrest charge of those processed at Baltimore's Central Booking & Intake Facility. This data does not contain those who have been processed through Juvenile Booking.
Content
The data set was created on October 18, 2011. The data set was last updated on November 18, 2016. It is updated on a monthly basis.
Metadata
Arrest-ID
Age
Sex
Race
ArrestDate
ArrestTime
ArrestLocation
IncidentOffense
IncidentLocation
Charge
ChargeDescription
District
Post
Neighborhood
Location1(Location Coordinates)
Past Research
I have done my own analysis on the data which can be found on the following GitHub repository. Feel free to give any suggestions regarding the data.
Github Link
Inspiration
How arrests vary across different gender, race, age ?
Which area in Baltimore has most number of arrests made ?
What are the top offences and/or charges made while making arrests ?
Acknowledgements
The data is hosted on:
Data set Source
Baltimore Police Depratment's website:
Baltimore Police Department"
Campaign Finance versus Election Results,Can an election be predicted from the preceding campaign finance reports?,danerbland,22,"Version 1,2016-12-08","finance
politics",CSV,594 KB,CC0,"5,072 views",541 downloads,7 kernels,2 topics,https://www.kaggle.com/danerbland/electionfinance,"Context
This dataset was assembled to investigate the possibility of predicting congressional election results by campaign finance reports from the period leading up to the election.
Content
Each row represents a candidate, with information on their campaign including the state, district, office, total contributions, total expenditures, etc. The content is specific to the year leading up to the 2016 election: (1/1/2015 through 10/19/2016).
Acknowledgements
Campaign finance information came directly from FEC.gov. Election results and vote totals for house races were taken from CNN's election results page.
Inspiration
How much of an impact does campaign spending and fundraising have on an election? Is the impact greater in certain areas? Given this dataset, to what degree of accuracy could we have predicted the election results?"
Home Price Index,Housing indexed prices from January 1991 to August 2016,Randy Betancourt,22,"Version 1,2016-12-07",home,CSV,8 MB,CC0,"9,819 views","1,198 downloads",8 kernels,0 topics,https://www.kaggle.com/PythonforSASUsers/hpindex,"Context
The Federal Housing Finance Agency House Price Index (HPI) is a broad measure of the movement of single-family house prices. The HPI is a weighted, repeat-sales index, meaning that it measures average price changes in repeat sales or refinancings on the same properties. The technical methodology for devising the index, collection, and publishing the data is at: http://www.fhfa.gov/PolicyProgramsResearch/Research/PaperDocuments/1996-03_HPI_TechDescription_N508.pdf
Content
Contains monthly and quarterly time series from January 1991 to August 2016 for the U.S., state, and MSA categories. Analysis variables are the aggregate non-seasonally adjusted value and seasonally adjusted index values. The index value is 100 beginning January 1991.
Acknowledgements
This data is found on Data.gov
Inspiration
Can this data be combined with the corresponding census growth projections either at the state or MSA level to forecast 24 months out the highest and lowest home index values?"
Global Food & Agriculture Statistics,Land use and farming inputs,United Nations,22,"Version 2,2017-11-17|Version 1,2017-11-15",agriculture,CSV,453 MB,Other,"3,563 views",433 downloads,,0 topics,https://www.kaggle.com/unitednations/global-food-agriculture-statistics,"FAOSTAT provides access to over 3 million time-series and cross sectional data relating to food and agriculture. The full FAO data can be found in the large zipfile, while a (somewhat out of date) summary of FAOSTAT is in the top level csv files. FAOSTAT contains data for 200 countries and more than 200 primary products and inputs in its core data set. The national version of FAOSTAT, CountrySTAT, is being implemented in about 20 countries and three regions. It offers a two-way bridge amongst sub-national, national, regional and international statistics on food and agriculture.
Acknowledgements
This dataset was kindly published by the United Nation on the UNData site. You can find the original dataset here.
License
Per the UNData terms of use: all data and metadata provided on UNdata’s website are available free of charge and may be copied freely, duplicated and further distributed provided that UNdata is cited as the reference."
"Chronic illness: symptoms, treatments and triggers",How do treatments and environmental stressors impact symptoms?,Flaredown,22,"Version 4,2017-11-03|Version 3,2017-06-24|Version 2,2017-06-06|Version 1,2017-04-17","healthcare
diseases
epidemiology
+ 2 more...",CSV,134 MB,CC4,"3,803 views",433 downloads,5 kernels,0 topics,https://www.kaggle.com/flaredown/flaredown-autoimmune-symptom-tracker,"Introduction
Flaredown is an app that helps patients of chronic autoimmune and invisible illnesses improve their symptoms by avoiding triggers and evaluating their treatments. Each day, patients track their symptom severity, treatments and doses, and any potential environmental triggers (foods, stress, allergens, etc) they encounter.
About the data
Instead of coupling symptoms to a particular illness, Flaredown asks users to create their unique set of conditions, symptoms and treatments (“trackables”). They can then “check-in” each day and record the severity of symptoms and conditions, the doses of treatments, and “tag” the day with any unexpected environmental factors.
User: includes an ID, age, sex, and country.
Condition: an illness or diagnosis, for example Rheumatoid Arthritis, rated on a scale of 0 (not active) to 4 (extremely active).
Symptom: self-explanatory, also rated on a 0–4 scale.
Treatment: anything a patient uses to improve their symptoms, along with an optional dose, which is a string that describes how much they took during the day. For instance “3 x 5mg”.
Tag: a string representing an environmental factor that does not occur every day, for example “ate dairy” or “rainy day”.
Food: food items were seeded from the publicly-available USDA food database. Users have also added many food items manually.
Weather: weather is pulled automatically for the user's postal code from the Dark Sky API. Weather parameters include a description, precipitation intensity, humidity, pressure, and min/max temperatures for the day.
If users do not see a symptom, treatment, tag, or food in our database (for instance “Abdominal Pain” as a symptom) they may add it by simply naming it. This means that the data requires some cleaning, but it is patient-centered and indicates their primary concerns.
Suggested Questions
Does X treatment affect Y symptom positively/negatively/not at all? What are the most strongly-correlated symptoms and treatments?
Are there subsets within our current diagnoses that could more accurately represent symptoms and predict effective treatments?
Can we reliably predict what triggers a flare for a given user or all users with a certain condition?
Could we recommend treatments more effectively based on similarity of users, rather than specific symptoms and conditions? (Netflix recommendations for treatments)
Can we quantify a patient’s level of disease activity based on their symptoms? How different is it from our existing measures?
Can we predict which symptom should be treated to have the greatest effect on a given illness?
How accurately can we guess a condition based on a user’s symptoms?
Can we detect new interactions between treatments?
Please email logan@flaredown.com if you have questions about the project"
Bengali Digit Recognition in the Wild (BDRW),BDRW is a real-world image dataset for recognizing digits in Bengali,DebdootSheet,21,"Version 1,2016-08-18","writing
image data
multiclass classification",Other,1 MB,CC4,"4,063 views",384 downloads,17 kernels,0 topics,https://www.kaggle.com/debdoot/bdrw,"Context: BDRW is a real-world image dataset for developing machine learning and vision algorithms with minimal requirement on data pre-processing and formatting to identify digits of the decimal number system appearing in Bengali script. It can be seen as similar in flavor to SVHN (e.g., the images are of small cropped digits), but incorporates higher visual heterogeneity and comes from a significantly harder, unsolved, real world problem (recognizing digits and numbers in natural scene images). BDRW is obtained from numbers appearing in photographs, printed materials, sign boards, wall writings, calendar or book pages, etc.
File: BDRW_train.zip (contains BDRW_train_1.zip, BDRW_train_2.zip)
The data in the two zip files are to be used together and together contain a set of .jpg images of different sized which are cropped from different photographs, magazine prints, wall writing images, etc. Each image represents a digit from the decimal number system written in Bengali (https://en.wikipedia.org/wiki/Bengali_numerals). The file labels.xls contains the number represented in each image which can be used as the ground truth labels for training a learning based system to recognize the Bengali numbers.
Inspiration: This dataset is released for a machine vision challenge being hosted at IEEE TechSym 2016. The challenge will also include a testing set which includes samples not present in the training set released here and would be released after the challenge is closed."
Filipino Family Income and Expenditure,Annual Household Income and Expenses in the Philippines,Francis Paul Flores,21,"Version 1,2017-10-05","income
finance
demographics",CSV,22 MB,CC0,"3,691 views",571 downloads,4 kernels,2 topics,https://www.kaggle.com/grosvenpaul/family-income-and-expenditure,"Context
The Philippine Statistics Authority (PSA) spearheads the conduct of the Family Income and Expenditure Survey (FIES) nationwide. The survey, which is undertaken every three (3) years, is aimed at providing data on family income and expenditure, including, among others, levels of consumption by item of expenditure, sources of income in cash, and related information affecting income and expenditure levels and patterns in the Philippines.
Content
Inside this data set is some selected variables from the latest Family Income and Expenditure Survey (FIES) in the Philippines. It contains more than 40k observations and 60 variables which is primarily comprised of the household income and expenditures of that specific household
Acknowledgements
The Philippine Statistics Authority for providing the publisher with their raw data
Inspiration
Socio-economic classification models in the Philippines has been very problematic. In fact, not one SEC model has been widely accepted. Government bodies uses their own SEC models and private research entities uses their own. We all know that household income is the greatest indicator of one's socio-economic classification that's why the publisher would like to find out the following:
1) Best model in predicting household income 2) Key drivers of household income, we want to make the model as sparse as possible 3) Some exploratory analysis in the data would also be useful"
Weather Conditions in World War Two,Daily Weather Summaries from 1940-1945,Shane Smith,21,"Version 1,2017-11-01","weather
history
war",CSV,11 MB,Other,"2,591 views",363 downloads,,0 topics,https://www.kaggle.com/smid80/weatherww2,"Context
While exploring the Aerial Bombing Operations of World War Two dataset (https://www.kaggle.com/usaf/world-war-ii), and recalling that the D-Day landings were nearly postponed due to poor weather, I sought out weather reports from the period to compare with missions in the bombing operations dataset.
Content
The dataset contains information on weather conditions recorded on each day at various weather stations around the world. Information includes precipitation, snowfall, temperatures, wind speed and whether the day included thunder storms or other poor weather conditions.
Acknowledgements
The data are taken from the United States National Oceanic and Atmospheric Administration (https://www.kaggle.com/noaa) National Centres for Environmental Information website: https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/world-war-ii-era-data
Inspiration
This dataset is mostly to assist with the analysis of the Aerial Bombing Operations dataset, also hosted on Kaggle."
Question-Answer Dataset,Can you use NLP to answer these questions?,Rachael Tatman,21,"Version 1,2017-09-29","languages
linguistics
artificial intelligence",Other,5 MB,CC3,"3,436 views",398 downloads,,0 topics,https://www.kaggle.com/rtatman/questionanswer-dataset,"Context:
Being able to automatically answer questions accurately remains a difficult problem in natural language processing. This dataset has everything you need to try your own hand at this task. Can you correctly generate the answer to questions given the Wikipedia article text the question was originally generated from?
Content:
There are three question files, one for each year of students: S08, S09, and S10, as well as 690,000 words worth of cleaned text from Wikipedia that was used to generate the questions.
The ""question_answer_pairs.txt"" files contain both the questions and answers. The columns in this file are as follows:
ArticleTitle is the name of the Wikipedia article from which questions and answers initially came.
Question is the question.
Answer is the answer.
DifficultyFromQuestioner is the prescribed difficulty rating for the question as given to the question-writer.
DifficultyFromAnswerer is a difficulty rating assigned by the individual who evaluated and answered the question, which may differ from the difficulty in field 4.
ArticleFile is the name of the file with the relevant article
Questions that were judged to be poor were discarded from this data set.
There are frequently multiple lines with the same question, which appear if those questions were answered by multiple individuals.
Acknowledgements:
These data were collected by Noah Smith, Michael Heilman, Rebecca Hwa, Shay Cohen, Kevin Gimpel, and many students at Carnegie Mellon University and the University of Pittsburgh between 2008 and 2010. It is released here under CC BY_SA 3.0. Please cite this paper if you write any papers involving the use of the data above:
Smith, N. A., Heilman, M., & Hwa, R. (2008, September). Question generation as a competitive undergraduate course project. In Proceedings of the NSF Workshop on the Question Generation Shared Task and Evaluation Challenge.
You may also like:
Question-Answer Jokes: Jokes of the question-answer form from Reddit's r/jokes
Stanford Question Answering Dataset: New Reading Comprehension Dataset on 100,000+ Question-Answer Pairs
Question Pairs Dataset: Can you identify duplicate questions?"
Kaggle Blog: Winners' Posts,Examine trends in machine learning by analyzing winners' posts on No Free Hunch,Kaggle,21,"Version 5,2016-09-21|Version 4,2016-09-21|Version 3,2016-09-21|Version 2,2016-09-21|Version 1,2016-09-21",artificial intelligence,CSV,2 MB,CC0,"7,854 views",294 downloads,26 kernels,0 topics,https://www.kaggle.com/kaggle/kaggle-blog-winners-posts,"In 2010, Kaggle launched its first competition, which was won by Jure Zbontar, who used a simple linear model. Since then a lot has changed. We've seen the rebirth of neural networks, the rise of Python, the creation of powerful libraries like XGBoost, Keras and Tensorflow.
This is data set is a dump of all winners' posts from the Kaggle blog starting with Jure Zbontar. It allows us to track trends in the techniques, tools and libraries that win competitions.
This is a simple dump. If there's demand, I can upload more detail (including comments and tags)."
Stack Overflow Tag Network,Network (links and nodes) of Stack Overflow tags based on Developer Stories,Stack Overflow,21,"Version 1,2017-09-29","internet
programming languages
networks",CSV,18 KB,CC3,"4,188 views",519 downloads,3 kernels,,https://www.kaggle.com/stackoverflow/stack-overflow-tag-network,"Context
On the data team at Stack Overflow, we spend a lot of time and energy thinking about tech ecosystems and how technologies are related to each other. One way to get at this idea of relationships between technologies is tag correlations, how often technology tags at Stack Overflow appear together relative to how often they appear separately. One place we see developers using tags at Stack Overflow is on their Developer Stories, or professional profiles/CVs/resumes. If we are interested in how technologies are connected and how they are used together, developers' own descriptions of their work and careers is a great place to get that.
Content
A network of technology tags from Developer Stories on the Stack Overflow online developer community website.
This is organized as two tables:
stack_network_links contains links of the network, the source and target tech tags plus the value of the the link between each pair stack_network_nodes contains nodes of the network, the name of each node, which group that node belongs to (calculated via a cluster walktrap), and a node size based on how often that technology tag is used
Acknowledgements
All Stack Overflow user contributions are licensed under CC-BY-SA 3.0 with attribution required."
Near-Earth Comets,Heliocentric orbital data for comets that make approaches near Earth's orbit,NASA,21,"Version 1,2016-11-05",space,CSV,25 KB,CC0,"3,930 views",327 downloads,6 kernels,0 topics,https://www.kaggle.com/nasa/near-earth-comets,"NASA tracks about 15,000 near-Earth objects -- small Solar System bodies whose orbits bring them less than 1.3 AU from the Sun (i.e., within 130% of the the average distance between the Earth and the Sun). Of these 15,000, 160 are comets. This dataset provides orbital data for these comets.
The Data
Notes on Time and Space
Timing information for each of these comets is given in Barycentric Dynamical Time, or TDB. This is, very roughly, the number of days since January 1st, 4713 BC (see the Wikipedia article on Julian Day for more info). Check out those Wikipedia articles for details.
For information on inclination, argument, and longitude of the ascending node, look at this article.
The non-gravitational forces are effects that accelerate or decelerate the comet, such as jets of gas.
This dataset contains the following fields:
Object: the name of the comet
Epoch: the epoch for the comet, in TDB
TP: time of perihelion passage, in TDB; this is the time when the comet was closest to the Sun
e: the orbital eccentricity of the comet
i: Inclination of the orbit with respect to the ecliptic plane and the equinox of J2000 (J2000-Ecliptic), in degrees
w: Argument of perihelion (J2000-Ecliptic), in degrees
Node: Longitude of the ascending node (J2000-Ecliptic), in degrees
q: comet's distance at perihelion, in AU
Q: comet's distance at aphelion, in AU
P: orbital period, in Julian years
A1: Non-gravitational force parameter A1
A2: Non-gravitational force parameter A2
A3: Non-gravitational force parameter A3
MOID (AU): Minimum orbit intersection distance (the minimum distance between the osculating orbits of the NEO and the Earth)
ref: Orbital solution reference
What Should We Try?
What can we do with this dataset? - plot the comets' orbits - combine with Earth's orbital data to predict close approaches
Acknowledgements
This dataset was downloaded from the NASA data portal."
CMS Open Payments Dataset 2013,Creating Public Transparency into Industry-Physician Financial Relationship,Centers for Medicare & Medicaid Services,21,"Version 2,2016-11-07|Version 1,2016-11-07","healthcare
finance
health",CSV,2 GB,Other,"3,944 views",311 downloads,2 kernels,0 topics,https://www.kaggle.com/cms/cms-open-payments-dataset-2013,"Context
Open Payments is a national disclosure program created by the Affordable Care Act (ACA) and managed by Centers for Medicare & Medicaid Services (CMS). The purpose of the program is to promote transparency into the financial relationships between pharmaceutical and medical device industries, and physicians and teaching hospitals. The financial relationships may include consulting fees, research grants, travel reimbursements, and payments from industry to medical practitioners.
Content
There are 3 datasets that represent 3 different payment types:
General Payments: Payments not made in connection with a research agreement. This dataset contains 65 variables.
Research Payments: Payments made in connection with a research agreement. This dataset contains 166 variables.
Physician Ownership or Investment Interest: Information about physicians who hold ownership or investment interest in the manufacturer/GPO or who have an immediate family member holding such interest. This dataset contains 29 variables.
Deleted/Removed Records: Contains any deleted/removed records.
A comprehensive methodology overview and data dictionary for each dataset can be found here.
Acknowledgements
The original datasets can be found here.
Inspiration
Using the General Payments dataset, can you determine any trends in the total amount of payment to hospitals and physicians across the medical specialties or by the form/nature of the payments?
According to the Research Payments dataset, which area(s) of research or the type of drug/medical device receive the most amount of payment?"
International Financial Statistics,Global indicators from 1960-2010,United Nations,21,"Version 1,2017-11-15",economics,CSV,7 MB,Other,"3,819 views",501 downloads,2 kernels,0 topics,https://www.kaggle.com/unitednations/international-financial-statistics,"International Financial Statistics (IFS) is a standard source of international statistics on all aspects of international and domestic finance. It reports, for most countries of the world, current data needed in the analysis of problems of international payments and of inflation and deflation, i.e., data on exchange rates, international liquidity, international banking, money and banking, interest rates, prices, production, international transactions, government accounts, and national accounts. Last update in UNdata: 14 May 2010 If you need more current data, the IMF has made their current database available for bulk download for personal use.
Acknowledgements
This dataset was kindly published by the United Nations on the UNData site. You can find the original dataset here.
License
Per the UNData terms of use: all data and metadata provided on UNdata’s website are available free of charge and may be copied freely, duplicated and further distributed provided that UNdata is cited as the reference."
Seattle Library Checkout Records,Twelve years of checkout records,Seattle Public Library,21,"Version 1,2017-10-24","books
libraries",CSV,7 GB,Other,"2,903 views",401 downloads,2 kernels,0 topics,https://www.kaggle.com/seattle-public-library/seattle-library-checkout-records,"Context
This dataset includes a log of all physical item checkouts from Seattle Public Library. The dataset begins with checkouts occurring in April 2005, and is regularly updated. Renewals are not included.
Content
The dataset contains several types of files:
Checkout records hold the raw data. I've dropped several columns from these files in order to shrink the total dataset size down from a couple of dozen gigabytes; they can be rebuilt by merging with the library collection inventory.
The data dictionary allows you to decode the 'ItemType' column from the checkout records.
The library collection inventory is a dataset in its own right and stores important metadata about each title, such as the author and subjects.
Inspiration
Can you predict which books will be checked out in the coming month? SPL posts fresh data every month, so you can check your forecast by downloading the new data from them.
With a bit of imagination, this can be a great dataset for logistics modeling. Make some assumptions about each location's storage capacity and where a book was checked out from and you've got a warehouse resource allocation problem.
Acknowledgements
This dataset was kindly made available by the Seattle Public Library. You can find the original copies of the three component datasets at the following links: - Collection data - Data dictionary - Checkout records
Disclaimer
The data made available here has been modified for use from its original source, which is the City of Seattle. Neither the City of Seattle nor the Office of the Chief Technology Officer (OCTO) makes any claims as to the completeness, timeliness, accuracy or content of any data contained in this application; makes any representation of any kind, including, but not limited to, warranty of the accuracy or fitness for a particular use; nor are any such warranties to be implied or inferred with respect to the information or data furnished herein. The data is subject to change as modifications and updates are complete. It is understood that the information contained in the web feed is being used at one's own risk.
For the complete terms of use, please see the City of Seattle Data Policy."
Chat messages,Urban night city chat messages,Oleksii Nidzelskyi,21,"Version 3,2017-03-07|Version 2,2017-02-14|Version 1,2017-02-14","cities
linguistics
telecommunications",CSV,119 MB,CC0,"6,857 views",539 downloads,7 kernels,,https://www.kaggle.com/onidzelskyi/chat-messages,"Context
Collection of chat messages in night urban city between boys and girls.
Content
Data set of messages (more than 1 million of rows) in Russian language from teenager population taken in period from 2012 to 2016 inclusive
Acknowledgements
All personal info in the message' body were taken from public web source, and, though, are free of use.
Inspiration
This dataset can be used to classify chat messages as male / female.
Key objectives
Extract phone numbers from messages. All phone numbers are located in Ukraine and belongs to one from next operators
+380 50
+380 95
+380 66
+380 99
+380 63
+380 73
+380 93
+380 68
+380 67
+380 96
+380 97
+380 98
Classify chat messages by gender (male/female)"
India Air Quality Data,India's air pollution levels over the years,Shruti Bhargava,21,"Version 1,2017-07-22","india
pollution",CSV,60 MB,Other,"6,331 views","1,135 downloads",2 kernels,0 topics,https://www.kaggle.com/shrutibhargava94/india-air-quality-data,"Context
Since industrialization, there has been an increasing concern about environmental pollution. As mentioned in the WHO report 7 million premature deaths annually linked to air pollution , air pollution is the world's largest single environmental risk. Moreover as reported in the NY Times article, India’s Air Pollution Rivals China’s as World’s Deadliest it has been found that India's air pollution is deadlier than even China's.
Using this dataset, one can explore India's air pollution levels at a more granular scale.
Content
This data is combined(across the years and states) and largely clean version of the Historical Daily Ambient Air Quality Data released by the Ministry of Environment and Forests and Central Pollution Control Board of India under the National Data Sharing and Accessibility Policy (NDSAP).
Visualization of the Mean RSPM values over the years
Inspiration
Can we detect local trends? Can we relate the air quality changes to changes in Environmental policy in India?
Acknowledgements
Vishal Subbiah (Data downloading)"
Run or Walk,A dataset containing labeled sensor data from accelerometer and gyroscope,Viktor Malyi,21,"Version 2,2017-07-19|Version 1,2017-07-13","running
walking",CSV,7 MB,CC4,"5,951 views",591 downloads,13 kernels,,https://www.kaggle.com/vmalyi/run-or-walk,"Context
This dataset complements https://github.com/vmalyi/run-or-walk project which aims to detect whether the person is running or walking based on deep neural network and sensor data collected from iOS device.
This dataset has been accumulated with help of ""Data Collection"" iOS app specially developed for this purpose: https://github.com/vmalyi/run-or-walk/tree/master/ios_app_data_collection.
Please note that this app is not available in the AppStore yet.
Content
Currently, the dataset contains a single file which represents 88588 sensor data samples collected from accelerometer and gyroscope from iPhone 5c in 10 seconds interval and ~5.4/second frequency. This data is represented by following columns (each column contains sensor data for one of the sensor's axes):
acceleration_x
acceleration_y
acceleration_z
gyro_x
gyro_y
gyro_z
There is an activity type represented by ""activity"" column which acts as label and reflects following activities:
""0"": walking
""1"": running
Apart of that, the dataset contains ""wrist"" column which represents the wrist where the device was placed to collect a sample on:
""0"": left wrist
""1"": right wrist
Additionally, the dataset contains ""date"", ""time"" and ""username"" columns which provide information about the exact date, time and user which collected these measurements."
Classified Ads for Cars,Used cars for sale in Germany and Czech Republic since 2015,Miroslav Zoricak,21,"Version 1,2017-03-17",automobiles,CSV,400 MB,CC0,"10,579 views","1,766 downloads",4 kernels,,https://www.kaggle.com/mirosval/personal-cars-classifieds,"Context
The data was scraped from several websites in Czech Republic and Germany over a period of more than a year. Originally I wanted to build a model for estimating whether a car is a good buy or a bad buy based on the posting. But I was unable to create a model I could be satisfied with and now have no use for this data. I'm a great believer in open data, so here goes.
Content
The scrapers were tuned slowly over the course of the year and some of the sources were completely unstructured, so as a result the data is dirty, there are missing values and some values are very obviously wrong (e.g. phone numbers scraped as mileage etc.)
There are roughly 3,5 Million rows and the following columns:
maker - normalized all lowercase
model - normalized all lowercase
mileage - in KM
manufacture_year
engine_displacement - in ccm
engine_power - in kW
body_type - almost never present, but I scraped only personal cars, no motorcycles or utility vehicles
color_slug - also almost never present
stk_year - year of the last emission control
transmission - automatic or manual
door_count
seat_count
fuel_type - gasoline, diesel, cng, lpg, electric
date_created - when the ad was scraped
date_last_seen - when the ad was last seen. Our policy was to remove all ads older than 60 days
price_eur - list price converted to EUR
Inspiration
Which factors determine the price of a car?
With what accuracy can the price be predicted?
Can a model trained on all cars be used to accurately predict prices of models with only a few samples?
In my analysis, there is too much variance even within individual models to reliably predict the price, can you prove me wrong? I would love to understand what I did wrong if you can."
1 M+ Real Time stock market data [NSE/BSE],Real time price volume data for select Nifty 50 stocks from both NSE/BSE,Dipanjan,21,"Version 3,2017-06-24|Version 2,2017-06-20|Version 1,2017-06-15",finance,CSV,211 MB,CC4,"9,073 views",517 downloads,,,https://www.kaggle.com/deeiip/1m-real-time-stock-market-data-nse,"Context
Starting something in FinTech is the most difficult thing. You have no open data. These days I'm trying to do some algo-trading. Maybe not in true sense, because it's not high frequency scalping. But anyway that's that.
What?
The data gives almost-Realtime data for half of the Nifty 50 stocks for last week of May and first 2 Weeks of July.
Now here is the obvious question. The dataset does not have timestamp. That's because it is collected via Web-Socket streaming as it happens. Sometimes once in a couple of seconds, sometimes 10-15 times in the same span. So there is no point to timestamp IMHO. Anyway it'll be client-side timestamp, so not a true timestamp.
Description
tick_data.csv contains only the price-volume data.
volume: total volumes traded for the day
last_price: denotes the quote price for latest trade
List item instrument_list.csv contains description of the underlying instrument.
P.S:
*All the data points are not tick-by-tick update. Rather it is mostly an update after 600 ms, provided a trade happened *"
Donald Trump Comments on Reddit,"Over 183,000 full text comments with post metadata","AndrewMalinow, PhD",21,"Version 5,2017-08-31|Version 4,2017-07-13|Version 3,2017-02-04|Version 2,2017-01-30|Version 1,2017-01-24","politics
linguistics
internet",CSV,22 MB,CC4,"5,105 views",444 downloads,10 kernels,0 topics,https://www.kaggle.com/amalinow/donald-trump-comments-on-reddit,"What is the world saying about Donald Trump? Find out in this dataset of over 37,000 Reddit comments about the new US president.
Photo credit: Gage Skidmore, CC BY-SA 2.0"
UFC Fight Data,Fight-by-fight list of all UFC fights from 2013,Karmanya Aggarwal,21,"Version 5,2017-06-12|Version 4,2017-05-08|Version 3,2017-04-29|Version 2,2017-04-29|Version 1,2017-04-29",sports,CSV,2 MB,CC0,"8,292 views",866 downloads,11 kernels,,https://www.kaggle.com/calmdownkarm/ufcdataset,"Context
List of all UFC fights since 2013 with summed up entries of each fighter's round by round record preceding that fight. Created in the attempt to create a UFC fight winner predictor. Dataset may not be great, I'm still new to this thing so appreciate any tips on cleaning up the set.
Content
Each row represents a single fight - with each fighter's previous records summed up prior to the fight. blank stats mean its the fighter's first fight since 2013 which is where granular data for UFC fights beings
Acknowledgements
https://github.com/valish/ufc-api for the UFC api Beautifulsoup and it's creators and Hitkul my partner in crime
Inspiration
Can we draw decent predictions from this dataset?"
Shakespeare plays,"All of shakespeares plays, characters, lines, and acts in one CSV",LiamLarsen,21,"Version 4,2017-04-28|Version 3,2017-04-26|Version 2,2017-03-25|Version 1,2017-03-24","writing
languages
literature",Other,14 MB,Other,"4,849 views",641 downloads,23 kernels,2 topics,https://www.kaggle.com/kingburrito666/shakespeare-plays,"Context
This is all of Shakespeare's plays.
Content
This is a dataset comprised of all of Shakespeare's plays. It includes the following:
The first column is the Data-Line, it just keeps track of all the rows there are.
The second column is the play that the lines are from.
The third column is the actual line being spoken at any given time.
The fourth column is the Act-Scene-Line from which any given line is from.
The fifth column is the player who is saying any given line.
The sixth column is the line being spoken.
Inspiration
I've been doing Shakespeare for a while and I wanted to make a Shakespearean chatbot."
US Adult Income,Data set of adult income,John Olafenwa,21,"Version 1,2017-07-14","income
economics",CSV,6 MB,CC0,"10,046 views","1,497 downloads",12 kernels,2 topics,https://www.kaggle.com/johnolafenwa/us-census-data,"US Adult Census data relating income to social factors such as Age, Education, race etc.
The Us Adult income dataset was extracted by Barry Becker from the 1994 US Census Database. The data set consists of anonymous information such as occupation, age, native country, race, capital gain, capital loss, education, work class and more. Each row is labelled as either having a salary greater than "">50K"" or ""<=50K"".
This Data set is split into two CSV files, named adult-training.txt and adult-test.txt.
The goal here is to train a binary classifier on the training dataset to predict the column income_bracket which has two possible values "">50K"" and ""<=50K"" and evaluate the accuracy of the classifier with the test dataset.
Note that the dataset is made up of categorical and continuous features. It also contains missing values The categorical columns are: workclass, education, marital_status, occupation, relationship, race, gender, native_country
The continuous columns are: age, education_num, capital_gain, capital_loss, hours_per_week
This Dataset was obtained from the UCI repository, it can be found on
https://archive.ics.uci.edu/ml/datasets/census+income, http://mlr.cs.umass.edu/ml/machine-learning-databases/adult/
USAGE This dataset is well suited to developing and testing wide linear classifiers, deep neutral network classifiers and a combination of both. For more info on Combined Deep and Wide Model classifiers, refer to the Research Paper by Google https://arxiv.org/abs/1606.07792
Refer to this kernel for sample usage : https://www.kaggle.com/johnolafenwa/wage-prediction
Complete Tutorial is available from http://johnolafenwa.blogspot.com.ng/2017/07/machine-learning-tutorial-1-wage.html?m=1"
Automobile Dataset,Dataset consist of various characteristic of an auto,Ramakrishnan Srinivasan,21,"Version 2,2017-05-24|Version 1,2017-05-20",automobiles,CSV,24 KB,Other,"13,231 views","1,741 downloads",27 kernels,,https://www.kaggle.com/toramky/automobile-dataset,"Context
This dataset consist of data From 1985 Ward's Automotive Yearbook. Here are the sources
Sources:
1) 1985 Model Import Car and Truck Specifications, 1985 Ward's Automotive Yearbook. 2) Personal Auto Manuals, Insurance Services Office, 160 Water Street, New York, NY 10038 3) Insurance Collision Report, Insurance Institute for Highway Safety, Watergate 600, Washington, DC 20037
Content
This data set consists of three types of entities: (a) the specification of an auto in terms of various characteristics, (b) its assigned insurance risk rating, (c) its normalized losses in use as compared to other cars. The second rating corresponds to the degree to which the auto is more risky than its price indicates. Cars are initially assigned a risk factor symbol associated with its price. Then, if it is more risky (or less), this symbol is adjusted by moving it up (or down) the scale. Actuarians call this process ""symboling"". A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe.
The third factor is the relative average loss payment per insured vehicle year. This value is normalized for all autos within a particular size classification (two-door small, station wagons, sports/speciality, etc...), and represents the average loss per car per year.
Note: Several of the attributes in the database could be used as a ""class"" attribute.
Inspiration
Please bring it on whatever inferences you can get it."
Homelessness,"Homelessness in the United States, 2007 to 2016",def love(x):,21,"Version 5,2017-08-07|Version 4,2017-08-07|Version 3,2017-08-07|Version 2,2017-07-25|Version 1,2017-07-24","communities
united states
sociology",CSV,7 MB,CC0,"4,951 views",614 downloads,6 kernels,2 topics,https://www.kaggle.com/adamschroeder/homelessness,"Context
The previous New York City policies eliminated all housing resources for homeless families and single adults. I wanted to see the consequences.
Content
""These raw data sets contain Point-in-Time (PIT) estimates and national PIT estimates of homelessness as well as national estimates of homelessness by state and estimates of chronic homelessness from 2007 - 2016. Estimates of homeless veterans are also included beginning in 2011. The accompanying Housing Inventory Count (HIC) data is available as well from 2007 - 2016."" (Department of Housing and Urban Development
Acknowledgements
I would like to thank Matthew Schnars for providing this dataset from: https://www.hudexchange.info/resource/3031/pit-and-hic-data-since-2007/
Inspiration
Many of our fellow human beings go through hardships that we would never know about. But it's our obligation as a society to take care of one another. That's why I was hoping this dataset shine light on some of the challenges our cities and states are still facing in this topic."
News Headlines Of India,16 years of categorized headlines focusing on India,Rohk,21,"Version 3,2018-01-10|Version 2,2017-12-23|Version 1,2017-12-03","news agencies
cities
historiography",CSV,62 MB,CC4,"2,758 views",241 downloads,8 kernels,0 topics,https://www.kaggle.com/therohk/india-headlines-news-dataset,"Context
This dataset is a compilation of 2.7 million news headlines published by Times of India from 2001 to 2017, 17 years.
A majority of the data is focusing on Indian local news including national, city level and entertainment.
Agency Website: https://timesofindia.indiatimes.com
Prepared by Rohit Kulkarni
Content
CSV Rows: 2,735,347
publish_date: Date of the article being published online in yyyyMMdd format
headline_category: Category of the headline, ascii, dot delimited, lowercase values
headline_text: Text of the Headline in English, very rare non-ascii characters
Start Date: 2001-01-01 End Date: 2017-12-31
See This Kernal for Overview of Trends and Categories
Inspiration
This News Dataset is a persistent historical archive of noteable events in the Indian subcontinent from start-2001 to end-2017, recorded in real time by the journalists of India.
Times Group as a news agency, reaches out a very wide audience across Asia and drawfs every other agency in the quantity of English Articles published per day. Due to the heavy daily volume (avg. 650 articles) over multiple years, this data offers a deep insight into Indian society, its priorities, events, issues and talking points and how they have unfolded over time.
It is possible to chop this dataset into a smaller piece for a more focused analysis, based on one or more facets.
Time Range: Records during 2014 election, 2006 Mumbai Bombings
One or more Categories: like Mumbai, Movie Releases, ICC updates, Magazine, Middle East
One or more Keywords: like crime or ecology related words; names of political parties, celebrities, corporations.
Acknowledgements
The headlines are extracted from several GB of raw HTML files using Jsoup, Java and Bash. The entire process takes 3.5 minutes.
This logic also : chooses the best worded headline for each article (longest one is usually picked) ; clusters about 17k categories to 200 large groups ; removes records where the date is ambiguous (9k cases) ; finally cleans the selected headline via a string 'domestication' function (which I use for any wild text from the internet).
The final categories are as per the latest sitemap. Around 1.5k rare categories remain and these records (~20k) can be filtered out easily during analysis. The category is unknown for ~200k records.
Similar news datasets exploring other attributes, countries and topics can be seen on my profile.
Citation for usage:
Rohit Kulkarni (2017), News Headlines of India 2001-2017 [CSV data file], doi:10.7910/DVN/J7BYRX, Retrieved from: [this url]"
Auto-mpg dataset,Mileage per gallon performances of various cars,UCI Machine Learning,21,"Version 3,2017-07-02|Version 2,2017-06-28|Version 1,2017-06-28",automobiles,CSV,18 KB,CC0,"9,018 views",771 downloads,12 kernels,3 topics,https://www.kaggle.com/uciml/autompg-dataset,"Context
The data is technical spec of cars. The dataset is downloaded from UCI Machine Learning Repository
Content
Title: Auto-Mpg Data
Sources: (a) Origin: This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University. The dataset was used in the 1983 American Statistical Association Exposition. (c) Date: July 7, 1993
Past Usage:
See 2b (above)
Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.
Relevant Information:
This dataset is a slightly modified version of the dataset provided in the StatLib library. In line with the use by Ross Quinlan (1993) in predicting the attribute ""mpg"", 8 of the original instances were removed because they had unknown values for the ""mpg"" attribute. The original dataset is available in the file ""auto-mpg.data-original"".
""The data concerns city-cycle fuel consumption in miles per gallon, to be predicted in terms of 3 multivalued discrete and 5 continuous attributes."" (Quinlan, 1993)
Number of Instances: 398
Number of Attributes: 9 including the class attribute
Attribute Information:
mpg: continuous
cylinders: multi-valued discrete
displacement: continuous
horsepower: continuous
weight: continuous
acceleration: continuous
model year: multi-valued discrete
origin: multi-valued discrete
car name: string (unique for each instance)
Missing Attribute Values: horsepower has 6 missing values
Acknowledgements
Dataset: UCI Machine Learning Repository Data link : https://archive.ics.uci.edu/ml/datasets/auto+mpg
Inspiration
I have used this dataset for practicing my exploratory analysis skills."
Tobacco Use 1995-2010,Prevalence and Trends: Four Level Smoking Data,Centers for Disease Control and Prevention,21,"Version 1,2016-11-17",health,CSV,78 KB,Other,"9,878 views","1,456 downloads",35 kernels,,https://www.kaggle.com/cdc/tobacco-use,"Context
This dataset contains the prevalence and trends of tobacco use for 1995-2010. Percentages are weighted to population characteristics. Data are not available if it did not meet Behavioral Risk Factor Surveillance System (BRFSS) stability requirements. For more information on these requirements, as well as risk factors and calculated variables, see the Technical Documents and Survey Data for a specific year - http://www.cdc.gov/brfss/annual_data/annual_data.htm.
Content
This dataset has 7 variables:
Year
State
Smoke everyday
Smoke some days
Former smoker
Never smoked
Location 1
Acknowledgements
The original dataset can be found here.
Recommended citation: Centers for Disease Control and Prevention (CDC). Behavioral Risk Factor Surveillance System. Atlanta, Georgia: U.S. Department of Health and Human Services, Centers for Disease Control and Prevention, [appropriate year].
Inspiration
How does tobacco use change over time?
Does tobacco use differ by state?"
Poverty and Equity Database,Poverty and Inequality Indicators from International Sources,World Bank,21,"Version 1,2016-11-25","income
international relations",CSV,1 MB,Other,"6,239 views",727 downloads,5 kernels,,https://www.kaggle.com/theworldbank/poverty-and-equity-database,"Context
Latest poverty and inequality indicators compiled from officially recognized international sources. Poverty indicators include the poverty headcount ratio, poverty gap, and number of poor at both international and national poverty lines. Inequality indicators include the Gini index and income or consumption distributions. The database includes national, regional and global estimates.
Content
This dataset contains country names and indicator variables from 1974 until 2015. Additional materials and detailed descriptions of the datasets can be downloaded from here.
Acknowledgement
The original datasets and data dictionaries can be found here.
Inspiration
Some ideas for exploring the dataset:
How does the poverty headcount ratio differ across countries? Can you visualize the temporal trend?
Which countries have the highest or lowest GINI index as estimated by the World Bank? Is this indicator correlated with other indicators such as urban/rural poverty or income status?"
Gun violence database,"Archive of U.S. gun violence incidents collected from over 2,000 sources",Gun Violence Archive,21,"Version 1,2016-11-27",crime,CSV,420 KB,Other,"9,064 views","1,773 downloads",13 kernels,,https://www.kaggle.com/gunviolencearchive/gun-violence-database,"Context
The Gun Violence Archive is an online archive of gun violence incidents collected from over 2,000 media, law enforcement, government and commercial sources daily in an effort to provide near-real time data about the results of gun violence. GVA in an independent data collection and research group with no affiliation with any advocacy organization.
Content
This dataset includes files that separate gun violence incidents by category, including deaths and injuries of children and teens, and a collection of mass shootings.
Inspiration
What has been the trend of gun violence in the past few years?
What states have the highest incidents per capita per year? How has this metric changed over time?
Are officer involved shootings on the rise? Where are they most concentrated? Do they correlate with the rates of accidental deaths and mass shootings?
Acknowledgements
This dataset is owned by the Gun Violence Archive, and can be accessed in its original form here."
2016 Olympics in Rio de Janeiro,"Athletes, medals, and events from summer games",Rio 2016,21,"Version 2,2017-01-10|Version 1,2016-12-24",olympic games,CSV,775 KB,CC0,"9,779 views","1,737 downloads",28 kernels,0 topics,https://www.kaggle.com/rio2016/olympic-games,"This dataset consists of the official statistics on the 11,538 athletes and 306 events at the 2016 Olympic Games in Rio de Janeiro. The athletes file includes the name, nationality (as a three letter IOC country code), gender, age (as date of birth), height in meters, weight in kilograms, sport, and quantity of gold, silver, and/or bronze medals won for every Olympic athlete at Rio. The events file lists the name, sport, discipline (if available), gender of competitors, and venue(s) for every Olympic event at Rio 2016.
CREDITS
Source Data: Rio 2016 website
Data Files: GitHub user flother"
Innerwear Data from Victoria's Secret and Others,"600,000+ innerwear product data extracted from popular retail sites",PromptCloud,21,"Version 1,2017-08-09","business
internet",CSV,506 MB,CC4,"6,240 views",811 downloads,,,https://www.kaggle.com/PromptCloudHQ/innerwear-data-from-victorias-secret-and-others,"Context
These datasets provides an opportunity to perform analyses on the fashion trend of innerwear and swimwear products.
Content
They were created by extracting data from from the popular retail sites via PromptCloud's data extraction solutions.
Sites covered:
Amazon
Victoria's Secret
Btemptd
Calvin Klein
Hanky Panky
American Eagle
Macy's
Nordstrom
Topshop USA
Time period: June, 2017 to July, 2017
Inspiration
Some of the most common questions that can be answered are:
How does the pricing differ depending on the brand?
Topic modelling on the product description
What are the most common color used by different brands?
Analyses on the product ratings (wherever applicable)
Common style attributes (wherever applicable)"
Synthetic data from a financial payment system,Synthetic datasets generated by the BankSim payments simulator,TESTIMON @ NTNU,21,"Version 1,2017-07-11",finance,CSV,78 MB,CC4,"5,766 views",630 downloads,3 kernels,,https://www.kaggle.com/ntnu-testimon/banksim1,"Context
BankSim is an agent-based simulator of bank payments based on a sample of aggregated transactional data provided by a bank in Spain. The main purpose of BankSim is the generation of synthetic data that can be used for fraud detection research. Statistical and a Social Network Analysis (SNA) of relations between merchants and customers were used to develop and calibrate the model. Our ultimate goal is for BankSim to be usable to model relevant scenarios that combine normal payments and injected known fraud signatures. The data sets generated by BankSim contain no personal information or disclosure of legal and private customer transactions. Therefore, it can be shared by academia, and others, to develop and reason about fraud detection methods. Synthetic data has the added benefit of being easier to acquire, faster and at less cost, for experimentation even for those that have access to their own data. We argue that BankSim generates data that usefully approximates the relevant aspects of the real data.
Content
We ran BankSim for 180 steps (approx. six months), several times and calibrated the parameters in order to obtain a distribution that get close enough to be reliable for testing. We collected several log files and selected the most accurate. We injected thieves that aim to steal an average of three cards per step and perform about two fraudulent transactions per day. We produced 594643 records in total. Where 587443 are normal payments and 7200 fraudulent transactions. Since this is a randomised simulation the values are of course not identical to original data.
Acknowledgements
This research was conducted during my PhD studies in Sweden at Blekinge Institute of Technology (BTH ww.bth.se). More about it: http://edgarlopez.net
Original paper
Please refer to this dataset using the following citations:
Lopez-Rojas, Edgar Alonso ; Axelsson, Stefan Banksim: A bank payments simulator for fraud detection research Inproceedings 26th European Modeling and Simulation Symposium, EMSS 2014, Bordeaux, France, pp. 144–152, Dime University of Genoa, 2014, ISBN: 9788897999324. https://www.researchgate.net/publication/265736405_BankSim_A_Bank_Payment_Simulation_for_Fraud_Detection_Research"
Religious Texts Used By ISIS,"A compilation of 2,685 religious texts cited by ISIS over a 3 year period",Fifth Tribe,21,"Version 1,2017-09-01","islam
politics
terrorism",CSV,1 MB,CC0,"6,015 views",311 downloads,,2 topics,https://www.kaggle.com/fifthtribe/isis-religious-texts,"Context
Religious texts play a key role in ISIS ideology, propaganda, and recruitment. This dataset is a compilation of all of the religious and ideological texts (Muslim, Christian, Jewish, and other) used in ISIS English-based magazines.
Content
We scraped 15 issues of Dabiq (6/2014 to 7/2016) and 9 issues of Rumiyah (9/2016 to 5/2017) producing a total of 2,685 texts. We classified the data as follows:
Magazine: Dabiq or Rumiyah
Issue #
Date (Month and Year)
Type of Text (Qur'an, Hadeeth, Religious Figure, etc)
Source: What the source of the text was
The quote itself
Purpose: Support ISIS, Refute Another Group
The article from which the quote is derived
Acknowledgements
We would like to thank Asma Shah for helping to compile this dataset. Asma is a junior at the University of Maryland, College Park, where she is studying criminal justice and computer science. She is currently an intern with the Department of Justice where she does data science work. She has previously done counter-terrorism research with Fifth Tribe and the National Consortium for the Studies of Terrorism and Responses to Terrorism.
We would also like to express our gratitude to Abdul Aziz Suraqah (Canada), Saif ul Hadi (India), and Abdulbasit Kassam (Nigeria) for helping with the classification of some of the more obscure texts.
Inspiration
We would like this data to be analyzed by religious clerics to develop rebuttals of ISIS propaganda, data scientists to generate insights from the texts, and policymakers to understand how faith can shape countering violent extremism efforts. We also need help classifying some of the data that could not be identified and has been marked 'unknown.'"
DJIA 30 Stock Time Series,Historical stock data for DIJA 30 companies (2006-01-01 to 2018-01-01),szrlee,21,"Version 3,2018-01-03|Version 2,2018-01-02|Version 1,2018-01-01","stochastic processes
time series
finance",CSV,6 MB,CC0,"2,136 views",420 downloads,5 kernels,0 topics,https://www.kaggle.com/szrlee/stock-time-series-20050101-to-20171231,"Context
The script used to acquire all of the following data can be found in this GitHub repository. This repository also contains the modeling codes and will be updated continually, so welcome starring or watching!
Stock market data can be interesting to analyze and as a further incentive, strong predictive models can have large financial payoff. The amount of financial data on the web is seemingly endless. A large and well structured dataset on a wide array of companies can be hard to come by. Here provided a dataset with historical stock prices (last 12 years) for 29 of 30 DJIA companies (excluding 'V' because it does not have the whole 12 years data).
      ['MMM', 'AXP', 'AAPL', 'BA', 'CAT', 'CVX', 'CSCO', 'KO', 'DIS', 'XOM', 'GE',

      'GS', 'HD', 'IBM', 'INTC', 'JNJ', 'JPM', 'MCD', 'MRK', 'MSFT', 'NKE', 'PFE',

      'PG', 'TRV', 'UTX', 'UNH', 'VZ', 'WMT', 'GOOGL', 'AMZN', 'AABA']
In the future if you wish for a more up to date dataset, this can be used to acquire new versions of the .csv files.
Content
The data is presented in a couple of formats to suit different individual's needs or computational limitations. I have included files containing 13 years of stock data (in the all_stocks_2006-01-01_to_2018-01-01.csv and corresponding folder) and a smaller version of the dataset (all_stocks_2017-01-01_to_2018-01-01.csv) with only the past year's stock data for those wishing to use something more manageable in size.
The folder individual_stocks_2006-01-01_to_2018-01-01 contains files of data for individual stocks, labelled by their stock ticker name. The all_stocks_2006-01-01_to_2018-01-01.csv and all_stocks_2017-01-01_to_2018-01-01.csv contain this same data, presented in merged .csv files. Depending on the intended use (graphing, modelling etc.) the user may prefer one of these given formats.
All the files have the following columns: Date - in format: yy-mm-dd
Open - price of the stock at market open (this is NYSE data so all in USD)
High - Highest price reached in the day
Low Close - Lowest price reached in the day
Volume - Number of shares traded
Name - the stock's ticker name
Inspiration
This dataset lends itself to a some very interesting visualizations. One can look at simple things like how prices change over time, graph an compare multiple stocks at once, or generate and graph new metrics from the data provided. From these data informative stock stats such as volatility and moving averages can be easily calculated. The million dollar question is: can you develop a model that can beat the market and allow you to make statistically informed trades!
Acknowledgement
This Data description is adapted from the dataset named 'S&P 500 Stock data'. This data is scrapped from Google finance using the python library 'pandas_datareader'. Special thanks to Kaggle, Github and the Market."
WIDS_DataThon_2018,,Awalin Sopan,21,"Version 1,2018-02-03",,CSV,9 MB,CC0,888 views,120 downloads,,2 topics,https://www.kaggle.com/awalinsopan/wids2018,This dataset does not have a description yet.
Flowers Recognition,This dataset contains labled 4242 images of flowers.,Alexander Mamaev,21,"Version 1,2018-01-06","photography
plants
machine learning
+ 2 more...",Other,225 MB,Other,"2,997 views",525 downloads,,0 topics,https://www.kaggle.com/alxmamaev/flowers-recognition,"Context
This dataset contains 4242 images of flowers. The data collection is based on the data flicr, google images, yandex images. You can use this datastet to recognize plants from the photo.
Content
The pictures are divided into five classes: chamomile, tulip, rose, sunflower, dandelion. For each class there are about 800 photos. Photos are not high resolution, about 320x240 pixels. Photos are not reduced to a single size, they have different proportions!"
HappyDB,"A Corpus of 100,000 Crowdsourced Happy Moments",Recruit Institute of Technology,21,"Version 1,2018-01-13","personal life
emotion
linguistics",CSV,5 MB,CC0,"2,832 views",161 downloads,,0 topics,https://www.kaggle.com/ritresearch/happydb,"Description:
HappyDB is a corpus of more than 100,000 happy moments crowd-sourced via Amazon’s Mechanical Turk.
Each worker is given the following task: What made you happy today? Reflect on the past 24 hours, and recall three actual events that happened to you that made you happy. Write down your happy moment in a complete sentence. (Write three such moments.)
The goal of the corpus is to advance the understanding of the causes of happiness through text-based reflection.
More information is available on the HappyDB website (https://rit-public.github.io/HappyDB/).
Content:
cleaned_hm.csv: the cleaned-up corpus of 100,000 crowd-sourced happy moments. For cleaning up, we have done spell checking over the whole corpus, and remove empty or one-word statements. The raw happy moments are retained for reference, and the author of each happy moment is represented by the his/her worker ID.
demographic.csv: the demographic information of the worker who provided the moment. The information includes worker id, age, country, gender, marital status, and status of parenthood.
Have fun with the data! Feel free to contact us with any questions.
Sample questions:
To provide some inspiration, here are a few sample interesting exploration questions.
What are the popular sports/movies/books/purchased products/tourist destinations/... that make people happy?
Can we predict gender/marriage status/parenthood/age groups based on happy moment texts?
How many indoor and outdoor activities are in the corpus respectively?
Can we find interesting ways of clustering happy moments?
Citation:
Please cite the following publication if you are using the dataset for your work:
HappyDB: A Corpus of 100,000 Crowdsourced Happy Moments, LREC 2018 (to appear)
Akari Asai, Sara Evensen, Behzad Golshan, Alon Halevy, Vivian Li, Andrei Lopatenko, Daniela Stepanov, Yoshihiko Suhara, Wang-Chiew Tan and Yinzhan Xu"
Articles sharing and reading from CI&T DeskDrop,Logs of users interactions on shared articles for content Recommender Systems,Gabriel Moreira,20,"Version 5,2017-08-28|Version 4,2017-08-28|Version 3,2017-08-28|Version 2,2017-08-28|Version 1,2017-08-28","web sites
human-computer interaction
internet",CSV,29 MB,ODbL,"2,511 views",328 downloads,3 kernels,0 topics,https://www.kaggle.com/gspmoreira/articles-sharing-reading-from-cit-deskdrop,"Context
Deskdrop is an internal communications platform developed by CI&T, focused in companies using Google G Suite. Among other features, this platform allows companies employees to share relevant articles with their peers, and collaborate around them.
Content
This rich and rare dataset contains a real sample of 12 months logs (Mar. 2016 - Feb. 2017) from CI&T's Internal Communication platform (DeskDrop).
I contains about 73k logged users interactions on more than 3k public articles shared in the platform.
This dataset features some distinctive characteristics:
Item attributes: Articles' original URL, title, and content plain text are available in two languages (English and Portuguese).
Contextual information: Context of the users visits, like date/time, client (mobile native app / browser) and geolocation.
Logged users: All users are required to login in the platform, providing a long-term tracking of users preferences (not depending on cookies in devices).
Rich implicit feedback: Different interaction types were logged, making it possible to infer the user's level of interest in the articles (eg. comments > likes > views).
Multi-platform: Users interactions were tracked in different platforms (web browsers and mobile native apps)
If you like it, please upvote!
Take a look in these featured Python kernels:
- Deskdrop datasets EDA: Exploratory analysis of the articles and interactions in the dataset
- DeskDrop Articles Topic Modeling: A statistical analysis of the main articles topics using LDA
- Recommender Systems in Python 101: A practical introduction of the main Recommender Systems approaches: Popularity model, Collaborative Filtering, Content-Based Filtering and Hybrid Filtering.
Acknowledgements
We thank CI&T for the support and permission to share a sample of real usage data from its internal communication platform: Deskdrop.
Inspiration
The two main approaches for Recommender Systems are Collaborative Filtering and Content-Based Filtering.
In the RecSys community, there are some popular datasets available with users ratings on items (explicit feedback), like MovieLens and Netflix Prize, which are useful for Collaborative Filtering techniques.
Therefore, it is very difficult to find open datasets with additional item attributes, which would allow the application of Content-Based filtering techniques or Hybrid approaches, specially in the domain of ephemeral textual items (eg. articles and news).
News datasets are also reported in academic literature as very sparse, in the sense that, as users are usually not required to log in in news portals, IDs are based on device cookies, making it hard to track the users page visits in different portals, browsing sessions and devices.
This difficult scenario for research and experiments on Content Recommender Systems was the main motivation for the sharing of this dataset."
Brazilian Federal Legislative activity,"Datasets of congresspeople attendance, votes and propositions since past century",Irio Musskopf,20,"Version 2,2017-12-27|Version 1,2017-12-10","journalism
brazil
south america
politics",CSV,53 MB,CC0,990 views,50 downloads,,0 topics,https://www.kaggle.com/iriomk/brazilian-federal-legislative-activity,"Brazilian? You can read a Portuguese version of this article here.
Context
Last year, while I was attending a data science course in Germany, my country was impeaching its president. My colleagues asked me to explain what was happening in Brazil and the possible political outcomes in South America. Although I was able to give a general context and tell multiple arguments in favor and against the impeachment, deep inside, my answer was ""I really don't know"".
Understanding what happens in Politics is something that takes a lot of effort and research. When I decided I had to use my tech skills to make myself a better citizen, I dived into government data and started Operation Serenata de Amor.
After reporting hundreds of politicians for small acts of corruption and learning how to encourage the population to engage in the democratic processes, my studies drove me to understand the legislative activity.
Brazilians elect 594 citizens to be their representatives in the National Congress. How can we be sure that they are not defending their own interests or those who paid for their campaigns? My way, as a data scientist, is to ask the data.
Content
The National Congress of Brazil is composed of a Lower (Chamber of Deputies) and an Upper House (Federal Senate). In the first version of this dataset, you are going to find data only from the Chamber of Deputies. With 513 representatives, 86% of the congresspeople, I hope you have enough data to explore for some time.
Would be impossible for me, a citizen without government ties, to collect this data without the help of public servants. I processed 9,717 fixed-width files and 73 XML's made officially available by the Chamber of Deputies and created 5 CSV's containing the same information. Multiple fields of the same file telling the same thing (e.g. body_id, body_name and body_abbreviation) were removed.
Data on session attendance, votes, and propositions since past century were collected and scripted in a reproducible manner. The data collection and pre-processing scripts are available in a GitHub repository, under an open source license.
Everything was collected from the Chamber of Deputies website at December 27, 2017, containing the whole legislative activity of the year. Presence and votes date from 1999, propositions go as far as 1946.
When in question about the legislative process and how the sessions work in real world, the Internal Regulation of the Chamber of Deputies is the best Portuguese documentation for research. It's free!
Acknowledgements
Since the data was collected from a government website and the Brazilian law states that access to this information is free to any citizen, I am placing my own work published here in Public Domain.
I'd like to thank the hundreds of people financially supporting the work of Operation Serenata de Amor and those responsible for passing the Information Access bill in 2011.
Inspiration
The legislative activity should tell the history while it's happening. How much has the Congress changed over the past decades? Do the congresspeople maintain the same political views or they vary on a weekly basis? Do people vote together with their state or party peers? How often? Can you model an algorithm to tell us the real parties inside Brazilian Congress?"
Car Insurance Cold Calls,We help the guys and girls at the front to get out of Cold Call Hell,GregKondla,20,"Version 1,2017-06-16",business,CSV,951 KB,Other,"5,952 views",945 downloads,7 kernels,2 topics,https://www.kaggle.com/kondla/carinsurance,"Introduction
Here you find a very simple, beginner-friendly data set. No sparse matrices, no fancy tools needed to understand what's going on. Just a couple of rows and columns. Super simple stuff. As explained below, this data set is used for a competition. As it turns out, this competition tends to reveal a common truth in data science: KISS - Keep It Simple Stupid
What is so special about this data set is, given it's simplicity, it pays off to use ""simple"" classifiers as well. This year's competition was won by a C5.0 . Can you do better?
Description
We are looking at cold call results. Turns out, same salespeople called existing insurance customers up and tried to sell car insurance. What you have are details about the called customers. Their age, job, marital status, whether the have home insurance, a car loan, etc. As I said, super simple.
What I would love to see is some of you applying some crazy XGBoost classifiers, which we can square off against some logistic regressions. It would be curious to see what comes out on top. Thank you for your time, I hope you enjoy using the data set.
Acknowledgements
Thanks goes to the Decision Science and Systems Chair of Technical University of Munich (TUM) for getting the data set from a real world company and making it available to be shared publicly. Also Vladimir Fux, who oversees the challenge associated with this data set.
Inspiration
This is a data set used for teaching entry level data mining skills at the TUM. Every year there is a competition as part of the curriculum of a particular course. This Data Mining Cup teaches some of the very fundamentals that are always worthy to be revisited, especially by pros abundant at Kaggle. For some of my thoughts see the verbose comments in the Kernel."
Higher Education Analytics,Unit level survey data from 2011-12 to 2015-16.,Rajanand Ilangovan / இராஜ்ஆனந்த் இளங்கோவன்,20,"Version 1,2017-08-13","india
education",Other,2 GB,CC4,"5,020 views",542 downloads,,0 topics,https://www.kaggle.com/rajanand/aishe,"Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks.
Context
Ministry of Human Resource Development (MHRD), Govt of India has initiated an All India Survey on Higher Education (AISHE) in the year 2010-11 to build a robust database and to assess the correct picture of higher Education in the country.
The main objectives of the survey was to
identify & capture all the institutions of higher learning in the country
Collect the data from all the higher education institutions on various aspects of higher education.
Data was collected on following broad items
Institution’s Basic Details
Teacher’s Details
Details of Non-Teaching Staff
Programme conducted under various Faculties/Schools & Departments/Centres
Students enrolled in these Programme
Examination result of terminal year of each Programme
Financial Information such as Receipt and Expenditure under various heads
Availability of Infrastructure
Scholarships, Loans & Accreditation
source: AISHE(pdf)
Content
This dataset contains unit level data from AISHE from 2011-12 to 2015-16.
csv file list:
accreditation.csv
college.csv
college_institution.csv
college_institution_accreditation.csv
college_institution_department.csv
college_institution_faculty.csv
college_institution_non_teaching_staff_count.csv
college_institution_student_hostel.csv
college_institution_teaching_staff.csv
college_institution_teaching_staff_sanctioned_strength.csv
course.csv
course_enrolled_foreign_student_count.csv
course_enrolled_student_count.csv
course_examination_result.csv
department.csv
educational_institution_course.csv
enrolled_distance_student_university.csv
enrolled_distance_student_university_count.csv
enrolled_foreign_student_count.csv
enrolled_student_count.csv
examination_result.csv
faculty.csv
faculty_department.csv
infrastructure.csv
loan.csv
MetaData.csv
non_teaching_staff_count.csv
other_minority_college_regular .csv
other_minority_standalone_distance.csv
other_minority_standalone_regular .csv
other_minority_university_distance.csv
other_minority_university_regular .csv
persons_count_by_category.csv
private_students_result.csv
ref_broad_discipline_group.csv
ref_broad_discipline_group_category.csv
ref_college_institution_statutory_body.csv
ref_count_by_category_remarks.csv
ref_country.csv
ref_course_level.csv
ref_course_mode.csv
ref_course_type.csv
ref_diploma_course.csv
ref_district.csv
ref_examination_system.csv
ref_institute_type.csv
ref_institution_management.csv
ref_non_teaching_staff_group.csv
ref_non_teaching_staff_type.csv
ref_programme.csv
ref_programme_broad_discipline_group_and_category.csv
ref_programme_statutory_body.csv
ref_speciality.csv
ref_standalone_institution.csv
ref_state.csv
ref_state_body.csv
ref_student_hostel_type.csv
ref_teaching_staff_designation.csv
ref_teaching_staff_selection_mode.csv
ref_university.csv
ref_university_college_type.csv
ref_university_type.csv
regional_center.csv
scholarship.csv
staff_quarter.csv
standalone_institution.csv
standalone_institution_accreditation.csv
standalone_institution_department.csv
standalone_institution_faculty.csv
standalone_institution_non_teaching_staff_count.csv
standalone_institution_student_hostel.csv
standalone_institution_teaching_staff.csv
standalone_institution_teaching_staff_sanctioned_strength.csv
student_hostel.csv
teaching_staff.csv
teaching_staff_count.csv
teaching_staff_sanctioned_strength.csv
university.csv
university_accreditation.csv
university_department.csv
university_enrolled_distance_student.csv
university_faculty.csv
university_non_teaching_staff_count.csv
university_private_students_result.csv
university_student_hostel.csv
university_teaching_staff.csv
university_teaching_staff_sanctioned_strength.csv
Acknowledgements
Ministry of Human Resource Development (MHRD), Govt of India has published this dataset on Open Govt Data India Platform under Govt. open data license - India.
MHRD has also published some reports from this survey.
Inspiration
This is an interesting dataset to get the holistic picture of higher education system in India. One of the main objective of dept. of higher education is to increase the gross enrolment ratio (GRT) to 15% by 2011-12 and to 21% by 12th five year plan (2012-17). One can look at things like the objective like this has been achieved or can be achieved based on the progress of past data. There are several other things that can be analysed from this dataset.
Pupil-Teacher Ratio (PTR)
Out-Turn
Gender Parity Index (GPI) etc.,"
Weekly Sales Transactions,Weekly purchase quantities of over 800 products over 52 weeks,Chris Crawford,20,"Version 1,2017-08-23","timelines
time series
business
product",CSV,310 KB,Other,"7,307 views","1,119 downloads",,,https://www.kaggle.com/crawford/weekly-sales-transactions,"Context
Contains weekly purchased quantities of 800 over products over 52 weeks. These data were used in the paper ""Time series clustering: A superior alternative for market basket analysis"" by Tan, Swee Chuan and San Lau, Jess Pei.
Content
Each row represents a different product
Each column represents a week of the year (52 total weeks). The last half of the columns are normalized for you.
Values represent quantity of the products sold during the week
52 weeks: W0, W1, ..., W51
Normalised vlaues of weekly data: Normalised 0, Normalised 1, ..., Normalised 51
Acknowledgements
Tan, Swee Chuan and San Lau, Jess Pei, Time series clustering: A superior alternative for market basket analysis.
This dataset was downloaded from the UCI Machine Learning Repository. https://archive.ics.uci.edu/ml/datasets/Sales_Transactions_Dataset_Weekly"
Saturday Night Live,Over 40 seasons of hilarious data,Hendrik Hilleckes,20,"Version 11,2018-02-09|Version 10,2017-11-21|Version 9,2017-10-18|Version 8,2017-10-05|Version 7,2017-06-08|Version 6,2017-04-20|Version 5,2017-03-22|Version 4,2017-03-09|Version 3,2017-02-17|Version 2,2017-02-10|Version 1,2017-02-10","popular culture
film",CSV,1 MB,Other,"6,831 views",653 downloads,18 kernels,2 topics,https://www.kaggle.com/hhllcks/snldb,"Context
I was thinking about a dataset that I could provide and when I was reading through the LiveFromNewYork subreddit I got the idea: what about a Saturday Night Live dataset? Wouldn't it be fun to analyze the data about a TV show that airs since the 70s?
Content
I aim to improve the dataset over time and update the files with more data. But I think that I have enough already so that people can work with it.
There are files for the following objects:
season
episode
title
actor
actor_title (mapping of actors and titles)
rating (episode rating from IMDb.com)
Acknowledgements
A lot of the data comes from http://www.snlarchives.net where Joel Navaroli (@snlmedia) created a great snl archive. You can find everything about SNL there. Want to know about the 5th sketch in the 3rd episode in season 13? Go there to find out!
I got the ratings from IMDb.com.
I used Scrapy to get the data from the websites.
Inspiration
Since SNL is such a long running TV show I thought it would be interesting to see how it developed over time. There are also some prejudices around, like ""there was a big slump from season X to Y"". Do the user ratings reflect that? I provided an example analysis, so that everyone can get started easily with the data.
Source
You can find everything about the dataset in the GitHub repository: http://www.github.com/hhllcks/snldb"
Airline Fleets,The top 100+ airlines and their fleet specifics.,traceyvanp,20,"Version 1,2017-02-10",aviation,CSV,100 KB,Other,"6,671 views",846 downloads,24 kernels,,https://www.kaggle.com/traceyvanp/airlinefleet,"Planespotters.net has a full database on airlines around the world and the airplanes that each owns and operates. This dataset collects the top 100+ airlines in the world (by the size of their fleet). It is combined with information found on Wikipedia on the respective airline's fleet and the average value/cost of the manufactured airplane.
Updated January 2017.
Dataset includes:
Parent Airline: i.e. International Airlines Group (IAG)
Airline: i.e. Iberia, Aer Lingus, British Airways...etc. which are owned by IAG
Aircraft Type: Manufacturer & Model
Current: Quantity of airplanes in Operation
Future: Quantity of airplanes on order, from planespotter.net
Order: Quantity airplanes on order, from Wikipedia
Unit Cost: Average unit cost ($M) of Aircraft Type, as found by Wikipedia and various google searches
Total Cost: Current quantity * Unit Cost ($M)
Average Age: Average age of ""Current"" airplanes by ""Aircraft Type""
Sources: Planespotters.net Wikipedia.org"
EURUSD - 15m - 2010-2016,"FOREX currency rates data for EURUSD, 15 minute candles, BID, years 2010-2016",Michal Januszewski,20,"Version 2,2017-02-22|Version 1,2017-02-22","finance
money",CSV,14 MB,CC4,"4,508 views",483 downloads,11 kernels,3 topics,https://www.kaggle.com/meehau/EURUSD,"Context
I've always wanted to have a proper sample Forex currency rates dataset for testing purposes, so I've created one.
Content
The data contains Forex EURUSD currency rates in 15-minute slices (OHLC - Open High Low Close, and Volume). BID price only. Spread is not provided, so be careful.
(Quick reminder: Bid price + Spread = Ask price)
The dates are in the yyyy-mm-dd hh:mm format, GMT. Volume is in Units.
Acknowledgements
Dukascopy Bank SA https://www.dukascopy.com/swiss/english/marketwatch/historical/
Inspiration
Just would like to see if there is still an way to beat the current Forex market conditions, with the prop traders' advanced automatic algorithms running in the wild."
World Language Family Map,Where are the world’s language families used?,Rachael Tatman,20,"Version 1,2017-07-21","languages
geography
linguistics",CSV,198 MB,CC4,"2,250 views",252 downloads,,,https://www.kaggle.com/rtatman/world-language-family-map,"Context:
Glottolog (http://glottolog.org) provides a comprehensive catalogue of the world's languages, language families and dialects. It assigns a unique and stable identifier (the Glottocode) to (in principle) all languoids, i.e. all families, languages, and dialects.
Content:
This dataset contains information on 1) the geographic location of languages and dialects, 2) their familial relationships and 3) a list of scholarly sources where information on languages was found.
Acknowledgements:
This dataset was the current version of Glottolog as of July 20, 2017. If you publish work using this dataset, please use the following citation:
Hammarström, Harald & Haspelmath, Martin & Forkel, Robert. 2017. Glottolog 3.0. Jena: Max Planck Institute for the Science of Human History. (Available online at http://glottolog.org, Accessed on 2017-03-23.)
Inspiration:
Can you plot the geographic location of each language family or langoid?
Where are most extinct languages found?
Can you find which language was documented in what decade? Which areas of the focus of more or less documentation?
You may also be interested in:
Atlas of Pidgin and Creole Language Structures
The Sign Language Analyses (SLAY) Database
World Atlas of Language Structures: Information on the linguistic structures in 2,679 languages"
"London Crime Data, 2008-2016","13M Rows of Crime Counts, by Borough, Category, and Month",Jacob Boysen,20,"Version 1,2017-08-03",crime,CSV,890 MB,CC0,"4,444 views",642 downloads,,,https://www.kaggle.com/jboysen/london-crime,"Context:
Crime in major metropolitan areas, such as London, occurs in distinct patterns. This data covers the number of criminal reports by month, LSOA borough, and major/minor category from Jan 2008-Dec 2016.
Content:
13M rows containing counts of criminal reports by month, LSOA borough, and major/minor category.
Acknowledgements:
Txt file was pulled from Google Cloud Platform and converted to csv. Photo by James Sutton.
Inspiration:
Are there seasonal or time-of-week/day changes in crime occurrences? Any boroughs where particular crimes are increasing or decreasing? Policy makers use this data to plan upcoming budgets and deployment--can you use previous year crime reports to reliably predict later trends? If you normalize by borough population, can you find any areas where crime is more or less likely?"
PM2.5 Data of Five Chinese Cities,"Measurements for Shenyang, Chengdu, Beijing, Guangzhou, and Shanghai",UCI Machine Learning,20,"Version 1,2017-08-23","cities
pollution",CSV,15 MB,Other,"3,401 views",485 downloads,,0 topics,https://www.kaggle.com/uciml/pm25-data-for-five-chinese-cities,"Context
PM2.5 readings are often included in air quality reports from environmental authorities and companies. PM2.5 refers to atmospheric particulate matter (PM) that have a diameter less than 2.5 micrometers. In other words, it's used as a measure of pollution.
Content
The time period for this data is between Jan 1st, 2010 to Dec 31st, 2015. Missing data are denoted as NA.
No: row number
year: year of data in this row
month: month of data in this row
day: day of data in this row
hour: hour of data in this row
season: season of data in this row
PM: PM2.5 concentration (ug/m^3)
DEWP: Dew Point (Celsius Degree)
TEMP: Temperature (Celsius Degree)
HUMI: Humidity (%)
PRES: Pressure (hPa)
cbwd: Combined wind direction
Iws: Cumulated wind speed (m/s)
precipitation: hourly precipitation (mm)
Iprec: Cumulated precipitation (mm)
Acknowledgements
Liang, X., S. Li, S. Zhang, H. Huang, and S. X. Chen (2016), PM2.5 data reliability, consistency, and air quality assessment in five Chinese cities, J. Geophys. Res. Atmos., 121, 10220â€“10236.
The files were downloaded from the UCI Machine Learning Repository and have not been modified. https://archive.ics.uci.edu/ml/datasets/PM2.5+Data+of+Five+Chinese+Cities#"
Hazardous Air Pollutants,A summary of daily Hazardous Air Pollutants from 1990 to 2017,US Environmental Protection Agency,20,"Version 1,2017-07-01","environment
pollution",CSV,2 GB,CC0,"3,557 views",411 downloads,,0 topics,https://www.kaggle.com/epa/hazardous-air-pollutants,"Context:
Hazardous air pollutants, also known as toxic air pollutants or air toxics, are those pollutants that are known or suspected to cause cancer or other serious health effects, such as reproductive effects or birth defects, or adverse environmental effects. The Environmental Protection Agency (EPA) tracks 187 air pollutants. See https://www.epa.gov/haps/ for more information.
Content:
The daily summary file contains data for every monitor (sampled parameter) in the Environmental Protection Agency (EPA) database for each day. This file will contain a daily summary record that is: 1. The aggregate of all sub-daily measurements taken at the monitor. 2. The single sample value if the monitor takes a single, daily sample (e.g., there is only one sample with a 24-hour duration). In this case, the mean and max daily sample will have the same value.
Fields Descriptions: 1. State Code: The Federal Information Processing Standards (FIPS) code of the state in which the monitor resides.
County Code: The FIPS code of the county in which the monitor resides.
Site Num: A unique number within the county identifying the site.
Parameter Code: The AQS code corresponding to the parameter measured by the monitor.
POC: This is the “Parameter Occurrence Code” used to distinguish different instruments that measure the same parameter at the same site.
Latitude: The monitoring site’s angular distance north of the equator measured in decimal degrees.
Longitude: The monitoring site’s angular distance east of the prime meridian measured in decimal degrees.
Datum: The Datum associated with the Latitude and Longitude measures.
Parameter Name: The name or description assigned in AQS to the parameter measured by the monitor. Parameters may be pollutants or non-pollutants.
Sample Duration: The length of time that air passes through the monitoring device before it is analyzed (measured). So, it represents an averaging period in the atmosphere (for example, a 24-hour sample duration draws ambient air over a collection filter for 24 straight hours). For continuous monitors, it can represent an averaging time of many samples (for example, a 1-hour value may be the average of four one-minute samples collected during each quarter of the hour).
Pollutant Standard: A description of the ambient air quality standard rules used to aggregate statistics. (See description at beginning of document.)
Date Local: The calendar date for the summary. All daily summaries are for the local standard day (midnight to midnight) at the monitor.
Units of Measure: The unit of measure for the parameter. QAD always returns data in the standard units for the parameter. Submitters are allowed to report data in any unit and EPA converts to a standard unit so that we may use the data in calculations.
Event Type: Indicates whether data measured during exceptional events are included in the summary. A wildfire is an example of an exceptional event; it is something that affects air quality, but the local agency has no control over. No Events means no events occurred. Events Included means events occurred and the data from them is included in the summary. Events Excluded means that events occurred but data form them is excluded from the summary. Concurred Events Excluded means that events occurred but only EPA concurred exclusions are removed from the summary. If an event occurred for the parameter in question, the data will have multiple records for each monitor.
Observation Count: The number of observations (samples) taken during the day.
Observation Percent: The percent representing the number of observations taken with respect to the number scheduled to be taken during the day. This is only calculated for monitors where measurements are required (e.g., only certain parameters).
Arithmetic Mean: The average (arithmetic mean) value for the day.
1st Max Value: The highest value for the day.
1st Max Hour: The hour (on a 24-hour clock) when the highest value for the day (the previous field) was taken.
AQI: The Air Quality Index for the day for the pollutant, if applicable.
Method Code: An internal system code indicating the method (processes, equipment, and protocols) used in gathering and measuring the sample. The method name is in the next column.
Method Name: A short description of the processes, equipment, and protocols used in gathering and measuring the sample.
Local Site Name: The name of the site (if any) given by the State, local, or tribal air pollution control agency that operates it.
Address: The approximate street address of the monitoring site.
State Name: The name of the state where the monitoring site is located.
County Name: The name of the county where the monitoring site is located.
City Name: The name of the city where the monitoring site is located. This represents the legal incorporated boundaries of cities and not urban areas.
CBSA Name: The name of the core bases statistical area (metropolitan area) where the monitoring site is located.
Date of Last Change: The date the last time any numeric values in this record were updated in the AQS data system.
Acknowledgements:
These data came from the EPA and are current up to May 01, 2017. You can use Kernels to analyze, share, and discuss this data on Kaggle, but if you’re looking for real-time updates and bigger data, check out the data on BigQuery, too: https://cloud.google.com/bigquery/public-data/epa.
Inspiration:
People exposed to toxic air pollutants at sufficient concentrations and durations may have an increased chance of getting cancer or experiencing other serious health effects. These health effects can include damage to the immune system, as well as neurological, reproductive (e.g., reduced fertility), developmental, respiratory and other health problems. In addition to exposure from breathing air toxics, some toxic air pollutants such as mercury can deposit onto soils or surface waters, where they are taken up by plants and ingested by animals and are eventually magnified up through the food chain. Like humans, animals may experience health problems if exposed to sufficient quantities of air toxics over time. Use this dataset to find out where the highest concentrations of hazardous air pollutants are for each state. You could also use the GPS locations to find out where the EPA has the most monitoring stations and identify places that could use more."
Stanford Natural Language Inference Corpus,A collection of 570k labeled human-written English sentence pairs,Stanford University,20,"Version 1,2017-07-22","languages
linguistics",CSV,373 MB,CC4,"2,609 views",214 downloads,,0 topics,https://www.kaggle.com/stanfordu/stanford-natural-language-inference-corpus,"The SNLI corpus (version 1.0) is a collection of 570k human-written English sentence pairs manually labeled for balanced classification with the labels entailment, contradiction, and neutral, supporting the task of natural language inference (NLI), also known as recognizing textual entailment (RTE). We aim for it to serve both as a benchmark for evaluating representational systems for text, especially including those induced by representation learning methods, as well as a resource for developing NLP models of any kind.
Acknowledgements
This dataset was kindly made available bye the Stanford Natural Language Processing Group. Please cite it as:
Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)
Inspiration
This dataset has been used to evaluate academic work on sentence encoding-based models for 3 way classification, with previous scores tabulated at https://nlp.stanford.edu/projects/snli/. Most of the entries use deep learning. How close to those scores (peak of 88.8% test accuracy) can you get with less computationally intensive methods?"
"Foodborne Disease Outbreaks, 1998-2015",What contaminant has caused the most hospitalizations and fatalities?,Centers for Disease Control and Prevention,20,"Version 1,2017-02-16","food and drink
public health",CSV,1 MB,CC0,"6,466 views","1,034 downloads",11 kernels,0 topics,https://www.kaggle.com/cdc/foodborne-diseases,"Context
Next time you take a bite, consider this: roughly one in six (or 48 million) people in the United States get sick from eating contaminated food per year. More than 250 pathogens and toxins have been known to cause foodborne illness and almost all of them can cause an outbreak.
A foodborne disease outbreak occurs when two or more people get the same illness from the same contaminated food or drink. While most foodborne illnesses are not part of a recognized outbreak, outbreaks provide important information on how germs spread, which foods cause illness, and how to prevent infection.
Public health agencies in all 50 states, the District of Columbia, U.S. territories, and Freely Associated States have primary responsibility for identifying and investigating outbreaks and use a standard form to report outbreaks voluntarily to CDC. During 1998–2008, reporting was made through the electronic Foodborne Outbreak Reporting System (eFORS).
Content
This dataset provides data on foodborne disease outbreaks reported to CDC from 1998 through 2015. Data fields include year, state (outbreaks occurring in more than one state are listed as ""multistate""), location where the food was prepared, reported food vehicle and contaminated ingredient, etiology (the pathogen, toxin, or chemical that caused the illnesses), status (whether the etiology was confirmed or suspected), total illnesses, hospitalizations, and fatalities. In many outbreak investigations, a specific food vehicle is not identified; for these outbreaks, the food vehicle variable is blank.
Inspiration
Are foodborne disease outbreaks increasing or decreasing? What contaminant has been responsible for the most illnesses, hospitalizations, and deaths? What location for food preparation poses the greatest risk of foodborne illness?"
US Traffic Fatality Records,Fatal car crashes for 2015-2016,Department of Transportation,20,"Version 1,2017-12-02","automobiles
bigquery",BigQuery,580 MB,CC0,"4,772 views",0 downloads,494 kernels,,https://www.kaggle.com/usdot/nhtsa-traffic-fatalities,"Fatality Analysis Reporting System (FARS) was created in the United States by the National Highway Traffic Safety Administration (NHTSA) to provide an overall measure of highway safety, to help suggest solutions, and to help provide an objective basis to evaluate the effectiveness of motor vehicle safety standards and highway safety programs.
FARS contains data on a census of fatal traffic crashes within the 50 States, the District of Columbia, and Puerto Rico. To be included in FARS, a crash must involve a motor vehicle traveling on a trafficway customarily open to the public and result in the death of a person (occupant of a vehicle or a non-occupant) within 30 days of the crash. FARS has been operational since 1975 and has collected information on over 989,451 motor vehicle fatalities and collects information on over 100 different coded data elements that characterizes the crash, the vehicle, and the people involved.
FARS is vital to the mission of NHTSA to reduce the number of motor vehicle crashes and deaths on our nation's highways, and subsequently, reduce the associated economic loss to society resulting from those motor vehicle crashes and fatalities. FARS data is critical to understanding the characteristics of the environment, trafficway, vehicles, and persons involved in the crash.
NHTSA has a cooperative agreement with an agency in each state government to provide information in a standard format on fatal crashes in the state. Data is collected, coded and submitted into a micro-computer data system and transmitted to Washington, D.C. Quarterly files are produced for analytical purposes to study trends and evaluate the effectiveness highway safety programs.
Content
There are 40 separate data tables. You can find the manual, which is too large to reprint in this space, here.
Querying BigQuery tables
You can use the BigQuery Python client library to query tables in this dataset in Kernels. Note that methods available in Kernels are limited to querying data. Tables are at bigquery-public-data.nhtsa_traffic_fatalities.[TABLENAME]. Fork this kernel to get started.
Acknowledgements
This dataset was provided by the National Highway Traffic Safety Administration."
World Factbook Country Profiles,Textual profiles describing every country in the world,Department of Defense,20,"Version 1,2017-09-16","military
politics
demographics
economics",{}JSON,7 MB,CC0,"2,569 views",407 downloads,,0 topics,https://www.kaggle.com/usdod/world-factbook-country-profiles,"Context
This dataset is a snapshot of all of the country profiles provided in the World Factbook as of early 2017. The World Factbook is a reference almanac published by the United States Central Intelligence Agency on a continual basis. It is often used as a reference text in other academic works.
Content
This dataset includes high-level textual information on the economy, politics, demography, culture, military, and society of every country in the world.
Acknowledgements
This data was scraped here, then concatenated into a single entity before upload to Kaggle.
Inspiration
This dataset is an ideal basis of comparison for various world countries.
Analyzing international data? This dataset is a rich mix-in dataset for contextualizing such analyses."
Database of Android Apps,"Over 400,000 Android apps scraped from Google Play",Orges Leka,20,"Version 1,2016-12-07",,CSV,80 MB,CC0,"7,081 views",636 downloads,,,https://www.kaggle.com/orgesleka/android-apps,"This database consist of over 400000 infos for android Apps scraped with Scrapy from Google Play. Those fields are included:
name
datePublished
numDownloadsMin
fileSize
packageName
price
aggregateRating
softwareVersion
ratingCount
dateCrawled
url"
Finding and Measuring Lungs in CT Data,"A collection of CT images, manually segmented lungs and measurements in 2/3D",Kevin Mader,20,"Version 2,2017-04-26|Version 1,2017-04-26",healthcare,Other,632 MB,Other,"8,000 views","1,393 downloads",19 kernels,2 topics,https://www.kaggle.com/kmader/finding-lungs-in-ct-data,"Context
Competitions like LUNA (http://luna16.grand-challenge.org) and the Kaggle Data Science Bowl 2017 (https://www.kaggle.com/c/data-science-bowl-2017) involve processing and trying to find lesions in CT images of the lungs. In order to find disease in these images well, it is important to first find the lungs well. This dataset is a collection of 2D and 3D images with manually segmented lungs.
Challenge
Come up with an algorithm for accurately segmenting lungs and measuring important clinical parameters (lung volume, PD, etc)
Percentile Density (PD)
The PD is the density (in Hounsfield units) the given percentile of pixels fall below in the image. The table includes 5 and 95% for reference. For smokers this value is often high indicating the build up of other things in the lungs."
Numenta Anomaly Benchmark (NAB),Dataset and scoring for detecting anomalies in streaming data,BoltzmannBrain,20,"Version 1,2016-08-19","computer science
information technology",Other,9 MB,Other,"7,798 views",864 downloads,13 kernels,0 topics,https://www.kaggle.com/boltzmannbrain/nab,"The Numenta Anomaly Benchmark (NAB) is a novel benchmark for evaluating algorithms for anomaly detection in streaming, online applications. It is comprised of over 50 labeled real-world and artificial timeseries data files plus a novel scoring mechanism designed for real-time applications. All of the data and code is fully open-source, with extensive documentation, and a scoreboard of anomaly detection algorithms: github.com/numenta/NAB. The full dataset is included here, but please go to the repo for details on how to evaluate anomaly detection algorithms on NAB.
NAB Data Corpus
The NAB corpus of 58 timeseries data files is designed to provide data for research in streaming anomaly detection. It is comprised of both real-world and artifical timeseries data containing labeled anomalous periods of behavior. Data are ordered, timestamped, single-valued metrics. All data files contain anomalies, unless otherwise noted.
The majority of the data is real-world from a variety of sources such as AWS server metrics, Twitter volume, advertisement clicking metrics, traffic data, and more. All data is included in the repository, with more details in the data readme. We are in the process of adding more data, and actively searching for more data. Please contact us at nab@numenta.org if you have similar data (ideally with known anomalies) that you would like to see incorporated into NAB.
The NAB version will be updated whenever new data (and corresponding labels) is added to the corpus; NAB is currently in v1.0.
Real data
realAWSCloudwatch/
AWS server metrics as collected by the AmazonCloudwatch service. Example metrics include CPU Utilization, Network Bytes In, and Disk Read Bytes.
realAdExchange/
Online advertisement clicking rates, where the metrics are cost-per-click (CPC) and cost per thousand impressions (CPM). One of the files is normal, without anomalies.
realKnownCause/
This is data for which we know the anomaly causes; no hand labeling.
ambient_temperature_system_failure.csv: The ambient temperature in an office setting.
cpu_utilization_asg_misconfiguration.csv: From Amazon Web Services (AWS) monitoring CPU usage – i.e. average CPU usage across a given cluster. When usage is high, AWS spins up a new machine, and uses fewer machines when usage is low.
ec2_request_latency_system_failure.csv: CPU usage data from a server in Amazon's East Coast datacenter. The dataset ends with complete system failure resulting from a documented failure of AWS API servers. There's an interesting story behind this data in the Numenta blog.
machine_temperature_system_failure.csv: Temperature sensor data of an internal component of a large, industrial mahcine. The first anomaly is a planned shutdown of the machine. The second anomaly is difficult to detect and directly led to the third anomaly, a catastrophic failure of the machine.
nyc_taxi.csv: Number of NYC taxi passengers, where the five anomalies occur during the NYC marathon, Thanksgiving, Christmas, New Years day, and a snow storm. The raw data is from the NYC Taxi and Limousine Commission. The data file included here consists of aggregating the total number of taxi passengers into 30 minute buckets.
rogue_agent_key_hold.csv: Timing the key holds for several users of a computer, where the anomalies represent a change in the user.
rogue_agent_key_updown.csv: Timing the key strokes for several users of a computer, where the anomalies represent a change in the user.
realTraffic/
Real time traffic data from the Twin Cities Metro area in Minnesota, collected by the Minnesota Department of Transportation. Included metrics include occupancy, speed, and travel time from specific sensors.
realTweets/
A collection of Twitter mentions of large publicly-traded companies such as Google and IBM. The metric value represents the number of mentions for a given ticker symbol every 5 minutes.
Artificial data
artificialNoAnomaly/
Artifically-generated data without any anomalies.
artificialWithAnomaly/
Artifically-generated data with varying types of anomalies.
Acknowledgments
We encourage you to publish your results on running NAB, and share them with us at nab@numenta.org. Please cite the following publication when referring to NAB:
Lavin, Alexander and Ahmad, Subutai. ""Evaluating Real-time Anomaly Detection Algorithms – the Numenta Anomaly Benchmark"", Fourteenth International Conference on Machine Learning and Applications, December 2015. [PDF]"
Demonetization in India,Withdrawal of 500 and 1000 bills in India,shan,20,"Version 3,2017-01-17|Version 2,2016-12-05|Version 1,2016-11-27","money
economics",CSV,39 MB,CC0,"12,391 views","1,443 downloads",13 kernels,,https://www.kaggle.com/shan4224/demonetization-in-india,"Withdrawal of a particular form of currency (such a gold coins, currency notes) from circulation is known as demonetization .
Context:
On November 8th, India’s Prime Minister announced that 86% of the country’s currency would be rendered null and void in 50 days and it will withdraw all 500 and 1,000 rupee notes — the country’s most popular currency denominations from circulation, while a new 2,000 rupee note added in. It was posited as a move to crackdown on corruption and the country’s booming under-regulated and virtually untaxed grassroots economy.
Content:
The field names are following:
ID
QUERY
TWEET_ID
INSERTED DATE
TRUNCATED
LANGUAGE
possibly_sensitive coordinates
retweeted_status
created_at_text
created_at
CONTENT
from_user_screen_name
from_user_id from_user_followers_count
from_user_friends_count
from_user_listed_count
from_user_statuses_count
from_user_description
from_user_location
from_user_created_at
retweet_count
entities_urls
entities_urls_counts
entities_hashtags
entities_hashtags_counts
entities_mentions
entities_mentions_counts
in_reply_to_screen_name
in_reply_to_status_id
source
entities_expanded_urls
json_output
entities_media_count
media_expanded_url
media_url
media_type
video_link
photo_link
twitpic
Acknowledgements:
Dataset is created by pulling tweets by hashtag from twitter.
Inspiration:
Dataset can be used to understand trending tweets. Dataset can be used for sentiment analysis and topic mining. Dataset can be used for time series analysis of tweets.
What questions would you like answered by the community ?
What is the general sentiment of tweets ?
Conclusion regarding tweet sentiments varying over time.
What feedback would be helpful on the data itself ?
An in depth analysis of data."
NBA Draft Value,Data from NBA Drafts and Seasons to evaluate draft effectiveness,Aaron Miles,20,"Version 1,2016-08-22",basketball,CSV,490 KB,Other,"7,720 views","1,000 downloads",15 kernels,0 topics,https://www.kaggle.com/amiles/nbadraftvalue,"These datasets contain information from NBA draft classes and subsequent advanced stats years. The idea is to evaluate which draft classes are better than others, and in which ways they are better (did they produce more stars in the top 10, or more solid role players at the middle or end. I posted an analysis of this data here, but there's a lot more to be done. For example, my initial analysis just looks at drafts as a whole, not breaking it down by top 10, top 30, or top 60 picks. I also just analyzed drafts since 2000, but the dataset I uploaded has info all the way back to 1978. This data was scraped from baskeball-reference.com
There are 2 datasets: season78, which has the fields:
Season: The NBA season data is drawn from. The later year is the Season value (e.g. 2015-2016 season is 2016)
Player: Name of the player
WS: Win Shares produced that season.
draft78 has the fields:
Pick: The draft pick the player was.
Player: Name of the player
Yrs: Number of years the player played in the NBA
Draft: Year of the draft."
"Ground State Energies of 16,242 Molecules",Predict molecular properties from a database of atomic level simulations,BurakH,20,"Version 1,2017-04-13","chemistry
physics",CSV,162 MB,CC0,"3,878 views",338 downloads,7 kernels,5 topics,https://www.kaggle.com/burakhmmtgl/energy-molecule,"Context
This dataset contains ground state energies of 16,242 molecules calculated by quantum mechanical simulations.
Content
The data contains 1277 columns. The first 1275 columns are entries in the Coulomb matrix that act as molecular features. The 1276th column is the Pubchem Id where the molecular structures are obtained. The 1277th column is the atomization energy calculated by simulations using the Quantum Espresso package.
In the csv file, the first column (X1) is the data index and unused.
Past Research
The data is used for a publication in Journal of Chemical Physics. A blog post was also published explaining the data and the research behind it in less technical terms.
A Github repository is available that contains the source code used for generating the data, as well as some of the R scripts used for analysis.
Inspiration
Simulations of molecular properties are computationally expensive. The purpose of this project is to use machine learning methods to come up with a model that can predict molecular properties from a database of simulations. If this can be done with high accuracy, properties of new molecules can be calculated using the trained model. This could open up many possibilities in computational design and discovery of molecules, compounds and new drugs.
The purpose is to use the 1275 molecular features to predict the atomization energy. This is a regression problem so mean squared error is minimized during training.
I am looking for Kagglers to find the best model and reduce mean squared error as much as possible!"
Religious Terrorist Attacks,Use data from The Religion of Peace to predict attacks and save lives,FelipeArgolo,20,"Version 3,2016-07-27|Version 2,2016-07-20|Version 1,2016-07-19",,CSV,3 MB,GPL,"11,731 views",819 downloads,35 kernels,2 topics,https://www.kaggle.com/argolof/predicting-terrorism,"These three extremists attacks happened in the last 24 hours (as of 18th of July, 2016):
Extremists send rockets into a residential neighborhood, taking out a child and two women. Aleppo, Syria.
Suicide bombers attack Yemeni army checkpoints, killing 10. Yemen, Mukalla.
5 killed, 9 injured in Almaty terrorist attack on police station [GRAPHIC]. Kazakhstan,Almaty.
Can we come together to predict where the next terrorist attacks will likely occur?
The best weapon to fight extremists might be information. In fact, machine learning is already being used to predict and prevent terrorist attacks. We can do the same with data gathered from The Religion of Peace website.
This website gathers events that resulted in killings and which were committed out of religious duty since 2002. This dataset contains a table summary of their data, including:
Date
Country
City
(# of people) Killed
(# of people) Injured
Description (including type of attack descriptors such as ""bomb"",""car"",""shooting"",""rocket"")
I webscraped the data using R rvest.
# Webscraping in terror data by ping_freud
# You can use the following gadget to find out which nodes to select in a page 
# https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html
library(dplyr) #For %>% pipe flow
library(ggplot2)
library(rvest) #Webscraping

# Building strings and declaring variables.
# Link with data.
data.link <-""https://www.thereligionofpeace.com/attacks/attacks.aspx?""
Years <- as.character(2002:2016)
links <- paste(data.link,""Yr="",Years,sep = """")
terror.nodes <- terror.data <- vector(""list"",length(links))

# For loop to extract data
for (i in 1:length(links)){ 
  terror.nodes[[i]] <- read_html(links[i]) %>% html_nodes(xpath=""//table"")
  #11 is the node where the table with data is 
  terror.data[[i]] <- as.data.frame(html_table(terror.nodes[[i]][11])) 
  terror.data[[i]] <- terror.data[[i]][nrow(terror.data[[i]]):1,]
}

# Combines data frames
terror.alldata <- do.call(""rbind"",terror.data)
# Convert strings with dates to date format
terror.alldata$Date <- as.Date(terror.alldata$Date,""%Y.%m.%d"")
row.names(terror.alldata) <- as.character(1:nrow(terror.alldata))
write.csv(terror.alldata,""attacks_data.csv"")
I have not worked on the analysis yet, but we have geospacial distribution, type (hidden in the Description strings) and magnitude of the attacks. There's also the possibility of using socioeconomical data available for the places listed."
Fantasy Premier League,"Dataset regarding users, fixtures, points of Fantasy English Premier League",Chaitanya Bapat,20,"Version 2,2017-05-17|Version 1,2017-05-14",association football,Other,685 MB,CC0,"7,337 views",811 downloads,6 kernels,2 topics,https://www.kaggle.com/chaibapat/fantasy-premier-league,"Context
Fantasy Premier League is the online global competition for Football Enthusiasts to try their luck at picking up their ""Dream Team"" and collect points. It involves a deep understanding of the Sport, Clubs, Players and the Fixtures apart from many other things. All this makes for a compelling Data Science (read Machine Learning Problem).
English Premier League (EPL) - one of the famous leagues in the sport of football. Most viewed and followed across the globe. FPL provides an opportunity for enthusiasts to try their hand at decision making. To predict the best set of players who perform every game. Points are given based on various parameters.
Goal - To get the maximum Team Score every week
Content
Time Period - Year 2016-17 Season
Dataset consists of
FPL users data (basic information)
Fixtures (Week-wise)
Points earned by every user (Gameweek-wise)
Data Extraction
Detailed Information about the Code Github repo - https://github.com/ChaiBapchya/fantasypremierleague-datascience
Acknowledgements
Thanks to Fantasy Premier League, without which this data would not have been available.
Inspiration
Diego Costa scores a goal every 15 minutes of the match he plays.
Harry Kane is the youngest player to have scored 100 Premier League goals.
Such statistics (if true) are a compelling read.
But, to know -
Nathaniel Chalobah has 70% probability of scoring against Stoke City this weekend.
Alexis Sanchez will score around 2 goals this month.
Cesc Fabregas is going to assist this weekend.
There's 70% David De Gea is going to have a clean-sheet.
Such statistics and much more lend so much credibility to decision making. It would enable Fantasy Premier League team owners to decide :-
a. When to choose which player?
b. When is the right time to use the WildCard?
c. If it is time to be patient with a signing?
d. Who to watch out for?
In order to do this, one needs data to back your predictions. Hence, I was keen on retrieving all this data.
Future Scope -
Predict the best team for the upcoming week.
Predict the best player (Highest value for money) (Goalkeeper, Defender, MId-fielder, Attacker)
Suggest possible changes in formation if need be."
ADS-16 Computational Advertising Dataset,A collection of 300 real ads voted by 120 unacquainted individuals,GiorgioRoffo,20,"Version 1,2017-01-15",marketing,Other,754 MB,Other,"7,276 views",846 downloads,8 kernels,,https://www.kaggle.com/groffo/ads16-dataset,"Context
In the last decade, new ways of shopping online have increased the possibility of buying products and services more easily and faster than ever. In this new context, personality is a key determinant in the decision making of the consumer when shopping. A person's buying choices are influenced by psychological factors like impulsiveness; indeed some consumers may be more susceptible to making impulse purchases than others. Since affective metadata are more closely related to the user's experience than generic parameters, accurate predictions reveal important aspects of user's attitudes, social life, including attitude of others and social identity. This work proposes a highly innovative research that uses a personality perspective to determine the unique associations among the consumer's buying tendency and advert recommendations. In fact, the lack of a publicly available benchmark for computational advertising do not allow both the exploration of this intriguing research direction and the evaluation of recent algorithms. We present the ADS Dataset, a publicly available benchmark consisting of 300 real advertisements (i.e., Rich Media Ads, Image Ads, Text Ads) rated by 120 unacquainted individuals, enriched with Big-Five users' personality factors and 1,200 personal users' pictures.
Content
The content of the zip files are folders. The directory tree of this disk is as follows:
20 Ads folder: Ads belong to 20 product/service categories. all the ads are here. 120 Users Folders: Each folder contains data for one of the involved subjects. 300 real advertisements have been scored, Ratings according to the users’ interests (1 star to 5 stars), ~1,200 personal pictures (labelled as positive/negative), Big-Five personality scores (O-C-E-A-N).
Data can be easily analysed in Matlab, or Python
Acknowledgements
If you use our dataset please cite:
[1] Roffo, G., & Vinciarelli, A. (2016, August). Personality in computational advertising: A benchmark. In 4 th Workshop on Emotions and Personality in Personalized Systems (EMPIRE) 2016 (p. 18).
Inspiration
We collected and introduced a representative benchmark for computational advertising enriched with affective-like metadata such as personality factors. The benchmark allows to (i) explore the relationship between consumer characteristics, attitude toward online shopping and advert recommendation, (ii) identify the underlying dimensions of consumer shopping motivations and attitudes toward online in-store conversions, and (iii) have a reference benchmark for comparison of state-of-the-art advertisement recommender systems (ARSs). To the best of our knowledge, the ADS dataset is the first attempt at providing a set of advertisements scored by the users according to their interest into the content. We hope that this work motivates researchers to take into account the use of personality factors as an integral part of their future work, since there is a high potential that incorporating these kind of users' characteristics into ARS could enhance recommendation quality and user experience."
FCC Net Neutrality Comments (4/2017 - 10/2017),FCC Proceeding #17-108 (text and dupe counts only),Jeff Kao,20,"Version 1,2017-11-26","politics
linguistics
internet",CSV,198 MB,CC0,"2,819 views",132 downloads,,0 topics,https://www.kaggle.com/jeffkao/proc_17_108_unique_comments_text_dupe_count,"Recent Updates (11-27-2017)
I've posted a clustered full dataset. This might give you a boost on the work you're doing with the data!
I've posted a vectorized subset w/ 100,000 data points sampled after manual reduction of the dataset after EDA.
Context
Cleaned data used in researching public comments for FCC Proceeding 17-108 (Net Neutrality Repeal).
Data collected from the beginning of submissions (April 2017) until Oct 27th, 2017. The long-running comment scraping script suffered from a couple of disconnections and I estimate that I lost ~50,000 comments because of it. Even though the Net Neutrality Public Comment Period ended on August 30, 2017, the FCC ECFS system continued to take comments afterward, which were included in the analysis.
I did a write-up on the results here: https://hackernoon.com/more-than-a-million-pro-repeal-net-neutrality-comments-were-likely-faked-e9f0e3ed36a6
Content
Document id, Text of the post, and number of duplicates found for that text was all that was necessary to generate the results. Text is raw & unchanged from the original. I'm working hard to get online a fuller set of data w/ other important metadata fields.
Cleaned-up notebooks used are available on github. I am posting the notebook for Exploratory Data Analysis first, and will include others as they are cleaned up. Please share with the rest of us what interesting insights you glean from the data! Tweet at me @jeffykao."
EmojiNet,A machine-readable dictionary of emoji meanings,Rachael Tatman,20,"Version 1,2017-11-03","popular culture
linguistics
internet",{}JSON,7 MB,CC4,"3,585 views",233 downloads,,0 topics,https://www.kaggle.com/rtatman/emojinet,"Content
EmojiNet is the largest machine-readable emoji sense inventory that links Unicode emoji representations to their English meanings extracted from the Web. EmojiNet is a dataset consisting of:
12,904 sense labels over 2,389 emoji, which were extracted from the web and linked to machine-readable sense definitions seen in BabelNet
context words associated with each emoji sense, which are inferred through word embedding models trained over Google News corpus and a Twitter message corpus for each emoji sense definition
specification of the most likely platform-based emoji sense for a selected set of emoji (since emoji presentation is different on different platforms)
Acknowledgements:
EmojiNet was developed by Sanjaya Wijeratne, Lakshika Balasuriya, Amit Sheth and Derek Doran. EmojiNet is licensed under aCreative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported (CC BY-NC-SA 3.0) license.. Please cite the following paper when using EmojiNet dataset(s):
Sanjaya Wijeratne, Lakshika Balasuriya, Amit Sheth, Derek Doran. EmojiNet: An Open Service and API for Emoji Sense Discovery. In 11th International AAAI Conference on Web and Social Media (ICWSM 2017). Montreal, Canada; 2017.
You can also find more information about the dataset on the project website.
The banner photo is by Frank Behrens and is licensed under a CC BY-SA 2.0 license.
Inspiration:
Can you use these senses to create a sentiment lexicon for emoji?
Can you cluster emoji based on their sense?
Which emoji are the most different across platforms?"
Speech Accent Archive,Parallel English speech samples from 177 countries,Rachael Tatman,20,"Version 2,2017-11-07|Version 1,2017-11-07","languages
acoustics
linguistics",Other,863 MB,CC4,"1,812 views",158 downloads,,,https://www.kaggle.com/rtatman/speech-accent-archive,"Context:
Everyone who speaks a language, speaks it with an accent. A particular accent essentially reflects a person's linguistic background. When people listen to someone speak with a different accent from their own, they notice the difference, and they may even make certain biased social judgments about the speaker.
The speech accent archive is established to uniformly exhibit a large set of speech accents from a variety of language backgrounds. Native and non-native speakers of English all read the same English paragraph and are carefully recorded. The archive is constructed as a teaching tool and as a research tool. It is meant to be used by linguists as well as other people who simply wish to listen to and compare the accents of different English speakers.
This dataset allows you to compare the demographic and linguistic backgrounds of the speakers in order to determine which variables are key predictors of each accent. The speech accent archive demonstrates that accents are systematic rather than merely mistaken speech.
All of the linguistic analyses of the accents are available for public scrutiny. We welcome comments on the accuracy of our transcriptions and analyses.
Content:
This dataset contains 2140 speech samples, each from a different talker reading the same reading passage. Talkers come from 177 countries and have 214 different native languages. Each talker is speaking in English.
This dataset contains the following files:
reading-passage.txt: the text all speakers read
speakers_all.csv: demographic information on every speaker
recording: a zipped folder containing .mp3 files with speech
Acknowledgements:
This dataset was collected by many individuals (full list here) under the supervision of Steven H. Weinberger. The most up-to-date version of the archive is hosted by George Mason University. If you use this dataset in your work, please include the following citation:
Weinberger, S. (2013). Speech accent archive. George Mason University.
This datasets is distributed under a CC BY-NC-SA 2.0 license.
Inspiration:
The following types of people may find this dataset interesting:
ESL teachers who instruct non-native speakers of English
Actors who need to learn an accent
Engineers who train speech recognition machines
Linguists who do research on foreign accent
Phoneticians who teach phonetic transcription
Speech pathologists
Anyone who finds foreign accent to be interesting"
Kepler Exoplanet Search Results,10000 exoplanet candidates examined by the Kepler Space Observatory,NASA,20,"Version 2,2017-10-11|Version 1,2017-10-11","astronomy
space",CSV,4 MB,CC0,"2,834 views",229 downloads,2 kernels,0 topics,https://www.kaggle.com/nasa/kepler-exoplanet-search-results,"Context
The Kepler Space Observatory is a NASA-build satellite that was launched in 2009. The telescope is dedicated to searching for exoplanets in star systems besides our own, with the ultimate goal of possibly finding other habitable planets besides our own. The original mission ended in 2013 due to mechanical failures, but the telescope has nevertheless been functional since 2014 on a ""K2"" extended mission.
Kepler had verified 1284 new exoplanets as of May 2016. As of October 2017 there are over 3000 confirmed exoplanets total (using all detection methods, including ground-based ones). The telescope is still active and continues to collect new data on its extended mission.
Content
This dataset is a cumulative record of all observed Kepler ""objects of interest"" — basically, all of the approximately 10,000 exoplanet candidates Kepler has taken observations on.
This dataset has an extensive data dictionary, which can be accessed here. Highlightable columns of note are:
kepoi_name: A KOI is a target identified by the Kepler Project that displays at least one transit-like sequence within Kepler time-series photometry that appears to be of astrophysical origin and initially consistent with a planetary transit hypothesis
kepler_name: [These names] are intended to clearly indicate a class of objects that have been confirmed or validated as planets—a step up from the planet candidate designation.
koi_disposition: The disposition in the literature towards this exoplanet candidate. One of CANDIDATE, FALSE POSITIVE, NOT DISPOSITIONED or CONFIRMED.
koi_pdisposition: The disposition Kepler data analysis has towards this exoplanet candidate. One of FALSE POSITIVE, NOT DISPOSITIONED, and CANDIDATE.
koi_score: A value between 0 and 1 that indicates the confidence in the KOI disposition. For CANDIDATEs, a higher value indicates more confidence in its disposition, while for FALSE POSITIVEs, a higher value indicates less confidence in that disposition.
Acknowledgements
This dataset was published as-is by NASA. You can access the original table here. More data from the Kepler mission is available from the same source here.
Inspiration
How often are exoplanets confirmed in the existing literature disconfirmed by measurements from Kepler? How about the other way round?
What general characteristics about exoplanets (that we can find) can you derive from this dataset?
What exoplanets get assigned names in the literature? What is the distribution of confidence scores?
See also: the Kepler Labeled Time Series and Open Exoplanets Catalogue datasets."
Men's Professional Basketball,"Stats on players, teams, and coaches in men's pro basketball leagues, 1937-2012",Open Source Sports,20,"Version 2,2016-11-14|Version 1,2016-11-14",basketball,CSV,7 MB,Other,"11,434 views","1,802 downloads",7 kernels,3 topics,https://www.kaggle.com/open-source-sports/mens-professional-basketball,"This dataset contains stats on players, coaches, and teams in men's professional basketball leagues from 1937 to 2012.
Acknowledgments
This dataset was downloaded from the Open Source Sports website. It did not come with an explicit license, but based on other datasets from Open Source Sports, we treat it as follows:
This database is copyright 1996-2015 by Sean Lahman.
This work is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License. For details see: http://creativecommons.org/licenses/by-sa/3.0/
The Data
This dataset contains 11 files, each corresponding to a data table. There are five main tables:
master: biographical information for all the players and coaches
teams: stats on each team, per year
players: stats for each player, per year
coaches: stats for each coach, per year
series_post: information on post-season winners, per year
And there are six supplementary tables:
abbrev: a key to the abbreviations used in other tables
awards_coaches: coaching awards, per year
awards_players: player awards, per year
draft: draft information, per year
hof: Hall of Fame information, per year
player_allstar: individual player stats for the All-Star Game, per year"
"The Counted: Killed by Police, 2015-2016",Use of deadly force by police officers in United States,The Guardian,20,"Version 2,2017-01-07|Version 1,2017-01-04",crime,CSV,333 KB,CC4,"5,448 views",766 downloads,27 kernels,,https://www.kaggle.com/the-guardian/the-counted,"The Counted is a project by the Guardian – and you – working to count the number of people killed by police and other law enforcement agencies in the United States throughout 2015 and 2016, to monitor their demographics and to tell the stories of how they died.
The database will combine Guardian reporting with verified crowdsourced information to build a more comprehensive record of such fatalities. The Counted is the most thorough public accounting for deadly use of force in the US, but it will operate as an imperfect work in progress – and will be updated by Guardian reporters and interactive journalists frequently.
Any deaths arising directly from encounters with law enforcement will be included in the database. This will inevitably include, but will likely not be limited to, people who were shot, tasered and struck by police vehicles as well those who died in police custody. Self-inflicted deaths during encounters with law enforcement or in police custody or detention facilities will not be included.
The US government has no comprehensive record of the number of people killed by law enforcement. This lack of basic data has been glaring amid the protests, riots and worldwide debate set in motion by the fatal police shooting of Michael Brown in August 2014. The Guardian agrees with those analysts, campaign groups, activists and authorities who argue that such accounting is a prerequisite for an informed public discussion about the use of force by police.
Contributions of any information that may improve the quality of our data will be greatly welcomed as we work toward better accountability. Please contact us at thecounted@theguardian.com.
CREDITS
Research and Reporting: Jon Swaine, Oliver Laughland, Jamiles Lartey
Design and Production: Kenan Davis, Rich Harris, Nadja Popovich, Kenton Powell"
Hong Kong Horse Racing Results 2014-17 Seasons,"Race details include track, time, and horse",Lantana Camara,20,"Version 5,2017-08-18|Version 4,2017-08-18|Version 3,2017-01-31|Version 2,2017-01-31|Version 1,2017-01-22",horse racing,CSV,8 MB,Other,"10,197 views","1,024 downloads",30 kernels,6 topics,https://www.kaggle.com/lantanacamara/hong-kong-horse-racing,"Can You Predict The Result?
Horse racing is one of the sport which involved many gambling activities. Million of people in the world tried to find their 'winning formula' in order to gain profit from betting. Since there are many factors which could affect the race result, data analysis on horse racing became much interesting.
Hong Kong horse racing is especially interesting due to the follow reasons:
- The handicap system made the race more competitive
- Horse pool is small compared to other countries so that horses will meet their rivalries very often in the races
- Limited number of jockey/trainer
- Data are well managed by the official :)
The Dataset
The dataset contains the race result of 1561 local races throughout Hong Kong racing seasons 2014-16 and more information will be added into the dataset. The dataset is divided into two tables (which can be joined by race_id). Most of the column description can be found below with one extra piece of information:
finishing_position - For special incident, please refer to here
So, can you find any pattern for the winner under some condition? Did you spot out a winning strategy? (FYI, betting on all horse equally will bring a loss of ~17.5% on average) Which jockey/trainer is worth to follow?
Don't wait and start the data analysis! You may find some of the kernels I created useful. Enjoy! And please remember to share your finding with the community!
Acknowledgement
The data are extracted from the website of The Hong Kong Jockey Club
How to get started?
In case you are not familiar with Hong Kong horse racing, please see this notebook as a get started tutorial."
"Refugees in the United States, 2006-2015",Where do most people granted refugee or asylum status come from?,Department of Homeland Security,20,"Version 1,2017-01-20","demographics
international relations",CSV,15 KB,CC0,"6,127 views","1,013 downloads",20 kernels,0 topics,https://www.kaggle.com/dhs/refugee-report,"Context
A refugee is a person outside his or her country of nationality who is unable or unwilling to return to his or her country of nationality because of persecution or a well-founded fear of persecution on account of race, religion, nationality, membership in a particular social group, or political opinion. An asylee is a person who meets the definition of refugee and is already present in the United States or is seeking admission at a port of entry. Refugees are required to apply for lawful permanent resident (“green card”) status one year after being admitted, and asylees may apply for green card status one year after being granted asylum.
Content
The Office of Immigration Statistics (OIS) Annual Flow Reports on refugees and asylees contain information obtained from the Worldwide Refugee Admissions Processing System (WRAPS) of the Bureau of Population, Refugees, and Migration of the US Department of State on the numbers and demographic profiles of persons admitted to the United States as refugees and those granted asylum status during a given fiscal year."
Snake Eyes,Tiny dice images with translation and rotation for image classification,nic,19,"Version 1,2017-11-26","geometry
machine learning
image data
multiclass classification",Other,126 MB,CC0,"3,119 views",204 downloads,8 kernels,2 topics,https://www.kaggle.com/nicw102168/snake-eyes,"Context
Invariance to translation and rotation is an important attribute we would like image classifiers to have in many applications. For many problems, even if there doesn't seem to be a lot of translation in the data, augmenting it with these transformations is often beneficial. There are not many datasets where these transformations are clearly relevant, though. The ""Snake Eyes"" dataset seeks to provide a problem where rotation and translation are clearly a fundamental aspect of the problem, and not just something intuitively believed to be involved.
Image classifiers are frequently utilized in a pipeline where a bounding box is first extracted from the complete image, and this process might provide centered data to the classifier. Some translation might still be present in the data the classifier sees, though, making the phenomenon relevant to classification nevertheless. A Snake Eyes classifier can clearly benefit from such a pre-processing. But the point here is trying to learn how much a classifier can learn to do by itself. In special we would like to demonstrate the ""built-in"" invariance to translations from CNNs.
Content
Snake Eyes contains artificial images simulating the a roll of one or two dice. The face patterns were modified to contain at most 3 black spots, making it impossible to solve the problem by merely counting them. The data was synthesized using a Python program, each image produced from a set of floating-point parameters modeling the position and angle of each dice.
The data format is binary, with records of 401 bytes. The first byte contains the class (1 to 12, notice it does not start at 0), and the other 400 bytes are the image rows. We offer 1 million images, split in 10 files with 100k records each, and an extra test set with 10,000 images.
Inspiration
We were inspired by the popular ""tiny image"" datasets often studied in ML research: MNIST, CIFAR-10 and Fashion-MNIST. Our dataset has smaller images, though, only 20x20, and 12 classes. The reduced proportions should help approximate the actual 3D and 6D manifolds of each class with the available number of data points (1 million images).
The data is artificial, with limited and very well-defined patterns, noise-free and properly anti-aliased. This is not about improving from 95% to 97% accuracy and wondering if 99% is possible with a deeper network. We don't expect less than 100% precision to be achieved with any method eventually. What we are interested to see is how do different methods compare in efficiency, how hard is it to train different models, and how the translation and rotation invariance is enforced or achieved.
We are also interested in studying the concept of manifold learning. The data has some intra-class variability due to different possible face combinations with two dice. But most of the variation comes from translation and rotation. We hope to have sampled enough data to really allow for the extraction of these manifolds in 400 dimensions, and to investigate topics such as the role of pre-training, and the relation between modeling the manifold of the whole data and of the separate classes.
Translations alone already create quite non-convex manifolds, but our classes also have the property that some linear combinations are actually a different class (e.g. two images from the ""2"" face make an image from the ""4"" class). We are curious to see how this property can make the problem more challenging to different techniques.
We are also secretly hoping to have created the image-detection version of the infamous ""spiral"" problem for neural networks. We are offering the prize of one ham sandwich, collected at my local café, to the first person who manages to train a neural network to solve this problem, convolutional or not, and using just traditional techniques such as logistic or ReLU activation functions and SGD training. 99% accuracy is enough. The resulting network may be susceptible to adversarial instances, this is fine, but we'll be constantly complaining about it in your ear while you eat the sandwich."
Where's Waldo,Find Waldo with image recognition,Aleksey Bilogur,19,"Version 2,2017-10-25|Version 1,2017-10-24","popular culture
visual arts
image data
object detection",Other,125 MB,ODbL,"2,727 views",286 downloads,,0 topics,https://www.kaggle.com/residentmario/wheres-waldo,"Context
Where's Waldo is a popular children's book series where the reader is presented with a sequence of scenes. Each scene contains potentially hundreds of individuals doing different things. Exactly one of these figures is Waldo: a tall man in a striped red shirt, red beanie, and glasses, and the objective of the game is to find Waldo is the least time possible. This dataset is raw data from the books for these challenges.
Content
This dataset contains a number of cuts of Where's Waldo scenes, including scenes. See the complimentary kernel to learn more about the dataset contents!
Acknowledgements
This dataset was collected and published as-is by Valentino Constantinou (vc1492a) on GitHub (here).
Inspiration
Can you come up with a strategy better than randomly scanning the page for this task? Can you identify Waldos and not-Waldos?"
Business and Industry Reports,"7,000 economics time series for 1956-2017",US Census Bureau,19,"Version 1,2017-10-18","finance
economics",CSV,40 MB,CC0,"4,191 views",429 downloads,,2 topics,https://www.kaggle.com/census/business-and-industry-reports,"Context
Along with their core mission of counting the US population, the United States Census Bureau gathers a wide range of economic data. This dataset covers 16 of their economic reports and surveys:
Advance Monthly Sales for Retail and Food Services
Construction Spending
Housing Vacancies and Homeownership
Manufactured Housing Survey (1980-2013)
Manufactured Housing Survey (Current)
Manufacturers' Shipments, Inventories, and Orders
Manufacturing and Trade Inventories and Sales
Monthly Retail Trade and Food Services
Monthly Wholesale Trade: Sales and Inventories
New Home Sales
New Residential Construction
Quarterly Financial Report
Quarterly Services Survey
Quarterly Summary of State & Local Taxes
Quarterly Survey of Public Pensions
U.S. International Trade in Goods and Services
Content
The data csv is arranged in a long format, with the time_series_code column tying it back to the metadata csv. If you're trying to figure out what data is available, you'll want to start with the metadata.
Just over a third of the time series store error codes, usually confidence intervals, rather than actual values. The metadata for these time series will have values in the columns et_code, et_desc, and et_unit.
All of the dates are stored as complete beginning of the period dates, but all of the time series are at either monthly, quarterly, or annual resolution. Exact days and months are provided for convenience when aligning time series and so that you don't have to unpack period codes like 'Q22009'.
There may be many time series bundled under a given data category or description. For example, the largest category (taxes) contains dozens of types of tax categories, and each of those contains a separate time series for each state in the country.
Two of the error code time series have non-numeric values. To convert the values column into reasonable units you'll need to drop all entries equal to the string Less than .05 percent.
The data have been substantially reformatted from how they are provided by the Census Bureau. You can find the script I used to prepare the data here.
Acknowledgements
This data was kindly made available by the United States Census. You can find the original data here. If you enjoyed this dataset you might also like one of the other US Census datasets available on Kaggle.
Inspiration
The National Bureau of Economic Research's macroeconomic history of the United States covers many similar time series, but before the census data was reported. Can you integrate it with this census data? This should allow you to generate many time series stretching from the present back to the 19th century."
MIAS Mammography,Looking for breast cancer,Kevin Mader,19,"Version 3,2017-11-01|Version 2,2017-11-01|Version 1,2017-11-01","healthcare
health",Other,206 MB,Other,"2,696 views",314 downloads,6 kernels,,https://www.kaggle.com/kmader/mias-mammography,"Content
The data is images and labels / annotations for mammography scans. More about the database can be found at MIAS. The 'Preview' kernel shows how the Info.txt and PGM files can be parsed correctly.
Labels
1st column: MIAS database reference number.
2nd column: Character of background tissue: F Fatty G Fatty-glandular D Dense-glandular
3rd column: Class of abnormality present: CALC Calcification CIRC Well-defined/circumscribed masses SPIC Spiculated masses MISC Other, ill-defined masses ARCH Architectural distortion ASYM Asymmetry NORM Normal
4th column: Severity of abnormality; B Benign M Malignant
5th, 6th columns: x,y image-coordinates of centre of abnormality.
7th column: Approximate radius (in pixels) of a circle enclosing the abnormality. There are also several things you should note:
The list is arranged in pairs of films, where each pair represents the left (even filename numbers) and right mammograms (odd filename numbers) of a single patient. The size of all the images is 1024 pixels x 1024 pixels. The images have been centered in the matrix. When calcifications are present, centre locations and radii apply to clusters rather than individual calcifications. Coordinate system origin is the bottom-left corner. In some cases calcifications are widely distributed throughout the image rather than concentrated at a single site. In these cases centre locations and radii are inappropriate and have been omitted.
Acknowledgements/LICENCE
MAMMOGRAPHIC IMAGE ANALYSIS SOCIETY MiniMammographic Database
                   LICENCE AGREEMENT
This is a legal agreement between you, the end user and the Mammographic Image Analysis Society (""MIAS""). Upon installing the MiniMammographic database (the ""DATABASE"") on your system you are agreeing to be bound by the terms of this Agreement.
GRANT OF LICENCE MIAS grants you the right to use the DATABASE, for research purposes ONLY. For this purpose, you may edit, format, or otherwise modify the DATABASE provided that the unmodified portions of the DATABASE included in a modified work shall remain subject to the terms of this Agreement.
COPYRIGHT The DATABASE is owned by MIAS and is protected by United Kingdom copyright laws, international treaty provisions and all other applicable national laws. Therefore you must treat the DATABASE like any other copyrighted material. If the DATABASE is used in any publications then reference must be made to the DATABASE within that publication.
OTHER RESTRICTIONS You may not rent, lease or sell the DATABASE.
LIABILITY To the maximum extent permitted by applicable law, MIAS shall not be liable for damages, other than death or personal injury, whatsoever (including without limitation, damages for negligence, loss of business, profits, business interruption, loss of business information, or other pecuniary loss) arising out of the use of or inability to use this DATABASE, even if MIAS has been advised of the possibility of such damages. In any case, MIAS's entire liability under this Agreement shall be limited to the amount actually paid by you or your assignor, as the case may be, for the DATABASE.
Inspiration
Automatically finding lesions would be a very helpful tool for physicians, also predicting malignancy based on a found/marked lesion"
Carbon Emissions,Carbon emissions from electicity production,Jason McNeill,19,"Version 1,2016-11-06","environment
energy",CSV,612 KB,Other,"7,789 views",987 downloads,14 kernels,,https://www.kaggle.com/txtrouble/carbon-emissions,"Monthly/Annual carbon dioxide emissions from electricity generation from the Energy Information Administration. Data is broken down by fuel type.
http://www.eia.gov/electricity/data.cfm#elecenv"
Multispectral Image Classification,Handwritten numbers (0-9) from six different people and two different pens,Little Boat,19,"Version 1,2017-03-14","writing
artificial intelligence
image data
multiclass classification",Other,5 GB,Other,"5,449 views",480 downloads,10 kernels,,https://www.kaggle.com/xiaozhouwang/multispectralimages,"Introduction
With multispectral images, we can capture more data per pixel, and understand objects based on their chemical composition or the variation of composition that encompasses an object. Examples of this might be, is the image you see an apple or an orange? Further, is the apple or the orange real? If it is plastic, was it made in Mexico or India? Real life impacts of using spectral data as part of object detection in images could one day save a life, if a self driving car could not only detect faces, but also the difference between skin and plastic, a lone pedestrian could avoid being if it was a choice between them or a group of three manikins.
This sample data contains a series of multispectral images of handwritten numbers between 0 and 9, from six different peoples, using two different pens. And here I am asking the great kagglers to explore and build models to tell each of the numbers from one another and with what ink each was written in.
Data
Each csv file contains pixels for 10 grayscale images (350 * 350) that represent 10 channels for the multispectral image, where X, Y represent the location of the pixel, and channel0 - channel9 represent channels.
And we also have a labels csv that contains labels for each pixel csv file.
Licence
You can do whatever you want with the data."
Diversity Index of US counties,Simpson Diversity Index to quantify racial diversity of US counties,Mike Johnson Jr,19,"Version 1,2016-08-22",demographics,CSV,188 KB,CC0,"5,618 views",752 downloads,10 kernels,0 topics,https://www.kaggle.com/mikejohnsonjr/us-counties-diversity-index,"Context: Diversity of United States Counties
Content: Diversity Index of Every US County using the Simpson Diversity Index: D = 1 - ∑(n/N)^2 (where n = number of people of a given race and N is the total number of people of all races, to get the probability of randomly selecting two people and getting two people of different races (ecological entropy))"
Hong Kong Marathon 2016 results,A race participated by 12k+ athletes from 50+ countries,melvincheung,19,"Version 2,2016-10-04|Version 1,2016-10-02","running
walking",CSV,967 KB,Other,"9,392 views","1,138 downloads",35 kernels,2 topics,https://www.kaggle.com/melvincheung/hong-kong-marathon-2016,"From the data, you will have:
- Results from 12k+ participants, with the fastest one of 2hr12mins from world class athlete - Midway time at 10km, halfway and 30km
- Overall and gender ranking
The original source
The data are captured from its official site
http://www.hkmarathon.com/Results/Search_2016_Results.htm
Only marathon results are included (but not 10km nor half marathon) because only this results has midway time, which can serve better analysis purposes.
The fields:
Race No: runner ID
Category: gender and age group. (e.g. MMS and MFS denote male and female while the age group are the same.)
Official Time: the ""gun time""
Net Time: the time between one passes the starting line and final line. It is usually a few minutes less than Official Time.
10km Time, Half Way Time, 30km Time: they are the midway times as described
The files
Marathon challenge and Marathon Run 1 uses the same running path for racing but with a different starting time. Athletes in challenge group are generally run faster.
Improving the dataset:
- Comparing the results of different marathons all over the world to find which one is the toughest or having the best participants, etc. - Please let me know if there is any centralized database collecting the results from different races."
MNIST as .jpg,Kaggle Digit Recognizer Competition Dataset as .jpg Image Files,Stuart Colianni,19,"Version 1,2017-05-15",,Other,18 MB,CC0,"7,464 views","1,770 downloads",4 kernels,,https://www.kaggle.com/scolianni/mnistasjpg,"Context
The Digit Recognizer competition uses the popular MNIST dataset to challenge Kagglers to classify digits correctly. In this dataset, the images are represented as strings of pixel values in train.csv and test.csv. Often, it is beneficial for image data to be in an image format rather than a string format. Therefore, I have converted the aforementioned datasets from text in .csv files to organized .jpg files.
Content
This dataset is composed of four files:
trainingSet.tar.gz (10.2 MB) - This file contains ten sub folders labeled 0 to 9. Each of the sub folders contains .jpg images from the Digit Recognizer competition's train.csv dataset, corresponding to the folder name (ie. folder 2 contains images of 2's, etc.). In total, there are 42,000 images in the training set.
testSet.tar.gz (6.8 MB) - This file contains the .jpg images from the Digit Recognizer competition's test.csv dataset. In total, there are 28,000 images in the test set.
trainingSample.zip (407 KB) - This file contains ten sub folders labeled 0 to 9. Each sub folder contains 60 .jpg images from the training set, for a total of 600 images.
testSample.zip (233 KB) - This file contains a 350 image sample from the test set.
Acknowledgements
As previously mentioned, all data presented here is simply a cleaned version of the data presented in Kaggle's Digit Recognizer competition. The division of the MNIST dataset into training and test sets exactly mirrors that presented in the competition.
Inspiration
I created this dataset when exploring TensorFlow's Inception model. Inception is a massive CNN built by Google to compete in the ImageNet competition. By way of Transfer Learning, the final layer of Inception can be retrained, rendering the model useful for general classification tasks. In retraining the model, .jpg images must be used, thereby necessitating to the creation of this dataset.
My hope in experimenting with Inception was to achieve an accuracy of around 98.5% or higher on the MNIST dataset. Unfortunately, the maximum accuracy I reached with Inception was only 95.314%. If you are interested in my code for said attempt, it is available on my GitHub repository Kaggle MNIST Inception CNN.
To learn more about retraining Inception, check out TensorFlow for Poets."
"USA Income Tax Data by ZIP Code, 2014",Distribution of income for 6 earning brackets,williamnowak,19,"Version 1,2016-11-15","geography
income
demographics",CSV,160 MB,CC0,"6,775 views","1,287 downloads",8 kernels,0 topics,https://www.kaggle.com/wpncrh/zip-code-income-tax-data-2014,"The Statistics of Income (SOI) division bases its ZIP code data on administrative records of individual income tax returns (Forms 1040) from the Internal Revenue Service (IRS) Individual Master File (IMF) system. Included in these data are returns filed during the 12-month period, January 1, 2015 to December 31, 2015. While the bulk of returns filed during the 12-month period are primarily for Tax Year 2014, the IRS received a limited number of returns for tax years before 2014 and these have been included within the ZIP code data.
There is data for more years here:
https://www.irs.gov/uac/soi-tax-stats-individual-income-tax-statistics-zip-code-data-soi
See documentation file attached. Crucially:
ZIPCODE - 5-digit Zip code  
AGI_STUB - Size of adjusted gross income
1 = $1 under $25,000 2 = $25,000 under $50,000 3 = $50,000 under $75,000 4 = $75,000 under $100,000 5 = $100,000 under $200,000 6 = $200,000 or more"
PokemonGO,151 Pokemon and battle stats,Alberto Barradas,19,"Version 1,2016-08-26",video games,CSV,17 KB,CC0,"17,150 views","1,278 downloads",27 kernels,3 topics,https://www.kaggle.com/abcsds/pokemongo,"This is a database of the first 151 pokemon; the ones you can find in the PokemonGO game. The stats include Pokemon Number, Name, First and Second Type, Max CP, Max HP and a url from the bulbagarden.net gallery.
Pokemon No: Number or ID of the pokemon.
Name: The original name of the pokemon.
First Type: What type of pokemon it is.
Second Type: Some pokemon can have two types, if they don't, this cell is empty.
Max CP: This is the maximum amount of damage a pokemon can infringe.
Max HP: The maximum amount of damage a pokemon can receive.
URL: This is a link to the pokemon's image on bulbagarden.
This database presents a great way of helping new generations of pokemon players learn about data science and pokemon at the same time. This data was scrapped from http://handbooks.bulbagarden.net/pokemongo/pokemon-index"
Election Day Tweets,"Tweets scraped from Twitter on November 8, 2016",Ed King,19,"Version 1,2016-11-27","politics
internet",CSV,209 MB,CC0,"4,735 views",692 downloads,6 kernels,0 topics,https://www.kaggle.com/kinguistics/election-day-tweets,"Tweets scraped by Chris Albon on the day of the 2016 United States elections.
Chris Albon's site only posted tweet IDs, rather than full tweets. We're in the process of scraping the full information, but due to API limiting this is taking a very long time. Version 1 of this dataset contains just under 400k tweets, about 6% of the 6.5 million originally posted.
This dataset will be updated as more tweets become available.
Acknowledgements
The original data was scraped by Chris Albon, and tweet IDs were posted to his Github page.
The Data
Since I (Ed King) used my own Twitter API key to scrape these tweets, this dataset contains a couple of fields with information on whether I have personally interacted with particular users or tweets. Since Kaggle encouraged me to not remove any data from a dataset, I'm leaving it in; feel free to build a classifier of the types of users I follow.
The dataset consists of the following fields:
text: text of the tweet
created_at: date and time of the tweet
geo: a JSON object containing coordinates [latitude, longitude] and a `type'
lang: Twitter's guess as to the language of the tweet
place: a Place object from the Twitter API
coordinates: a JSON object containing coordinates [longitude, latitude] and a `type'; note that coordinates are reversed from the geo field
user.favourites_count: number of tweets the user has favorited
user.statuses_count: number of statuses the user has posted
user.description: the text of the user's profile description
user.location: text of the user's profile location
user.id: unique id for the user
user.created_at: when the user created their account
user.verified: bool; is user verified?
user.following: bool; am I (Ed King) following this user?
user.url: the URL that the user listed in their profile (not necessarily a link to their Twitter profile)
user.listed_count: number of lists this user is on (?)
user.followers_count: number of accounts that follow this user
user.default_profile_image: bool; does the user use the default profile pic?
user.utc_offset: positive or negative distance from UTC, in seconds
user.friends_count: number of accounts this user follows
user.default_profile: bool; does the user use the default profile?
user.name: user's profile name
user.lang: user's default language
user.screen_name: user's account name
user.geo_enabled: bool; does user have geo enabled?
user.profile_background_color: user's profile background color, as hex in format ""RRGGBB"" (no '#')
user.profile_image_url: a link to the user's profile pic
user.time_zone: full name of the user's time zone
id: unique tweet ID
favorite_count: number of times the tweet has been favorited
retweeted: is this a retweet?
source: if a link, where is it from (e.g., ""Instagram"")
favorited: have I (Ed King) favorited this tweet?
retweet_count: number of times this tweet has been retweeted
I've also included a file called bad_tweets.csv , which includes all of the tweet IDs that could not be scraped, along with the error message I received while trying to scrape them. This typically happens because the tweet has been deleted, the user has deleted their account (or been banned), or the user has made their tweets private. The fields in this file are id and exception.response."
Diagnose Specific Language Impairment in Children,Explore and create models using data derived from transcripts in CHILDES,dgoke1,19,"Version 6,2017-11-23|Version 5,2017-11-23|Version 4,2017-11-23|Version 3,2017-11-23|Version 2,2017-04-11|Version 1,2017-04-11","healthcare
children
linguistics",CSV,618 KB,CC0,"3,943 views",341 downloads,2 kernels,0 topics,https://www.kaggle.com/dgokeeffe/specific-language-impairment,"Context
Specific Language Impairment is a condition that effects roughly 7% of 5-year old children. It is characterised by a lack of language ability in comparison to your peers but with no obvious mental or physical disability. Diagnosis can tend to be laborious, thus automating this process using NLP and ML techniques might be of interest to paediatricians and speech pathologists.
Content
This study evaluated three datasets obtained via the CHILDES project. All the datasets consist of narratives from a child attempting to complete a wordless picture task. The choice to use only narrative corpora was based on previous research which indicated it has the best ability to distinguish a language impairment in children. The first dataset consists of samples from British adolescents, the second from Canadian children aged 4 to 9, and the third from U.S. children aged 4 to 12.
Unfortunately finding transcript data of this kind is rare, I have tried to find more data to no avail, so 1163 samples will have to do.
Conti-Ramsden 4:
The Conti-Ramsden 4 dataset was collected for a study to assess the effectiveness of narrative tests on adolescents. It consists of 99 TD and 19 SLI samples of children between the ages of 13.10 and 15.90. Ideally all the corpora would only be from children, as SLI is most prominent in children aged five years old, and is best detected early. However, it was included to enable a direct comparison between classifiers created by Gabani and this study.
The corpus contains transcripts of a story telling task based on Mayer’s wordless picture book “Frog, Where Are You”. The children first viewed the picture book in their own time before being prompted to retell the story in the past tense. If the children started telling the story in the present tense the interviewer would prompt them with the phrase “What happened next?” in order to attempt to revert them back to the past tense. If they failed to start to retell the story in the past tense after two prompts no further prompts were made.
ENNI
The ENNI dataset was collected during the course of a study aimed at identifying SLI children using an index of storytelling ability based on the story grammar model. The corpus consists of 300 TD and 77 SLI samples of children aged between 4 and 9 years old. Each child was presented with two wordless picture stories with one more complicated than the other. Unlike Conti-Ramsden 4 the examiner held the book and turned the page after the child appeared to be finished telling the story for a particular picture. The children were also given the opportunity to practice on a training story, where the examiner gave more explicit prompts to the child about what to do.
Gillam
The Gillam dataset is based on another tool for narrative assessment known as “The Test of Narrative Language (TNL). It consists of 250 language impaired children, and 520 controls aged between 5 and 12. A detailed description of each of the participants does not exist. The TNL consists of four storytelling tasks, the first is a recall of a script based story, the rest being wordless picture books. The first picture set depicts a story with a main protagonist having repeated attempts at the goal, and the rest are single picture stories. The single picture stories require more input from the child, and thus is better suited to older children. The TNL appears to be intermediary in difficulty compared to ENNI.
Features
Attribute | Name | Description
Y | The label | 0 for typically developing children 1 for language impaired
child_TNW | Total Number of Words | The total number of words in the transcript
child_TNS | Total Number of Sentences | Children with SLI are more likely to speak in short sentences
group | Same as Y | BEWARE: Is the same as Y but easier to graph in Python and R
examiner_TNW | Total Number of Words spoken by the examiner | Children with SLI are more likely to need support
freq_ttr | Frequency of Word Types to Word Token Ratio | Divides word types by word tokens and provides a rough measure of lexical diversity.
r_2_i_verbs| Ratio of raw to inflected verbs | Children with SLI often have difficulty with the morphemes -ed, -s, be, and do. This results in the use of raw verbs instead of their inflected forms.
mor_words | Number of words in the %mor tier |
num_pos_tags | Number of different Part-of-Speech tags |
n_dos | Number of Do's | The number of time the word 'do' is used
repetition | Number of Repetitions | Counts the number of repetitions as tagged in the CHAT format inside square brackets e.g., milk milk milk milk = milk [x 4]
retracing | Number of Retracings | A retracing is defined as when a speaker abandons an utterance but then continues again.
fillers | Number of Fillers | Counts the number of fillers used in total. A list of fillers was created by searching through the entire corpus (all 1038 samples) for all common variants of fillers such as um, umm, uh, uhh, etc.
s_1g_ppl | Perplexity of 1-gram SLI | The perplexity of this sample in comparison to a language model trained on all the SLI group for this corpora except the sample
s_2g_ppl | Perplexity of 2-gram SLI | Same as above but with a 2-gram LM
s_3g_ppl | Perplexity of 3-gram SLI | Same as above but with a 3-gram LM
d_1g_ppl | Perplexity of 1-gram TD | The perplexity of this sample in comparison to a language model trained on all the TD group for this corpora except the sample
d_2g_ppl | Perplexity of 2-gram TD | Same as above but with a 2-gram LM
d_3g_ppl | Perplexity of 3-gram TD | Same as above but with a 3-gram LM
z_mlu_sli | Sample Z-score using SLI group's Mean Length of Utterance |
z_mlu_td | Sample Z-score using TD group's Mean Length of Utterance |
z_ndw_sli | Sample Z-score using SLI group's Raw:Inflected Verbs Ratio |
z_ndw_td | Sample Z-score using TD group's Raw:Inflected Verbs Ratio |
z_ipsyn_sli | Sample Z-score using SLI group's Developmental Sentence Score |
z_ipsyn_td | Sample Z-score using TD group's Developmental Sentence Score |
z_utts_sli |Sample Z-score using SLI group's Number of Verb Utterances |
z_utts_td |Sample Z-score using TD group's Number of Verb Utterances |
total_syl | Total number of Syllables | Using a technique from
average_syl | Average number of Syllables per word |
mlu_words | Mean Length of Utterance of Words |
mlu_morphemes | Mean Length of Utterance of Morphemes |
mlu100_utts | Mean Length of Utterance of 1st 100 words |
verb_utt | Number of verb utterances |
dss | Developmental Sentence Score |
ipsyn_total | Index of Productive Syntax Score |
The following fields are counts of instances of Brown's Stages of Morphological Development (see https://www.speech-language-therapy.com/index.php?option=com_content&view=article&id=33:brown&catid=2:uncategorised&Itemid=117)
present_progressive
propositions_in
propositions_on
plural_s
irregular_past_tense
possessive_s
uncontractible_copula
articles
regular_past_ed
regular_3rd_person_s
irregular_3rd_person
uncontractible_aux
contractible_copula
contractible_aux
Back to normal
word_errors | Number of Word Errors | As marked in the transcripts
f_k | Flesch-Kincaid Score | See https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests
n_v | Number of Nouns followed immediately by a verb
n_aux | Number of Nouns followed immediately by an Auxillary verb
n_3s_v | Number of Third Singular Nouns followed immediately by a verb
det_n_pl | * Number of Determinant Nouns followed by a Personal Pronoun*
det_pl_n | * Number of Determinant Pronouns followed by a Noun
pro_aux | * Pronouns followed by Auxillary Verb*
pro_3s_v | * 3rd. singular nominative pronoun followed by Verb*
total_error | Total number of morphosyntactic errors | Sum of the columns from nouns verbs down
This table will take some time to finish, will get to it within a few days
Past Research
I have spent the last few months playing around with this data, I have uploaded it here mainly to speed up the computation of the analysis I have already done. But I'm excited to see what other people can do with this. There are some nice graphs to be made; especially using the MLU attributes.
Thus far using the combined corpora the best I have managed to get in terms of creating a predictive classifier is using Neural Networks with Feature Extraction and SMOTE to get a mean ROC of 0.8709 under 10-repeated-10-k-folds CV. You'll find that Random Forest and SVM with an RBF kernel do comparably well with SMOTE.
Acknowledgements
All the data here was derived by me using the open source transcripts provided on the CHILDES Talkbank (http://childes.talkbank.org/). The methods I used are very close to those in:
K. Gabani, T. Solorio, Y. Liu, K.-n. Hassanali, and C. A. Dollaghan, “Exploring a corpus-based approach for detecting language impairment in monolingual english-speaking children,” Artificial Intelligence in Medicine, vol. 53, no. 3, pp. 161–170, 2011.
Conti-4: D. Wetherell, N. Botting, and G. Conti-Ramsden, “Narrative skills in adolescents with a history of SLI in relation to non-verbal IQ scores,” Child Language Teaching and Therapy, vol. 23, no. 1, pp. 95–113, 2007.
ENNI: P. Schneider, D. Hayward, and R. V. Dub, “Storytelling from pictures using the Edmonton Narrative Norms Instrument,” 2006.
Gillam: R. Gillam and N. Pearson, Test of Narrative Language. Austin, TX: Pro-Ed Inc., 2004.
Inspiration
I'm hoping somebody can beat my score. I'm keen to learn more and see if this can become anything that might be of use to some child/family someday."
Insurance Data,Agency Performance Model,ADM2752836,19,"Version 1,2016-12-02",,CSV,45 MB,ODbL,"6,622 views",707 downloads,7 kernels,2 topics,https://www.kaggle.com/moneystore/agencyperformance,"Context
An insurance group consists of 10 property and casualty insurance, life insurance and insurance brokerage companies. The property and casualty companies in the group operate in a 17-state region. The group is a major regional property and casualty insurer, represented by more than 4,000 independent agents who live and work in local communities through a six-state region. Define the metrics to analyse agent performance based on several attributes like demography, products sold, new business, etc. The goal is to improve their existing knowledge used for agent segmentation in a supervised predictive framework.
Inspiration
Vizualization:
a. Summary stats by agency
b. Product line: Commercial Line/Personal Line wise analysis
c. Agency wise - state wise - distribution of Retention ratio (Top 10)
d. Quote system wise hit ratio distribution (also by PL/CL)
e. Add few more based on your understanding of the data
Growth rates from 2006 to 2013 have to be computed and converted to independent attributes and include in the data for modeling.
Compute Hit ratio based on bounds and quotes for each Quotes system
Compute required aggregations at Agency id and state and year
Decide if binning the data works for this situation
Some suggested approaches:
a. Model Building - Either Regression or classification
b. Pattern extraction - Classification Model
c. Patterns from the data using Decision Trees"
(Better) - Donald Trump Tweets!,A collection of all of Donald Trump tweets--better than its predecessors,LiamLarsen,19,"Version 2,2017-04-16|Version 1,2017-04-16","politics
internet",CSV,2 MB,Other,"9,943 views",822 downloads,23 kernels,0 topics,https://www.kaggle.com/kingburrito666/better-donald-trump-tweets,"Context
Unlike This dataset, (which proved to be unusable). And This one which was filled with unnecessary columns; This Donald trump dataset has the cleanest usability and consists of over 7,000 tweets, no nonsense
You may need to use a decoder other than UTF-8 if you want to see the emojis
Content
Data consists of:
-Date
-Time
-Tweet_Text
-Type
-Media_Type
-Hashtags
-Tweet_Id
-Tweet_Url
-twt_favourites_IS_THIS_LIKE_QUESTION_MARK
-Retweets
I scrapped this from someone on reddit"
AWS Spot Pricing Market,"This includes price, region, instance size, and OS for AWS Spot Instances",Benjamin Visser,19,"Version 4,2017-05-17|Version 3,2017-05-07|Version 2,2017-05-06|Version 1,2017-05-06","business
computing",CSV,2 GB,Other,"4,503 views",296 downloads,11 kernels,2 topics,https://www.kaggle.com/noqcks/aws-spot-pricing-market,"Context
AWS spot Instances allow users to bid on spare server capacity. You set a bid threshold for an instance that is usually upwards of 30% cheaper than standard on-demand AWS instances. You can save a lot of money with AWS spot instances.
Data Content
I pulled this data from the AWS CLI with the describe-spot-price-history command. I took a lot of time to acquire and transform, which is why I decided to provide it here.
There are various time periods per region (I acquired all that I could). The columns are all fairly self-evident. Please comment if you have any questions about the data or columns.
The data includes the following column fields:
price: the current Spot price
datetime: the date and time
instance_type: the Spot instance type
os: the Spot instance operating system
region: the region and availability zone (AZ) for the Spot instance
Inspiration
While AWS spot instances are significantly cheaper than on-demand instances, there is only one problem with spot instances: once the spot market price of an instance exceeds the bid threshold you purchased an instance for, the instance is terminated and given to others with higher bids. So while hourly server costs are cheaper, your server is liable to terminate without notice. But, there is a difference between regions and spot pricing. Sometimes there is an arbitrage between regions, and some regions have more stable prices than others (fewer price spikes). If you can find which region/AZ is most stable, you can worry less about your instance terminating without notice.
I started collecting this data because I wanted answers to two questions:
Which region/AZ is historically cheapest for instance X
Which region/AZ is historically most stable for instance X
We could also use this data to predict which regions are likely to stay under a certain $ Spot price, which would allow you to say with some amount of certainty whether a SPOT instance lasts the next [6,12,18]+ hours."
Facebook V Results: Predicting Check Ins,The competition solution along with each top teams' final submission,Facebook,19,"Version 2,2016-08-03|Version 1,2016-07-09","geography
internet",CSV,3 GB,Other,"10,716 views",996 downloads,24 kernels,0 topics,https://www.kaggle.com/facebook/facebook-v-results,"This dataset contains the solution and the top 20 team's submissions for Facebook's 5th recruiting competition on predicting checkin places.
Here's how an ensemble model leveraging the top results would have performed in the competition:"
El Nino Dataset,Meteorological readings taken from a series of buoys in the equatorial Pacific,UCI Machine Learning,19,"Version 1,2016-11-07","oceans
climate",CSV,10 MB,Other,"6,775 views",690 downloads,9 kernels,0 topics,https://www.kaggle.com/uciml/el-nino-dataset,"Context
This dataset contains oceanographic and surface meteorological readings taken from a series of buoys positioned throughout the equatorial Pacific. This data was collected with the Tropical Atmosphere Ocean (TAO) array, which consists of nearly 70 moored buoys spanning the equatorial Pacific, measuring oceanographic and surface meteorological variables critical for improved detection, understanding and prediction of seasonal-to-interannual climate variations originating in the tropics.
Content
The data consists of the following variables: date, latitude, longitude, zonal winds (west<0, east>0), meridional winds (south<0, north>0), relative humidity, air temperature, sea surface temperature and subsurface temperatures down to a depth of 500 meters. Data taken from the buoys from as early as 1980 for some locations. Other data that was taken in various locations are rainfall, solar radiation, current levels, and subsurface temperatures.
The latitude and longitude in the data showed that the bouys moved around to different locations. The latitude values stayed within a degree from the approximate location. Yet the longitude values were sometimes as far as five degrees off of the approximate location.
There are missing values in the data. Not all buoys are able to measure currents, rainfall, and solar radiation, so these values are missing dependent on the individual buoy. The amount of data available is also dependent on the buoy, as certain buoys were commissioned earlier than others.
All readings were taken at the same time of day.
Acknowledgement
This dataset is part of the UCI Machine Learning Repository, and the original source can be found here. The original owner is the NOAA Pacific Marine Environmental Laboratory.
Inspiration
How can the data be used to predict weather conditions throughout the world?
How do the variables relate to each other?
Which variables have a greater effect on the climate variations?
Does the amount of movement of the buoy effect the reliability of the data?"
Police Officer Deaths in the U.S.,On-Duty Police Officer Deaths from 1971-2016,FiveThirtyEight,19,"Version 1,2016-11-06","death
crime",CSV,4 MB,Other,"6,701 views",997 downloads,35 kernels,0 topics,https://www.kaggle.com/fivethirtyeight/police-officer-deaths-in-the-us,"Context
This dataset contains data behind the story, The Dallas Shooting Was Among The Deadliest For Police In U.S. History. The data are scraped from ODMP and capture information on all tracked on-duty police officer deaths in the U.S. broken down by cause from 1971 until 2016.
Content
This dataset tags every entry as human or canine. There are 10 variables:
person
dept: Department
eow: End of watch
cause: Cause of death
cause_short: Shortened cause of death
date: Cleaned EOW
year: Year from EOW
canine
dept_name
state
Inspiration
Using the data, can you determine the temporal trend of police officer deaths by cause? By state? By department?
Acknowledgements
The primary source of data is the Officer Down Memorial Page (ODMP), started in 1996 by a college student who is now a police officer and who continues to maintain the database. The original data and code can be found on the FiveThirtyEight GitHub."
Human Mobility During Natural Disasters,Twitter geolocation for users during 15 natural disasters,Dryad Digital Repository,19,"Version 2,2017-08-31|Version 1,2016-11-07","geography
demographics
internet",CSV,285 MB,CC0,"5,416 views",511 downloads,13 kernels,0 topics,https://www.kaggle.com/dryad/human-mobility-during-natural-disasters,"This dataset contains geolocation information for thousands of Twitter users during natural disasters in their area.
Abstract
(from original paper)
Natural disasters pose serious threats to large urban areas, therefore understanding and predicting human movements is critical for evaluating a population’s vulnerability and resilience and developing plans for disaster evacuation, response and relief. However, only limited research has been conducted into the effect of natural disasters on human mobility. This study examines how natural disasters influence human mobility patterns in urban populations using individuals’ movement data collected from Twitter. We selected fifteen destructive cases across five types of natural disaster and analyzed the human movement data before, during, and after each event, comparing the perturbed and steady state movement data. The results suggest that the power-law can describe human mobility in most cases and that human mobility patterns observed in steady states are often correlated with those in perturbed states, highlighting their inherent resilience. However, the quantitative analysis shows that this resilience has its limits and can fail in more powerful natural disasters. The findings from this study will deepen our understanding of the interaction between urban dwellers and civil infrastructure, improve our ability to predict human movement patterns during natural disasters, and facilitate contingency planning by policymakers.
Acknowledgments
The original journal article for which this dataset was collected:
Wang Q, Taylor JE (2016) Patterns and limitations of urban human mobility resilience under the influence of multiple types of natural disaster. PLoS ONE 11(1): e0147299. http://dx.doi.org/10.1371/journal.pone.0147299
The Dryad page that this dataset was downloaded from:
Wang Q, Taylor JE (2016) Data from: Patterns and limitations of urban human mobility resilience under the influence of multiple types of natural disaster. Dryad Digital Repository. http://dx.doi.org/10.5061/dryad.88354
The Data
This dataset contains the following fields:
disaster.event: the natural disaster during which the observation was collected. One of:
-- one of:
--- *01_Wipha*, *02_Halong*, *03_Kalmaegi*, *04_Rammasun_Manila* (typhoons)
--- *11_Bohol*, *12_Iquique*, *13_Napa* (earthquakes)
--- *21_Norfolk*, *22_Hamburg*, *23_Atlanta* (winter storms)
--- *31_Phoenix*, *32_Detroit*, *33_Baltimore* (thunderstorms)
--- *41_AuFire1*, *42_AuFire2* (wildfires)
user.anon: an anonymous user id; unique within each disaster event
latitude: latitude of user's tweet
longitude.anon: longitude of user's tweet; shifted to preserve anonymity
time: the date and time of the tweet"
Basic Income Survey - 2016 European Dataset,A survey about Europeans' opinions toward basic income,Dalia Research,19,"Version 1,2017-05-10",income,CSV,3 MB,CC4,"4,501 views",532 downloads,29 kernels,,https://www.kaggle.com/daliaresearch/basic-income-survey-european-dataset,"About this Dataset
Dalia Research conducted the first representative poll on basic income across Europe in the Spring of 2016. The results, first presented together with NEOPOLIS at the Future of Work conference in Zurich, showed that two thirds of Europeans would vote for basic income. Dalia's basic income poll is now an annual survey, and the first wave of results from 2016 are now being made public. Although Dalia's latest research on basic income is not yet public, you can visit here to see the results from the most recent Spring 2017 survey.
The study was conducted by Dalia Research in April 2016 on public opinion across 28 EU Member States. The sample of n=9.649 was drawn across all 28 EU Member States, taking into account current population distributions with regard to age (14-65 years), gender and region/country.
Enjoy perusing the dataset and exploring interesting connections between demographics and support for basic income."
"Tobacco Use and Mortality, 2004-2015","Hospital admissions, prescriptions, and fatalities in England",National Health Service,19,"Version 2,2017-01-18|Version 1,2017-01-18",healthcare,CSV,423 KB,CC0,"7,368 views","1,230 downloads",10 kernels,0 topics,https://www.kaggle.com/nhs/tobacco-use,"Context
Conditions that could be caused by smoking resulted in 1.7 million admissions to hospitals in England, for adults aged 35 and over, in 2014-2015 -- an average of 4,700 admissions per day! These figures refer to admissions with a primary diagnosis of a disease that can be caused by smoking, but for which smoking may or may not have actually been the cause.
Content
The Statistics on Smoking in England report aims to present a broad picture of health issues relating to smoking in England and covers topics such as smoking prevalence, habits, behaviours, and attitudes, smoking-related health issues and mortality, and associated costs.
Acknowledgements
This report contains data and information previously published by the Health and Social Care Information Centre (HSCIC), Department of Health, the Office for National Statistics, and Her Majesty’s Revenue and Customs."
Food searches on Google since 2004,How do we search for food? Google search interest can reveal key food trends over the years.,Google News Lab,19,"Version 1,2017-11-01","journalism
food and drink
internet",CSV,4 MB,CC4,"3,309 views",465 downloads,,0 topics,https://www.kaggle.com/GoogleNewsLab/food-searches-on-google-since-2004,"This is the data behind the Rhythm of Food visualisation by Moritz Stefaner. It shows seasonal food searches in different food types around the world since 2004. The data is indexed, with 0 being the least and 100 being the highest search interest.
Find out more here: http://rhythm-of-food.net/"
Flower Color Images,Set for Classification,Olga Belitskaya,19,"Version 2,2017-11-01|Version 1,2017-09-22","photography
nature
plants
+ 2 more...",Other,49 MB,Other,"4,509 views",522 downloads,4 kernels,,https://www.kaggle.com/olgabelitskaya/flower-color-images,"History
I made the database from the fragments of my own photos of flowers. The images are selected to reflect the flowering features of these plant species.
Content
The content is very simple: 210 images (128x128x3) with 10 species of flowering plants and the file with labels flower-labels.csv. Photo files are in the .png format and the labels are the integers.
Label => Name
0 => phlox; 1 => rose; 2 => calendula; 3 => iris; 4 => leucanthemum maximum; 5 => bellflower; 6 => viola; 7 => rudbeckia laciniata (Goldquelle); 8 => peony; 9 => aquilegia.
Acknowledgements
As an owner of this database, I have published it for absolutely free using by any site visitor.
Usage
Accurate classification of plant species with a small number of images isn't a trivial task. I hope this set can be interesting for training skills in this field. A wide spectrum of algorithms can be used for classification."
Football Matches of Spanish League,Soccer matches of 1st and 2nd division from season 1970-71 to 2016-17,Ricardo Moya,19,"Version 2,2017-12-15|Version 1,2017-06-14",association football,CSV,375 KB,CC4,"3,606 views",468 downloads,5 kernels,2 topics,https://www.kaggle.com/ricardomoya/football-matches-of-spanish-league,"Context
Data Set with the football matches of the Spanish league of the 1st and 2nd division from the 1970-71 to 2016-17 season, has been created with the aim of opening a line of research in the Machine Learning, for the prediction of results (1X2) of football matches.
Content
This file contains information about a football matches with the follow features:
4808,1977-78,1,8,Rayo Vallecano,Real Madrid,3,2,30/10/1977,247014000
id (4808): Unique identifier of football match
season (1977-78): Season in which the match was played
division (1): División in which the match was played (1st '1', 2nd '2')
round (8): round in which the match was played
localTeam (Rayo Vallecano): Local Team name
visitorTeam (Real Madrid): Visitor Team name
localGoals (3): Goals scored by the local team
visitorGoals (2): Goals scored by the visitor team
fecha (30/10/1977): Date in which the match was played
date (247014000): Timestamp in which the match was played
Acknowledgements
Scraping made from:
http://www.bdfutbol.com
http://www.resultados-futbol.com"
Amazon reviews: Kindle Store Category,Amazon reviews: Kindle Store category,Bharadwaj Srigiriraju,19,"Version 1,2017-12-11","business
linguistics
internet",{}JSON,265 MB,Other,"2,767 views",342 downloads,,0 topics,https://www.kaggle.com/bharadwaj6/kindle-reviews,"Context
A small subset of dataset of product reviews from Amazon Kindle Store category.
Content
5-core dataset of product reviews from Amazon Kindle Store category from May 1996 - July 2014. Contains total of 982619 entries. Each reviewer has at least 5 reviews and each product has at least 5 reviews in this dataset.
Columns
asin - ID of the product, like B000FA64PK
helpful - helpfulness rating of the review - example: 2/3.
overall - rating of the product.
reviewText - text of the review (heading).
reviewTime - time of the review (raw).
reviewerID - ID of the reviewer, like A3SPTOKDG7WBLN
reviewerName - name of the reviewer.
summary - summary of the review (description).
unixReviewTime - unix timestamp.
Acknowledgements
This dataset is taken from Amazon product data, Julian McAuley, UCSD website. http://jmcauley.ucsd.edu/data/amazon/
License to the data files belong to them.
Inspiration
Sentiment analysis on reviews.
Understanding how people rate usefulness of a review/ What factors influence helpfulness of a review.
Fake reviews/ outliers.
best rated product IDs, or similarity between products based on reviews alone (not the best idea ikr).
Any other interesting analysis."
Multilingual word vectors in 78 languages,fastText vectors of 78 languages,Rachael Tatman,19,"Version 1,2017-09-01","languages
linguistics",Other,168 MB,CC4,"2,338 views",170 downloads,2 kernels,2 topics,https://www.kaggle.com/rtatman/multilingual-word-vectors-in-78-languages,"Context:
Word embeddings define the similarity between two words by the normalised inner product of their vectors. The matrices in this repository place languages in a single space, without changing any of these monolingual similarity relationships. When you use the resulting multilingual vectors for monolingual tasks, they will perform exactly the same as the original vectors.
Facebook recently open-sourced word vectors in 89 languages. However these vectors are monolingual; meaning that while similar words within a language share similar vectors, translation words from different languages do not have similar vectors. In this dataset are 78 matrices, which can be used to align the majority of the fastText languages in a single space.
Contents:
This repository contains 78 matrices, which can be used to align the majority of the fastText languages in a single space.
This dataset was obtained by first getting the 10,000 most common words in the English fastText vocabulary, and then using the Google Translate API to translate these words into the 78 languages available. This vocabulary was then split in two, assigning the first 5000 words to the training dictionary, and the second 5000 to the test dictionary. The alignment procedure is discribed in this blog. It takes two sets of word vectors and a small bilingual dictionary of translation pairs in two languages; and generates a matrix which aligns the source language with the target. Sometimes Google translates an English word to a non-English phrase, in these cases we average the word vectors contained in the phrase. To place all 78 languages in a single space, every matrix is aligned to the English vectors (the English matrix is the identity). You can find more information on this dataset in the authors’ GitHub repository, here.
Acknowledgements:
This dataset was produced by Samuel Smith, David Turban, Steven Hamblin and Nils Hammerly. If you use this repository, please cite: Offline bilingual word vectors, orthogonal transformations and the inverted softmax. Samuel L. Smith, David H. P. Turban, Steven Hamblin and Nils Y. Hammerla. ICLR 2017 (conference track)
Inspiration:
Can you use the word embeddings in this dataset to cluster languages into their families?
Can you create a visualization of the relationship between words for similar concepts across languages?"
Between Our Worlds: An Anime Ontology,"A Linked Open Dataset of Over 390,000 Anime",Rachael Tatman,19,"Version 1,2017-09-09","popular culture
information technology
internet",CSV,97 MB,Other,"1,309 views",70 downloads,,,https://www.kaggle.com/rtatman/between-our-worlds-an-anime-ontology,"Context:
Linked data: “Linked open data is linked data that is open content. In computing, linked data (often capitalized as Linked Data) is a method of publishing structured data so that it can be interlinked and become more useful through semantic queries. It builds upon standard Web technologies such as HTTP, RDF and URIs, but rather than using them to serve web pages for human readers, it extends them to share information in a way that can be read automatically by computers. This enables data from different sources to be connected and queried.” -- “Linked Open Data” on Wikipedia
Anime: “Anime is a Japanese term for hand-drawn or computer animation. The word is the abbreviated pronunciation of ""animation"" in Japanese, where this term references all animation. Outside Japan, anime is used to refer specifically to animation from Japan or as a Japanese-disseminated animation style often characterized by colorful graphics, vibrant characters and fantastical themes.” -- “Anime” on Wikipedia
This dataset is a linked open dataset that contains information on 391706 anime titles.
Content:
This dataset contains two files. The first is the native N-Triples format, which is suitable for tasks. The second is a .csv containing three columns:
Anime: the title of the anime
Concept: the concept
Value: the value of the concept for that anime
The .csv is not a true linked data dataset, since it has removed many of the relevant URL’s. However, it should prove easier for data analysis.
Acknowledgements:
This dataset has been collected and maintained by Pieter Heyvaert. It is © Between Our Worlds and reproduced here under an MIT license. You can find more information on this dataset and the most recent version here.
Inspiration:
Many anime have summaries, under the “description” concept. Can you use these to identify common themes in anime? What about training an anime description generator?
Can you plot the number of titles released over time? Has the rate of anime production increased or decreased over time?"
Animal Bites,"Data on over 9,000 bites, including rabies tests",Rachael Tatman,19,"Version 1,2017-09-16","healthcare
animals
crime
violence",CSV,675 KB,CC0,"4,441 views",590 downloads,31 kernels,0 topics,https://www.kaggle.com/rtatman/animal-bites,"Context:
In the United States, animal bites are often reported to law enforcement (such as animal control). The main concern with an animal bite is that the animal may be rabid. This dataset includes information on over 9,000 animal bites which occurred near Louisville, Kentucky from 1985 to 2017 and includes information on whether the animal was quarantined after the bite occurred and whether that animal was rabid.
Content:
Attributes of animal bite incidents reported to and investigated by Louisville Metro Department of Public Health and Wellness. Personal/identifying data has been removed. This dataset is a single .csv with the following fields.
bite_date: The date the bite occurred
SpeciesIDDesc: The species of animal that did the biting
BreedIDDesc: Breed (if known)
GenderIDDesc: Gender (of the animal)
color: color of the animal
vaccination_yrs: how many years had passed since the last vaccination
vaccination_date: the date of the last vaccination
victim_zip: the zipcode of the victim
AdvIssuedYNDesc: whether advice was issued
WhereBittenIDDesc: Where on the body the victim was bitten
quarantine_date: whether the animal was quarantined
DispositionIDDesc: whether the animal was released from quarantine
head_sent_date: the date the animal’s head was sent to the lab
release_date: the date the animal was released
ResultsIDDesc: results from lab tests (for rabies)
Acknowledgements:
Attributes of animal bite incidents reported to and investigated by Louisville Metro Department of Public Health and Wellness. This data is in the public domain.
Inspiration:
Which animals are most likely to bite humans?
Are some dog breeds more likely to bite?
What factors are most strongly associated with a positive rabies ID?"
OECD Better Life Index 2017,Cleaned dataset for the 2017 OECD Better Life Index,Joel Jacobsen,19,"Version 1,2017-12-13","politics
demographics
international relations",CSV,5 KB,Other,"2,383 views",482 downloads,4 kernels,0 topics,https://www.kaggle.com/jej13b/oecd-better-life-index,"Content
This is the Better Life Index for 2017 gathered from the OECD stats page. Grouping labels have been removed and the row for units of measurment for each column has been removed with the units added to the end of each column label as such: (Percentage: 'as pct'; Ratio: 'as rat'; US Dollar: 'in usd'; Average score: 'as avg score'; Years: 'in years'; Micrograms per cubic metre: 'in ugm3'; Hours: 'in hrs'). Also, although included in the report, Brazil, Russia, and South Africa are non-OECD economies at the time of reporting
Acknowledgements
OECD stats page For full index and others please visit: http://stats.oecd.org/Index.aspx?DataSetCode=BLI"
Consumer Price Index,Statistical measures of change in prices of consumer goods,US Bureau of Labor Statistics,19,"Version 2,2017-06-28|Version 1,2017-06-28","business
finance",CSV,63 MB,CC0,"3,667 views",411 downloads,,0 topics,https://www.kaggle.com/bls/consumer-price-index,"Context:
The Bureau of Labor Statistics defines the Consumer Price Index (CPI) as “a statistical measure of change, over time, of the prices of goods and services in major expenditure groups--such as food, housing, apparel, transportation, and medical care--typically purchased by urban consumers. Essentially, it compares the cost of a sample of goods and services in a specific month relative to the cost of the same ""market basket"" in an earlier reference period.
Make sure to read the cu.txt for more descriptive summaries on each data file and how to use the unique identifiers.
Content:
This dataset was collected June 27th, 2017 and may not be up-to-date.
The revised CPI introduced by the BLS in 1998 includes indexes for two populations; urban wage earners and clerical workers (CW), and all urban consumers (CU). This dataset covers all urban consumers (CU).
The Consumer Price Index (CPI) is a statistical measure of change, over time, of the prices of goods and services in major expenditure groups--such as food, housing, apparel, transportation, and medical care--typically purchased by urban consumers. Essentially, it compares the cost of a sample ""market basket"" of goods and services in a specific month relative to the cost of the same ""market basket"" in an earlier reference period. This reference period is designated as the base period.
As a result of the 1998 revision, both the CW and the CU utilize updated expenditure weights based upon data tabulated from three years (1982, 1983, and 1984) of the Consumer Expenditure Survey and incorporate a number of technical improvements, including an updated and revised item structure.
To construct the two indexes, prices for about 100,000 items and data on about 8,300 housing units are collected in a sample of 91 urban places. Comparison of indexes for individual CMSA's or cities show only the relative change over time in prices between locations. These indexes cannot be used to measure interarea differences in price levels or living costs.
Summary Data Available: U.S. average indexes for both populations are available for about 305 consumer items and groups of items. In addition, over 100 of the indexes have been adjusted for seasonality. The indexes are monthly with some beginning in 1913. Semi-annual indexes have been calculated for about 100 items for comparison with semi-annual areas mentioned below. Semi-annual indexes are available from 1984 forward.
Area indexes for both populations are available for 26 urban places. For each area, indexes are published for about 42 items and groups. The indexes are published monthly for three areas, bimonthly for eleven areas, and semi-annually for 12 urban areas.
Regional indexes for both populations are available for four regions with about 55 items and groups per region. Beginning with January 1987, indexes are monthly, with some beginning as early as 1966. Semi-annual indexes have been calculated for about 42 items for comparison with semi-annual areas mentioned above. Semi-annual indexes have been calculated for about 42 items in the 27 urban places for comparison with semi-annual areas.
City-size indexes for both populations are available for three size classes with about 55 items and groups per class. Beginning with January 1987, indexes are monthly and most begin in 1977. Semi-annual indexes have been calculated for about 42 items for comparison with semi-annual areas mentioned below.
Region/city-size indexes for both populations are available cross classified by region and city-size class. For each of 13 cross calculations, about 42 items and groups are available. Beginning with January 1987, indexes are monthly and most begin in 1977. Semi-annual indexes have been calculated for about 42 items in the 26 urban places for comparison with semi-annual areas.
Frequency of Observations: U.S. city average indexes, some area indexes, and regional indexes, city-size indexes, and region/city-size indexes for both populations are monthly. Other area indexes for both populations are bimonthly or semi-annual.
Annual Averages: Annual averages are available for all unadjusted series in the CW and CU.
Base Periods: Most indexes have a base period of 1982-1984 = 100. Other indexes, mainly those which have been added to the CPI program with the 1998 revision, are based more recently. The base period value is 100.0, except for the ""Purchasing Power"" values (AAOR and SAOR) where the base period value is 1.000.
Data Characteristics: Indexes are stored to one decimal place, except for the ""Purchasing Power"" values which are stored to three decimal places.
References: BLS Handbook of Methods, Chapter 17, ""Consumer Price Index"", BLS Bulletin 2285, April 1988.
Acknowledgements:
This dataset was taken directly from the U.S. Bureau of Labor Statistics website at http://www.bls.gov/data/ and converted to CSV format.
Inspiration:
The Bureau of Labor Statistics has done a great job of providing this source of information for the public to explore. You can use this information to compare the cost of living in urban areas around the United States. What are the top 10 most expensive places to live? Which cities have the most expensive snacks or college textbooks? Coffee? Beer?"
Behavioral Risk Factor Surveillance System,Public health surveys of 400k people from 2011-2015,Centers for Disease Control and Prevention,19,"Version 1,2017-08-24","mental health
public health",CSV,3 GB,CC0,"4,051 views",534 downloads,,2 topics,https://www.kaggle.com/cdc/behavioral-risk-factor-surveillance-system,"The objective of the BRFSS is to collect uniform, state-specific data on preventive health practices and risk behaviors that are linked to chronic diseases, injuries, and preventable infectious diseases in the adult population. Factors assessed by the BRFSS include tobacco use, health care coverage, HIV/AIDS knowledge or prevention, physical activity, and fruit and vegetable consumption. Data are collected from a random sample of adults (one per household) through a telephone survey.
The Behavioral Risk Factor Surveillance System (BRFSS) is the nation's premier system of health-related telephone surveys that collect state data about U.S. residents regarding their health-related risk behaviors, chronic health conditions, and use of preventive services. Established in 1984 with 15 states, BRFSS now collects data in all 50 states as well as the District of Columbia and three U.S. territories. BRFSS completes more than 400,000 adult interviews each year, making it the largest continuously conducted health survey system in the world.
Content
Each year contains a few hundred columns. Please see one of the annual code books for complete details.
These CSV files were converted from a SAS data format using pandas; there may be some data artifacts as a result.
If you like this dataset, you might also like the data for 2001-2010.
Acknowledgements
This dataset was released by the CDC. You can find the original dataset and additional years of data here."
Project Tycho: Contagious Diseases,"Weekly case reports for polio, smallpox, and other diseases in the United States",University of Pittsburgh,19,"Version 1,2017-02-03","healthcare
diseases",CSV,20 MB,Other,"4,757 views",783 downloads,11 kernels,2 topics,https://www.kaggle.com/pitt/contagious-diseases,"Context
The Project Tycho database was named after the Danish nobleman Tycho Brahe, who is known for his detailed astronomical and planetary observations. Tycho was not able to use all of his data for breakthrough discoveries, but his assistant Johannes Kepler used Tycho's data to derive the laws of planetary motion. Similarly, this project aims to advance the availablity of large scale public health data to the worldwide community to accelerate advancements in scientific discovery and technological progress.
Content
The Project Tycho database (level one) includes standardized counts at the state level for smallpox, polio, measles, mumps, rubella, hepatitis A, and whooping cough from weekly National Notifiable Disease Surveillance System (NNDSS) reports for the United States. The time period of data varies per disease somewhere between 1916 and 2010. The records include cases and incidence rates per 100,000 people based on historical population estimates. These data have been used by investigators at the University of Pittsburgh to estimate the impact of vaccination programs in the United States, recently published in the New England Journal of Medicine.
Acknowledgements
The Project Tycho database was digitized and standardized by a team at the University of Pittsburgh, including Professor Wilbert van Panhuis, MD, PhD, Professor John Grefenstette, PhD, and Dean Donald Burke, MD."
GeoNames database,Geographical database covering all countries with over eleven million placenames,GeoNames,19,"Version 4,2017-08-30|Version 3,2017-08-30|Version 2,2017-08-30|Version 1,2017-08-30",geography,Other,1 GB,CC3,"2,321 views",277 downloads,,,https://www.kaggle.com/geonames/geonames-database,"Context
The GeoNames geographical database contains over 10 million geographical names and consists of over 9 million unique features with 2.8 million populated places and 5.5 million alternate names. All features are categorized into one out of nine feature classes and further subcategorized into one out of 645 feature codes.
Content
The main 'geoname' table has the following fields :
geonameid : integer id of record in geonames database
name : name of geographical point (utf8) varchar(200)
asciiname : name of geographical point in plain ascii characters, varchar(200)
alternatenames : alternatenames, comma separated, ascii names automatically transliterated, convenience attribute from alternatename table, varchar(10000)
latitude : latitude in decimal degrees (wgs84)
longitude : longitude in decimal degrees (wgs84)
feature class : see http://www.geonames.org/export/codes.html, char(1)
feature code : see http://www.geonames.org/export/codes.html, varchar(10)
country code : ISO-3166 2-letter country code, 2 characters
cc2 : alternate country codes, comma separated, ISO-3166 2-letter country code, 200 characters
admin1 code : fipscode (subject to change to iso code), see exceptions below, see file admin1Codes.txt for display names of this code; varchar(20)
admin2 code : code for the second administrative division, a county in the US, see file admin2Codes.txt; varchar(80)
admin3 code : code for third level administrative division, varchar(20)
admin4 code : code for fourth level administrative division, varchar(20)
population : bigint (8 byte int)
elevation : in meters, integer
dem : digital elevation model, srtm3 or gtopo30, average elevation of 3''x3'' (ca 90mx90m) or 30''x30'' (ca 900mx900m) area in meters, integer. srtm processed by cgiar/ciat.
timezone : the iana timezone id (see file timeZone.txt) varchar(40)
modification date : date of last modification in yyyy-MM-dd format
AdminCodes:
Most adm1 are FIPS codes. ISO codes are used for US, CH, BE and ME. UK and Greece are using an additional level between country and fips code. The code '00' stands for general features where no specific adm1 code is defined. The corresponding admin feature is found with the same countrycode and adminX codes and the respective feature code ADMx.
feature classes:
A: country, state, region,...
H: stream, lake, ...
L: parks,area, ...
P: city, village,...
R: road, railroad
S: spot, building, farm
T: mountain,hill,rock,...
U: undersea
V: forest,heath,...
Acknowledgements
Data Sources: http://www.geonames.org/data-sources.html"
U.S. Healthcare Data,"Population Health, Diseases, Drugs, Nutritions, Health-plans",BuryBuryZymon,18,"Version 1,2017-12-22","united states
healthcare
diseases
nutrition",CSV,38 MB,CC0,"1,943 views",198 downloads,,0 topics,https://www.kaggle.com/maheshdadhich/us-healthcare-data,"Context
Health care in the United States is provided by many distinct organizations. Health care facilities are largely owned and operated by private sector businesses. 58% of US community hospitals are non-profit, 21% are government owned, and 21% are for-profit. According to the World Health Organization (WHO), the United States spent more on healthcare per capita ($9,403), and more on health care as percentage of its GDP (17.1%), than any other nation in 2014. Many different datasets are needed to portray different aspects of healthcare in US like disease prevalences, pharmaceuticals and drugs, Nutritional data of different food products available in US. Such data is collected by surveys (or otherwise) conducted by Centre of Disease Control and Prevention (CDC), Foods and Drugs Administration, Center of Medicare and Medicaid Services and Agency for Healthcare Research and Quality (AHRQ). These datasets can be used to properly review demographics and diseases, determining start ratings of healthcare providers, different drugs and their compositions as well as package informations for different diseases and for food quality. We often want such information and finding and scraping such data can be a huge hurdle. So, Here an attempt is made to make available all US healthcare data at one place to download from in csv files.
Content
Nhanes Survey (National Health and Nutrition Examination Survey) - The National Health and Nutrition Examination Survey (NHANES) is a program of studies designed to assess the health and nutritional status of adults and children in the United States. The survey is unique in that it combines interviews and physical examinations. NHANES is a major program of the National Center for Health Statistics (NCHS). NCHS is part of the Centers for Disease Control and Prevention (CDC) and has the responsibility for producing vital and health statistics for the Nation. The NHANES interview includes demographic, socioeconomic, dietary, and health-related questions. The examination component consists of medical, dental, and physiological measurements, as well as laboratory tests administered by highly trained medical personnel. The diseases, medical conditions, and health indicators to be studied include: Anemia, Cardiovascular disease, Diabetes, Environmental exposures, Eye diseases, Hearing loss, Infectious diseases, Kidney disease, Nutrition, Obesity, Oral health, Osteoporosis, Physical fitness and physical functioning, Reproductive history and sexual behavior, Respiratory disease (asthma, chronic bronchitis, emphysema), Sexually transmitted diseases, Vision. 10000 individuals are surveyed to represent US statistics. Five files in this datasets represent current recent Nhanes data -
*Nhanes_2005_2006.csv*
*Nhanes_2007_2008.csv*
*Nhanes_2009_2010.csv*
*Nhanes_2011_2012.csv*
*Nhanes_2013_2014.csv*
Data fields' description -
Nhanes_2005_2006.csv - Demographic, Dietary, Examinations, Laboratory
Nhanes_2007_2008.csv - Demographic, Dietary, Examinations, Laboratory
Nhanes_2009_2010.csv - Demographic, Dietary, Examinations, Laboratory
Nhanes_2011_2012.csv - Demographic, Dietary, Examinations, Laboratory
Nhanes_2013_2014.csv - Demographic, Dietary, Examinations, Laboratory
US Drugs datasets - FDA provides a database for searching all the published drugs and all the unpublished drugs on their website, This database provides all the information about package of drugs and compositions of drugs their NDC codes. Description of variables for this datasets are as follows -
*Drugs_product (current and unfinished)*
PRODUCTID - Id of the product
PRODUCTNDC - National drug code of the product
PRODUCTTYPENAME - Type of the product
PROPRIETARYNAME - Proprietary name of the product
PROPRIETARYNAMESUFFIX - Proprietary name Suffix
NONPROPRIETARYNAME - Non- proprietary (common name) of the product
DOSAGEFORMNAME - Dosage information
ROUTENAME - Route of taking drugs (Oral / Injections)
STARTMARKETINGDATE - Date on which marketing for the drug has started
ENDMARKETINGDATE - Date on which the marketing for the drug has stopped
MARKETINGCATEGORYNAME - Marketing category name
APPLICATIONNUMBER - Application number for registering drug
LABELERNAME - Labeler name
SUBSTANCENAME - Names of the substances in drug
ACTIVE_NUMERATOR_STRENGTH - Strength of the drug
ACTIVE_INGRED_UNIT - Unit of strength
PHARM_CLASSES - Pharmaceutical class of the drugs
DEASCHEDULE - DEA schedule
Drugs Package (current and unfinished)
PRODUCTID - Id of the product
PRODUCTNDC - National drug code of the product
NDCPACKAGECODE National drug code of the package
PACKAGEDESCRIPTION - description of the p[ackage
Nutritions Data from USDA - Whenever we buy a packaged food product, we find the nutritional fact written on it. United States Department of Agriculture Agricultural Research Service’s Food composition database. This database contains all kinds food products available in US and provides description of their nutritions. This dataset is web scrapped and converted into a csv file. Variables are self-explanatory names yet the descriptions can be found at this link - variables descriptions -( All values are per 100 grams) -
Data fields' description -
NDB_No - Nutrition database number
Shrt_Desc - Short description
Water_(g) - water in grams per 100 grams
Energ_Kcal - Energy in Kcal
Protein_(g) - Protein
Lipid_Tot_(g) - Total Lipid
Ash_(g) - Ash
Carbohydrt_(g) - Carbohydrate, by difference
Fiber_TD_(g) - Fiber, total dietary
Sugar_Tot_(g) - Total Sugars
Calcium_(mg) - Calcium
Iron_(mg) - Iron
Magnesium_(mg) - Magnesium
Phosphorus_(mg) - Phosphorus
Potassium_(mg) - Potassium
Zinc_(mg) - Zinc
Copper_(mg) - Copper
Manganese_(mg) - Manganese
Selenium_(æg) - Selenium
Vit_C_(mg) - Vitamin C, total ascorbic acid
Thiamin_(mg) - Thiamin
Riboflavin_(mg) - Riboflavin
Niacin_(mg) - Niacin
Panto_Acid_(mg) - Pantothenic acid
Vit_B6_(mg) - Vitamin B6
Folate_Tot_(æg) - Folate, total
Folic_Acid_(æg) - Folic acid
Food_Folate_(æg) - Folate, food
Folate_DFE_(æg) - Folate, DFE
Choline_Tot_ (mg) - Choline, total
Vit_B12_(æg) - Vitamin B-12
Vit_A_IU - Vitamin A, IU
Vit_A_RAE - Vitamin A, RAE
Retinol_(æg) - Retinol
Alpha_Carot_(æg) - Carotene, alpha
Beta_Carot_(æg) - Carotene, beta
Beta_Crypt_(æg) - Cryptoxanthin, beta
Lycopene_(æg) - Lycopene
Lut+Zea_ (æg) - Lutein + zeaxanthin
Vit_E_(mg) - Vitamin E (alpha-tocopherol)
Vit_D_æg - Vitamin D (D2 + D3)
Vit_D_IU - Vitamin D
Vit_K_(æg) - Vitamin K (phylloquinone)
FA_Sat_(g) - Fatty acids, total saturated
FA_Mono_(g) - Fatty acids, total monounsaturated
FA_Poly_(g) - Fatty acids, total polyunsaturated
Cholestrl_(mg) - Cholesterol
GmWt_1 - gram weight 1
GmWt_Desc1 gram weight 1 descriptions
GmWt_2 - gram weight 2
GmWt_Desc2 - gram weight 2 description
Star rating of health care plans with HOS-CAHPS measures - HOS CAHPS survey measures are the base of determining star rating of healthcare plan. Files related to star rating have two types of measures which are used to determine star rating of the healthcare plans - Part C and Part D. Part C is has three type of information 1. Chronic conditions (disease) 2. Tests and Vaccines 3. Member experience with healthcare plans. All variables starting with C01 to C32 are related to part C of the surveys. Similarly Part D of the survey is related to Drugs plans customer services. In data variables starting with D01 to D15 is related to part D. Surveys such as HOS CAHPS etc contains questions whose final standing results into C01 to C32, and D01 to D15 measures. Dataset has two star rating and measurements data released in fall 2015 and Spring 2016. Files description -
Star_rating_fall/spring_2015_C_cutoff.csv - Contains information about different cut off used in determining star rating of part C measures.
Star_rating_fall/spring_2016_D_cutoff - Contains information about different cut off used in determining star rating of part D measures.
Star_rating_fall/spring_domain.csv - Contains information about domain rating of plans
Star_rating_fall/spring_high_performing_plans.csv - List of high performing plans
Star_rating_fal/spring_low_performing_plans.csv - List of low performing plans
Star_rating_fall/spring_master_data.csv - Contains information on all the measures of all plans
Star_rating_fall/spring_plans_final_star_rating.csv - Having information of star rating of healthcare plans
Description -
CONTRACT_ID - Healthcare plan id
Organization Type - Type of the organizer - employer/demo/local cpp etc
Contract Name - Name of the contract
Organization Marketing Name - Self explanatory
Parent Organization - Healthcare provider
HD1: Staying Healthy: Screenings, Tests and Vaccines (domain)
C01: Breast Cancer Screening
C02: Colorectal Cancer Screening
C03: Annual Flu Vaccine
C04: Improving or Maintaining Physical Health
C05: Improving or Maintaining Mental Health
C06: Monitoring Physical Activity
C07: Adult BMI Assessment
HD2: Managing Chronic (Long Term) Conditions (domain)
C08: Special Needs Plan (SNP) Care Management
C09: Care for Older Adults – Medication Review
C10: Care for Older Adults – Functional Status Assessment
C11: Care for Older Adults – Pain Assessment
C12: Osteoporosis Management in Women who had a Fracture
C13: Diabetes Care – Eye Exam
C14: Diabetes Care – Kidney Disease Monitoring
C15: Diabetes Care – Blood Sugar Controlled
C16: Controlling Blood Pressure
C17: Rheumatoid Arthritis Management
C18: Reducing the Risk of Falling
C19: Plan All-Cause Readmissions
HD3: Member Experience with Health Plan (domain)
C20: Getting Needed Care
C21: Getting Appointments and Care Quickly
C22: Customer Service
C23: Rating of Health Care Quality
C24: Rating of Health Plan
C25: Care Coordination
HD4: Member Complaints and Changes in the Health Plan's Performance (domain)
C26: Complaints about the Health Plan
C27: Members Choosing to Leave the Plan
C28: Beneficiary Access and Performance Problems
C29: Health Plan Quality Improvement
HD5: Health Plan Customer Service (domain)
C30: Plan Makes Timely Decisions about Appeals
C31: Reviewing Appeals Decisions
C32: Call Center – Foreign Language Interpreter and TTY Availability
DD1: Drug Plan Customer Service
D01: Call Center – Foreign Language Interpreter and TTY Availability
D02: Appeals Auto–Forward
D03: Appeals Upheld
DD2: Member Complaints and Changes in the Drug Plan’s Performance
D04: Complaints about the Drug Plan
D05: Members Choosing to Leave the Plan
D06: Beneficiary Access and Performance Problems
D07: Drug Plan Quality Improvement
DD3: Member Experience with the Drug Plan
D08: Rating of Drug Plan
D09: Getting Needed Prescription Drugs
DD4: Drug Safety and Accuracy of Drug Pricing
D10: MPF Price Accuracy
D11: High Risk Medication
D12: Medication Adherence for Diabetes Medications
D13: Medication Adherence for Hypertension (RAS antagonists)
D14: Medication Adherence for Cholesterol (Statins)
D15: MTM Program Completion Rate for CMR
SNP - Are they offering special plans
Sanction Deduction - If sanction is deducted from last survey to this survey
2016 Part C Summary - 2016 Part C rating
2016 Part D Summary - 2016 Part D rating
2016 Overall - 2016 Overall star rating of the plan
Rated-as - Category name
Highest rating - category -C/D/Overall for which rating is high
Rating - Star rating of the plan
Acknowledgements
I have collected these files from various data websites and data sources listed below -
Nhanes - from CDS's National Health and Nutrition Examination Survey. Link
Drugs' dataset - from FDA drug database. link
Nutritions' dataset - USDA Food composition databsase. link
Star rating dataset - CMS website. link
Inspiration
These datasets are used for hundreds of publications per year worldwide. Link"
10 Monkey Species,Image dataset for fine-grain classification,Mario,18,"Version 1,2018-01-21","animals
image data",Other,547 MB,CC0,"1,624 views",257 downloads,3 kernels,0 topics,https://www.kaggle.com/slothkong/10-monkey-species,"Content
The dataset consists of two files, training and validation. Each folder contains 10 subforders labeled as n0~n9, each corresponding a species form Wikipedia's monkey cladogram. Images are 400x300 px or larger and JPEG format (almost 1400 images). Images were downloaded with help of the googliser open source code.
Label mapping:
> Label, Latin Nama
> n0, alouatta_palliata
> n1, erythrocebus_patas
> n2, cacajao_calvus
> n3, macaca_fuscata
> n4, cebuella_pygmea
> n5, cebus_capucinus
> n6, mico_argentatus
> n7, saimiri_sciureus
> n8, aotus_nigriceps
> n9, trachypithecus_johnii
For more information on the monkey species and number of images per class make sure to check monkey_labels.txt file.
Aim
This dataset is intended as a test case for fine-grain classification tasks, perhaps best used in combination with transfer learning. Hopefully someone can help us expand the number of classes or number of images.
Acknowledgements
Thanks to Romain Renard for his help with the code implementation. Also, thanks to Gustavo Montoya, Jacky Zhang and Sofia Loaiciga for their help with the dataset curation.
Notes
Some demo code for usage of the dataset in combination with Keras can be found in this repo."
#Charlottesville on Twitter,A snapshot of American history in the making,VincentLa,18,"Version 2,2017-08-22|Version 1,2017-08-18","politics
linguistics
twitter
internet",CSV,178 MB,CC4,"2,873 views",298 downloads,5 kernels,2 topics,https://www.kaggle.com/vincela9/charlottesville-on-twitter,"Charlottesville, Virgina
Charlottesville is home to a statue of Robert E. Lee which is slated to be removed. (For those unfamiliar with American history, Robert E. Lee was a US Army general who defected to the Confederacy during the American Civil War and was considered to be one of their best military leaders.) While many Americans support the move, believing the main purpose of the Confederacy was to defend the institution of slavery, many others do not share this view. Furthermore, believing Confederate symbols to be merely an expression of Southern pride, many have not taken its planned removal lightly.
As a result, many people--including white nationalists and neo-Nazis--have descended to Charlottesville to protest its removal. This in turn attracted many counter-protestors. Tragically, one of the counter-protestors--Heather Heyer--was killed and many others injured after a man intentionally rammed his car into them. In response, President Trump blamed ""both sides"" for the chaos in Charlottesville, leading many Americans to denounce him for what they see as a soft-handed approach to what some have called an act of ""domestic terrorism.""
This dataset below captures the discussion--and copious amounts of anger--revolving around this past week's events.
The Data
Description
This data set consists of a random sample of 50,000 tweets per day (in accordance with the Twitter Developer Agreement) of tweets mentioning Charlottesville or containing ""#charlottesville"" extracted via the Twitter Streaming API, starting on August 15. The files were copied from a large Postgres database containing--currently--over 2 million tweets. Finally, a table of tweet counts per timestamp was created using the whole database (not just the Kaggle sample). The data description PDF provides a full summary of the attributes found in the CSV files.
Note: While the tweet timestamps are in UTC, the cutoffs were based on Eastern Standard Time, so the August 16 file will have timestamps ranging from 2017-08-16 4:00:00 UTC to 2017-08-17 4:00:00 UTC.
Format
The dataset is available as either separate CSV files or a single SQLite database.
License
I'm releasing the dataset under the CC BY-SA 4.0 license. Furthermore, because this data was extracted via the Twitter Streaming API, its use must abide by the Twitter Developer Agreement. Most notably, the display of individual tweets should satisfy these requirements. More information can be found in the data description file, or on Twitter's website.
Acknowledgements
Obviously, I would like to thank Twitter for providing a fast and reliable streaming service. I'd also like to thank the developers of the Python programming language, psycopg2, and Postgres for creating amazing software with which this data set would not exist.
Image Credit
The banner above is a personal modification of these images:
Evan Nesterak: Image Source Image License
Wikipedia user Cville Dog Image Source
The Associated Press Image Source
Inspiration
I almost removed the header ""inspiration"" from this section, because this is a rather sad and dark data set. However, this is preciously why this is an important data set to analyze. Good history books have never shied away from unpleasant events, and never should we.
This data set provides a rich opportunity for many types of research, including:
Natural language processing
Sentiment analysis
Data visualization
Furthermore, given the political nature of this dataset, there are a lot of social science questions that can potentially be answered, or at least piqued, by this data."
World Atlas of Language Structures,"Information on the linguistic structures in 2,679 languages",Rachael Tatman,18,"Version 1,2017-09-08","languages
linguistics",CSV,13 MB,Other,"2,475 views",209 downloads,,0 topics,https://www.kaggle.com/rtatman/world-atlas-of-language-structures,"Context:
There are over 7,000 human languages in the world. The World Atlas of Language Structures (WALS) contains information on the structure of 2,679 of them. It also includes information about where languages are used. WALS is widely-cited and used in the linguistics research community.
Content:
The World Atlas of Language Structures (WALS) is a large database of structural (phonological, grammatical, lexical) properties of languages gathered from descriptive materials (such as reference grammars) by a team of 55 authors. The atlas provides information on the location, linguistic affiliation and basic typological features of a great number of the world's languages
WALS Online is a publication of the (Max Planck Institute for Evolutionary Anthropology)[http://www.eva.mpg.de/]. It is a separate publication, edited by Dryer, Matthew S. & Haspelmath, Martin (Leipzig: Max Planck Institute for Evolutionary Anthropology, 2013) The main programmer is Robert Forkel.
This dataset includes three files:
source.bib: A BibTex file with all of the sources cited in the dataset in it
language.csv: A file with a list of all the languages included in WALS
wals-data.csv: A file containing information on the features associated with each individual language
Acknowledgements:
This dataset is licensed under a Creative Commons Attribution 4.0 International License .
The World Atlas of Language Structures was edited by Matthew Dryer and Martin Haspelmath. If you use this data in your work, please include the following citation:
Dryer, Matthew S. & Haspelmath, Martin (eds.) 2013. The World Atlas of Language Structures Online. Leipzig: Max Planck Institute for Evolutionary Anthropology. (Available online at http://wals.info, Accessed on September 7, 2017.)
Inspiration:
This dataset was designed to make interactive maps of language features. Can you make an interactive map that shows different linguistic features? You might find it helpful to use Leaflet (for R) or Plotly (for Python). This blog post is a great resource to help you get started.
There’s a lot of discussion of “linguistic universals” in linguistics. These are specific features that every language (should) have. Can you identify any features that you think may be universals from this dataset?
You may also like:
Atlas of Pidgin and Creole Language Structures: Information on 76 Creole and Pidgin Languages
World Language Family Map
The Sign Language Analyses (SLAY) Database"
Chess Game Dataset (Lichess),"20,000+ Lichess Games, including moves, victor, rating, opening details and more",Mitchell J,18,"Version 1,2017-09-04","board games
internet",CSV,7 MB,CC0,"7,386 views",374 downloads,3 kernels,,https://www.kaggle.com/datasnaek/chess,"General Info
This is a set of just over 20,000 games collected from a selection of users on the site Lichess.org, and how to collect more. I will also upload more games in the future as I collect them. This set contains the:
Game ID;
Rated (T/F);
Start Time;
End Time;
Number of Turns;
Game Status;
Winner;
Time Increment;
White Player ID;
White Player Rating;
Black Player ID;
Black Player Rating;
All Moves in Standard Chess Notation;
Opening Eco (Standardised Code for any given opening, list here);
Opening Name;
Opening Ply (Number of moves in the opening phase)
For each of these separate games from Lichess. I collected this data using the Lichess API, which enables collection of any given users game history. The difficult part was collecting usernames to use, however the API also enables dumping of all users in a Lichess team. There are several teams on Lichess with over 1,500 players, so this proved an effective way to get users to collect games from.
Possible Uses
Lots of information is contained within a single chess game, let alone a full dataset of multiple games. It is primarily a game of patterns, and data science is all about detecting patterns in data, which is why chess has been one of the most invested in areas of AI in the past. This dataset collects all of the information available from 20,000 games and presents it in a format that is easy to process for analysis of, for example, what allows a player to win as black or white, how much meta (out-of-game) factors affect a game, the relationship between openings and victory for black and white and more."
Internet Advertisements Data Set,This dataset represents a set of possible advertisements on Internet pages,UCI Machine Learning,18,"Version 1,2017-09-01",,CSV,10 MB,ODbL,"6,006 views",502 downloads,3 kernels,,https://www.kaggle.com/uciml/internet-advertisements-data-set,"Context
The task is to predict whether an image is an advertisement (""ad"") or not (""nonad"").
Content
There are 1559 columns in the data.Each row in the data represent one image which is tagged as ad or nonad in the last column.column 0 to 1557 represent the actual numerical attributes of the images
Acknowledgements
Lichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.
Here is a BiBTeX citation as well:
@misc{Lichman:2013 , author = ""M. Lichman"", year = ""2013"", title = ""{UCI} Machine Learning Repository"", url = ""http://archive.ics.uci.edu/ml"", institution = ""University of California, Irvine, School of Information and Computer Sciences"" } https://archive.ics.uci.edu/ml/citation_policy.html"
Names Corpus,5001 female names and 2943 male names,NLTK Data,18,"Version 1,2017-08-16",linguistics,Other,55 KB,Other,"1,566 views",252 downloads,,0 topics,https://www.kaggle.com/nltkdata/names,"Context
This corpus contains 5001 female names and 2943 male names, sorted alphabetically, one per line created by Mark Kantrowitz and redistributed in NLTK.
The names.zip file includes
README: The readme file.
female.txt: A line-delimited list of words.
male.txt: A line-delimited list of words.
License/Usage
Names Corpus, Version 1.3 (1994-03-29)
Copyright (C) 1991 Mark Kantrowitz
Additions by Bill Ross

This corpus contains 5001 female names and 2943 male names, sorted
alphabetically, one per line.

You may use the lists of names for any purpose, so long as credit is
given in any published work. You may also redistribute the list if you
provide the recipients with a copy of this README file. The lists are
not in the public domain (I retain the copyright on the lists) but are
freely redistributable.  If you have any additions to the lists of
names, I would appreciate receiving them.

Mark Kantrowitz <mkant+@cs.cmu.edu>
http://www-2.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/
Inspiration
This corpus is used for the text classification chapter in the NLTK book."
Adverse Pharmaceuticals Events,FDA Adverse Event Reporting System (FAERS) Data,Food and Drug Administration,18,"Version 1,2017-09-08","government agencies
pharmaceutical industry
pharmaceuticals policy",Other,4 GB,CC0,"3,351 views",244 downloads,,,https://www.kaggle.com/fda/adverse-pharmaceuticals-events,"Context:
Identification of adverse drug reactions (ADRs) during the post-marketing phase is one of the most important goals of drug safety surveillance. Spontaneous reporting systems (SRS) data, which are the mainstay of traditional drug safety surveillance, are used for hypothesis generation and to validate the newer approaches. The publicly available US Food and Drug Administration (FDA) Adverse Event Reporting System (FAERS) data requires substantial curation before they can be used appropriately, and applying different strategies for data cleaning and normalization can have material impact on analysis results.
Content:
We provide a curated and standardized version of FAERS removing duplicate case records, applying standardized vocabularies with drug names mapped to RxNorm concepts and outcomes mapped to SNOMED-CT concepts, and pre-computed summary statistics about drug-outcome relationships for general consumption. This publicly available resource, along with the source code, will accelerate drug safety research by reducing the amount of time spent performing data management on the source FAERS reports, improving the quality of the underlying data, and enabling standardized analyses using common vocabularies.
Acknowledgements:
Data available from this source.
When using this data, please cite the original publication:
Banda JM, Evans L, Vanguri RS, Tatonetti NP, Ryan PB, Shah NH (2016) A curated and standardized adverse drug event resource to accelerate drug safety research. Scientific Data 3: 160026. http://dx.doi.org/10.1038/sdata.2016.26
Additionally, please cite the Dryad data package:
Banda JM, Evans L, Vanguri RS, Tatonetti NP, Ryan PB, Shah NH (2016) Data from: A curated and standardized adverse drug event resource to accelerate drug safety research. Dryad Digital Repository. http://dx.doi.org/10.5061/dryad.8q0s4
Inspiration:
This is a large-ish dataset (~4.5 gb uncompressed), so try out your batch processing skills in a Kernel
What groups of drugs are most risky?
What medical conditions are most at risk to drug-associated risks?"
Powerlifting Database,"Over 3,000 meets and 300,000 lifts from competitions worldwide",OpenPowerlifting,18,"Version 1,2018-02-02",weight training,CSV,9 MB,CC0,"1,859 views",277 downloads,3 kernels,,https://www.kaggle.com/open-powerlifting/powerlifting-database,"Context
This dataset is a snapshot of the OpenPowerlifting database as of February 2018. OpenPowerlifting is an organization which tracks meets and competitor results in the sport of powerlifting, in which competitors complete to lift the most weight for their class in three separate weightlifting categories.
Content
This dataset includes two files. meets.csv is a record of all meets (competitions) included in the OpenPowerlifting database. competitors.csv is a record of all competitors who attended those meets, and the stats and lifts that they recorded at them.
For more on how this dataset was collected, see the OpenPowerlifting FAQ.
Acknowledgements
This dataset is republished as-is from the OpenPowerlifting source.
Inspiration
How much influence does overall weight have on lifting capacity?
How big of a difference does gender make? What is demographic of lifters more generally?"
IMDB Movies Dataset,"Over 14,000 movies from IMDB",Orges Leka,18,"Version 1,2016-11-15",,CSV,3 MB,CC0,"14,145 views","1,840 downloads",6 kernels,,https://www.kaggle.com/orgesleka/imdbmovies,"Context
The IMDB Movies Dataset contains information about 14,762 movies. Information about these movies was downloaded with wget for the purpose of creating a movie recommendation app. The data was preprocessed and cleaned to be ready for machine learning applications.
Content
title,
wordsInTitle,
url,
imdbRating,
ratingCount,
duration,
year,
type,
nrOfWins,
nrOfNominations,
nrOfPhotos,
nrOfNewsArticles,
nrOfUserReviews,
nrOfGenre,
The rest of the fields are dummy (0/1) variables indicating if the movie has the given genre:
Action,
Adult,
Adventure,
Animation,
Biography,
Comedy,
Crime,
Documentary,
Drama,
Family,
Fantasy,
FilmNoir,
GameShow,
History,
Horror,
Music,
Musical,
Mystery,
News,
RealityTV,
Romance,
SciFi,
Short,
Sport,
TalkShow,
Thriller,
War,
Western
Past Research
Movie Recommendation App (Learning to Rank)
https://github.com/orgesleka/filmempfehlung,
Inspiration:
Movie Recommendation App: https://github.com/orgesleka/filmempfehlung
Learning to Rank: https://play.google.com/store/apps/details?id=de.leka.orges.filmempfehlung&hl=de"
Trump's World,Help us map Trump's World,SkyLord,18,"Version 1,2017-02-22","geography
politics",CSV,397 KB,CC0,"6,418 views",395 downloads,17 kernels,,https://www.kaggle.com/skylord/trumpworld,"Context
The dataset has been downloaded from a BuzzFeed news article that was posted on Jan 15, 2017. The link to the original source can be checked in the Acknowledgements section.
The authors have created a database of more than 1500 people/organization who have a connection with the Trump family, his top advisors or his cabinet picks.
The dataset can help us to capture how policy decisions may be impacted by these varied connections.
Content
You have three datasets to play with.
Person_Person.csv: Each row represents a connection between a person and another person (eg. Charles Kushner and Alan Hammer)
Person_Org.csv: Each row represents a connection between a person and an organization (eg. 401 NORTH WABASH VENTURE LLC. and Donald J. Trump)
Org_Org.csv: Each row represents a connection between an organization and another organization (eg. TRUMP COMMERCIAL CHICAGO LLC and 401 NORTH WABASH VENTURE LLC. )
All the three files are in the following format:
Column1: Person or Organization (A)
Column2: Person or Organization (B)
Column3: Connection between (A) and (B)
Column4: Source url from which the connection is derived
Acknowledgements
Source: https://www.buzzfeed.com/johntemplon/help-us-map-trumpworld
Inspiration
https://github.com/BuzzFeedNews/trumpworld
This is an incomplete database, and you are free to add more connections."
Agricultural Survey of African Farm Households,Survey of 9500+ households to study impact of climate change on agriculture,Chris Crawford,18,"Version 1,2017-07-21","africa
climate
demographics
agriculture",Other,36 MB,CC0,"1,957 views",385 downloads,2 kernels,0 topics,https://www.kaggle.com/crawford/agricultural-survey-of-african-farm-households,"Context
Abstract: Surveys for more than 9,500 households were conducted in the growing seasons 2002/2003 or 2003/2004 in eleven African countries: Burkina Faso, Cameroon, Ghana, Niger and Senegal in western Africa; Egypt in northern Africa; Ethiopia and Kenya in eastern Africa; South Africa, Zambia and Zimbabwe in southern Africa. Households were chosen randomly in districts that are representative for key agro-climatic zones and farming systems. The data set specifies farming systems characteristics that can help inform about the importance of each system for a country’s agricultural production and its ability to cope with short- and long-term climate changes or extreme weather events. Further it informs about the location of smallholders and vulnerable systems and permits benchmarking agricultural systems characteristics.
Content
The data file contains survey data collected from different families and has 9597 rows that represent the households and 1753 columns with details about the households. The questionnaire was organized into seven sections and respondents were asked to relate the information provided to the previous 12 months’ farming season. There are too many columns to describe here, however they are described in detail in this paper: https://www.nature.com/articles/sdata201620?WT.ec_id=SDATA-201605
Questionnaire.pdf: This file contains the questionnaire used, a description for each variable name and the question ID.
SurveyManual.pdf: This file gives further information on the household questionnaire, the research design and surveying. It was produced for the team leaders and interviewers in the World Bank/GEF project.
AdaptationCoding.pdf: This file describes codes for variables ‘ad711’ to ‘ad7625’ from section VII of the questionnaire on adaptation options.
There is also some description in how the data was collected in Survey.pdf.
Acknowledgements
Waha, Katharina; Zipf, Birgit; Kurukulasuriya, Pradeep; Hassan, Rashid (2016): An agricultural survey for more than 9,500 African households. figshare. https://doi.org/10.6084/m9.figshare.c.1574094
https://www.nature.com/articles/sdata201620?WT.ec_id=SDATA-201605
The original DTA file was converted to CSV
Inspiration
This dataset contains a huge amount of information related to farming households in Africa. Data like these are important for studying the impact of global warming on African agriculture and farming families."
New York City Transport Statistics,"Periodic data recorded from NYC Buses - Location, Time, Schedule & more",MichaelStone,18,"Version 1,2017-07-18","transport
public transport",CSV,326 MB,Other,"4,079 views",399 downloads,,,https://www.kaggle.com/stoney71/new-york-city-transport-statistics,"Context
I wanted to find a better way to provide live traffic updates. We dont all have access to the data from traffic monitoring sensors or whatever gets uploaded from people's smart phones to Apple, Google etc plus I question how accurate the traffic congestion is on Google Maps or other apps. So I figured that since buses are also in the same traffic and many buses stream their GPS location and other data live, that would be an ideal source for traffic data. I investigated the data streams available from many bus companies around the world and found MTA in NYC to be very reliable.
Content
This dataset is from the NYC MTA buses data stream service. In roughly 10 minute increments the bus location, route, bus stop and more is included in each row. The scheduled arrival time from the bus schedule is also included, to give an indication of where the bus should be (how much behind schedule, or on time, or even ahead of schedule).
Data for the entire month of June 2017 is included.
Due to space limitations on Kaggle for datasets, only selected bus routes have been included.
Acknowledgements
Data is recorded from the MTA SIRI Real Time data feed and the MTA GTFS Schedule data.
Inspiration
I want to see what exploratory & discovery people come up with from this data. Feel free to download this dataset for your own use however I would appreciate as many Kernals included on Kaggle as we can get.
Based on the interest this generates I plan to collect more data for subsequent months down the track."
Worldnews on Reddit from 2008 to Today,Perfect for NLP or other tasks,Chris,18,"Version 1,2016-11-23","news agencies
linguistics
internet",CSV,78 MB,Other,"6,075 views",498 downloads,26 kernels,,https://www.kaggle.com/rootuser/worldnews-on-reddit,"Reddit is a social network which divide topics into so called 'subreddits'.
In subreddit 'worldnews', news of the whole world are published. The dataset contains following columns: time_created - a Unix timestamp of the submission creation date date_created - creation time in %Y-%m-%d up_votes - how often the submission was upvoted down_votes - how often the submission was downvoted title - the title of the submission over_18 - if the submission is for mature persons author - the reddit username of the author subreddit - this is always 'worldnews'
With the dataset, you can estimate several things in contrast to world politics and special events."
Missing Migrants Dataset,Explore missing migrants across the globe,jmataya,18,"Version 1,2017-06-16","demographics
international relations",CSV,326 KB,Other,"4,396 views",434 downloads,7 kernels,,https://www.kaggle.com/jmataya/missingmigrants,"About the Missing Migrants Data
This data is sourced from the International Organization for Migration. The data is part of a specific project called the Missing Migrants Project which tracks deaths of migrants, including refugees , who have gone missing along mixed migration routes worldwide. The research behind this project began with the October 2013 tragedies, when at least 368 individuals died in two shipwrecks near the Italian island of Lampedusa. Since then, Missing Migrants Project has developed into an important hub and advocacy source of information that media, researchers, and the general public access for the latest information.
Where is the data from?
Missing Migrants Project data are compiled from a variety of sources. Sources vary depending on the region and broadly include data from national authorities, such as Coast Guards and Medical Examiners; media reports; NGOs; and interviews with survivors of shipwrecks. In the Mediterranean region, data are relayed from relevant national authorities to IOM field missions, who then share it with the Missing Migrants Project team. Data are also obtained by IOM and other organizations that receive survivors at landing points in Italy and Greece. In other cases, media reports are used. IOM and UNHCR also regularly coordinate on such data to ensure consistency. Data on the U.S./Mexico border are compiled based on data from U.S. county medical examiners and sheriff’s offices, as well as media reports for deaths occurring on the Mexico side of the border. Estimates within Mexico and Central America are based primarily on media and year-end government reports. Data on the Bay of Bengal are drawn from reports by UNHCR and NGOs. In the Horn of Africa, data are obtained from media and NGOs. Data for other regions is drawn from a combination of sources, including media and grassroots organizations. In all regions, Missing Migrants Projectdata represents minimum estimates and are potentially lower than in actuality.
Updated data and visuals can be found here: https://missingmigrants.iom.int/
Who is included in Missing Migrants Project data?
IOM defines a migrant as any person who is moving or has moved across an international border or within a State away from his/her habitual place of residence, regardless of
    (1) the person’s legal status; 
    (2) whether the movement is voluntary or involuntary; 
    (3) what the causes for the movement are; or 
    (4) what the length of the stay is.[1]
Missing Migrants Project counts migrants who have died or gone missing at the external borders of states, or in the process of migration towards an international destination. The count excludes deaths that occur in immigration detention facilities, during deportation, or after forced return to a migrant’s homeland, as well as deaths more loosely connected with migrants’ irregular status, such as those resulting from labour exploitation. Migrants who die or go missing after they are established in a new home are also not included in the data, so deaths in refugee camps or housing are excluded. This approach is chosen because deaths that occur at physical borders and while en route represent a more clearly definable category, and inform what migration routes are most dangerous. Data and knowledge of the risks and vulnerabilities faced by migrants in destination countries, including death, should not be neglected, rather tracked as a distinct category.
How complete is the data on dead and missing migrants?
Data on fatalities during the migration process are challenging to collect for a number of reasons, most stemming from the irregular nature of migratory journeys on which deaths tend to occur. For one, deaths often occur in remote areas on routes chosen with the explicit aim of evading detection. Countless bodies are never found, and rarely do these deaths come to the attention of authorities or the media. Furthermore, when deaths occur at sea, frequently not all bodies are recovered - sometimes with hundreds missing from one shipwreck - and the precise number of missing is often unknown. In 2015, over 50 per cent of deaths recorded by the Missing Migrants Project refer to migrants who are presumed dead and whose bodies have not been found, mainly at sea.
Data are also challenging to collect as reporting on deaths is poor, and the data that does exist are highly scattered. Few official sources are collecting data systematically. Many counts of death rely on media as a source. Coverage can be spotty and incomplete. In addition, the involvement of criminal actors in incidents means there may be fear among survivors to report deaths and some deaths may be actively covered-up. The irregular immigration status of many migrants, and at times their families as well, also impedes reporting of missing persons or deaths.
The varying quality and comprehensiveness of data by region in attempting to estimate deaths globally may exaggerate the share of deaths that occur in some regions, while under-representing the share occurring in others.
What can be understood through this data?
The available data can give an indication of changing conditions and trends related to migration routes and the people travelling on them, which can be relevant for policy making and protection plans. Data can be useful to determine the relative risks of irregular migration routes. For example, Missing Migrants Project data show that despite the increase in migrant flows through the eastern Mediterranean in 2015, the central Mediterranean remained the more deadly route. In 2015, nearly two people died out of every 100 travellers (1.85%) crossing the Central route, as opposed to one out of every 1,000 that crossed from Turkey to Greece (0.095%). From the data, we can also get a sense of whether groups like women and children face additional vulnerabilities on migration routes.
However, it is important to note that because of the challenges in data collection for the missing and dead, basic demographic information on the deceased is rarely known. Often migrants in mixed migration flows do not carry appropriate identification. When bodies are found it may not be possible to identify them or to determine basic demographic information. In the data compiled by Missing Migrants Project, sex of the deceased is unknown in over 80% of cases. Region of origin has been determined for the majority of the deceased. Even this information is at times extrapolated based on available information – for instance if all survivors of a shipwreck are of one origin it was assumed those missing also came from the same region.
The Missing Migrants Project dataset includes coordinates for where incidents of death took place, which indicates where the risks to migrants may be highest. However, it should be noted that all coordinates are estimates.
Why collect data on missing and dead migrants?
By counting lives lost during migration, even if the result is only an informed estimate, we at least acknowledge the fact of these deaths. What before was vague and ill-defined is now a quantified tragedy that must be addressed. Politically, the availability of official data is important. The lack of political commitment at national and international levels to record and account for migrant deaths reflects and contributes to a lack of concern more broadly for the safety and well-being of migrants, including asylum-seekers. Further, it drives public apathy, ignorance, and the dehumanization of these groups.
Data are crucial to better understand the profiles of those who are most at risk and to tailor policies to better assist migrants and prevent loss of life. Ultimately, improved data should contribute to efforts to better understand the causes, both direct and indirect, of fatalities and their potential links to broader migration control policies and practices.
Counting and recording the dead can also be an initial step to encourage improved systems of identification of those who die. Identifying the dead is a moral imperative that respects and acknowledges those who have died. This process can also provide a some sense of closure for families who may otherwise be left without ever knowing the fate of missing loved ones.
Identification and tracing of the dead and missing
As mentioned above, the challenge remains to count the numbers of dead and also identify those counted. Globally, the majority of those who die during migration remain unidentified. Even in cases in which a body is found identification rates are low. Families may search for years or a lifetime to find conclusive news of their loved one. In the meantime, they may face psychological, practical, financial, and legal problems.
Ultimately Missing Migrants Project would like to see that every unidentified body, for which it is possible to recover, is adequately “managed”, analysed and tracked to ensure proper documentation, traceability and dignity. Common forensic protocols and standards should be agreed upon, and used within and between States. Furthermore, data relating to the dead and missing should be held in searchable and open databases at local, national and international levels to facilitate identification.
For more in-depth analysis and discussion of the numbers of missing and dead migrants around the world, and the challenges involved in identification and tracing, read our two reports on the issue, Fatal Journeys: Tracking Lives Lost during Migration (2014) and Fatal Journeys Volume 2, Identification and Tracing of Dead and Missing Migrants
Content
The data set records incidents of missing persons and deaths of migrants
columns in the data:
ID - unique key documenting incident
Cause of Death - reason for death
Region of Origin
Nationality
Missing Persons - counts
Dead - counts of deaths
Incident Region - region where incident was recorded
Date - the date when the incident was recorded. Note the data set includes records from 2014 to June 2017
Latitude - spatial coordinates
Longitude - spatial coordinates
Acknowledgements
This data set was created by the International Organization for Migration.
https://www.iom.int/about-iom
Established in 1951, IOM is the leading inter-governmental organization in the field of migration and works closely with governmental, intergovernmental and non-governmental partners.
With 166 member states, a further 8 states holding observer status and offices in over 100 countries, IOM is dedicated to promoting humane and orderly migration for the benefit of all. It does so by providing services and advice to governments and migrants.
IOM works to help ensure the orderly and humane management of migration, to promote international cooperation on migration issues, to assist in the search for practical solutions to migration problems and to provide humanitarian assistance to migrants in need, including refugees and internally displaced people.
The IOM Constitution recognizes the link between migration and economic, social and cultural development, as well as to the right of freedom of movement.
IOM works in the four broad areas of migration management:
Migration and development
Facilitating migration
Regulating migration
Forced migration.
IOM activities that cut across these areas include the promotion of international migration law, policy debate and guidance, protection of migrants' rights, migration health and the gender dimension of migration.
Start a new kernel"
"U.S. Homicide Reports, 1980-2014",Homicides between 1980 and 2014,jyzaguirre,18,"Version 2,2017-03-12|Version 1,2017-03-12","history
crime",Other,109 MB,CC0,"4,161 views",602 downloads,5 kernels,0 topics,https://www.kaggle.com/jyzaguirre/us-homicide-reports,"Context
This datasheet is an extension of the job of ""Murder Accountability Project"". In this datasheet is included a vectorial file of states to make easier the labour of geographical plotting.
Content
The Murder Accountability Project is the most complete database of homicides in the United States currently available. This dataset includes murders from the FBI's Supplementary Homicide Report from 1976 to the present and Freedom of Information Act data on more than 22,000 homicides that were not reported to the Justice Department. This dataset includes the age, race, sex, ethnicity of victims and perpetrators, in addition to the relationship between the victim and perpetrator and weapon used.
Acknowledgements
The data was compiled and made available by the Murder Accountability Project, founded by Thomas Hargrove.
Inspiration
Can you develop an algorithm to detect serial killer activity?"
Leading Causes of Death in the USA,Age-adjusted death rates for the top 10 leading causes of death in the US,LiamLarsen,18,"Version 2,2017-03-31|Version 1,2017-03-30","death
demographics",CSV,1 MB,Other,"4,572 views",617 downloads,16 kernels,,https://www.kaggle.com/kingburrito666/leading-causes-of-death-usa,"Content
Age-adjusted Death Rates for Selected Major Causes of Death: United States, 1900-2013
Age adjusting rates
is a way to make fairer comparisons between groups with different age distributions. For example, a county having a higher percentage of elderly people may have a higher rate of death or hospitalization than a county with a younger population, merely because the elderly are more likely to die or be hospitalized. (The same distortion can happen when comparing races, genders, or time periods.) Age adjustment can make the different groups more comparable. A ""standard"" population distribution is used to adjust death and hospitalization rates. The age-adjusted rates are rates that would have existed if the population under study had the same age distribution as the ""standard"" population. Therefore, they are summary measures adjusted for differences in age distributions.
Acknowledgements
Scrap data from data.gov"
FourSquare - NYC and Tokyo Check-ins,Check-ins in NYC and Tokyo collected for about 10 months,chetan,18,"Version 2,2017-04-27|Version 1,2017-04-27","cities
geography
internet",CSV,98 MB,Other,"4,787 views",463 downloads,18 kernels,0 topics,https://www.kaggle.com/chetanism/foursquare-nyc-and-tokyo-checkin-dataset,"Context
This dataset contains check-ins in NYC and Tokyo collected for about 10 month (from 12 April 2012 to 16 February 2013). It contains 227,428 check-ins in New York city and 573,703 check-ins in Tokyo. Each check-in is associated with its time stamp, its GPS coordinates and its semantic meaning (represented by fine-grained venue-categories). This dataset is originally used for studying the spatial-temporal regularity of user activity in LBSNs.
Content
This dataset includes long-term (about 10 months) check-in data in New York city and Tokyo collected from Foursquare from 12 April 2012 to 16 February 2013. It contains two files in tsv format. Each file contains 8 columns, which are:
User ID (anonymized)
Venue ID (Foursquare)
Venue category ID (Foursquare)
Venue category name (Fousquare)
Latitude
Longitude
Timezone offset in minutes (The offset in minutes between when this check-in occurred and the same time in UTC)
UTC time
The file dataset_TSMC2014_NYC.txt contains 227428 check-ins in New York city. The file dataset_TSMC2014_TKY.txt contains 537703 check-ins in Tokyo.
Acknowledgements
This dataset is acquired from here
Following is the citation of the dataset author's paper:
Dingqi Yang, Daqing Zhang, Vincent W. Zheng, Zhiyong Yu. Modeling User Activity Preference by Leveraging User Spatial Temporal Characteristics in LBSNs. IEEE Trans. on Systems, Man, and Cybernetics: Systems, (TSMC), 45(1), 129-142, 2015. PDF
Inspiration
One of the questions that I am trying to answer is if there is a pattern in users' checkin behaviour. For example, if it's a Friday evening, what all places they might be interested to visit."
Movie Genre from Its Poster,Predicting the genre of a movie by analyzing its poster,Neha,18,,"film
visual arts",CSV,26 MB,Other,,,,,https://www.kaggle.com/neha1703/movie-genre-from-its-poster,
Cervical cancer tumor vs matched control,gene expression profiling data from tumor and matched normal samples (29 each),Thomas Nelson,18,"Version 1,2017-05-08",healthcare,CSV,105 KB,Other,"4,389 views",410 downloads,7 kernels,2 topics,https://www.kaggle.com/thomasnelson/cervical-cancer-tumor-vs-matched-control,"Context
If you use this data, please be sure to give credit to Witten, et. al., since it is their data set.
Cervical cancer tumor vs matched control data. Data set is gene expression profiling data from tumor and matched normal samples (29 each). The data are the raw read counts (not normalized) from sequencing of microRNA. This is not my data, but was published by:
Witten, D., et al. (2010) Ultra-high throughput sequencing-based small RNA discovery and discrete statistical biomarker analysis in a collection of cervical tumours and matched controls. BMC Biology, 8:58
Content
The rows are each micro RNA name and the columns are the sample names (N=normal, T=tumor). The values are raw read counts.
Acknowledgements
Witten, D., et al. (2010) Ultra-high throughput sequencing-based small RNA discovery and discrete statistical biomarker analysis in a collection of cervical tumours and matched controls. BMC Biology, 8:58
Inspiration
Use this data to practice making predictive models from machine learning/deep learning algorithms on gene expression profiling data."
Horse Colic Dataset,Can you predict the life or death of a horse?,UCI Machine Learning,18,"Version 1,2017-06-07","veterinary medicine
animals",Other,59 KB,CC0,"5,400 views",717 downloads,18 kernels,2 topics,https://www.kaggle.com/uciml/horse-colic,"Context
Predict whether or not a horse can survive based upon past medical conditions.
Noted by the ""outcome"" variable in the data.
Content
All of the binary representation have been converted into the words they actually represent. However, a fuller description is provided by the data dictionary (datadict.txt).
There are a lot of NA's in the data. This is the real struggle here. Try to find a way around it through imputation or other means.
Acknowledgements
This dataset was originally published by the UCI Machine Learning Database: http://archive.ics.uci.edu/ml/datasets/Horse+Colic"
Pesticide Data Program (2015),Study of pesticide residues in food,United States Department of Agriculture,18,"Version 1,2016-11-16","food and drink
agriculture",CSV,123 MB,Other,"4,828 views",622 downloads,6 kernels,,https://www.kaggle.com/usdeptofag/pesticide-data-program-2015,"Context
This dataset contains information on pesticide residues in food. The U.S. Department of Agriculture (USDA) Agricultural Marketing Service (AMS) conducts the Pesticide Data Program (PDP) every year to help assure consumers that the food they feed themselves and their families is safe. Ultimately, if EPA determines a pesticide is not safe for human consumption, it is removed from the market.
The PDP tests a wide variety of domestic and imported foods, with a strong focus on foods that are consumed by infants and children. EPA relies on PDP data to conduct dietary risk assessments and to ensure that any pesticide residues in foods remain at safe levels. USDA uses the data to better understand the relationship of pesticide residues to agricultural practices and to enhance USDA’s Integrated Pest Management objectives. USDA also works with U.S. growers to improve agricultural practices.
Content
While the original 2015 MS Access database can be found [here (https://www.ams.usda.gov/datasets/pdp/pdpdata), the data has been transferred to a SQLite database for easier, more open use. The database contains two tables, Sample Data and Results Data. Each sampling includes attributes such as extraction method, the laboratory responsible for the test, and EPA tolerances among others. These attributes are labeled with codes, which can be referenced in PDF format here, or integrated into the database using the included csv files.
Inspiration
What are the most common types of pesticides tested in this study?
Do certain states tend to use one particular pesticide type over another?
Does pesticide type correspond more with crop type or location (state)?
Are any produce types found to have higher pesticide levels than assumed safe by EPA standards?
By combining databases from several years of PDP tests, can you see any trends in pesticide use?
Acknowledgement
This dataset is part of the USDA PDP yearly database, and the original source can be found here."
Vegetarian and Vegan Restaurants,"A list of over 18,000 restaurants that serve vegetarian or vegan food in the US.",Datafiniti,18,"Version 1,2016-11-17","databases
food and drink
business",CSV,79 MB,CC4,"6,404 views",831 downloads,7 kernels,0 topics,https://www.kaggle.com/datafiniti/vegetarian-vegan-restaurants,"About This Data
This is a list of over 18,000 restaurants in the US that serve vegetarian or vegan food provided by Datafiniti's Business Database. The dataset includes address, city, state, business name, business categories, menu data, phone numbers, and more.
What You Can Do With This Data
You can use this data to determine the most vegetarian and vegan-friendly cities in the US. E.g.:
How many restaurants in each metro area offers vegetarian options?
Which metros among the 25 most popular metro areas have the most and least vegetarian restaurants per 100,000 residents?
Which metros with at least 10 vegetarian restaurants have the most vegetarian restaurants per 100,000 residents?
How many restaurants in each metro area offers vegan options?
Which metros among the 25 most popular metro areas have the most and least vegan restaurants per 100,000 residents?
Which metros with at least 10 vegan restaurants have the most vegan restaurants per 100,000 residents?
Which cuisines are served the most at vegetarian restaurants?
Data Schema
A full schema for the data is available in our support documentation.
About Datafiniti
Datafiniti provides instant access to web data. We compile data from thousands of websites to create standardized databases of business, product, and property information. Learn more.
Want More?
You can get more data like this by joining Datafiniti or requesting a demo."
Health Care Access/Coverage for 1995-2010,Prevalence and Trends of Health Care Acess,Centers for Disease Control and Prevention,18,"Version 1,2016-11-17",healthcare,CSV,257 KB,Other,"6,422 views",712 downloads,6 kernels,0 topics,https://www.kaggle.com/cdc/health-care-access-coverage,"Context
This dataset contains the prevalence and trends of health care access/coverage for 1995-2010. Percentages are weighted to population characteristics. Data are not available if it did not meet Behavioral Risk Factor Surveillance System (BRFSS) stability requirements. For more information on these requirements, as well as risk factors and calculated variables, see the Technical Documents and Survey Data for a specific year - http://www.cdc.gov/brfss/annual_data/annual_data.htm.
Content
This dataset has 7 variables:
Year
State
Yes
No
Category
Condition
Location 1
Acknowledgements
The original dataset can be found here.
Recommended citation: Centers for Disease Control and Prevention (CDC). Behavioral Risk Factor Surveillance System. Atlanta, Georgia: U.S. Department of Health and Human Services, Centers for Disease Control and Prevention, [appropriate year].
Inspiration
How does health care coverage change over time?
Does health care access differ by state?"
World Gender Statistics,"Sex-disaggregated and gender-specific data on demographics, education, etc.",World Bank,18,"Version 1,2016-11-28","gender
education
demographics",CSV,76 MB,Other,"8,010 views","1,026 downloads",5 kernels,,https://www.kaggle.com/theworldbank/world-gender-statistics,"The Gender Statistics database is a comprehensive source for the latest sex-disaggregated data and gender statistics covering demography, education, health, access to economic opportunities, public life and decision-making, and agency.
The Data
The data is split into several files, with the main one being Data.csv. The Data.csv contains all the variables of interest in this dataset, while the others are lists of references and general nation-by-nation information.
Data.csv contains the following fields:
Data.csv
Country.Name: the name of the country
Country.Code: the country's code
Indicator.Name: the name of the variable that this row represents
Indicator.Code: a unique id for the variable
1960 - 2016: one column EACH for the value of the variable in each year it was available
The other files
I couldn't find any metadata for these, and I'm not qualified to guess at what each of the variables mean. I'll list the variables for each file, and if anyone has any suggestions (or, even better, actual knowledge/citations) as to what they mean, please leave a note in the comments and I'll add your info to the data description.
Country-Series.csv
CountryCode
SeriesCode
DESCRIPTION
Country.csv
Country.Code
Short.Name
Table.Name
Long.Name
2-alpha.code
Currency.Unit
Special.Notes
Region
Income.Group
WB-2.code
National.accounts.base.year
National.accounts.reference.year
SNA.price.valuation
Lending.category
Other.groups
System.of.National.Accounts
Alternative.conversion.factor
PPP.survey.year
Balance.of.Payments.Manual.in.use
External.debt.Reporting.status
System.of.trade
Government.Accounting.concept
IMF.data.dissemination.standard
Latest.population.census
Latest.household.survey
Source.of.most.recent.Income.and.expenditure.data
Vital.registration.complete
Latest.agricultural.census
Latest.industrial.data
Latest.trade.data
Latest.water.withdrawal.data
FootNote.csv
CountryCode
SeriesCode
Year
DESCRIPTION
Series-Time.csv
SeriesCode
Year
DESCRIPTION
Series.csv
Series.Code
Topic
Indicator.Name
Short.definition
Long.definition
Unit.of.measure
Periodicity
Base.Period
Other.notes
Aggregation.method
Limitations.and.exceptions
Notes.from.original.source
General.comments
Source
Statistical.concept.and.methodology
Development.relevance
Related.source.links
Other.web.links
Related.indicators
License.Type
Acknowledgements
This dataset was downloaded from The World Bank's Open Data project. The summary of the Terms of Use of this data is as follows:
You are free to copy, distribute, adapt, display or include the data in other products for commercial and noncommercial purposes at no cost subject to certain limitations summarized below.
You must include attribution for the data you use in the manner indicated in the metadata included with the data.
You must not claim or imply that The World Bank endorses your use of the data by or use The World Bank’s logo(s) or trademark(s) in conjunction with such use.
Other parties may have ownership interests in some of the materials contained on The World Bank Web site. For example, we maintain a list of some specific data within the Datasets that you may not redistribute or reuse without first contacting the original content provider, as well as information regarding how to contact the original content provider. Before incorporating any data in other products, please check the list: Terms of use: Restricted Data.
-- [ed. note: this last is not applicable to the Gender Statistics database]
The World Bank makes no warranties with respect to the data and you agree The World Bank shall not be liable to you in connection with your use of the data.
This is only a summary of the Terms of Use for Datasets Listed in The World Bank Data Catalogue. Please read the actual agreement that controls your use of the Datasets, which is available here: Terms of use for datasets. Also see World Bank Terms and Conditions."
United States Commutes,Visualizing over 4 million home-to-work commutes in the United States,figshare,18,"Version 1,2016-12-04",transport,CSV,278 MB,CC0,"3,742 views",270 downloads,2 kernels,0 topics,https://www.kaggle.com/figshare/united-states-commutes,"Context
The emergence in the United States of large-scale “megaregions” centered on major metropolitan areas is a phenomenon often taken for granted in both scholarly studies and popular accounts of contemporary economic geography.
This dataset comes from a paper (Nelson & Rae, 2016. An Economic Geography of the United States: From Commutes to Megaregions) that uses a data set of more than 4,000,000 commuter flows as the basis for an empirical approach to the identification of such megaregions.
Content
This dataset consists of two files: one contains the commuting data, and one is a gazetteer describing the population and locations of the census tracts referred to by the commuting data. The fields Ofips and Dfips (FIPS codes for the originating and destination census tracts, respectively) in commute_data.csv refer to the GEOID field in census_tracts_2010.csv.
commute_data
This file contains information on over 4 million commute flows. It has the following fields:
Ofips: the full FIPS code for the origin census tract of an individual flow line
*Dfips *: the full FIPS code for the destination census tract of an individual flow line
Ostfips: the FIPS code for the origin state of an individual flow line
Octfips: the FIPS code for the origin county of an individual flow line
Otrfips: the FIPS code for the destination census tract of an individual flow line
Dstfips: the FIPS code for the destination state of an individual flow line
Dctfips: the FIPS code for the destination county of an individual flow line
Dtrfips: the FIPS code for the destination census tract of an individual flow line
Flow: the total number of commuters associated with this individual point to point flow line (i.e. the total number of journeys to work)
Moe: margin of error of the Flow value above
LenKM: length of each flow line, in Kilometers
ESTDIVMOE: the Flow value divided by the Margin of Error of the estimate
census_tracts_2010
This file contains the following fields, which represent information about different U.S. Census Tracts:
USPS: United States Postal Service State Abbreviation
GEOID: Geographic Identifier - fully concatenated geographic code (State FIPS and County FIPS)
ANSICODE: American National Standards Institute code
NAME: Name
POP10: 2010 Census population count.
HU10: 2010 Census housing unit count.
ALAND: Land Area (square meters) - Created for statistical purposes only.
AWATER: Water Area (square meters) - Created for statistical purposes only.
ALAND_SQMI: Land Area (square miles) - Created for statistical purposes only.
AWATER_SQMI: Water Area (square miles) - Created for statistical purposes only.
INTPTLAT: Latitude (decimal degrees) First character is blank or ""-"" denoting North or South latitude respectively.
INTPTLONG: Longitude (decimal degrees) First character is blank or ""-"" denoting East or West longitude respectively.
Acknowledgements
This dataset comes from the following article:
Nelson & Rae, 2016. An Economic Geography of the United States: From Commutes to Megaregions
The full dataset (in GIS shapefile format) can be found on figshare here"
Volcanic Eruptions in the Holocene Period,"Name, location, and type of volcanoes active in the past 10,000 years",The Smithsonian Institution,18,"Version 1,2017-01-24",geology,CSV,254 KB,CC0,"4,297 views",542 downloads,13 kernels,0 topics,https://www.kaggle.com/smithsonian/volcanic-eruptions,"Content
The Smithsonian Institution's Global Volcanism Program (GVP) documents Earth's volcanoes and their eruptive history over the past 10,000 years. The GVP reports on current eruptions from around the world and maintains a database repository on active volcanoes and their eruptions. The GVP is housed in the Department of Mineral Sciences, part of the National Museum of Natural History, on the National Mall in Washington, D.C.
The GVP database includes the names, locations, types, and features of more than 1,500 volcanoes with eruptions during the Holocene period (approximately the last 10,000 years) or exhibiting current unrest."
Aerial Bombing Operations in World War II,"Target, aircraft used, and bombs deployed for every mission in WWII",United States Air Force,18,"Version 1,2017-01-31","history
war",CSV,27 MB,CC0,"3,646 views",429 downloads,9 kernels,0 topics,https://www.kaggle.com/usaf/world-war-ii,"Content
This dataset consists of digitized paper mission reports from WWII. Each record includes the date, conflict, geographic location, and other data elements to form a live-action sequence of air warfare from 1939 to 1945. The records include U.S. and Royal Air Force data, in addition to some Australian, New Zealand and South African air force missions.
Acknowledgements
Lt Col Jenns Robertson of the US Air Force developed the Theater History of Operations Reports (THOR) and posted them online after receiving Department of Defense approval."
"Person of the Year, 1927-Present",Who has been featured on the magazine cover as Man/Woman of the Year?,Time Magazine,18,"Version 1,2017-03-07","news agencies
history",CSV,11 KB,Other,"4,441 views",538 downloads,26 kernels,0 topics,https://www.kaggle.com/timemagazine/magazine-covers,"Context
TIME's Person of the Year hasn't always secured his or her place in the history books, but many honorees remain unforgettable: Gandhi, Khomeini, Kennedy, Elizabeth II, the Apollo 8 astronauts Anders, Borman and Lovell. Each has left an indelible mark on the world.
TIME's choices for Person of the Year are often controversial. Editors are asked to choose the person or thing that had the greatest impact on the news, for good or ill — guidelines that leave them no choice but to select a newsworthy, not necessarily praiseworthy, cover subject. Controversial choices have included Adolf Hitler (1938), Joseph Stalin (1939, 1942), and Ayatullah Khomeini (1979).
TIME's choices for Person of the Year are often politicians and statesmen. Eleven American presidents, from FDR to George W. Bush, have graced the Person of the Year cover, many of them more than once. As commander in chief of one of the world's greatest nations, it's hard not to be a newsmaker.
Content
This dataset includes a record for every Time Magazine cover which has honored an individual or group as ""Men of the Year"", ""Women of the Year"", or (as of 1999) ""Person of the Year"".
Acknowledgements
The data was scraped from Time Magazine's website.
Inspiration
Who has been featured on the magazine cover the most times? Did any American presidents not receive the honor for their election victory? How has the selection of Person of the Year changed over time? Have the magazine's choices become more or less controversial?"
Non-invasive Blood Pressure Estimation,Vital signals and reference blood pressure values acquired from 26 subjects.,Mohammad Kachuee,18,"Version 1,2017-10-16","healthcare
health sciences",Other,186 MB,Other,"3,650 views",310 downloads,,2 topics,https://www.kaggle.com/mkachuee/noninvasivebp,"Abstract
This dataset provides a collection of vital signals and reference blood pressure values acquired from 26 subjects that can be used for the purpose of non-invasive cuff-less blood pressure estimation.
Source
Creators: Amirhossein Esmaili, Mohammad Kachuee, Mahdi Shabany Department of Electrical Engineering, Sharif University of Technology, Tehran, Iran Date: October 2017
Relevant Information
This dataset is to be used for research methods trying to estimate blood pressure in a cuff-less non-invasive manner. For each subject in this dataset, phonocardiogram (PCG), electrocardiogram (ECG), and photoplethysmogram (PPG) signals are acquired. Alongside the acquisition of the signals per subject, a number of reference BPs are measured. Here, a signal from a force-sensing resistor (FSR), placed under the cuff of the BP reference device, is used to distinguish exact moments of reference BP measurements, which are corresponding to the inflation and deflation of the cuff. The signal from FSR is also included in our dataset. For each subject, age, weight, and height are also recorded.
Attribute Information
In the dataset, corresponding to each subject there is a “.json” file. In each file, we have the following attributes:
“UID”: Subject number
“age”: Age of the subject
“weight”: Weight of the subject (Kg)
“height”: Height of the subject (cm)
“data_PCG”: Acquired PCG signal from chest (Fs = 1 KHz)
“data_ECG”: Acquired ECG signal (Fs = 1 KHz)
“data_PPG”: Acquired PPG signal from fingertip (Fs = 1 KHz)
“data_FSR”: Acquired FSR signal (Fs = 1 KHz)
“data_BP”: reference systolic blood pressure (SBP) and diastolic blood pressure (DBP) values acquired from the subjects. To distinguish the time instances in the signals in which SBP and DBP values are measured, FSR signal can be used. For more details, please refer to our paper.
Please note that the FSR signal should be inverted prior to any analysis to reflect the instantaneous cuff pressure. Also, the moment of each reference device measurement is approximately the time at which the cuff pressure drops with a considerable negative slope.
Relevant Papers
A. Esmaili, M. Kachuee, M. Shabany, Nonlinear Cuffless Blood Pressure Estimation of Healthy Subjects Using Pulse Transit Time and Arrival Time, IEEE Transactions on Instrumentation and Measurement, 2017.
A. Esmaili, M. Kachuee, M. Shabany, Non-invasive Blood Pressure Estimation Using Phonocardiogram, IEEE International Symposium on Circuits and Systems (ISCAS'17), 2017.
Citation Request
A. Esmaili, M. Kachuee, M. Shabany, Nonlinear Cuffless Blood Pressure Estimation of Healthy Subjects Using Pulse Transit Time and Arrival Time, IEEE Transactions on Instrumentation and Measurement, 2017."
Horse Racing in HK,Data on thoroughbred racing in Hong Kong for fun and machine learning,Graham Daley,18,"Version 2,2016-12-15|Version 1,2016-12-14",horse racing,CSV,11 MB,CC0,"7,828 views",873 downloads,20 kernels,2 topics,https://www.kaggle.com/gdaley/hkracing,"Can you beat the market?
Horse racing has always intrigued me - not so much from the point of view as a sport, but more from the view of it as a money market. Inspired by the pioneers of computerised horse betting, I'm sharing this dataset in the hope of finding more data scientists willing to take up the challenge and find new ways of exploiting it!
As always, the goal for most of us is to find information in the data that can be used to generate profit, usually by finding information that has not already been considered by the other players in the game. But I'm always interested in finding new uses for the data, whatever they may be.
Horse racing is a huge business in Hong Kong, which has two race tracks in a city that is only 1,104 square km. The betting pools are bigger than all US racetracks combined, which means that the opportunity is unlimited for those who are successful.
So are you up for it?
Content
The data was obtained from various free sources and is presented in CSV format. Personally-identifiable information, such as horse and jockey names, has not been included. However these should have no relevance to the purpose of this dataset, which is purely for experimental use.
There are two files:
races.csv
Each line describes the condition of an individual race.
race_id - unique identifier for the race
date - date of the race, in YYYY-MM-DD format (note that the dates given have been obscured and are not the real ones, although the durations between each race should be correct)
venue - a 2-character string, representing which of the 2 race courses this race took place at: ST = Shatin, HV = Happy Valley
race_no - race number of the race in the day's meeting
config - race track configuration, mostly related to the position of the inside rail. For more details, see the HKJC website.
surface - a number representing the type of race track surface: 1 = dirt, 0 = turf
distance - distance of the race, in metres
going - track condition. For more details, see the HKJC website.
horse_ratings - the range of horse ratings that may participate in this race
prize - the winning prize, in HK Dollars
race_class - a number representing the class of the race
sec_time1 - time taken by the leader of the race to reach the end of the end of the 1st sectional point (sec)
sec_time2 - time taken by the leader of the race to reach the end of the 2nd sectional point (sec)
sec_time3 - time taken by the leader of the race to reach the end of the 3rd sectional point (sec)
sec_time4 - time taken by the leader of the race to reach the end of the 4th sectional point, if any (sec)
sec_time5 - time taken by the leader of the race to reach the end of the 5th sectional point, if any (sec)
sec_time6 - time taken by the leader of the race to reach the end of the fourth sectional point, if any (sec)
sec_time7 - time taken by the leader of the race to reach the end of the fourth sectional point, if any (sec)
time1 - time taken by the leader of the race in the 1st section only (sec)
time2 - time taken by the leader of the race in the 2nd section only (sec)
time3 - time taken by the leader of the race in the 3rd section only (sec)
time4 - time taken by the leader of the race in the 4th section only, if any (sec)
time5 - time taken by the leader of the race in the 5th section only, if any (sec)
time6 - time taken by the leader of the race in the 6th section only, if any (sec)
time7 - time taken by the leader of the race in the 7th section only, if any (sec)
place_combination1 - placing horse no (1st)
place_combination2 - placing horse no (2nd)
place_combination3 - placing horse no (3rd)
place_combination4 - placing horse no (4th)
place_dividend1 - placing dividend paid (for place_combination1)
place_dividend2 - placing dividend paid (for place_combination2)
place_dividend3 - placing dividend paid (for place_combination2)
place_dividend4 - placing dividend paid (for place_combination2)
win_combination1 - winning horse no
win_dividend1 - winning dividend paid (for win_combination1)
win_combination2 - joint winning horse no, if any
win_dividend2 - winning dividend paid (for win_combination2, if any)
runs.csv
Each line describes the characteristics of one horse run, in one of the races given in races.csv.
race_id - unique identifier for the race
horse_no - the number assigned to this horse, in the race
horse_id - unique identifier for this horse
result - finishing position of this horse in the race
won - whether horse won (1) or otherwise (0)
lengths_behind - finishing position, as the number of horse lengths behind the winner
horse_age - current age of this horse at the time of the race
horse_country - country of origin of this horse
horse_type - sex of the horse, e.g. 'Gelding', 'Mare', 'Horse', 'Rig', 'Colt', 'Filly'
horse_rating - rating number assigned by HKJC to this horse at the time of the race
horse_gear - string representing the gear carried by the horse in the race. An explanation of the codes used may be found on the HKJC website.
declared_weight - declared weight of the horse and jockey, in lbs
actual_weight - actual weight carried by the horse, in lbs
draw - post position number of the horse in this race
position_sec1 - position of this horse (ranking) in section 1 of the race
position_sec2 - position of this horse (ranking) in section 2 of the race
position_sec3 - position of this horse (ranking) in section 3 of the race
position_sec4 - position of this horse (ranking) in section 4 of the race, if any
position_sec5 - position of this horse (ranking) in section 5 of the race, if any
position_sec6 - position of this horse (ranking) in section 6 of the race, if any
behind_sec1 - position of this horse (lengths behind leader) in section 1 of the race
behind_sec2 - position of this horse (lengths behind leader) in section 2 of the race
behind_sec3 - position of this horse (lengths behind leader) in section 3 of the race
behind_sec4 - position of this horse (lengths behind leader) in section 4 of the race, if any
behind_sec5 - position of this horse (lengths behind leader) in section 5 of the race, if any
behind_sec6 - position of this horse (lengths behind leader) in section 6 of the race, if any
time1 - time taken by the horse to pass through the 1st section of the race (sec)
time2 - time taken by the horse to pass through the 2nd section of the race (sec)
time3 - time taken by the horse to pass through the 3rd section of the race (sec)
time4 - time taken by the horse to pass through the 4th section of the race, if any (sec)
time5 - time taken by the horse to pass through the 5th section of the race, if any (sec)
time6 - time taken by the horse to pass through the 6th section of the race, if any (sec)
finish_time - finishing time of the horse in this race (sec)
win_odds - win odds for this horse at start of race
place_odds - place (finishing in 1st, 2nd or 3rd position) odds for this horse at start of race
trainer_id - unique identifier of the horse's trainer at the time of the race
jockey_id - unique identifier of the jockey riding the horse in this race
Acknowledgements
None of this research would have even started without me standing on the shoulders of giants such as William Benter, Ruth Bolton and Randall Chapman and many others who have published the results of their research.
Inspiration
It is probably not going to be enough to just take this dataset and feed it into Google Cloud Machine Learning, Azure MI, etc... but let me know if you find otherwise!
Questions that need to be answered include:
Feature engineering - what features are needed and how best to estimate them from the data given?
Modelling - what kind of model works best? Maybe more than one model?
Other data - is there any other data needed, apart from that given in this data set?"
Overlapping chromosomes,Learn to resolve them,Jeanpat,18,"Version 3,2016-10-12|Version 2,2016-10-12|Version 1,2016-07-21",human genetics,Other,23 MB,CC4,"8,025 views",491 downloads,26 kernels,,https://www.kaggle.com/jeanpat/overlapping-chromosomes,"As a cytogeneticist studying some features of the chromosomes (their telomeres, structural anomalies ...) you need to take pictures from chromosomal preparations fixed on glass slides . Unfortunately, like the sticks in the mikado game, sometime a chromosome or more can fall on an other one yielding overlapping chromosomes in the image, the plague of cytogeneticists. Before computers and images processing with photography, chromosomes were cut from a paper picture and then classified (at least two paper pictures were required), with computers and quantitative analysis, automatic methods were developped to overcome this problem (Lunsteen & Piper, Minaee et al.)
This dataset modelizes overlapping chromosomes. Couples of non overlapping chromosomes were combined by relative translations and rotations to generate two kind of images:
a grey scaled image of the two overlapping chromosomes combining a DAPI stained chromosome and the labelling of the telomeres (showing the chromosomes extremities)
a label image (the ground truth) were the value equal to 3 labels the pixels (displayed in red) belonging to the overlapping domain.
The images were saved in a numpy array as an hdf5 file. The following minimalist python 2 code can load the data (assuming that the unzipped data are in the same folder than the code) :
import h5py
import numpy as np
from matplotlib import pyplot as plt

#h5f = h5py.File('overlapping_chromosomes_examples.h5','r')
h5f = h5py.File('LowRes_13434_overlapping_pairs.h5','r')
pairs = h5f['dataset_1'][:]
h5f.close()

grey = pairs[220,:,:,0]
mask = pairs[220,:,:,1]
#%matplotlib inline
plt.subplot(121)
plt.imshow(grey)
plt.title('max='+str(grey.max()))
plt.subplot(122)
plt.imshow(mask)
I hope this dataset to be suitable to apply supervised learning methods, possibly similar to segnet or its implementation with keras."
Minneapolis Incidents & Crime,"What's been goin' on in Minneapolis, MN (2010 to 2016)",Megan Risdal,18,"Version 2,2016-08-20|Version 1,2016-08-20",crime,CSV,74 MB,CC3,"6,244 views",784 downloads,25 kernels,0 topics,https://www.kaggle.com/mrisdal/minneapolis-incidents-crime,"Thinking of making a move to the lovely Twin Cities? First check out this dataset (curtesy of Open Data Minneapolis) before you pack your bags for the ""Little Apple."" The datasets included contain information about 311 calls and crimes committed between 2010 to 2016 which will help you convince your friends, family, and loved ones that Minneapolis is the place to be (or not). Snow plow noise complaints be darned!"
Brazilian congress,Patterns in the Brazilian congress voting behavior,FelipeLeiteAntunes,18,"Version 2,2016-11-14|Version 1,2016-11-12","crime
politics",CSV,116 MB,ODbL,"2,857 views",201 downloads,11 kernels,2 topics,https://www.kaggle.com/felipeleiteantunes/braziliancongress,"Patterns in the Brazilian congress voting behavior
The Brazilian Government House of Representatives maintains a public database, that contains legislative information since 1970. One type of information that is available are the records of bills. For each bill, the database gives a list of votes choices, state and party of each deputy, and a list of details about the bill itself like type, year, text of proposal, benches orientations and situation (a bill can be voted more than one time, in this work we will treat each votation as a single one). We retrieved more than 100000 bills (propList), where less than 1% was voted (propVotList) until November 2016.
Our objective is detect regularity patterns of legislative behavior, institutional arrangements, and legislative outcome.
Raw data from: http://www2.camara.leg.br/transparencia/dados-abertos/dados-abertos-legislativo/webservices/proposicoes-1/proposicoes"
Iowa Liquor Sales,12 million alcoholic beverage sales in the Midwest,Aleksey Bilogur,18,"Version 1,2017-11-15","food and drink
alcohol",CSV,731 MB,CC0,"4,108 views",685 downloads,3 kernels,2 topics,https://www.kaggle.com/residentmario/iowa-liquor-sales,"Context
The Iowa Department of Commerce requires that every store that sells alcohol in bottled form for off-the-premises consumption must hold a class ""E"" liquor license (an arrangement typical of most of the state alcohol regulatory bodies). All alcoholic sales made by stores registered thusly with the Iowa Department of Commerce are logged in the Commerce department system, which is in turn published as open data by the State of Iowa.
Content
This dataset contains information on the name, kind, price, quantity, and location of sale of sales of individual containers or packages of containers of alcoholic beverages.
This dataset is relatively straightforward, but one source of further information on the contents of the data is this Gist.
Acknowledgements
This data was originally published by the State of Iowa here and has been republished as-is on Kaggle.
Inspiration
This data is probably a representative sample of sale activity for alcohol in the United States, and can be used to answer many questions thereof, like: how much alcohol is sold and consumed in the United States? What kind? What are the most popular brands and labels? What are the most popular mixers? What is the distribution of prices paid in-store? Etcetera."
Urban Dictionary Words And Definitions,Corpus of 2.6 million words with ratings from urban dictionary,Rohk,18,"Version 2,2017-11-16|Version 1,2017-11-02","linguistics
internet",CSV,238 MB,CC0,"2,575 views",202 downloads,,0 topics,https://www.kaggle.com/therohk/urban-dictionary-words-dataset,"Context
This dataset contains 2.6 million words from Urban Dictionary, including their definitions and votes in CSV format.
Source: https://www.urbandictionary.com
To lookup a word in the dataset via urban dictionary api: http://api.urbandictionary.com/v0/define?defid={word_id}
Warning that this dataset contains a lot of profanity and racial slurs.
Content
Rows: 2,606,522
Column 1: word_id - for usage in urban dictionary api
Column 2: word - the text being defined
Column 3: up_votes - thumbs up count as of may 2016
Column 4: down_votes - thumbs down count as of may 2016
Column 5: author - hash of username of submitter
Column 6: definition - text with possible utf8 chars, double semi-colon denotes a newline
Acknowledgements
Remixed from data posted anonymously on reddit.
https://archive.org/details/UrbanDictionary1999-May2016DefinitionsCorpus
Inspiration
Modified and cleaned the original source which is in a very un-friendly format."
Patent Grant Full Text,Contains the full text of each patent grant issued weekly for 10/11/2016,US Patent and Trademark Office,18,"Version 1,2016-10-20",law,Other,569 MB,Other,"5,576 views",305 downloads,15 kernels,0 topics,https://www.kaggle.com/uspto/patent-grant-full-text,"Grant Red Book (GRB) Full Text
Context
Every Tuesday, the US Patent and Trademark Office (USPTO) issues approximately 6,000 patent grants (patents) and posts the full text of the patents online. These patent grant documents contain much of the supporting details for a given patent. From this data, we can track trends in innovation across industries.
Frequency: Weekly (Tuesdays)
Period: 10/11/2016
Content
The fields include patent number, series code and application number, type of patent, filing date, title, issue date, applicant information, inventor information, assignee(s) at time of issue, foreign priority information, related US patent documents, classification information (IPCR, CPC, US), US and foreign references, attorney, agent or firm/legal representative, examiner, citations, Patent Cooperation Treaty (PCT) information, abstract, specification, and claims.
Inspiration
How many times will you find “some assembly required”? What inventions are at the cutting edge of machine learning? To answer these questions and any others you may have about this catalog of knowledge, fork the kernel Library of Inventions and Innovations which demonstrates how to work with XML files in Python.
Acknowledgements
The USPTO owns the dataset. These files are a subset and concatenation of the Patent Grant Data/XML Version 4.5 ICE (Grant Red Book). Because of the concatenation of the individual XML documents, these files will not parse successfully or open/display by default in Internet Explorer. They also will not import into MS Excel. Each XML document within the file should have one start tag and one end tag. Concatenation creates a file that contains 6,000 plus start/end tag combinations. If you take one document out of the Patent Grant Full Text file and place it in a directory with the correct DTD and then double click that individual document, Internet Explorer will parse/open the document successfully. NOTE: You may receive a warning about Active X controls. NOTE: All Patent Grant Full Text files will open successfully in MS Word; NotePad; WordPad; and TextPad.
License
Creative Commons - Public Domain Mark 1.0"
SherLock,A long-term smartphone sensor dataset with a high temporal resolution,The BGU Cyber Security Research Center,18,"Version 1,2016-12-07",computer science,CSV,468 MB,Other,"6,767 views",516 downloads,5 kernels,,https://www.kaggle.com/BGU-CSRC/sherlock,"What is the SherLock dataset?
A long-term smartphone sensor dataset with a high temporal resolution. The dataset also offers explicit labels capturing the to activity of malwares running on the devices. The dataset currently contains 10 billion data records from 30 users collected over a period of 2 years and an additional 20 users for 10 months (totaling 50 active users currently participating in the experiment).
The primary purpose of the dataset is to help security professionals and academic researchers in developing innovative methods of implicitly detecting malicious behavior in smartphones. Specifically, from data obtainable without superuser (root) privileges. However, this dataset can be used for research in domains that are not strictly security related. For example, context aware recommender systems, event prediction, user personalization and awareness, location prediction, and more. The dataset also offers opportunities that aren't available in other datasets. For example, the dataset contains the SSID and signal strength of the connected WiFi access point (AP) which is sampled once every second, over the course of many months.
To gain full free access to the SherLock Dataset, follow these two steps:
1) Read, complete and sign the license agreement. The general restrictions are:
-The license lasts for 3 years, afterwhich the data must be deleted.
-Do not share the data with those who are not bound by the license agreement.
-Do not attempt to de-anonymize the individuals (volunteers) who have contributed the data.
-Any of your publication that benefit from the SherLock project must cite the following article: Mirsky, Yisroel, et al. ""SherLock vs Moriarty: A Smartphone Dataset for Cybersecurity Research."" Proceedings of the 2016 ACM Workshop on Artificial Intelligence and Security. ACM, 2016.
2)Send the scanned document as a PDF to bgu.sherlock@gmail.com and provide a gmail account to share a google drive folder with.
More information can be found here, or in this publication (download link).
A 2 week data sample from a single user is provided on this Kaggle page. To access the full dataset for free, please visit our site. Note: The format of the sample dataset may differ from the full dataset."
"Health searches by US Metropolitan Area, 2005-2017",Data from Google trends showing who searches for what and where,Google News Lab,18,"Version 1,2017-11-04","journalism
healthcare
diseases
+ 2 more...",CSV,84 KB,CC4,"3,011 views",472 downloads,3 kernels,0 topics,https://www.kaggle.com/GoogleNewsLab/health-searches-us-county,"This is the Google Search interest data that powers the Visualisation Searching For Health. Google Trends data allows us to see what people are searching for at a very local level. This visualization tracks the top searches for common health issues in the United States, from Cancer to Diabetes, and compares them with the actual location of occurrences for those same health conditions to understand how search data reflects life for millions of Americans.
How does search interest for top health issues change over time? From 2004–2017, the data shows that search interest gradually increased over the past few years. Certain regions show a more significant increase in search interest than others. The increase in search activity is greatest in the Midwest and Northeast, while the changes are noticeably less dramatic in California, Texas, and Idaho. Are people generally becoming more aware of health conditions and health risks?
The search interest data was collected using the Google Trends API. The visualisation also brings in incidences of each condition so they can be compared. The health conditions were hand-selected from the Community Health Status Indicators (CHSI) which provides key indicators for local communities in the United States. The CHSI dataset includes more than 200 measures for each of the 3,141 United States counties. More information about the CHSI can be found on healthdata.gov.
Many striking similarities exist between searches and actual conditions—but the relationship between the Obesity and Diabetes maps stands out the most. “There are many risk factors for type 2 diabetes such as age, race, pregnancy, stress, certain medications, genetics or family history, high cholesterol and obesity. However, the single best predictor of type 2 diabetes is overweight or obesity. Almost 90% of people living with type 2 diabetes are overweight or have obesity. People who are overweight or have obesity have added pressure on their body's ability to use insulin to properly control blood sugar levels, and are therefore more likely to develop diabetes.” —Obesity Society via obesity.org"
Four Shapes,"16,000 images of four basic shapes (star, circle, square, triangle)",smeschke,17,"Version 2,2017-11-21|Version 1,2017-11-19","beginner
image data
multiclass classification",Other,22 MB,CC0,"1,972 views",218 downloads,2 kernels,,https://www.kaggle.com/smeschke/four-shapes,"This dataset contains 16,000 images of four shapes; square, star, circle, and triangle. Each image is 200x200 pixels.
The data was collected using a Garmin Virb 1080p action camera. The shapes were cut from poster board, and then painted green. I held each shape in view of the camera for two minutes. While the camera was recording the shape, I moved the shape around and rotated it.
The four videos were then processed using OpenCV in Python. Using colorspaces, the green shape is cropped out of the image and resized to 200x200 pixels. The data is arranged into four folders; square, circle, triangle, and star. The images are labeled 0.png, 1.png, etc...
A fifth video was taken with all of the shapes in the frame. This fifth video is for testing purposes. The goal is to classify the shapes in the test video using a model created with the training data. These classifications were made using a model made in Keras.
How is this different than the MINST handwritten digits dataset? There are 10 classes in the MINST dataset and 4 in this shapes dataset. The images in this data set are rotated, and the digits in the MINST data set are not."
Annual Nominal Fish Catches,Explore the impact of overfishing in the Northeast Atlantic region.,Victor Genin,17,"Version 2,2016-05-29|Version 1,2016-05-15","fishing
environmental engineering",CSV,4 MB,CC0,"7,135 views",980 downloads,17 kernels,0 topics,https://www.kaggle.com/victorgenin/ices-fish-catch,"The datasets provides data of annual nominal catches of more than 200 species of fish and shellfish in the Northeast Atlantic region, which are officially submitted by 20 International Council for the Exploration of the Sea (ICES) member countries between 2006 and 2014."
Italy's Earthquakes,Data about the earthquakes that hit Italy between August and November 2016.,Gabriele Angeletti,17,"Version 1,2016-12-06",earth sciences,CSV,386 KB,CC0,"5,876 views",615 downloads,31 kernels,,https://www.kaggle.com/blackecho/italy-earthquakes,"Context
This dataset contains data about the earthquakes that hit the center of Italy between August and November 2016. For some simple visualizations of this dataset you can checkout this post.
Content
The dataset contains events from 2016-08-24 to 2016-11-30. It's a single .csv file with the following header:
Time,Latitude,Longitude,Depth/Km,Magnitude
The dataset contains 8087 rows (8086 of data + 1 of header)
Acknowledgements
The dataset was collected from this real-time updated list from the Italian Earthquakes National Center.
Inspiration
I hope that someone in the kaggle community will find this dataset interesting to analyze and/or visualize."
Home Advantage in Soccer and Basketball,Home and away performance of 9K+ Teams from 88 leagues around the world,DrGuillermo,17,"Version 1,2017-01-21","association football
basketball",CSV,808 KB,Other,"4,616 views",723 downloads,8 kernels,,https://www.kaggle.com/drgilermo/home-advantage-in-soccer-and-basketball,"Context
The data set includes information about different leagues in different sports (Basketball and Soccer) all around the world, as well as some basic facts about each country, regarding the home advantage phenomenon in sports.
Content
The data is comprised of 3 data sets:
The home and away performance of 8365 soccer teams from a few dozens of countries during the years 2010-2016.
The home and away performance of 1216 NBA teams during the years 1968-2010
General facts about 88 countries including soccer data such as their FIFA rank, the average attendance of soccer matches and the Home Advatnage Factor of the leagure
Acknowledgement
The soccer data was scraped from here: http://footballdatabase.com/competitions-index
The NBA data was scraped from NBA.com.
The world facts were copied from Wikipedia."
Birds' Bones and Living Habits,Measurements of bones and ecological groups of birds,zjf,17,"Version 1,2017-01-18","biology
ecology",CSV,25 KB,Other,"4,558 views",528 downloads,60 kernels,0 topics,https://www.kaggle.com/zhangjuefei/birds-bones-and-living-habits,"Context
There are many kinds of birds: pigeons, ducks, ostriches, penguins... Some are good at flying, others can't fly but run fast. Some swim under water, others wading in shallow pool.
According to their living environments and living habits, birds are classified into different ecological groups. There are 8 ecological groups of birds:
Swimming Birds
Wading Birds
Terrestrial Birds
Raptors
Scansorial Birds
Singing Birds
Cursorial Birds (not included in dataset)
Marine Birds (not included in dataset)
First 6 groups are main and are covered by this dataset.
Apparently, birds belong to different ecological groups have different appearances: flying birds have strong wings and wading birds have long legs. Their living habits are somewhat reflected in their bones' shapes. As data scientists we may think of examining the underlying relationship between sizes of bones and ecological groups , and recognising birds' ecological groups by their bones' shapes.
Content
There are 420 birds contained in this dataset. Each bird is represented by 10 measurements (features):
Length and Diameter of Humerus
Length and Diameter of Ulna
Length and Diameter of Femur
Length and Diameter of Tibiotarsus
Length and Diameter of Tarsometatarsus
All measurements are continuous float numbers (mm) with missing values represented by empty strings. The skeletons of this dataset are collections of Natural History Museum of Los Angeles County. They belong to 21 orders, 153 genera, 245 species.
Each bird has a label for its ecological group:
SW: Swimming Birds
W: Wading Birds
T: Terrestrial Birds
R: Raptors
P: Scansorial Birds
SO: Singing Birds
Acknowledgements
This dataset is provided by Dr. D. Liu of Beijing Museum of Natural History.
Inspiration
This dataset is a 420x10 size continuous values unbalanced multi-class dataset. What can be done include:
Data Visualisation
Statical Analysis
Supervised Classification
Unsupervised Clustering
License
Please do not publish or cite this dataset in research papers or other public publications."
Rolling Stone's 500 Greatest Albums of All Time,Data about Rolling Stone magazine's (2012) top 500 albums of all time list,Gibs,17,"Version 1,2017-01-06","critical theory
music",CSV,37 KB,CC0,"6,069 views",612 downloads,23 kernels,0 topics,https://www.kaggle.com/notgibs/500-greatest-albums-of-all-time-rolling-stone,"From Wikipedia:
""The 500 Greatest Albums of All Time"" is a 2003 special issue of American magazine Rolling Stone, and a related book published in 2005. The lists presented were compiled based on votes from selected rock musicians, critics, and industry figures, and predominantly feature American and British music from the 1960s and the 1970s.
In 2012, Rolling Stone published a revised edition of the list drawing on the original and a later survey of albums in the 2000s. It was made available in ""bookazine"" format on newsstands in the US from April 27 to July 25. The new list contained 38 albums not present in the previous one, 16 of them released after 2003.
I took the albums from MusicBrainz but the genres weren't listed. I wrote a Python script to get the genres and subgenres of each album from the Discogs API.
The data collected are:
Position on the list
Year of release
Album name
Artist name
Genre name
Subgenre name
Some of the genres/subgenres may not be entirely correct - Discogs seems to not consider some of the smaller genres. Let me know if there are any glaring issues and I'll try to fix them."
WTA Matches and Rankings,All Women's Tennis Association data you ever wanted,Joao Pedro Evangelista,17,"Version 4,2017-11-15|Version 3,2017-11-15|Version 2,2017-11-14|Version 1,2017-11-14","tennis
sports",Other,20 MB,CC4,"2,370 views",362 downloads,9 kernels,2 topics,https://www.kaggle.com/joaoevangelista/wta-matches-and-rankings,"Context
The WTA (Women's Tennis Association) is the principal organizing body of women's professional tennis, it governs its own tour worldwide. On its website, it provides a lot of data about the players as individuals as well the tour matches with results and the current rank during it.
Luckily for us, Jeff Sackmann scraped the website and collected everything from there and put in a nice way into easily consumable datasets.
On Jeff's GitHub account you can find a lot more data about tennis!
Content
The dataset present here is directly downloaded from the source, no alteration on the data was made, the files were only placed in subdirectories so one can easily locate them.
It covers statistics of players registered on the WTA, the matches that happened on each tour by year, with results, as well some qualifying matches for the tours.
As a reminder, you may not find all data of the matches prior to 2006, so be warned when working with those sets.
Acknowledgements
Thanks to Jeff Sackmann for maintaining such collection and making it public!
Also, a thank you for WTA for collecting those stats and making them accessible to anyone on their site.
Inspiration
Here are some things to start:
Which player did the most rapidly climb the ranks through the years?
Does the rank correlates with the money earn by the player?
What can we find about the age?
There is some deterministic factor to own the match?"
2013 and 2015 Israeli Elections,Vote statistics per party and booth for two Israeli election cycles,Itamar Mushkin,17,"Version 6,2017-12-17|Version 5,2017-12-17|Version 4,2017-10-17|Version 3,2017-10-16|Version 2,2017-10-11|Version 1,2017-10-05",politics,CSV,1 MB,CC0,"1,930 views",237 downloads,6 kernels,4 topics,https://www.kaggle.com/itamarmushkin/israeli-elections-2015-2013,"Context
This dataset contains results from the 2015 and 2013 elections in Israel. Results are given by voting booths (of comparable sizes of 0-800) and not by settlements (which are very varied - think Tel Aviv compared to a small kibbutz).
Content
The first seven columns are information about each settlement and voting booth, and from the eighth to the end is the number of votes each party has received in each booth.
Acknowledgements
This data is freely available at http://votes20.gov.il/ and http://www.votes-19.gov.il/nationalresults, I just translated the column headers into English. Settlement names are translated according to this key from the Central Bureau of Statistics, which uses the same settlement_code as the election results.
Inspiration
Personally, I've viewed this dataset in order to map out the relationships between different parties (i.e which are 'closer', which are more 'central'). This question is significant in Israel, where the composition of the parliament is determined almost directly by the popular vote (e.g a party with 25% of the total proper votes will recieve 25% of seats in parliament), but the government is formed by a coalition of parties (so the head of the largest party in parliament will not necessarily be the Prime Minister)."
Stanford Mass Shootings in America (MSA),"A high quality dataset from 1966-2016 with method, definitions and references",Carlos Paradis,17,"Version 1,2017-10-08","united states
crime
violence
terrorism",CSV,2 MB,Other,"2,969 views",480 downloads,2 kernels,0 topics,https://www.kaggle.com/carlosparadis/stanford-msa,"https://www.youtube.com/watch?v=A8syQeFtBKc
Context
The Stanford Mass Shootings in America (MSA) is a dataset released under Creative Commons Attribution 4.0 international license by the Stanford Geospatial Center. While not an exhaustive collection of mass shootings, it is a high-quality dataset ranging from 1966 to 2016 with well-defined methodology, definitions and source URLs for user validation.
This dataset can be used to validate other datasets, such as us-mass-shootings-last-50-years, which contains more recent data, or conduct other analysis, as more information is provided.
Content
This dataset contains data by the MSA project both from it's website and from it's Github account. The difference between the two sources is only on the data format (i.e. .csv versus .geojson for the data, or .csv versus .pdf for the dictionary).
mass_shooting_events_stanford_msa_release_06142016
Contains a nonexaustive list of US Mass Shootings from 1966 to 2016 in both .csv and .geojson formats.
dictionary_stanford_msa_release_06142016
Contains the data dictionary in .csv and .pdf formats. Note the .pdf format provides an easier way to visualize sub-fields.
Note the data was reproduced here without any modifications other than file renaming for clarity, the content is the same as in the source.
The following sections are reproduced from the dataset creators website. For more details, please see the source.
Project background
The Stanford Mass Shootings of America (MSA) data project began in 2012, in reaction to the mass shooting in Sandy Hook, CT. In our initial attempts to map this phenomena it was determined that no comprehensive collection of these incidents existed online. The Stanford Geospatial Center set out to create, as best we could, a single point repository for as many mass shooting events as could be collected via online media. The result was the Stanford MSA.
What the Stanford MSA is
The Stanford MSA is a data aggregation effort. It is a curated set of spatial and temporal data about mass shootings in America, taken from online media sources. It is an attempt to facilitate research on gun violence in the US by making raw data more accessible.
What the Stanford MSA is not
The Stanford MSA is not a comprehensive, longitudinal research project. The data collected in the MSA are not investigated past the assessment for inclusion in the database. The MSA is not an attempt to answer specific questions about gun violence or gun laws.
The Stanford Geospatial Center does not provide analysis or commentary on the contents of this database or any derivatives produced with it.
Data collection methodology
The information collected for the Stanford MSA is limited to online resources. An initial intensive investigation was completed looking back over existing online reports to fill in the historic record going back to 1966. Contemporary records come in as new events occur and are cross referenced against a number of online reporting sources. In general a minimum of three corroborating sources are required to add the full record into the MSA (as many as 6 or 7 sources may have been consulted in many cases). All sources for each event are listed in the database.
Due to the time involved in vetting the details of any new incident, there is often a 2 to 4 week lag between a mass shooting event and its inclusion in the public release database.
It is important to note the records in the Stanford MSA span a time from well before the advent of online media reporting, through its infancy, to the modern era of web based news and information resources. Researchers using this database need to be aware of the reporting bias these changes in technology present. A spike in incidents for recent years is likely due to increased online reporting and not necessarily indicative of the rate of mass shootings alone. Researchers should look at this database as a curated collection of quality checked data regarding mass shootings, and not an exhaustive research data set itself. Independent verification and analysis will be required to use this data in examining trends in mass shootings over time.
Definition of Mass Shooting
The definition of mass shooting used for the Stanford database is 3 or more shooting victims (not necessarily fatalities), not including the shooter. The shooting must not be identifiably gang, drug, or organized crime related.
Acknowledgements
The Stanford Mass Shootings in America (MSA) is a dataset released under Creative Commons Attribution 4.0 international license by the Stanford Geospatial Center.
How to cite the MSA
The Stanford MSA is released under a Creative Commons Attribution 4.0 international license. Please cite the MSA as “Stanford Mass Shootings in America, courtesy of the Stanford Geospatial Center and Stanford Libraries”.
Inspiration
There is already a great number of interesting datasets in Kaggle surrounding the subject of Mass Shootings, however, little has been done leveraging information from multiple sources. Can you see a story among them? Can we learn anything, for example, comparing the different sources by city or state?
From a bigger picture
Leading Causes of Death in US: https://www.kaggle.com/cdc/mortality
Gun Violence Database: https://www.kaggle.com/gunviolencearchive/gun-violence-database
Gun Deaths in US: https://www.kaggle.com/hakabuk/gun-deaths-in-the-us
Homicide Reports: https://www.kaggle.com/murderaccountability/homicide-reports
Global Terrorism Database: https://www.kaggle.com/START-UMD/gtd
Crime Rates in America: https://www.kaggle.com/marshallproject/crime-rates
""Prevention?
Firearms Provisions in US States: https://www.kaggle.com/jboysen/state-firearms
Trial and Terror: https://www.kaggle.com/jboysen/trial-and-terror
Connecticut Inmates Waiting trial: https://www.kaggle.com/Connecticut-open-data/connecticut-inmates-awaiting-trial
Are there warning signs?
Mental Health in Tech Survey: https://www.kaggle.com/osmi/mental-health-in-tech-2016/data (Not directly related but can be used to make a parenthesis about mental health being an issue in our surroundings)."
Median Listing Price (1 Bedroom),"Aggregated by neighborhood, price per square foot",Zillow,17,"Version 2,2016-11-07|Version 1,2016-11-07","cities
home",CSV,51 KB,Other,"5,341 views",599 downloads,24 kernels,0 topics,https://www.kaggle.com/zillow/median-listing-price-1-bedroom,"Context
This dataset includes the median list price divided by the square footage of a 1-bedroom home for a select number of neighborhoods around the United States.
Content
When available, data includes median price per square foot on a monthly basis between January 2010 and September 2016.
Selected neighborhoods include:
Upper East Side, New York, NY
Spring Valley, Las Vegas, NV
Hollywood, Los Angeles, CA
Williamsburg, New York, NY
Harlem, New York, NY
Enterprise, Las Vegas,NV
Downtown, San Jose, CA
Sheepshead Bay, New York, NY
Forest Hills, New York, NY
Jackson Heights, New York, NY
Gramercy, New York, NY
Flagami, Miami, FL
Downtown, Memphis, TN
Chelsea, New York, NY
Oak Lawn, Dallas, TX
Greater Uptown, Houston, TX
South Loop, Chicago, IL
Makiki-Lower Punchbowl-Tantalus, Honolulu, HI
Downtown, Los Angeles, CA
Capitol Hill, Seattle, WA
Clinton, New York, NY
Alexandria West, Alexandria, VA
Financial District, New York, NY
Flatiron District, New York, NY
Landmark-Van Dom, Alexandria, VA
Flamingo Lummus, Miami Beach, FL
Winchester, Las Vegas, NV
Brickell, Miami, FL
Waikiki, Honolulu, HI
Back Bay, Boston, MA
Sutton Place, New York, NY
and several others
Inspiration
What neighborhoods have the most expensive real estate per square foot? Least expensive?
Which neighborhoods and/or cities have the fastest growth rates in price?
Are there any neighborhoods that remain relatively steady in price?
Given that this metric is listing price per square foot, is there a similar dataset that could help you compare median square footage in a 1-bedroom home across neighborhoods?
Acknowledgement
This dataset is part of Zillow Data, and the original source can be found here, under the Neighborhoods link."
Russian Troll Tweets,"200,000 malicious-account tweets captured by NBC",vikas,17,"Version 2,2018-02-15|Version 1,2018-02-15","russia
politics
international relations
+ 2 more...",CSV,21 MB,CC0,"1,584 views",180 downloads,2 kernels,2 topics,https://www.kaggle.com/vikasg/russian-troll-tweets,"Context
As part of the House Intelligence Committee investigation into how Russia may have influenced the 2016 US Election, Twitter released the screen names of almost 3000 Twitter accounts believed to be connected to Russia’s Internet Research Agency, a company known for operating social media troll accounts. Twitter immediately suspended these accounts, deleting their data from Twitter.com and the Twitter API. A team at NBC News including Ben Popken and EJ Fox was able to reconstruct a dataset consisting of a subset of the deleted data for their investigation and were able to show how these troll accounts went on attack during key election moments. This dataset is the body of this open-sourced reconstruction.
For more background, read the NBC news article publicizing the release: ""Twitter deleted 200,000 Russian troll tweets. Read them here.""
Content
This dataset contains two CSV files. tweets.csv includes details on individual tweets, while users.csv includes details on individual accounts.
To recreate a link to an individual tweet found in the dataset, replace user_key in https://twitter.com/user_key/status/tweet_id with the screen-name from the user_key field and tweet_id with the number in the tweet_id field.
Following the links will lead to a suspended page on Twitter. But some copies of the tweets as they originally appeared, including images, can be found by entering the links on web caches like archive.org and archive.is.
Acknowledgements
If you publish using the data, please credit NBC News and include a link to this page. Send questions to ben.popken@nbcuni.com.
Inspiration
What are the characteristics of the fake tweets? Are they distinguishable from real ones?"
Face Images with Marked Landmark Points,,DrGuillermo,17,"Version 1,2017-09-20","celebrity
photography
humans",CSV,497 MB,Other,"1,830 views",238 downloads,4 kernels,,https://www.kaggle.com/drgilermo/face-images-with-marked-landmark-points,"Context
This is Kaggle's Facial Keypoint Detection dataset that is uploaded in order to allow kernels to work on it, as was also requested by a fellow kaggler in this discussion thread.
Content
The dataset contains 7049 facial images and up to 15 keypoints marked on them.
The keypoints are in the facial_keypoints.csv file.
The image are in the face_images.npz file.
Look at the exploration script for code that reads and presents the dataset.
Acknowledgements
This is a kaggle dataset, so all acknowledgements are to kaggle.
From the original dataset acknowledgements : ""The data set for this competition was graciously provided by Dr. Yoshua Bengio of the University of Montreal""
Inspiration
I hope the dataset can serve as a decent deep learning repository of code."
Indian Prison Statistics (2001 - 2013),Details of Inmates are classified according to 35+ factors. (35+ csv files),Rajanand Ilangovan / இராஜ்ஆனந்த் இளங்கோவன்,17,"Version 5,2017-09-05|Version 4,2017-09-01|Version 3,2017-08-04|Version 2,2017-08-03|Version 1,2017-08-03","india
crime",CSV,9 MB,CC4,"3,579 views",558 downloads,3 kernels,0 topics,https://www.kaggle.com/rajanand/prison-in-india,"Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks.
Context
This dataset contains the complete detail about the Prison and various characteristics of inmates. This will help to understand better about prison system in India.
Content
Details of Jail wise population of prison inmates
Details about the list of jails in India at the end of year 2015.
Jail category wise population of inmates.
Capacity of jails by inmate population.
Age group, nationality and gender wise population of inmates.
Religion and gender wise population of inmates.
Caste and gender wise population of inmates.
Education standards of inmates.
Domicile of inmates.
Incidence of recidivism.
Rehabilitation of prisoners.
Distribution of sentence periods of convicts in various jails by sex and age-groups.
Details of under trial prisoners by the type of IPC (Indian Penal Code) offences.
Details of convicts by the type of IPC (Indian Penal Code) offences.
Details of SLL (special & local law) Crime headwise distribution of inmates who convicted
Details of SLL (special & local law) Crime head wise distribution of inmates under trial
Details of educational facilities provided to prisoners.
Details of Jail breaks, group clashes and firing in jail (Tranquility).
Details of wages per day to convicts.
Details of Prison inmates trained under different vocational training.
Details of capital punishment (death sentence) and life imprisonment.
Details of prison inmates escaped.
Details of prison inmates released.
Details of Strength of officials
Details of Total Budget and Actual Expenditure during the year 2015-16.
Details of Budget
Details of Expenditure
Details of Expenditure on inmates
Details of Inmates suffering from mental ilness
Details of Period of detention of undertrials
Details of Number of women prisoners with children
Details of Details of inmates parole during the year
Details of Value of goods produced by inmates
Details of Number of vehicles available
Details of Training of Jail Officers
Details of Movements outside jail premises
Details of Details of electronic equipment used in prison
Inspiration
There are many questions about Indian prison with this dataset. Some of the interesting questions are
Percentage of jails over crowded. Is there any change in percentage over time?
How many percentage of inmates re-arrested?
Which state/u.t pay more wages to the inmates?
Which state/u.t has more capital punishment/life imprisonment inmates?
Inmates gender ratio per state
Acknowledgements
National Crime Records Bureau (NCRB), Govt of India has shared this dataset under Govt. Open Data License - India. NCRB has also shared prison data on their website."
NSE Stocks Data,The data is of National Stock Exchange of India for 2016 and 2017,Minat Verma,17,"Version 1,2018-01-02",strategy,CSV,30 MB,CC0,"3,169 views",559 downloads,,0 topics,https://www.kaggle.com/minatverma/nse-stocks-data,"Context
The data is of National Stock Exchange of India. The data is compiled to felicitate Machine Learning, without bothering much about Stock APIs.
Content
The data is of National Stock Exchange of India's stock listings for each trading day of 2016 and 2017. A brief description of columns. SYMBOL: Symbol of the listed company. SERIES: Series of the equity. Values are [EQ, BE, BL, BT, GC and IL] OPEN: The opening market price of the equity symbol on the date. HIGH: The highest market price of the equity symbol on the date. LOW: The lowest recorded market price of the equity symbol on the date. CLOSE: The closing recorded price of the equity symbol on the date. LAST: The last traded price of the equity symbol on the date. PREVCLOSE: The previous day closing price of the equity symbol on the date. TOTTRDQTY: Total traded quantity of the equity symbol on the date. TOTTRDVAL: Total traded volume of the equity symbol on the date. TIMESTAMP: Date of record. TOTALTRADES: Total trades executed on the day. ISIN: International Securities Identification Number.
Acknowledgements
All data is fetched from NSE official site. https://www.nseindia.com/
Inspiration
This dataset is compiled to felicitate Machine learning on Stocks."
"Data Stories of US Airlines, 1987-2008",Fight arrival and departure details for all commercial flights,Prajit Datta,17,"Version 1,2017-03-12",aviation,CSV,5 MB,CC0,"5,157 views",405 downloads,2 kernels,,https://www.kaggle.com/prajitdatta/data-stories-of-us-airlines,"About the Data
The data used in this project is real and is based on the collection of over 20 years. The total number of record in this dataset is roughly around 120 million rows and the size of the data is approximately 12GB. The data consists of flight arrival and departure details for all commercial flights within the USA, from October 1987 to April 2008. This is a large dataset. There are around 29 attributes.
How to get the data? The data originally comes from http://stat-computing.org/dataexpo/2009/the-data.html
You can download the data for each year by clicking the appropriate link in the above website (Remember the size is going to be more than 12GB).
(i) Problem Statement (a) Check the skewness of Distance travelled by airlines. (b) Calculate the mean, median and quantiles of the distance travelled by US Airlines (US). (c) Check the standard deviation of distance travelled by American Airlines (AA). (d) Draw a boxplot of UniqueCarrier with Distance. (e) Draw the direction of relationship between ArrDelay and DepDelay by drawing a scatterplot.
(ii) Problem Statement
(a) What is the probability that a flight which is landing/taking off is “WN” Airlines (marginal probability) (b) What is the probability that a flight which is landing/taking off is either “WN” or “AA” Airlines (disjoint events) (c) What is the joint probability that a flight is both “WN” and travels less than 600 miles (joint probability) (d) What is the conditional probability that the flight travels less than 2500 miles given that the flight is “AA” Airlines (conditional probability) (e) What is the joint probability of a flight getting cancelled and is supposed to travel less than 2500 miles given that the flight is “AA” Airlines (joint + conditional probability)
(iii) Problem Statement
(a) Suppose arrival delays of flights belonging to “AA” are normally distributed with mean 15 minutes and standard deviation 3 minutes. If the “AA” plans to announce a scheme where it will give 50% cash back if their flights are delayed by 20 minutes, how much percentage of the trips “AA” is supposed to loose this money. (Hint: pnorm) (b) Assume that 65% of flights are diverted due to bad weather through the Weather System. What is the probability that in a random sample of 10 flights, 6 are diverted through the Weather System. (Hint: dbinorm) (c) Do linear regression between the Arrival Delay and Departure Delay of the flights. (d) Find out the confidence interval of the fitted linear regression line. (e) Perform a multiple linear regression between the Arrival Delay along with the Departure Delay and Distance travelled by flights."
Top 980 Starred Open Source Projects on GitHub,Which types of projects are the most popular on GitHub?,Chase Willden,17,"Version 1,2017-06-25","internet
programming languages
programming",CSV,178 KB,CC0,"4,706 views",222 downloads,4 kernels,,https://www.kaggle.com/chasewillden/topstarredopensourceprojects,"Context
GitHub is the leader in hosting open source projects. For those who are not familiar with open source projects, a group of developers share and contribute to common code to develop software. Example open source projects include, Chromium (which makes Google Chrome), WordPress, and Hadoop. Open source projects are said to have disrupted the software industry (2008 Kansas Keynote).
Content
For this study, I crawled the leader in hosting open source projects, GitHub.com and extracted a list of the top starred open source projects. On GitHub, a user may choose the star a repository representing that they “like” the project. For each project, I gathered the repository username or Organization the project resided in, the repository name, a description, the last updated date, the language of the project, the number of stars, any tags, and finally the url of the project.
Acknowledgements
This data wouldn't be available if it weren't for GitHub. An example micro-study can be found at The Concept Center"
Hurricane Harvey Tweets,Recent tweets on Hurricane Harvey,Dan,17,"Version 6,2017-09-10|Version 5,2017-09-05|Version 4,2017-08-30|Version 3,2017-08-30|Version 2,2017-08-27|Version 1,2017-08-26","weather
internet",CSV,71 MB,CC0,"4,604 views",388 downloads,3 kernels,3 topics,https://www.kaggle.com/dan195/hurricaneharvey,"Context
Tweets containing Hurricane Harvey from the morning of 8/25/2017. I hope to keep this updated if computer problems do not persist.
*8/30 Update This update includes the most recent tweets tagged ""Tropical Storm Harvey"", which spans from 8/20 to 8/30 as well as the properly merged version of dataset including Tweets from when Harvey before it was downgraded back to a tropical storm.
Inspiration
What are the popular tweets?
Can we find popular news stories from this?
Can we identify people likely staying or leaving, and is there a difference in sentiment between the two groups?
Is it possible to predict popularity with respect to retweets, likes, and shares?"
Predict NHL Player Salaries,Build a model to predict the salaries of NHL players based on player data,Cam Nugent,17,"Version 2,2017-08-19|Version 1,2017-08-16","sports
money",CSV,438 KB,CC0,"4,137 views",542 downloads,4 kernels,0 topics,https://www.kaggle.com/camnugent/predict-nhl-player-salaries,"Context & Content
This dataset features the salaries of 874 nhl players for the 2016/2017 season. I have randomly split the players into a training (612 players) and test (262 players) populations. There are 151 predictor columns (described in column legend section, if you're not familiar with hockey the meaning of some of these may be a bit cryptic!) as well as a leading column with the players 2016/2017 annual salary. For the test population the actual salaries have been broken off into a separate .csv file.
Acknowledgements
Raw excel sheet was acquired http://www.hockeyabstract.com/
Inspiration
Can you build a model to predict NHL player's salaries? What are the best predictors of how much a player will make?
Column Legend
Acronym - Meaning
%FOT - Percentage of all on-ice faceoffs taken by this player.
+/- - Plus/minus
1G - First goals of a game
A/60 - Events Against per 60 minutes, defaults to Corsi, but can be set to another stat
A1 - First assists, primary assists
A2 - Second assists, secondary assists
BLK% - Percentage of all opposing shot attempts blocked by this player
Born - Birth date
C.Close - A player shot attempt (Corsi) differential when the game was close
C.Down - A player shot attempt (Corsi) differential when the team was trailing
C.Tied - A player shot attempt (Corsi) differential when the team was tied
C.Up - A player shot attempt (Corsi) differential when the team was in the lead
CA - Shot attempts allowed (Corsi, SAT) while this player was on the ice
Cap Hit - The player's cap hit
CBar - Crossbars hit
CF - The team's shot attempts (Corsi, SAT) while this player was on the ice
CF.QoC - A weighted average of the Corsi percentage of a player's opponents
CF.QoT - A weighted average of the Corsi percentage of a player's linemates
CHIP - Cap Hit of Injured Player is games lost to injury multiplied by cap hit per game
City - City of birth
Cntry - Country of birth
DAP - Disciplined aggression proxy, which is hits and takeaways divided by minor penalties
DFA - Dangerous Fenwick against, which is on-ice unblocked shot attempts weighted by shot quality
DFF - Dangerous Fenwick for, which is on-ice unblocked shot attempts weighted by shot quality
DFF.QoC - Quality of Competition metric based on Dangerous Fenwick, which is unblocked shot attempts weighted for shot quality
DftRd - Round in which the player was drafted
DftYr - Year drafted
Diff - Events for minus event against, defaults to Corsi, but can be set to another stat
Diff/60 - Events for minus event against, per 60 minutes, defaults to Corsi, but can be set to another stat
DPS - Defensive point shares, a catch-all stats that measures a player's defensive contributions in points in the standings
DSA - Dangerous shots allowed while this player was on the ice, which is rebounds plus rush shots
DSF - The team's dangerous shots while this player was on the ice, which is rebounds plus rush shots
DZF - Shifts this player has ended with an defensive zone faceoff
dzFOL - Faceoffs lost in the defensive zone
dzFOW - Faceoffs win in the defensive zone
dzGAPF - Team goals allowed after faceoffs taken in the defensive zone
dzGFPF - Team goals scored after faceoffs taken in the defensive zone
DZS - Shifts this player has started with an defensive zone faceoff
dzSAPF - Team shot attempts allowed after faceoffs taken in the defensive zone
dzSFPF - Team shot attempts taken after faceoffs taken in the defensive zone
E+/- - A player's expected +/-, based on his team and minutes played
ENG - Empty-net goals
Exp dzNGPF - Expected goal differential after faceoffs taken in the defensive zone, based on the number of them
Exp dzNSPF - Expected shot differential after faceoffs taken in the defensive zone, based on the number of them
Exp ozNGPF - Expected goal differential after faceoffs taken in the offensive zone, based on the number of them
Exp ozNSPF - Expected shot differential after faceoffs taken in the offensive zone, based on the number of them
F.Close - A player unblocked shot attempt (Fenwick) differential when the game was close
F.Down - A player unblocked shot attempt (Fenwick) differential when the team was trailing
F.Tied - A player unblocked shot attempt (Fenwick) differential when the team was tied
F.Up - A player unblocked shot attempt (Fenwick) differential when the team was in the lead. Not the best acronym.
F/60 - Events For per 60 minutes, defaults to Corsi, but can be set to another stat
FA - Unblocked shot attempts allowed (Fenwick, USAT) while this player was on the ice
FF - The team's unblocked shot attempts (Fenwick, USAT) while this player was on the ice
First Name -
FO% - Faceoff winning percentage
FO%vsL - Faceoff winning percentage against lefthanded opponents
FO%vsR - Faceoff winning percentage against righthanded opponents
FOL - The team's faceoff losses while this player was on the ice
FOL.Close - Faceoffs lost when the score was close
FOL.Down - Faceoffs lost when the team was trailing
FOL.Up - Faceoffs lost when the team was in the lead
FovsL - Faceoffs taken against lefthanded opponents
FovsR - Faceoffs taken against righthanded opponents
FOW - The team's faceoff wins while this player was on the ice
FOW.Close - Faceoffs won when the score was close
FOW.Down - Faceoffs won when the team was trailing
FOW.Up - Faceoffs won when the team was in the lead
G - Goals
G.Bkhd - Goals scored on the backhand
G.Dflct - Goals scored with deflections
G.Slap - Goals scored with slap shots
G.Snap - Goals scored with snap shots
G.Tip - Goals scored with tip shots
G.Wrap - Goals scored with a wraparound
G.Wrst - Goals scored with a wrist shot
GA - Goals allowed while this player was on the ice
Game - Game Misconduct penalties
GF - The team's goals while this player was on the ice
GP - Games Played
Grit - Defined as hits, blocked shots, penalty minutes, and majors
GS - The player's combined game score
GS/G - The player's average game score
GVA - The team's giveaways while this player was on the ice
GWG - Game-winning goals
GWG - Game-winning goals
HA - The team's hits taken while this player was on the ice
Hand - Handedness
HF - The team's hits thrown while this player was on the ice
HopFO - Opening faceoffs taken at home
HopFOW - Opening faceoffs won at home
Ht - Height
iBLK - Shots blocked by this individual
iCF - Shot attempts (Corsi, SAT) taken by this individual
iDS - Dangerous shots taken by this player, the sum of rebounds and shots off the rush
iFF - Unblocked shot attempts (Fenwick, USAT) taken by this individual
iFOL - Faceoff losses by this individual
iFOW - Faceoff wins by this individual
iGVA - Giveaways by this individual
iHA - Hits taken by this individual
iHDf - The difference in hits thrown by this individual minus those taken
iHF - Hits thrown by this individual
iMiss - Individual shots taken that missed the net.
Injuries - List of types of injuries incurred, if any
iPEND - Penalties drawn by this individual
iPenDf - The difference in penalties drawn minus those taken
iPENT - Penalties taken by this individual
IPP% - Individual points percentage, which is on-ice goals for which this player had the goal or an assist
iRB - Rebound shots taken by this individual
iRS - Shots off the rush taken by this individual
iSCF - All scoring chances taken by this individual
iSF - Shots on goal taken by this individual
iTKA - Takeaways by this individual
ixG - Expected goals (weighted shots) for this individual, which is shot attempts weighted by shot location
Last Name -
Maj - Major penalties taken
Match - Match penalties
MGL - Games lost due to injury
Min - Minor penalties taken
Misc - Misconduct penalties
Nat - Nationality
NGPF - Net Goals Post Faceoff. A differential of all goals within 10 seconds of a faceoff, relative to expectations set by the zone in which they took place
NHLid - NHL player id useful when looking at the raw data in game files
NMC - What kind of no-movement clause this player's contract has, if any
NPD - Net Penalty Differential is the player's penalty differential relative to a player of the same position with the same ice time per manpower situation
NSPF - Net Shots Post Faceoff. A differential of all shot attempts within 10 seconds of a faceoff, relative to expectations set by the zone in which they took place
NZF - Shifts this player has ended with a neutral zone faceoff
nzFOL - Faceoffs lost in the neutral zone
nzFOW - Faceoffs won in the neutral zone
nzGAPF - Team goals allowed after faceoffs taken in the neutral zone
nzGFPF - Team goals scored after faceoffs taken in the neutral zone
NZS - Shifts this player has started with a neutral zone faceoff
nzSAPF - Team shot attempts allowed after faceoffs taken in the neutral zone
nzSFPF - Team shot attempts taken after faceoffs taken in the neutral zone
OCA - Shot attempts allowed (Corsi, SAT) while this player was not on the ice
OCF - The team's shot attempts (Corsi, SAT) while this player was not on the ice
ODZS - Defensive zone faceoffs that occurred without this player on the ice
OFA - Unblocked shot attempts allowed (Fenwick, USAT) while this player was not on the ice
OFF - The team's unblocked shot attempts (Fenwick, USAT) while this player was not on the ice
OGA - Goals allowed while this player was not on the ice
OGF - The team's goals while this player was not on the ice
ONZS - Neutral zone faceoffs that occurred without this player on the ice
OOZS - Offensive zone faceoffs that occurred without this player on the ice
OpFO - Opening faceoffs taken
OpFOW - Opening faceoffs won
OppCA60 - A weighted average of the shot attempts (Corsi, SAT) the team allowed per 60 minutes of a player's opponents
OppCF60 - A weighted average of the shot attempts (Corsi, SAT) the team generated per 60 minutes of a player's opponents
OppFA60 - A weighted average of the unblocked shot attempts (Fenwick, USAT) the team allowed per 60 minutes of a player's opponents
OppFF60 - A weighted average of the unblocked shot attempts (Fenwick, USAT) the team generated per 60 minutes of a player's opponents
OppGA60 - A weighted average of the goals the team allowed per 60 minutes of a player's opponents
OppGF60 - A weighted average of the goals the team scored per 60 minutes of a player's opponents
OppSA60 - A weighted average of the shots on goal the team allowed per 60 minutes of a player's opponents
OppSF60 - A weighted average of the shots on goal the team generated per 60 minutes of a player's opponents
OPS - Offensive point shares, a catch-all stats that measures a player's offensive contributions in points in the standings
OSA - Shots on goal allowed while this player was not on the ice
OSCA - Scoring chances allowed while this player was not on the ice
OSCF - The team's scoring chances while this player was not on the ice
OSF - The team's shots on goal while this player was not on the ice
OTF - Shifts this player started with an on-the-fly change
OTG - Overtime goals
OTOI - The amount of time this player was not on the ice.
Over - Shots that went over the net
Ovrl - Where the player was drafted overall
OxGA - Expected goals allowed (weighted shots) while this player was not on the ice, which is shot attempts weighted by location
OxGF - The team's expected goals (weighted shots) while this player was not on the ice, which is shot attempts weighted by location
OZF - Shifts this player has ended with an offensive zone faceoff
ozFO - Faceoffs taken in the offensive zone
ozFOL - Faceoffs lost in the offensive zone
ozFOW - Faceoffs won in the offensive zone
ozGAPF - Team goals allowed after faceoffs taken in the offensive zone
ozGFPF - Team goals scored after faceoffs taken in the offensive zone
OZS - Shifts this player has started with an offensive zone faceoff
ozSAPF - Team shot attempts allowed after faceoffs taken in the offensive zone
ozSFPF - Team shot attempts taken after faceoffs taken in the offensive zone
Pace - The average game pace, as estimated by all shot attempts per 60 minutes
Pass - An estimate of the player's setup passes (passes that result in a shot attempt)
Pct% - Percentage of all events produced by this team, defaults to Corsi, but can be set to another stat
PDO - The team's shooting and save percentages added together, times a thousand
PEND - The team's penalties drawn while this player was on the ice
PENT - The team's penalties taken while this player was on the ice
PIM - Penalties in minutes
Position - Positions played. NHL source listed first, followed by those listed by any other source.
Post - Times hit the post
Pr/St - Province or state of birth
PS - Point shares, a catch-all stats that measures a player's contributions in points in the standings
PSA - Penalty shot attempts
PSG - Penalty shot goals
PTS - Points. Goals plus all assists
PTS/60 - Points per 60 minutes
QRelCA60 - Shot attempts allowed per 60 minutes relative to how others did against the same competition
QRelCF60 - Shot attempts per 60 minutes relative to how others did against the same competition
QRelDFA60 - Weighted unblocked shot attempts (Dangeorus Fenwick) allowed per 60 minutes relative to how others did against the same competition
QRelDFF60 - Weighted unblocked shot attempts (Dangeorus Fenwick) per 60 minutes relative to how others did against the same competition
RBA - Rebounds allowed while this player was on the ice. Two very different sources.
RBF - The team's rebounds while this player was on the ice. Two very different sources.
RelA/60 - The player's A/60 relative to the team when he's not on the ice
RelC/60 - Corsi differential per 60 minutes relative to his team
RelC% - Corsi percentage relative to his team
RelDf/60 - The player's Diff/60 relative to the team when he's not on the ice
RelF/60 - The player's F/60 relative to the team when he's not on the ice
RelF/60 - Fenwick differential per 60 minutes relative to his team
RelF% - Fenwick percentage relative to his team
RelPct% - The players Pct% relative to the team when he's not on the ice
RelZS% - The player's zone start percentage when he's on the ice relative to when he's not.
RopFO - Opening faceoffs taken at home
RopFOW - Opening faceoffs won at home
RSA - Shots off the rush allowed while this player was on the ice
RSF - The team's shots off the rush while this player was on the ice
S.Bkhd - Backhand shots
S.Dflct - Deflections
S.Slap - Slap shots
S.Snap - Snap shots
S.Tip - Tipped shots
S.Wrap - Wraparound shots
S.Wrst - Wrist shots
SA - Shots on goal allowed while this player was on the ice
Salary - The player's salary
SCA - Scoring chances allowed while this player was on the ice
SCF - The team's scoring chances while this player was on the ice
sDist - The average shot distance of shots taken by this player
SF - The team's shots on goal while this player was on the ice
SH% - The team's (not individual's) shooting percentage when the player was on the ice
SOG - Shootout Goals
SOGDG - Game-deciding shootout goals
SOS - Shootout Shots
Status - This player's free agency status
SV% - The team's save percentage when the player was on the ice
Team -
TKA - The team's takeaways while this player was on the ice
TMCA60 - A weighted average of the shot attempts (Corsi, SAT) the team allowed per 60 minutes of a player's linemates
TMCF60 - A weighted average of the shot attempts (Corsi, SAT) the team generated per 60 minutes of a player's linemates
TMFA60 - A weighted average of the unblocked shot attempts (Fenwick, USAT) the team allowed per 60 minutes of a player's linemates
TMFF60 - A weighted average of the unblocked shot attempts (Fenwick, USAT) the team generated per 60 minutes of a player's linemates
TMGA60 - A weighted average of the goals the team allowed per 60 minutes of a player's linemates
TMGF60 - A weighted average of the goals the team scored per 60 minutes of a player's linemates
TMSA60 - A weighted average of the shots on goal the team allowed per 60 minutes of a player's linemates
TMSF60 - A weighted average of the shots on goal the team generated per 60 minutes of a player's linemates
TmxGF - A weighted average of a player's linemates of the expected goals the team scored
TmxGA - A weighted average of a player's linemates of the expected goals the team allowed
TMGA - A weighted average of a player's linemates of the goals the team scored
TMGF - A weighted average of a player's linemates of the goals the team allowed
TOI - Time on ice, in minutes, or in seconds (NHL)
TOI.QoC - A weighted average of the TOI% of a player's opponents.
TOI.QoT - A weighted average of the TOI% of a player's linemates.
TOI/GP - Time on ice divided by games played
TOI% - Percentage of all available ice time assigned to this player.
Wide - Shots that went wide of the net
Wt - Weight
xGA - Expected goals allowed (weighted shots) while this player was on the ice, which is shot attempts weighted by location
xGF - The team's expected goals (weighted shots) while this player was on the ice, which is shot attempts weighted by location
xGF.QoC - A weighted average of the expected goal percentage of a player's opponents
xGF.QoT - A weighted average of the expected goal percentage of a player's linemates
ZS% - Zone start percentage, the percentage of shifts started in the offensive zone, not counting neutral zone or on-the-fly changes"
Movehub City Rankings,Compare key metrics for over 200 cities,Blitzer,17,"Version 1,2017-03-24",cities,CSV,98 KB,Other,"5,300 views","1,567 downloads",28 kernels,2 topics,https://www.kaggle.com/blitzr/movehub-city-rankings,"Context
Movehub city ranking as published on http://www.movehub.com/city-rankings
Content
movehubqualityoflife.csv
Cities ranked by
Movehub Rating: A combination of all scores for an overall rating for a city or country.
Purchase Power: This compares the average cost of living with the average local wage.
Health Care: Compiled from how citizens feel about their access to healthcare, and its quality.
Pollution: Low is good. A score of how polluted people find a city, includes air, water and noise pollution.
Quality of Life: A balance of healthcare, pollution, purchase power, crime rate to give an overall quality of life score.
Crime Rating: Low is good. The lower the score the safer people feel in this city.
movehubcostofliving.csv
Unit: GBP
City
Cappuccino
Cinema
Wine
Gasoline
Avg Rent
Avg Disposable Income
cities.csv
Cities to countries as parsed from Wikipedia https://en.wikipedia.org/wiki/List_of_towns_and_cities_with_100,000_or_more_inhabitants/cityname:_A (A-Z)
Acknowledgements
Movehub
http://www.movehub.com/city-rankings
Wikipedia
https://en.wikipedia.org/wiki/List_of_towns_and_cities_with_100,000_or_more_inhabitants/cityname:_A"
Question-Answer Jokes,Jokes of the question-answer form from Reddit's r/jokes,Jiri Roznovjak,17,"Version 2,2017-01-06|Version 1,2017-01-05","humor
linguistics",CSV,3 MB,Other,"8,872 views","1,063 downloads",3 kernels,2 topics,https://www.kaggle.com/jiriroz/qa-jokes,"This dataset contains 38,269 jokes of the question-answer form, obtained from the r/Jokes subreddit. The dataset contains a csv file, where a row contains a question (""Why did the chicken cross the road""), the corresponding answer (""To get to the other side"") and a unique ID.
The data comes from the end of 2016 all the way to 2008. The entries with a higher ID correspond to the ones submitted earlier.
An example of what one might do with the data is build a sequence-to-sequence model where the input is a question and the output is an answer. Then, given a question, the model should generate a funny answer. This is what I did as the final project for my fall 2016 machine learning class. The project page can be viewed here.
Disclaimer: The dataset contains jokes that some may find inappropriate.
License
Released under reddit's API terms"
Now That's What I Call Music (U.S. releases),A collection of all 61 original Now That is What I Call Music tracklistings,athontz,17,"Version 2,2017-02-02|Version 1,2017-01-31","popular culture
music",CSV,175 KB,CC0,"3,549 views",286 downloads,2 kernels,0 topics,https://www.kaggle.com/athontz/nowthatswhaticallmusic,"Context
I am working on building a classifier that will examine today's 'top 40' and determine whether or not they are worthy of appearing on the next 'Now That's What I Call Music' album.
Content
The dataset includes all 61 US released Now That's what I call Music tracklistings.
Columns are: volume_number - the album number corresponding with the volume. (ex: a value of 60 would represent the album 'Now That's What I Call Music Vol. 60)
artist - the name of the artist singing the track
title - the song name
number - the song's track number on it's album
duration - the song's length in seconds
Acknowledgements
Thanks to Wikipedia contributors for maintaining this data!
Improvements
I am currently working on adding another csv file that contains this same data joined with each song's audio features from the Spotify Web API.
In the future I would also be interested in scraping Now releases in other countries as well as the 'special' releases (ex: Now That's What I Call Christmas music, etc)."
Agricuture Crops Production In india,Various Crops Cultivation/Production,SrinivasRao,17,"Version 1,2017-08-14","india
business
agriculture",CSV,103 KB,Other,"4,171 views",900 downloads,2 kernels,0 topics,https://www.kaggle.com/srinivas1/agricuture-crops-production-in-india,"Context
Agricuture Production in India from 2001-2014
Content
This Dataset Describes the Agricuture Crops Cultivation/Production in india. This is from https://data.gov.in/ fully Licensed
Acknowledgements
This Dataset can solves the problems of various crops Cultivation/production in india.
Columns
crop:string, crop name Variety:string,crop subsidary name state: string,Crops Cultivation/production Place Quantity:Integer,no of Quintals/Hectars production:Integer,no of years Production Season:DateTime,medium(no of days),long(no of days) Unit:String , Tons Cost:Integer, cost of cutivation and Production Recommended Zone:String ,place(State,Mandal,Village)
Inspiration
Across The Globe India Is The Second Largest Country having People more than 1.3 Billion. Many People Are Dependent On The Agricuture And it is the Main Resource. In Agricuturce Cultivation/Production Having More Problems. I want to solve the Big problem in india and usefull to many more people"
Flights in Brazil,"Every flight tracked by the National Civil Aviation Agency in Brazil, 2016-17.",ramirobentes,17,"Version 2,2017-11-15|Version 1,2017-08-31","brazil
aviation",Other,41 MB,CC0,"2,434 views",314 downloads,2 kernels,2 topics,https://www.kaggle.com/ramirobentes/flights-in-brazil,"Context
These are all the flights tracked by the National Civil Aviation Agency, in Brazil, from January 2015 to August 2017. I plan on keeping this dataset updated and on gradually adding data from previous years as well.
Content
The dataset is in portuguese so I had to remove some characters that were not supported on Kaggle. You can see the translation for the columns in the main dataset description.
""Nao Identificado"" means ""Unidentified"".
UF means the State where the airport is located. For every airport not located in Brazil, the value is N/I.
Feel free to ask me if you need translation of any other word."
1k Pharmaceutical Pill Image Dataset,Speckled pills CNN feature extracted for unique individual identification,TruMedicines,17,"Version 1,2017-07-13","healthcare
pharmacy
image data
multiclass classification",Other,8 MB,ODbL,"3,303 views",293 downloads,,,https://www.kaggle.com/trumedicines/1k-pharmaceutical-pill-image-dataset,"Context
1K dataset of speckled pharmaceutical pills. Using a CNN to extract features and create binary hash code, these pills can be retrieved from a mobile device for remote identification. Every pill can be tracked using a mobile phone app.
Content
1 K pharmaceutical pills jpeg images that have been convoluted by: rotations, grey scale, noise, non-pill
Acknowledgements
Special thanks for Funding and support of Microsoft - Paul DeBaun and NWCadence- Steve Borg
Inspiration
The Pill Crisis in America 1) Fake Fentanyl - killing young people  2) Opioid Abuse - killing all ages of people  3) Fake Online Drugs - killing unknown numbers 4) Non-Compliance - killing older people
Non-Compliance  up to 90% of diabetics don't take their meds enough to benefit 
Up to 75% of hypertensive patients do not adhere to their medicine 
Less than 27% depressed patients adhere to their medication
41-59% of mentally ill take their meds infrequently or not at all
33% of patients with schizophrenia don’t take their medicine at all"
AWS Honeypot Attack Data,Visualizing Cyber Attacks,casimian2000,17,"Version 1,2018-01-26","crime
internet",CSV,6 MB,CC0,"1,747 views",163 downloads,2 kernels,0 topics,https://www.kaggle.com/casimian2000/aws-honeypot-attack-data,"Context
(U) My purpose is to analyze Amazon Web Services (AWS) honeypot data for any trends and/or correlations that could possibly be used in predictive cyber threat vectors. I spent a lot of time looking for data sets and most of the ones I found had no documentation and the data was hard to interpret just from the file. This data is well formatted and straight forward.
Content
(U) The AWS Honeypot Database is an open-source database including information on cyber attacks/attempts.
(U) Data has 451,581 data points collected from 9:53pm on 3 March 2013 to 5:55am on 8 September 2013.
Acknowledgements
http://datadrivensecurity.info/blog/pages/dds-dataset-collection.html Jay Jacobs & Bob Rudis
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
UjiIndoorLoc: An indoor localization dataset,Compare WiFi fingerprinting indoor localization algorithms,GIANT: Machine learning for smart environments,17,"Version 1,2016-12-21","geography
computing and society",CSV,43 MB,CC4,"5,378 views",285 downloads,13 kernels,,https://www.kaggle.com/giantuji/UjiIndoorLoc,"Context
This data set is focused on WLAN fingerprint positioning technologies and methodologies (also know as WiFi Fingerprinting). It was the official database used in the IPIN2015 competition.
Many real world applications need to know the localization of a user in the world to provide their services. Therefore, automatic user localization has been a hot research topic in the last years. Automatic user localization consists of estimating the position of the user (latitude, longitude and altitude) by using an electronic device, usually a mobile phone. Outdoor localization problem can be solved very accurately thanks to the inclusion of GPS sensors into the mobile devices. However, indoor localization is still an open problem mainly due to the loss of GPS signal in indoor environments. Although, there are some indoor positioning technologies and methodologies, this database is focused on WLAN fingerprint-based ones (also know as WiFi Fingerprinting).
Although there are many papers in the literature trying to solve the indoor localization problem using a WLAN fingerprint-based method, there still exists one important drawback in this field which is the lack of a common database for comparison purposes. So, UJIIndoorLoc database is presented to overcome this gap.
The UJIIndoorLoc database covers three buildings of Universitat Jaume I (http://www.uji.es) with 4 or more floors and almost 110.000m2. It can be used for classification, e.g. actual building and floor identification, or regression, e.g. actual longitude and latitude estimation. It was created in 2013 by means of more than 20 different users and 25 Android devices. The database consists of 19937 training/reference records (trainingData.csv file) and 1111 validation/test records (validationData.csv file)
The 529 attributes contain the WiFi fingerprint, the coordinates where it was taken, and other useful information.
Each WiFi fingerprint can be characterized by the detected Wireless Access Points (WAPs) and the corresponding Received Signal Strength Intensity (RSSI). The intensity values are represented as negative integer values ranging -104dBm (extremely poor signal) to 0dbM. The positive value 100 is used to denote when a WAP was not detected. During the database creation, 520 different WAPs were detected. Thus, the WiFi fingerprint is composed by 520 intensity values.
Then the coordinates (latitude, longitude, floor) and Building ID are provided as the attributes to be predicted.
The particular space (offices, labs, etc.) and the relative position (inside/outside the space) where the capture was taken have been recorded. Outside means that the capture was taken in front of the door of the space.
Information about who (user), how (android device & version) and when (timestamp) WiFi capture was taken is also recorded.
Content
Attributes 001 to 520 (WAP001-WAP520): Intensity value for WAP001. Negative integer values from -104 to 0 and +100. Positive value 100 used if WAP001 was not detected.
Attribute 521 (Longitude): Longitude. Negative real values from -7695.9387549299299000 to -7299.786516730871000
Attribute 522 (Latitude): Latitude. Positive real values from 4864745.7450159714 to 4865017.3646842018.
Attribute 523 (Floor): Altitude in floors inside the building. Integer values from 0 to 4.
Attribute 524 (BuildingID): ID to identify the building. Measures were taken in three different buildings. Categorical integer values from 0 to 2.
Attribute 525 (SpaceID): Internal ID number to identify the Space (office, corridor, classroom) where the capture was taken. Categorical integer values.
Attribute 526 (RelativePosition): Relative position with respect to the Space (1 - Inside, 2 - Outside in Front of the door). Categorical integer values.
Attribute 527 (UserID): User identifier (see below). Categorical integer values.
Attribute 528 (PhoneID): Android device identifier (see below). Categorical integer values.
Attribute 529 (Timestamp): UNIX Time when the capture was taken. Integer value.
Relevent Paper
More information can be found in this paper:
Joaquín Torres-Sospedra, Raúl Montoliu, Adolfo Martínez-Usó, Tomar J. Arnau, Joan P. Avariento, Mauri Benedito-Bordonau, Joaquín Huerta. UJIIndoorLoc: A New Multi-building and Multi-floor Database for WLAN Fingerprint-based Indoor Localization Problems. In Proceedings of the Fifth International Conference on Indoor Positioning and Indoor Navigation, 2014.
Available at: http://www.ipin2014.org/wp/pdf/4A-3.pdf
If your are going to use this dataset in your research, please cite this paper
Acknowledgements
The dataset was created by:
Joaquín Torres-Sospedra, Raul Montoliu, Adolfo Martínez-Usó, Tomar J. Arnau, Joan P. Avariento, Mauri Benedito-Bordonau, Joaquín Huerta, Yasmina Andreu, óscar Belmonte, Vicent Castelló, Irene Garcia-Martí, Diego Gargallo, Carlos Gonzalez, Nadal Francisco, Josep López, Ruben Martínez, Roberto Mediero, Javier Ortells, Nacho Piqueras, Ianisse Quizán, David Rambla, Luis E. Rodríguez, Eva Salvador Balaguer, Ana Sanchís, Carlos Serra, and Sergi Trilles.
Inspiration
The objective is to estimate the building, floor and coordinates (latitude and longitude) of the 1111 samples included in the validation set. Since the real values of the building, floor and coordinates are also included, it is posible to determine the localization error.
The formula used in the IPIN2015 competition was the mean of the localization error of each sample. The localization error of each sample can be estimated as follows:
Error = building_penality * building_error + floor_penality * floor_error + coordinates_error
where:
building_error is 1 if the estimated building is not equal to the real one. 0 otherwise
floor_error is 1 if the estimated floor is not equal to the real one. 0 otherwise
coordinates_error is sqrt( (estimated_latitude - real_latitude)^2 + (estimated_longitude-real_longitude)^2)
In the IPIN2015 competition building_penalty and floor_penalty where set to 50 and 4 meters, respectively."
"Executive Orders, 1789-2016","Name, years in office, and executive orders signed by every American president",National Archives,17,"Version 1,2017-02-02","history
politics",CSV,4 KB,CC0,"4,559 views",326 downloads,13 kernels,0 topics,https://www.kaggle.com/nationalarchives/executive-orders,"Content
Executive orders are official documents, numbered consecutively, through which the President of the United States manages the operations of the federal government. The text of executive orders appears in the daily Federal Register as each executive order is signed by the President and received by the Office of the Federal Register. The total number of Executive orders issued by each administration includes number-and-letter designated orders, such as 9577-A, 9616-A, etc.
Acknowledgements
The data was compiled and published by the National Archives as executive order disposition tables, available for Franklin D. Roosevelt and later presidents."
Hate Crime Classification,Let's stop hate crimes with the power of data science!,Team AI,17,"Version 1,2017-08-20",,CSV,3 MB,CC0,"3,336 views",285 downloads,2 kernels,,https://www.kaggle.com/team-ai/classification-of-hate-crime-in-the-us,"Context
We should definitely stop hate crimes. Let's use data science to stop them. This is mainly classification but any other approach is welcome. Example; prediction for next possible hate crime.
Content
3700 rows of CSV from Google Trend. Headline, date, location, URL.
Acknowledgements
Special thanks to ; http://googletrends.github.io/data/
Inspiration
Media data is mainly NLP CSV. We have to come up with other ways to add the value from it."
NYC Restaurant Inspections,~400k Rows of Restaurant Inspections Data,City of New York,17,"Version 1,2017-08-30","government agencies
food and drink
business",CSV,139 MB,CC0,"3,568 views",570 downloads,,0 topics,https://www.kaggle.com/new-york-city/nyc-inspections,"Context:
Restaurant inspections for permitted food establishments in NYC. Restaurants are graded on A-F scale with regular visits by city health department.
Content:
Dataset includes address, cuisine description, inspection date, type, action, violation code and description(s). Data covers all of NYC and starts Jan 1, 2010-Aug 29, 2017.
Acknowledgements:
Data was collected by the NYC Department of Health and is available here.
Inspiration:
Can you predict restaurant closings?
Are certain violations more prominent in certain neighborhoods? By cuisine?
Who gets worse grades--chain restaurants or independent establishments?"
Air quality data from extensive network of sensors,"PM1, PM2.5, PM10, temp, pres and hum data for 2017 year from Krakow, Poland",Airly,16,"Version 2,2017-12-29|Version 1,2017-12-28","atmospheric sciences
environment
pollution
health",CSV,8 MB,CC4,"2,193 views",259 downloads,,0 topics,https://www.kaggle.com/datascienceairly/air-quality-data-from-extensive-network-of-sensors,"Context
In the past, the ancient city of Krakow was known as the capital of Poland. In 2000, it became known as the official European Capital of Culture. Now, it is known for having some of the most polluted air in Europe... In a World Health Organiation (WHO) study Krakow has been rated amongst the most polluted in the world. In the report, city was ranked 8th among 575 cities for levels of PM2.5 and 145th among 1100 cities for levels of PM10. Hazardous air quality is a common problem particularly during the colder months when many residents use solid fuels (mostly coal) for household heating. Air pollution in Krakow poses a significant danger to human health and life. Krakow's poisoned air includes amongst other things: particulate matter, benzo(a)pyrene and nitrogen dioxide. The state-run network of monitoring stations consists of 8 monitoring stations in Krakow. We decided to go step further - to build network of low-cost air quality sensors that can be deployed across entire city. The first step in the fight against smog is to identify areas of problem and to raise awareness among residents and the authorities. It is very important to create a network of sensors – only then you can check the actual conditions in various areas of the city. The technology enables real-time monitoring of air quality via map.airly.eu, so the information about the air in a specific location is easily accessible and always up to date.
Content
This dataset consists air quality data (the concentrations of particulate matter PM1, PM2.5 and PM10, temperature, air pressure and humidity) from 2017 generated by network of 56 low-cost sensors located in Krakow, Poland. Each had its own location (6 of them where replaced during this time period and have almost the same latitude and longitude). Measurements are grouped in 12 files, one for each month. Resolution of data is 1 hour.
Known issues: - PM1 is not calibrated and therefore can be bigger than PM2.5 - PM2.5 can be bigger than PM10 within the limits of measurement error - for the first two months humidity and temperature were not calibrated and therefore can show inaccurate values
Acknowledgements
The data was generated by Airly network - the project is still in its beginning stage, but over 1000 sensors have already been implemented in Poland. Airly is a startup definitely worth watching, especially for citizens of the most polluted cities. After all, it’s clean air we all want to breathe.
Inspiration
I think that this dataset offers some great opportunities for predictive models and data visualization. Airly's goal is to develop an effective forecast and monitoring of air quality, employ Artificial Intelligence and utilise data from extensive sensor network. If anyone has any ideas, breakthroughs or other interesting models please post them.
Some questions worth exploring: - What are the best prediction models based on extensive sensor network - statistical or numerical forecast? - How weather affects air quality? - How much pollution comes from cars, factories and coal-fired power plants?"
Appliances Energy Prediction,Data driven prediction of energy use of appliances,Gokagglers,16,"Version 1,2017-09-16","automation
energy",CSV,11 MB,Other,"2,614 views",258 downloads,3 kernels,,https://www.kaggle.com/loveall/appliances-energy-prediction,"Context
Experimental data used to create regression models of appliances energy use in a low energy building.
Content
The data set is at 10 min for about 4.5 months. The house temperature and humidity conditions were monitored with a ZigBee wireless sensor network. Each wireless node transmitted the temperature and humidity conditions around 3.3 min. Then, the wireless data was averaged for 10 minutes periods. The energy data was logged every 10 minutes with m-bus energy meters. Weather from the nearest airport weather station (Chievres Airport, Belgium) was downloaded from a public data set from Reliable Prognosis (rp5.ru), and merged together with the experimental data sets using the date and time column. Two random variables have been included in the data set for testing the regression models and to filter out non predictive attributes (parameters).
Acknowledgements
Luis Candanedo, luismiguel.candanedoibarra '@' umons.ac.be, University of Mons (UMONS)
Luis M. Candanedo, Veronique Feldheim, Dominique Deramaix, Data driven prediction models of energy use of appliances in a low-energy house, Energy and Buildings, Volume 140, 1 April 2017, Pages 81-97, ISSN 0378-7788, [Web Link].
Inspiration
Data used include measurements of temperature and humidity sensors from a wireless network, weather from a nearby airport station and recorded energy use of lighting fixtures. data filtering to remove non-predictive parameters and feature ranking plays an important role with this data. Different statistical models could be developed over this dataset. Highlights: The appliances energy consumption prediction in a low energy house is the dataset content Weather data from a nearby station was found to improve the prediction.
Pressure, air temperature and wind speed are important parameters in the prediction.
Data from a WSN that measures temperature and humidity increase the pred. accuracy.
From the WSN, the kitchen, laundry and living room data ranked high in importance."
Cuff-Less Blood Pressure Estimation,Pre-processed and cleaned vital signals for cuff-less BP estimation,Mohammad Kachuee,16,"Version 5,2017-06-04|Version 4,2017-05-23|Version 3,2017-05-23|Version 2,2017-05-23|Version 1,2017-05-08","healthcare
health",Other,5 GB,Other,"5,882 views",616 downloads,10 kernels,6 topics,https://www.kaggle.com/mkachuee/BloodPressureDataset,"Data Set Information:
The main goal of this data set is providing clean and valid signals for designing cuff-less blood pressure estimation algorithms. The raw electrocardiogram (ECG), photoplethysmograph (PPG), and arterial blood pressure (ABP) signals are originally collected from the physionet.org and then some preprocessing and validation performed on them. (For more information about the process please refer to our paper)
Attribute Information:
This database consists of a cell array of matrices, each cell is one record part. In each matrix each row corresponds to one signal channel:
1: PPG signal, FS=125Hz; photoplethysmograph from fingertip
2: ABP signal, FS=125Hz; invasive arterial blood pressure (mmHg)
3: ECG signal, FS=125Hz; electrocardiogram from channel II
Relevant Papers:
M. Kachuee, M. M. Kiani, H. Mohammadzade, M. Shabany, Cuff-Less High-Accuracy Calibration-Free Blood Pressure Estimation Using Pulse Transit Time, IEEE International Symposium on Circuits and Systems (ISCAS'15), 2015.
A. Goldberger, L. Amaral, L. Glass, J. Hausdorff, P. Ivanov, R. Mark, J.Mietus, G. Moody, C. Peng and H. Stanley, â€œPhysiobank, physiotoolkit, and physionet components of a new research resource for complex physiologic signals,â€ Circulation, vol. 101, no. 23, pp. 215â€“220, 2000.
Citation Request:
If you found this data set useful please cite the following:
M. Kachuee, M. M. Kiani, H. Mohammadzade, M. Shabany, Cuff-Less High-Accuracy Calibration-Free Blood Pressure Estimation Using Pulse Transit Time, IEEE International Symposium on Circuits and Systems (ISCAS'15), 2015.
M. Kachuee, M. M. Kiani, H. Mohammadzadeh, M. Shabany, Cuff-Less Blood Pressure Estimation Algorithms for Continuous Health-Care Monitoring, IEEE Transactions on Biomedical Engineering, 2016."
Reddit Usernames,The usernames of all 26m reddit accounts that have commented since Dec. 2017,ColinMorris,16,"Version 1,2017-12-08","reddit
internet",CSV,213 MB,CC0,"4,413 views",140 downloads,,,https://www.kaggle.com/colinmorris/reddit-usernames,"Content
This dataset contains the username of any reddit account that has left at least one comment, and their number of comments.
This data was grabbed in December 2017 from the Reddit comments dataset hosted on Google BigQuery. It should be current up to November 2017.
Quick stats
26 million users
8 million have left only a single comment
13 million (50%) have left no more than 5 comments
42,000 usernames demand something via PM (e.g. PM_ME_PIX_OF_UR_CAT, PM_me_your_successes, PMmeyourRGB, and lots of less wholesome ones)
Acknowledgements
Thanks to /u/Stuck_In_the_Matrix, who collected and maintains the original comments dataset.
Inspiration
What words commonly appear in Reddit usernames?
Can you identify frequently occurring username 'recipes' using clustering techniques?
What numbers most commonly appear as suffixes in Reddit usernames?
Can you train a generative language model to output new usernames based on this dataset?"
Arabic Handwritten Digits Dataset,Arabic Handwritten Digits Data-set,Mohamed Loey,16,"Version 3,2017-06-23|Version 2,2017-06-23|Version 1,2017-06-23","writing
mathematics",Other,247 MB,ODbL,"1,934 views",267 downloads,2 kernels,,https://www.kaggle.com/mloey1/ahdd1,"Arabic Handwritten Digits Dataset
Abstract
In recent years, handwritten digits recognition has been an important area due to its applications in several fields. This work is focusing on the recognition part of handwritten Arabic digits recognition that face several challenges, including the unlimited variation in human handwriting and the large public databases. The paper provided a deep learning technique that can be effectively apply to recognizing Arabic handwritten digits. LeNet-5, a Convolutional Neural Network (CNN) trained and tested MADBase database (Arabic handwritten digits images) that contain 60000 training and 10000 testing images. A comparison is held amongst the results, and it is shown by the end that the use of CNN was leaded to significant improvements across different machine-learning classification algorithms.
The Convolutional Neural Network was trained and tested MADBase database (Arabic handwritten digits images) that contain 60000 training and 10000 testing images. Moreover, the CNN is giving an average recognition accuracy of 99.15%.
Context
The motivation of this study is to use cross knowledge learned from multiple works to enhancement the performance of Arabic handwritten digits recognition. In recent years, Arabic handwritten digits recognition with different handwriting styles as well, making it important to find and work on a new and advanced solution for handwriting recognition. A deep learning systems needs a huge number of data (images) to be able to make a good decisions.
Content
The MADBase is modified Arabic handwritten digits database contains 60,000 training images, and 10,000 test images. MADBase were written by 700 writers. Each writer wrote each digit (from 0 -9) ten times. To ensure including different writing styles, the database was gathered from different institutions: Colleges of Engineering and Law, School of Medicine, the Open University (whose students span a wide range of ages), a high school, and a governmental institution. MADBase is available for free and can be downloaded from (http://datacenter.aucegypt.edu/shazeem/) .
Acknowledgements
CNN for Handwritten Arabic Digits Recognition Based on LeNet-5 http://link.springer.com/chapter/10.1007/978-3-319-48308-5_54 Ahmed El-Sawy, Hazem El-Bakry, Mohamed Loey Proceedings of the International Conference on Advanced Intelligent Systems and Informatics 2016 Volume 533 of the series Advances in Intelligent Systems and Computing pp 566-575
Inspiration
Creating the proposed database presents more challenges because it deals with many issues such as style of writing, thickness, dots number and position. Some characters have different shapes while written in the same position. For example the teh character has different shapes in isolated position.
Arabic Handwritten Characters Dataset
https://www.kaggle.com/mloey1/ahcd1
Benha University
http://bu.edu.eg/staff/mloey
https://mloey.github.io/"
The freeCodeCamp 2017 New Coder Survey,"An open data survey of 20,000+ people who are new to software development",freeCodeCamp,16,"Version 3,2017-05-26|Version 2,2017-05-12|Version 1,2017-05-07","employment
computing and society
programming",CSV,13 MB,ODbL,"3,125 views",301 downloads,56 kernels,0 topics,https://www.kaggle.com/free-code-camp/the-freecodecamp-2017-new-coder-survey,"Free Code Camp is an open source community where you learn to code and build projects for nonprofits.
We surveyed more than 20,000 people who started coding within the past 5 years. We reached them through the twitter accounts and email lists of various organizations that help people learn to code.
Our goal was to understand these people's motivations in learning to code, how they're learning to code, their demographics, and their socioeconomic background.
We've written in depth about this dataset here: https://medium.freecodecamp.com/we-asked-20-000-people-who-they-are-and-how-theyre-learning-to-code-fff5d668969"
Earthquakes <-?-> Solar System objects?,Is there any relationship between Earthquakes and Solar System objects?,tusha kutusha,16,"Version 1,2017-04-23","geology
space",CSV,4 MB,CC4,"3,992 views",379 downloads,12 kernels,2 topics,https://www.kaggle.com/aradzhabov/earthquakes-solar-system-objects,"Context
We all know that there is relationship between tides and Moon. What if we try to find the match between the position of objects in our Solar System and Earthquakes? So I prepared the dataset to answer the question.
Content
The dataset has information about Earthquakes (magnitude 6.1+) occur between yyyy.mm.dd: 1986.05.04 to 2016.05.04 and position (and other params) of Solar System planets + Sun and Moon at the time when the specific Earthquake occured Each row has: 1) info about the specific Earthquake: date and time, where it occur: latitude and longitude, magnitude, place (it is useless field since we have latitude and longitude but I left the filed just to have humanreading meaning e.g. 7km SW of Ueki, Japan) 2) Planets, Moon and Sun information (position and etc) relatively latitude and longitude of place and time of the Earthquake.
Acknowledgements
Tha dataset has two sources of information. Both ones are free and available for public. 1) Earthquakes info: The USGS Earthquake Hazards Program https://earthquake.usgs.gov/ 2) Solar System objects info: NGINOV Astropositions: http://api.nginov note: NGINOV uses a bit specific calculation of azimuth (as far as i know more often the one calculates from geographicall north). The note from NGINOV: ""An astronomical azimuth is expressed in degree or schedules arcs. Taking as a reference point, geographically south of the place of observation and a dextrorotatory angular progression."" note: I am not quite strong in Astronomic stuff. I expressed the idea about possible relationship and wrote a simple app to match and merge the info from two data sources into the dataset which possibly help to answer the question.
Inspiration
May be there is a relationship between the position and other params of objects in our Solar System and Earthquakes? Hope someone will find the match! :) Good luck!!"
Bank_Loan_data,Analysis of loan defaults,Datastreamer,16,"Version 1,2016-12-18",,Other,681 KB,Other,"5,110 views","1,001 downloads",5 kernels,3 topics,https://www.kaggle.com/dataforyou/bankloan,"Data Dictionary:
Title: Credit data
Source: Credit One Bank
Number of Instances: 5000
Name of Dataset: Analysis_of_Default
Number of Attributes: 20 (7 numerical, 13 categorical)
Attribute description
Attribute 1: (Qualitative / Categorical) Status of existing checking account A11: ... < 0 USD A12: 0 <= ... < 10000 USD A13: ... >= 10000 USD A14: no checking account
Attribute 2: (numerical) Duration in month
Attribute 3: (Qualitative / Categorical) Credit history A30: no credits taken/all credits paid back duly A31: all credits at this bank paid back duly A32: existing credits paid back duly till now A33: delay in paying off in the past A34:critical account/other credits existing(not at this bank)
Attribute 4: (Qualitative / Categorical) Purpose A40: car (new) A41: car (used) A42: furniture/equipment A43: radio/television A44: domestic appliances A45: repairs A46: education A47: (vacation - does not exist?) A48: retraining A49: business A410: others
Attribute 5: (numerical) Credit amount
Attribute 6: (Qualitative / Categorical) Savings account/bonds A61: ... < 1000 USD A62: 1000 <= ... < 5000 USD A63: 5000 <= ... < 10000 USD A64: .. >= 10000 USD A65: unknown/ no savings account
Attribute 7: (Qualitative / Categorical) Present employment since A71: unemployed A72: ... < 1 year A73: 1 <= ... < 4 years
A74: 4 <= ... < 7 years A75: .. >= 7 years
Attribute 8: (numerical) Installment rate in percentage of disposable income
Attribute 9: (Qualitative / Categorical) Personal status and sex A91: male : divorced/separated A92: female: divorced/separated/married A93: male : single A94: male : married/widowed A95: female: single
Attribute 10: (Qualitative / Categorical) Other debtors / guarantors A101: none A102: co-applicant A103: guarantor
Attribute 11: (numerical) Present residence since
Attribute 12: (Qualitative / Categorical) Property A121: real estate A122: if not A121: building society savings agreement/ life insurance A123: if not A121/A122: car or other, not in attribute 6 A124: unknown / no property
Attribute 13: (numerical) Age in years
Attribute 14: (Qualitative / Categorical) Other installment plans A141: bank A142: stores A143: none
Attribute 15: (Qualitative / Categorical) Housing A151: rent A152: own A153: for free
Attribute 16: (numerical) Number of existing credits at this bank
Attribute 17: (Qualitative / Categorical) Job A171: unemployed/ unskilled - non-resident A172: unskilled - resident A173: skilled employee / official A174: management/ self-employed/ highly qualified employee/ officer
Attribute 18: (numerical) Number of people being liable to provide maintenance for
Attribute 19: (Qualitative / Categorical) Telephone A191: none A192: yes, registered under the customer’s name
Attribute 20: (Qualitative / Categorical) foreign worker A201: yes A202: no
Default on Payment due
1 (Defaulted) 0 (No Default)"
The Demographic /r/ForeverAlone Dataset,A survey taken by redditers in /r/ForeverAlone. You would be surprised.,LiamLarsen,16,"Version 2,2017-04-29|Version 1,2017-04-28","linguistics
sociology
internet",CSV,108 KB,Other,"4,002 views",440 downloads,9 kernels,0 topics,https://www.kaggle.com/kingburrito666/the-demographic-rforeveralone-dataset,"Why?
Last year a redditor created a survey to collect demographic data on the subreddit /r/ForeverAlone. Since then they have deleted their account but they left behind their data set.
Columns are below:
Content
Timestamp
DateTime
What is your Gender?
String
What is your sexual orientation?
String
How old are you?
DateTime
What is your level of income?
DateTime
What is your race?
String
How would you describe your body/weight?
String
Are you a virgin?
String
Is prostitution legal where you live?
String
Would you pay for sex?
String
How many friends do you have IRL?
DateTime
Do you have social anxiety/phobia?
String
Are you depressed?
String
What kind of help do you want from others? (Choose all that apply)
String
Have you attempted suicide?
String
Employment Status: Are you currently…?
String
What is your job title?
String
What is your level of education?
String
What have you done to try and improve yourself? (Check all that apply)"
NBA Finals Team Stats,Contains team totals game by game from 1980-2017 NBA Finals,DaveRosenman,16,"Version 5,2017-08-26|Version 4,2017-07-04|Version 3,2017-07-04|Version 2,2017-07-03|Version 1,2017-06-25",,CSV,76 KB,Other,"4,059 views",827 downloads,5 kernels,,https://www.kaggle.com/daverosenman/nba-finals-team-stats,"'champsdata.csv' and runnerupsdata.csv'
'champs.csv' contains game-by-game team totals for the championship team from every finals game between 1980 and 2017. 'runnerups.csv' contains game-by-game team totals for the runner-up team from every finals game between 1980 and 2017. The 1980 NBA Finals was the first Finals series since the NBA added the three point line.
Content
The data was scrapped from basketball-reference.com.
Variables in 'champs.csv' and 'runnerups.csv'
Year: The year the series was played
Team: The name of the team.
Win: 1 = Win. 0 = Loss
Home: 1 = Home team. 0 = Away team.
Game: Game #
MP - Total minutes played. Equals 240 (48x5=240) if game did not go to overtime. MP>240 if game went to overtime.
FG - Field goals made
FGA - Field goal attempts
FGP - Field Goal Percentage
TP - 3 Point Field Goals Made
TPA - Three point attempts
TPP - three point percentage
FT - Free throws made
FTA - Free throws attempted
FTP - Free throw percentage
ORB - Offensive rebounds
DRB - Defensive rebounds
TRB - Total rebounds
AST - Assists
STL - Steals
BLK - Blocks
TOV - Turnovers
PF - Personal fouls
PTS - points scored
Datasets created from 'champsionsdata.csv' and 'runnerupsdata.csv'
The R code that I used to make the three files listed below can be found here
'champs_and_runner_ups_series_averages.csv'
This data frame contains series averages for the champion and runnerup each year.
'champs_series_averages.csv'
This data frame contains series averages for just the champion each year.
'runner_ups_series_averages.csv'
This data frame contains decade-by-decade averages for champions and runners up."
The National University of Singapore SMS Corpus,"A corpus of more than 67,000 SMS messages in Singapore English & Mandarin",Rachael Tatman,16,"Version 1,2017-08-08","languages
linguistics
telecommunications",{}JSON,67 MB,Other,"2,655 views",411 downloads,,0 topics,https://www.kaggle.com/rtatman/the-national-university-of-singapore-sms-corpus,"Context:
Short Message Service (SMS) messages are short messages sent from one person to another from their mobile phones. They represent a means of personal communication that is an important communicative artifact in our current digital era. This dataset contains SMS messages that were collected from users who knew they were participating in a research project and that their messages would be shared publicly. This dataset contains two SMS messages in two languages: Singapore English and Mandarin Chinese.
Content:
This is a corpus of SMS (Short Message Service) messages collected for research at the Department of Computer Science at the National University of Singapore. This dataset consists of 67,093 SMS messages taken from the corpus on Mar 9, 2015. The messages largely originate from Singaporeans and mostly from students attending the University. These messages were collected from volunteers who were made aware that their contributions were going to be made publicly available. The data collectors opportunistically collected as much metadata about the messages and their senders as possible, so as to enable different types of analyses.
Acknowledgements:
This corpus was collected by Tao Chen and Min-Yen Kan. If you use this data, please cite the following paper:
Tao Chen and Min-Yen Kan (2013). Creating a Live, Public Short Message Service Corpus: The NUS SMS Corpus. Language Resources and Evaluation, 47(2)(2013), pages 299-355. URL: https://link.springer.com/article/10.1007%2Fs10579-012-9197-9
Inspiration:
This dataset contains a lot of short, informal texts and is ideal for trying your hand at various natural language processing tasks. There’s also a lot of information about the messages which might reveal interesting insights. Here are some ideas to get you started:
This dataset contains Singapore English. How well do tools trained on other varieties of English, like stemmers or part of speech taggers, work on it?
What time of day are most SMS messages sent? Is this different for the English and Mandarin datasets?
Unlike English, Mandarin does not have spaces between words, which can be made up of several characters. Can you build or implement a system for word identification?"
Massachusetts Public Schools Data,"Student body, funding levels, and outcomes (SAT, MCAS, APs, college attendance)",Nigel Dalziel,16,"Version 1,2017-08-22",education,CSV,2 MB,CC0,"3,248 views",618 downloads,3 kernels,0 topics,https://www.kaggle.com/ndalziel/massachusetts-public-schools-data,"Data Sources
This dataset compiles data from the following Massachusetts Department of Education reports:
Enrollment by Grade
Enrollment by Selected Population
Enrollment by Race/Gender
Class Size by Gender and Selected Populations
Teacher Salaries
Per Pupil Expenditure
Graduation Rates
Graduates Attending Higher Ed
Advanced Placement Participation
Advanced Placement Performance
SAT Performance
MCAS Achievement Results
Accountability Report
In each case, the data is the latest available data as of August 2017.
Data Dictionary
The data dictionary lists the report from which each field is sourced. It also includes the original field names - minor changes have been made to make the field names easier to understand. Data definitions can be found on the About the Data section of the MA DOE website.
Questions
What contributes to differences in schools outcomes?
Are there meaningful regional differences within MA?
Which schools do well despite limited resources?"
Arabic Natural Audio Dataset,Automatic Emotion Recognition,SamiraKlaylat,16,"Version 11,2017-12-01|Version 10,2017-12-01|Version 9,2017-12-01|Version 8,2017-12-01|Version 7,2017-10-23|Version 6,2017-10-16|Version 5,2017-10-10|Version 4,2017-10-08|Version 3,2017-09-18|Version 2,2017-08-04|Version 1,2017-08-03",,Other,560 MB,CC4,"2,448 views",227 downloads,2 kernels,0 topics,https://www.kaggle.com/suso172/arabic-natural-audio-dataset,"Emotion expression is an essential part of human interaction. The same text can hold different meanings when expressed with different emotions. Thus understanding the text alone is not enough for getting the meaning of an utterance. Acted and natural corpora have been used to detect emotions from speech. Many speech databases for different languages including English, German, Chinese, Japanese, Russian, Italian, Swedish and Spanish exist for modeling emotion recognition. Since there is no reported reference of an available Arabic corpus, we decided to collect the first Arabic Natural Audio Dataset (ANAD) to recognize discrete emotions.
Embedding an effective emotion detection feature in speech recognition system seems a promising solution for decreasing the obstacles faced by the deaf when communicating with the outside world. There exist several applications that allow the deaf to make and receive phone calls normally, as the hearing-impaired individual can type a message and the person on the other side hears the words spoken, and as they speak, the words are received as text by the deaf individual. However, missing the emotion part still makes these systems not hundred percent reliable. Having an effective speech to text and text to speech system installed in their everyday life starting from a very young age will hopefully replace the human ear. Such systems will aid deaf people to enroll in normal schools at very young age and will help them to adapt better in classrooms and with their classmates. It will help them experience a normal childhood and hence grow up to be able to integrate within the society without external help.
Eight videos of live calls between an anchor and a human outside the studio were downloaded from online Arabic talk shows. Each video was then divided into turns: callers and receivers. To label each video, 18 listeners were asked to listen to each video and select whether they perceive a happy, angry or surprised emotion. Silence, laughs and noisy chunks were removed. Every chunk was then automatically divided into 1 sec speech units forming our final corpus composed of 1384 records.
Twenty five acoustic features, also known as low-level descriptors, were extracted. These features are: intensity, zero crossing rates, MFCC 1-12 (Mel-frequency cepstral coefficients), F0 (Fundamental frequency) and F0 envelope, probability of voicing and, LSP frequency 0-7. On every feature nineteen statistical functions were applied. The functions are: maximum, minimum, range, absolute position of maximum, absolute position of minimum, arithmetic of mean, Linear Regression1, Linear Regression2, Linear RegressionA, Linear RegressionQ, standard Deviation, kurtosis, skewness, quartiles 1, 2, 3 and, inter-quartile ranges 1-2, 2-3, 1-3. The delta coefficient for every LLD is also computed as an estimate of the first derivative hence leading to a total of 950 features.
I would have never reached that far without the help of my supervisors. I warmly thank and appreciate Dr. Rached Zantout, Dr. Lama Hamandi, and Dr. Ziad Osman for their guidance, support and constant supervision."
Daily Fantasy Basketball - DraftKings NBA,Data for DraftKings NBA Daily Fantasy Basketball Contests,Alan Du,16,"Version 1,2017-12-30","basketball
covariance and correlation
decision theory
+ 2 more...",Other,124 MB,CC0,"1,181 views",132 downloads,,0 topics,https://www.kaggle.com/alandu20/daily-fantasy-basketball-draftkings,"Context
In Daily Fantasy Sports (DFS) contests, contestants construct a virtual lineup of players that score points based on their real-world performances. Unlike in season-long Fantasy Sports contests,in DFS contestants submit a new lineup for each set of games. DFS contests are held for several professional sports leagues, including the National Football League (NFL), National Basketball League (NBA), and National Hockey League (NHL). The leading DFS sites today are DraftKings and Fanduel, which control approximately 90% of the $3B DFS market.
There are three primary types of DFS games: Head-to-Heads (H2Hs), Double-Ups, and Guaranteed Prize Pools (GPPs). In H2H games, two contestants play for a single cash prize. In Double-Up games, a pool of contestants compete to place in the top 50% of lineups, which are awarded twice the entry fee. In GPPs, a pool of contestants compete for a fixed prize structure that tends to be very top heavy; some contests payout hundreds of thousands of dollars to the top finisher.
Over the last year, I have developed a winning system for daily fantasy football and baseball contests. Building this system from scratch was a fantastic compliment to the things I learned as a student, from machine learning and optimization to optimal learning and game theory. I hope others can join me in researching daily fantasy basketball and perhaps get involved with the burgeoning world of daily fantasy sports.
Content
This dataset contains 20 days of DraftKings NBA contest data scraped between 2017-11-27 and 2017-12-28. For DraftKings NBA daily fantasy basketball contest rules, see https://www.draftkings.com/help/rules/nba.
Format:
One folder per day
One folder per contest for a given day
Salary file (“DKSalaries.csv”), payout structure file (“payout_structure.csv”), and contest results file (“contest-standings.csv”) for a given contest. Column headers in each files are pretty self-explanatory.
Some additional files (e.g. “players.csv”, “covariance_mat_unfiltered.csv”, “hist_fpts_mat.csv”) for a given contest. These files were for my personal research, feel free to use or ignore.
“projections” folder contains projections data for each player from rotogrinders and daily fantasy nerd, labeled by date.
“contests.csv” contains information about each contest, e.g. entry fee, slate, and contest size.
Acknowledgements
Thank you to my friend from college, Michael Chiang, for contributions to this project.
Inspiration
A few ideas to get started:
What kind of position ""stacks"" tend to maximize correlation within a lineup?
How can you minimize correlation between lineups, such that you maximize your chances of winning a GPP?
What are the tendencies of some of the top DFS pros?
Can you improve rotogrinders and daily fantasy nerd player projections?
Can you predict which players are undervalued (i.e. high fantasy points / salary ratio)?
Can you predict the ownership percentage for each player in a given contest?"
Audio features of songs ranging from 1922 to 2011,A subset of the Million Song Database,UCI Machine Learning,16,"Version 1,2017-09-07","music
sound technology",CSV,423 MB,Other,"3,726 views",190 downloads,,,https://www.kaggle.com/uciml/msd-audio-features,"Context
The Million Song Dataset (MSD) is a freely-available collection of audio features and metadata for a million contemporary popular music tracks. This is a subset of the MSD and contains audio features of songs with the year of the song. The purpose being to predict the release year of a song from audio features.
Content
The owners recommend that you split the data like this to avoid the 'producer effect' by making sure no song from a given artist ends up in both the train and test set.
train: first 463,715 examples
test: last 51,630 examples
Field descriptions:
The first value is the year (target), ranging from 1922 to 2011.
Then there are 90 attributes
TimbreAverage[1-12]
TimbreCovariance[1-78]
These features were extracted from the 'timbre' features from The Echo Nest API. The authors took the average and covariance over all 'segments' and each segment was described by a 12-dimensional timbre vector.
Acknowledgements
Original dataset: Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian Whitman, and Paul Lamere. The Million Song Dataset. In Proceedings of the 12th International Society for Music Information Retrieval Conference (ISMIR 2011), 2
Subset downloaded from: https://archive.ics.uci.edu/ml/datasets/yearpredictionmsd
Inspiration
Use this dataset to predict the years that each song was released based on it's audio features"
Carbon Monoxide Daily Summary,A summary of daily CO levels from 1990 to 2017,US Environmental Protection Agency,16,"Version 1,2017-06-30",environment,CSV,2 GB,CC0,"2,549 views",314 downloads,,0 topics,https://www.kaggle.com/epa/carbon-monoxide,"Context:
Carbon Monoxide (CO) is a colorless, odorless gas that can be harmful when inhaled in large amounts. CO is released when something is burned. The greatest sources of CO to outdoor air are cars, trucks and other vehicles or machinery that burn fossil fuels. A variety of items in your home such as unvented kerosene and gas space heaters, leaking chimneys and furnaces, and gas stoves also release CO and can affect air quality indoors.
Content:
The daily summary file contains data for every monitor (sampled parameter) in the Environmental Protection Agency (EPA) database for each day. This file will contain a daily summary record that is:
The aggregate of all sub-daily measurements taken at the monitor.
The single sample value if the monitor takes a single, daily sample (e.g., there is only one sample with a 24-hour duration). In this case, the mean and max daily sample will have the same value.
Within the data file you will find these fields: 1. State Code: The Federal Information Processing Standards (FIPS) code of the state in which the monitor resides.
County Code: The FIPS code of the county in which the monitor resides.
Site Num: A unique number within the county identifying the site.
Parameter Code: The AQS code corresponding to the parameter measured by the monitor.
POC: This is the “Parameter Occurrence Code” used to distinguish different instruments that measure the same parameter at the same site.
Latitude: The monitoring site’s angular distance north of the equator measured in decimal degrees.
Longitude: The monitoring site’s angular distance east of the prime meridian measured in decimal degrees.
Datum: The Datum associated with the Latitude and Longitude measures.
Parameter Name: The name or description assigned in AQS to the parameter measured by the monitor. Parameters may be pollutants or non-pollutants.
Sample Duration: The length of time that air passes through the monitoring device before it is analyzed (measured). So, it represents an averaging period in the atmosphere (for example, a 24-hour sample duration draws ambient air over a collection filter for 24 straight hours). For continuous monitors, it can represent an averaging time of many samples (for example, a 1-hour value may be the average of four one-minute samples collected during each quarter of the hour).
Pollutant Standard: A description of the ambient air quality standard rules used to aggregate statistics. (See description at beginning of document.)
Date Local: The calendar date for the summary. All daily summaries are for the local standard day (midnight to midnight) at the monitor.
Units of Measure: The unit of measure for the parameter. QAD always returns data in the standard units for the parameter. Submitters are allowed to report data in any unit and EPA converts to a standard unit so that we may use the data in calculations.
Event Type: Indicates whether data measured during exceptional events are included in the summary. A wildfire is an example of an exceptional event; it is something that affects air quality, but the local agency has no control over. No Events means no events occurred. Events Included means events occurred and the data from them is included in the summary. Events Excluded means that events occurred but data form them is excluded from the summary. Concurred Events Excluded means that events occurred but only EPA concurred exclusions are removed from the summary. If an event occurred for the parameter in question, the data will have multiple records for each monitor.
Observation Count: The number of observations (samples) taken during the day.
Observation Percent: The percent representing the number of observations taken with respect to the number scheduled to be taken during the day. This is only calculated for monitors where measurements are required (e.g., only certain parameters).
Arithmetic Mean: The average (arithmetic mean) value for the day.
1st Max Value: The highest value for the day.
1st Max Hour: The hour (on a 24-hour clock) when the highest value for the day (the previous field) was taken.
AQI: The Air Quality Index for the day for the pollutant, if applicable.
Method Code: An internal system code indicating the method (processes, equipment, and protocols) used in gathering and measuring the sample. The method name is in the next column.
Method Name: A short description of the processes, equipment, and protocols used in gathering and measuring the sample.
Local Site Name: The name of the site (if any) given by the State, local, or tribal air pollution control agency that operates it.
Address: The approximate street address of the monitoring site.
State Name: The name of the state where the monitoring site is located.
County Name: The name of the county where the monitoring site is located.
City Name: The name of the city where the monitoring site is located. This represents the legal incorporated boundaries of cities and not urban areas.
CBSA Name: The name of the core bases statistical area (metropolitan area) where the monitoring site is located.
Date of Last Change: The date the last time any numeric values in this record were updated in the AQS data system.
Acknowledgements:
These data come from the EPA and are current up to May 1, 2017. You can use Kernels to analyze, share, and discuss this data on Kaggle, but if you’re looking for real-time updates and bigger data, check out the data on BigQuery, too: https://cloud.google.com/bigquery/public-data/epa.
Inspiration:
Breathing air with a high concentration of CO reduces the amount of oxygen that can be transported in the bloodstream to critical organs like the heart and brain. At very high levels, which are possible indoors or in other enclosed environments, CO can cause dizziness, confusion, unconsciousness and death. Very high levels of CO are not likely to occur outdoors. However, when CO levels are elevated outdoors, they can be of particular concern for people with some types of heart disease. These people already have a reduced ability for getting oxygenated blood to their hearts in situations where the heart needs more oxygen than usual. They are especially vulnerable to the effects of CO when exercising or under increased stress. In these situations, short-term exposure to elevated CO may result in reduced oxygen to the heart accompanied by chest pain also known as angina."
Breweries and Brew Pubs in the USA,"A list of over 7,000 breweries and brew pubs in the USA.",Datafiniti,16,"Version 2,2017-09-20|Version 1,2017-05-10","databases
food and drink
business",CSV,22 MB,CC4,"4,156 views",457 downloads,9 kernels,2 topics,https://www.kaggle.com/datafiniti/breweries-brew-pubs-in-the-usa,"About This Data
This is a list of over 7,000 breweries and brewpubs in the USA provided by Datafiniti's Business Database. The dataset includes the category, name, address, city, state, and more for each listing.
What You Can Do With This Data
You can use this geographical and categorical information for business locations to determine which cities and states have the most breweries. E.g.:
What is the number of breweries in each state?
What are the cities with the most breweries per person?
What are the states with the most breweries per person?
What are the top cities for breweries?
What are the top states for breweries?
What industry categories are typically grouped with breweries?
Data Schema
A full schema for the data is available in our support documentation.
About Datafiniti
Datafiniti provides instant access to web data. We compile data from thousands of websites to create standardized databases of business, product, and property information. Learn more.
Want More?
You can get more data like this by joining Datafiniti or requesting a demo."
Airbnb Property Data from Texas,"Dataset of 18,000+ properties",PromptCloud,16,"Version 1,2017-07-25","united states
hotels",CSV,9 MB,CC4,"4,808 views",479 downloads,3 kernels,2 topics,https://www.kaggle.com/PromptCloudHQ/airbnb-property-data-from-texas,"Context
Sharing economy and vacation rentals are among the hottest topics that has touched millions of lives across the globe. Airbnb has been instrumental in this space and currently operating in more than 191 countries. Hence, it'd be good idea to analyze this data and uncover insights.
Content
Dataset contains more than 18,000 property listings from Texas, United Staes. Given below are the data fields:
Rate per night Number of bedrooms City Joining month and year Longitude Latitude Property description Property title Property URL
The Airbnb data was extracted by PromptCloud’s Data-as-a-Service solution.
Initial Analysis
The following article covers spatial data visualization and topic modelling of the description text: http://www.kdnuggets.com/2017/08/insights-data-mining-airbnb.html
Inspiration
Some of the interesting analysis are related to spatial mapping and text mining of the description text apart from the exploratory analysis."
"Federal Reserve Interest Rates, 1954-Present","Interest rates, economic growth, unemployment, and inflation data",Federal Reserve,16,"Version 1,2017-03-16","history
finance",CSV,26 KB,CC0,"4,339 views",708 downloads,8 kernels,0 topics,https://www.kaggle.com/federalreserve/interest-rates,"Context
The Federal Reserve sets interest rates to promote conditions that achieve the mandate set by the Congress — high employment, low and stable inflation, sustainable economic growth, and moderate long-term interest rates. Interest rates set by the Fed directly influence the cost of borrowing money. Lower interest rates encourage more people to obtain a mortgage for a new home or to borrow money for an automobile or for home improvement. Lower rates encourage businesses to borrow funds to invest in expansion such as purchasing new equipment, updating plants, or hiring more workers. Higher interest rates restrain such borrowing by consumers and businesses.
Content
This dataset includes data on the economic conditions in the United States on a monthly basis since 1954. The federal funds rate is the interest rate at which depository institutions trade federal funds (balances held at Federal Reserve Banks) with each other overnight. The rate that the borrowing institution pays to the lending institution is determined between the two banks; the weighted average rate for all of these types of negotiations is called the effective federal funds rate. The effective federal funds rate is determined by the market but is influenced by the Federal Reserve through open market operations to reach the federal funds rate target. The Federal Open Market Committee (FOMC) meets eight times a year to determine the federal funds target rate; the target rate transitioned to a target range with an upper and lower limit in December 2008. The real gross domestic product is calculated as the seasonally adjusted quarterly rate of change in the gross domestic product based on chained 2009 dollars. The unemployment rate represents the number of unemployed as a seasonally adjusted percentage of the labor force. The inflation rate reflects the monthly change in the Consumer Price Index of products excluding food and energy.
Acknowledgements
The interest rate data was published by the Federal Reserve Bank of St. Louis' economic data portal. The gross domestic product data was provided by the US Bureau of Economic Analysis; the unemployment and consumer price index data was provided by the US Bureau of Labor Statistics.
Inspiration
How does economic growth, unemployment, and inflation impact the Federal Reserve's interest rates decisions? How has the interest rate policy changed over time? Can you predict the Federal Reserve's next decision? Will the target range set in March 2017 be increased, decreased, or remain the same?"
WUZZUF Job Posts (2014-2016),Explore jobs and job seekers applications on WUZZUF (2014-2016),WUZZUF,16,"Version 1,2017-04-11",employment,CSV,131 MB,CC4,"3,219 views",361 downloads,7 kernels,,https://www.kaggle.com/WUZZUF/wuzzuf-job-posts,"Context
One of the main challenges in any marketplace business is achieving the balance between demand and supply. At WUZZUF we optimize for demand, relevance and quality while connecting employers with the matching applicants, and recommending relevant jobs to the job seekers.
Content
The dataset includes:
Wuzzuf_Job_Posts_Sample : a sample of jobs posted on WUZZUF during 2014-2016.
Wuzzuf_Applications_Sample : the corresponding applications (Excluding some entries).
Note: The jobs are mainly in Egypt but other locations are included.
Exploration Ideas
There are several areas to explore, including, but not limited to:
Correlations between different features
Salaries trends
Insights about supply/demand
Growth opportunities
Data quality"
California Electricity Capacity,Are Californians paying for power they don't need?,LA Times Data Desk,16,"Version 1,2017-04-06","energy
electrical engineering",CSV,16 MB,Other,"3,143 views",419 downloads,5 kernels,,https://www.kaggle.com/la-times/california-electricity-capacity,"This data and analysis originally provided information for the February 5, 2017, Los Angeles Times story ""Californians are paying billions for power they don't need"" by documenting California's glut of power and the increasing cost to consumers. It also underpins a complementary interactive graphic. The data are drawn from the Energy Information Administration, a branch of the United States government.
Acknowledgements
Data and analysis originally published by Ryan Menezes and Ben Welsh on the LA Times Data Desk GitHub."
2016 and 2017 Kitefoil Race Results,This data set will contain the results from all the 2016-2017 kitefoil races.,Anthony Goldbloom,16,"Version 51,2017-10-09|Version 50,2017-10-08|Version 49,2017-10-08|Version 48,2017-09-05|Version 47,2017-08-13|Version 46,2017-08-13|Version 45,2017-05-14|Version 44,2017-05-14|Version 43,2017-05-14|Version 42,2017-05-14|Version 41,2017-05-14|Version 40,2017-05-14|Version 39,2017-05-14|Version 38,2017-05-14|Version 37,2017-05-14|Version 36,2017-05-14|Version 35,2017-05-14|Version 34,2017-05-14|Version 33,2017-03-05|Version 32,2017-01-26|Version 31,2016-12-18|Version 30,2016-12-17|Version 29,2016-12-17|Version 28,2016-12-17|Version 27,2016-11-13|Version 26,2016-11-13|Version 25,2016-11-13|Version 24,2016-11-13|Version 23,2016-11-13|Version 22,2016-09-21|Version 21,2016-09-21|Version 20,2016-09-21|Version 19,2016-09-21|Version 18,2016-09-18|Version 17,2016-09-16|Version 16,2016-09-16|Version 15,2016-09-16|Version 14,2016-08-20|Version 13,2016-08-20|Version 12,2016-08-17|Version 11,2016-08-17|Version 10,2016-08-14|Version 9,2016-08-14|Version 8,2016-08-14|Version 7,2016-08-14|Version 6,2016-08-14|Version 5,2016-08-14|Version 4,2016-08-14|Version 3,2016-08-14|Version 2,2016-08-13|Version 1,2016-08-11","oceans
sports",Other,383 KB,Other,"6,457 views",166 downloads,14 kernels,0 topics,https://www.kaggle.com/antgoldbloom/2016-kitefoil-race-results,This data set will contain the results from all the 2016 kitefoil races. This allows analysis to be done including the calculation of world rankings.
SF Bay Area Pokemon Go Spawns,"Find patterns in spawn location, type and time",kveykva,16,"Version 1,2016-09-15",video games,CSV,32 MB,Other,"10,116 views",740 downloads,22 kernels,,https://www.kaggle.com/kveykva/sf-bay-area-pokemon-go-spawns,"Pokemon Go type, latitude, longitude, and despawn times - July 26 to July 28.
This was an early mined dataset from Pokemon Go. Representing a fairly large spatial area within a thin time frame. Mostly useful in identifying potential spatial patterns in pokemon spawns."
Freesound: Content-Based Audio Retrieval,Find sounds with text-queries based on their acoustic content,David Schwertfeger,16,"Version 3,2016-12-27|Version 2,2016-12-25|Version 1,2016-12-24","linguistics
sound technology",CSV,5 GB,CC4,"3,551 views",264 downloads,4 kernels,0 topics,https://www.kaggle.com/dschwertfeger/freesound,"Context
People have been making music for tens of thousands of years [1]. Today, making music is easier and more accessible than ever before. The technological developments of the last few decades allow people to simulate playing every imaginable instrument on their computers. Audio sequencers enable users to arrange their songs on a time line, sample by sample. Digital audio workstations (DAWs) ship with virtual instruments and synthesizers which allow users to virtually play a whole band or orchestra in their bedrooms.
One challenge in working with DAWs is organizing samples and recordings in a structured way; so, users can easily access them. In addition to their own recordings, many users download samples. Browsing through sample collections to find the perfect sound is time consuming and may impede the user's creative flow [2]. On top of this, manually naming and tagging recordings is a time-consuming and tedious task, so not many users do [3]. The consequence is that finding the right sound at the right moment becomes a challenging problem [4].
Modeling the relationship between the acoustic content and semantic descriptions of sounds could allow users to retrieve sounds using text queries. This dataset was collected to support research on content-based audio retrieval systems, focused on sounds used in creative context.
Content
This dataset was collected from Freesound [5] in June 2016. It contains the frame-based MFCCs of about 230,000 sounds and the associated tags.
sounds.json: Sound metadata originally downloaded from the Freesound API. This file includes the id, associated tags, links to previews, and links to an analysis_frames file, which contains frame-based low-level features, for each sound.
preprocessed_tags.csv: Preprocessed tags. Contains only tags which are associated to at least 0.01% of sounds. Moreover, tags were split on hyphens and stemmed. Tags containing numbers and short tags with less than three characters were removed.
queries.csv: An aggregated query-log of real user-queries against the Freesound database, collected between May 11 and November 24 in 2016.
preprocessed_queries.csv Queries were preprocessed in the same way tags were preprocessed.
*_mfccs.csv.bz2: The original MFCCs for each sound, extracted from the URL provided in the analysis_frames field of sounds.json, split across ten files.
cb_{512|1024|2048|4096}_sparse.pkl: Codebook representation of sounds saved as sparse pd.DataFrame. The first-order and second-order derivatives of the 13 MFCCs were appended to the MFCC feature vectors of each sound. All frames were clustered using K-Means (Mini-Batch K-Means) to find {512|1024|2048|4096} cluster centers. Each frame was, then, assigned to its closest cluster center and the counts used to represent a sound as a single {512|1024|2048|4096}-dimensional vector.
Acknowledgements
Thanks to the Music Technology Group of the Universitat Pompeu Fabra in Barcelona for creating and maintaining the Freesound [5] database and for providing the aggregated query-logs.
Inspiration
Who can create the best content-based audio retrieval system measured by precision-at-k for values of k in {1, ..., 20} and mean average precision.
Getting started
Here's the accompanying GitHub repository: https://github.com/dschwertfeger/cbar
References
[1] N. L. Wallin and B. Merker, The Origins of Music. MIT Press, 2001.
[2] M. Csikszentmihalyi, Flow: The Psychology of Optimal Experience. New York: Harper Perennial Modern Classics, 2008.
[3] E. Pampalk, A. Rauber, and D. Merkl, ""Content-based organization and visualization of music archives"", in Proceedings of the tenth ACM international conference on Multimedia, 2002, pp. 570–579.
[4] T. Bertin-Mahieux, D. Eck, and M. Mandel, ""Automatic tagging of audio: The state-of-the-art"", Machine audition: Principles, algorithms and systems, pp. 334–352, 2010.
[5] F. Font, G. Roma, and X. Serra, ""Freesound technical demo"", 2013, pp. 411–412."
Codechef Competitive Programming,Problem statements and solutions provided by people on the codechef site,ArjoonnSharma,16,"Version 5,2017-01-24|Version 4,2017-01-08|Version 3,2016-11-29|Version 2,2016-11-27|Version 1,2016-11-26",programming,CSV,1 GB,CC0,"5,356 views",308 downloads,19 kernels,2 topics,https://www.kaggle.com/arjoonn/codechef-competitive-programming,"In the pursuit of any goal, the first step is invariably data collection. As put up on the OpenAI blog, writing a program which can write other programs is an incredibly important problem.
This dataset collects publicly available information from the Codechef site's practice section to provide about 1000 problem statements and a little over 1 million solutions in total to these problems in various languages.
The ultimate aim is to allow a program to learn program generation in any language to satisfy a given problem statement."
Paradise Papers,Data Scientists Against Corruption,Zeeshan-ul-hassan Usmani,16,"Version 1,2017-11-21","covariance and correlation
money
politics",CSV,7 MB,CC3,"1,709 views",111 downloads,,,https://www.kaggle.com/zusmani/paradise-papers,"Context
I've uploaded a dataset previously that contains Paradise Papers, Panama Papers, Bahamas and Offshore Leaks. I've been getting a lot of requests to upload Paradise papers alone to make it less confusing for my fellow data scientists. Here it is, the complete cache of Paradise Papers released so far. I will keep updating it.
The Paradise Papers is a cache of some 13GB of data that contains 13.4 million confidential records of offshore investment by 120,000 people and companies in 19 tax jurisdictions (Tax Heavens - an awesome video to understand this); that was published by the International Consortium of Investigative Journalists (ICIJ) on November 5, 2017. Subsequent data was released on November 20, 2017. Here is a brief video about the leak. The people include Queen Elizabeth II, the President of Columbia (Juan Manuel Santos), Former Prime Minister of Pakistan (Shaukat Aziz), U.S Secretary of Commerce (Wilbur Ross) and many more. According to an estimate by the Boston Consulting Group, the amount of money involved is around $10 trillion. The leak contains many famous companies, including Facebook, Apple, Uber, Nike, Walmart, Allianz, Siemens, McDonald’s and Yahoo.
It also contains a lot of U. S President Donald Trump allies including Rax Tillerson, Wilbur Ross, Koch Brothers, Paul Singer, Sheldon Adelson, Stephen Schwarzman, Thomas Barrack and Steve Wynn etc. The complete list of Politicians involve is available here.
I am calling all data scientists to help me stop the corruption and reveal the patterns and linkages invisible for the untrained eye.
Content
The data is the effort of more than 100 journalists from 60+ countries
The original data is available under creative common license and can be downloaded from this link.
I will keep updating the datasets with more leaks and data as it’s available
Acknowledgements
International Consortium of Investigative Journalists (ICIJ)
Inspiration
Some ideas worth exploring:
How many companies and individuals are there in all of the leaks data

How many countries involved

Total money involved

What is the biggest best tax heaven

Can we compare the corruption with human development index and make an argument that would correlate corruption with bad conditions in that country

Who are the biggest cheaters and where they live

What role Fortune 500 companies play in this game
I need your help to make this world corruption free in the age of NLP and Big Data"
The General Social Survey (GSS),"Longitudinal study of popular beliefs, attitudes, morality & behaviors in the US",NORC.org,16,"Version 2,2017-11-10|Version 1,2016-05-10","ethics
political science",CSV,2 GB,CC0,"5,637 views",818 downloads,3 kernels,0 topics,https://www.kaggle.com/norc/general-social-survey,"The GSS gathers data on contemporary American society in order to monitor and explain trends and constants in attitudes, behaviors, and attributes. Hundreds of trends have been tracked since 1972. In addition, since the GSS adopted questions from earlier surveys, trends can be followed for up to 70 years.
The GSS contains a standard core of demographic, behavioral, and attitudinal questions, plus topics of special interest. Among the topics covered are civil liberties, crime and violence, intergroup tolerance, morality, national spending priorities, psychological well-being, social mobility, and stress and traumatic events.
Altogether the GSS is the single best source for sociological and attitudinal trend data covering the United States. It allows researchers to examine the structure and functioning of society in general as well as the role played by relevant subgroups and to compare the United States to other nations. (Source)
This dataset is a csv version of the Cumulative Data File, a cross-sectional sample of the GSS from 1972-current."
Classification of Handwritten Letters,Images of Russian Letters,Olga Belitskaya,16,"Version 6,2017-12-04|Version 5,2017-11-23|Version 4,2017-11-23|Version 3,2017-11-23|Version 2,2017-11-22|Version 1,2017-11-22","languages
photography
deep learning
+ 2 more...",Other,73 MB,Other,"3,493 views",271 downloads,4 kernels,2 topics,https://www.kaggle.com/olgabelitskaya/classification-of-handwritten-letters,"History
I made the database from my own photos of Russian lowercase letters written by hand.
Content
The GitHub repository with examples
GitHub
The main dataset (letters.zip)
1650 (50x33) color images (32x32x3) with 33 letters and the file with labels letters.txt.
Photo files are in the .png format and the labels are integers and values.
Additional letters.csv file.
The file LetterColorImages.h5 consists of preprocessing images of this set: image tensors and targets (labels)
The additional dataset (letters2.zip)
5940 (180x33) color images (32x32x3) with 33 letters and the file with labels letters2.txt.
Photo files are in the .png format and the labels are integers and values.
Additional letters2.csv file.
The file LetterColorImages2.h5 consists of preprocessing images of this set: image tensors and targets (labels)
Letter Symbols => Letter Labels
а=>1, б=>2, в=>3, г=>4, д=>5, е=>6, ё=>7, ж=>8, з=>9, и=>10, й=>11, к=>12, л=>13, м=>14, н=>15, о=>16, п=>17, р=>18, с=>19, т=>20, у=>21, ф=>22, х=>23, ц=>24, ч=>25, ш=>26, щ=>27, ъ=>28, ы=>29, ь=>30, э=>31, ю=>32, я=>33
Background Images => Background Labels
striped=>0, gridded=>1, no background=>2
Acknowledgements
As an owner of this database, I have published it for absolutely free using by any site visitor.
Usage
Classification, image generation, etc. in a case of handwritten letters with a small number of images are useful exercises.
Improvement
There are lots of ways for increasing this set and the machine learning algorithms applying to it. For example: add the same images but written by other person or add capital letters."
Taekwondo Techniques Classification,Can you determine technique type & intensity of a Taekwondo impact?,Ali Ghafour,16,"Version 1,2017-01-19",sports,CSV,2 MB,CC4,"3,172 views",216 downloads,9 kernels,0 topics,https://www.kaggle.com/ali2020armor/taekwondo-techniques-classification,"Context:
This dataset contains information obtained from an impact sensor within a Taekwondo chest protector. Participants were asked to perform various Taekwondo techniques on this chest protector for analysis of sensor readings.
Content:
Data was obtained from 6 participants performing 4 different Taekwondo techniques – Roundhouse/Round Kick, Back Kick, Cut Kick & Punch. Participant details are summarized in Table 1. The table is organized in ascending order according to participant weight/experience level.
In the file 'Taekwondo_Technique_Classification_Stats.csv’, the data is organized as follows. The rows display:
Technique – Roundhouse/Round Kick (R), Back Kick (B), Cut Kick (C), Punch (P)
Participant ID – P1, P2, P3, P4, P5, P6
Trial # – For each Technique, each participant performed a total of 5 trials
Sensor Readings – Data shows the ADC readings obtained from a 12-bit ADC connected to the sensor (not listed as Voltage but can be converted to it)
The columns identify type of technique, participant, trial # and showcase the sensor readings. There are a total of 115 columns of sensor readings. Each participant performed 5 trials for each type of technique with hard intensity. The only exception is that Participant 6 (P6) did not perform Back Kicks.
Acknowledgements:
The dataset was collected at a local Taekwondo school. We would like to thank the instructor and students for taking their time to participate in our data collection!
Past Research:
Previous work to classify Taekwondo techniques included using a butter-worth low pass filter on Matlab and performing integration around the maximum signal peak. The goal was to observe a pattern in the resulting integration values to determine impact intensity for each type of technique.
Inspiration:
Main analysis questions:
1) Determine impact intensity that is proportional to the participant’s weight/experience level
Ideally, impact intensity should increase with the increasing participant weight/experience level or ID (Participant ID in Table 1 corresponds to ascending weight/experience level)
2) Classify or distinguish between types of impact (Round Kick, Back Kick, Cut Kick or Punch)
Each Taekwondo technique usually has a unique waveform to be identified"
Steven Wilson detector,Finding songs that match Steven Wilson's style,Daniel Grijalva,16,"Version 14,2017-11-27|Version 13,2017-11-24|Version 12,2017-11-22|Version 11,2017-11-22|Version 10,2017-11-22|Version 9,2017-11-18|Version 8,2017-11-17|Version 7,2017-11-16|Version 6,2017-11-15|Version 5,2017-11-15|Version 4,2017-11-15|Version 3,2017-11-15|Version 2,2017-11-14|Version 1,2017-11-14",music,CSV,535 KB,CC0,"1,247 views",33 downloads,4 kernels,,https://www.kaggle.com/danielgrijalvas/steven-wilson-analysis,"Context
I'm going straight to the point: I'm obsessed with Steven Wilson. I can't help it, I love his music. And I need more music with similar (almost identical) style. So, what I'm trying to solve here is, how to find songs that match SW's style with almost zero error?
I'm aware that Spotify gives you recommendations, like similar artists and such. But that's not enough -- Spotify always gives you varied music. Progressive rock is a very broad genre, and I just want those songs that sound very, very similar to Steven Wilson or Porcupine Tree.
BTW, Porcupine Tree was Steven Wilson's band, and they both sound practically the same. I made an analysis where I checked their musical similarities.
Content
I'm using the Spotify web API to get the data. They have an amazingly rich amount of information, especially the audio features.
This repository has 5 datasets:
StevenWilson.csv: contains Steven Wilson discography (65 songs)
PorcupineTree.csv: 65 Porcupine Tree songs
Complete Steven Wilson.csv: a merge between the past two datasets (Steven Wilson + Porcupine Tree)
Train.csv: 200 songs used to train KNN. 100 are Steven Wilson songs and the rest are totally different songs
Test.csv: 100 songs that may or may not be like Steven Wilson's. I picked this songs from various prog rock playlists and my Discover Weekly from Spotify.
Also, so far I've made two kernels:
Comparing Steven Wilson and Porcupine Tree
Finding songs that match SW's style using K-Nearest Neighbors
Data
There are 21 columns in the datasets.
Numerical: this columns were scraped using get_audio_features from the Spotify API.
acousticness: a confidence measure from 0.0 to 1.0 of whether the track is acoustic; 1.0 represents high confidence the track is acoustic
danceability: it describes how suitable a track is for dancing; a value of 0.0 is least danceable and 1.0 is most danceable
duration_ms: the duration of the track in milliseconds
energy: a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity
instrumentalness: predicts whether a track contains no vocals; values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0
liveness: detects the presence of an audience in the recording; 1.0 represents high confidence that the track was performed live
loudness: the overall loudness of a track in decibels (dB)
speechiness: detects the presence of spoken words in a track; the more exclusively speech-like the recording (e.g. talk show), the closer to 1.0 the attribute value
tempo: the overall estimated tempo of a track in beats per minute (BPM)
valence: a measure from 0.0 to 1.0 describing the musical positiveness; tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry)
Categorical: these features are categories represented as numbers.
key: the musical key the track is in. e.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on
mode: mode indicates the modality (major or minor); major is represented by 1 and minor is 0
time_signature: an estimated overall time signature of a track; it is a notational convention to specify how many beats are in each bar (or measure). e.g. 4/4, 4/3, 3/4, 8/4 etc.
Strings: these fields are mostly useless (except for name, album, artist and lyrics)
id: the Spotify ID of the song
name: name of the song
album: album of the song
artist: artist of the song
uri: the Spotify URI of the song
type: the type of the Spotify object
track_href: the Spotify API link of the song
analysis_url: the URL used for getting the audio features
lyrics: lyrics of the song in lower case
Future
Ever been obsessed with a song? an album? an artist? I'm planning on building a web app that solves this. It will help you find music extremely similar to other."
8a.nu Climbing Logbook,Analyze the world's largest rock climbing logbook!,David Cohen,16,"Version 1,2017-09-27","sports
internet",SQLite,445 MB,Other,"7,143 views",196 downloads,2 kernels,,https://www.kaggle.com/dcohen21/8anu-climbing-logbook,"Context
Analyze the world's largest rock climbing logbook!
Who's the biggest downgrader? Is it better to be short, tall, or average height? How many years does it take the average climber to send her first 5.12? After countless crag debates over these and similar topics, I set out to find the answers. Now you can prove statistically why that dude who tagged your multi-year project ""Soft"" is wrong. (Sorry, he might actually be right.)
Content
I used Python3 to build a web-scraper to collect all of the user and ascent information from the world's largest rock climbing logbook, https://www.8a.nu/. I actually ended up scraping their beta site, https://beta.8a.nu/, as it provided well-formed JSON objects. The scraper dumps all of the data into an SQLite database. Check out https://github.com/dcohen21/8a.nu-Scraper for more information about the project. This dataset was collected on 9/13/2017.
The database consists of four tables: User, Ascent, Method, and Grade.
Acknowledgements
Thanks to Andrew Cassidy (https://github.com/andrewcassidy) for the idea and mentorship. Thanks to Jens Larssen and 8a.nu for creating the logbook and maintaining a thriving community.
Inspiration
The sky's the limit!"
Darknet Market Cocaine Listings,How much does cocaine cost on the internet?,David Skipper Everling,16,"Version 1,2017-11-23","crime
illegal drugs
economics
internet",CSV,788 KB,CC4,"4,044 views",250 downloads,2 kernels,,https://www.kaggle.com/everling/cocaine-listings,"The dataset is approximately 1,400 cleaned and standardized product listings from Dream Market's ""Cocaine"" category. It was collected with web-scraping and text extraction techniques in July 2017.
Extracted features for each listing include:
product_title
ships_from_to
quantity in grams
quality
btc_price
vendor details
shipping dummy variables (true/false columns)
For further details on the creation of this dataset and what it contains, see the blog post here: https://medium.com/thought-skipper/dark-market-regression-calculating-the-price-distribution-of-cocaine-from-market-listings-10aeff1e89e0"
Cryptocurrency Historical Data,Daily data of the top 50 cryptocurrencies listed on coinmarketcap,Jack,16,"Version 3,2017-11-24|Version 2,2017-11-20|Version 1,2017-11-18",internet,Other,634 KB,CC0,"7,351 views",625 downloads,2 kernels,2 topics,https://www.kaggle.com/jackml/cryptocurrency-historical-data,"Context
Nowadays everybody is talking about cryptocurrencies and chances are this phenomenon will keep growing in the future. Not only Bitcoin and Ethereum, but also other cryptos are slowly attracting more and more investments. Here I have included all the available historical data of the top 50 cryptocurrencies listed on coinmarketcap.com (which at the time of writing is listing 1296 cryptocurrencies in total). Data is available at 1-day interval from when the crypto was listed on coinmarketcap.com up to 17 November 2017.
Good luck!
Content
As specified above, all the data was retrieved from coinmarketcap.com for the top 50 (at the time of writing) cryptocurrencies. Everytime marketcap was not available (listed as '-', usually during the first days after the launch) has been replaced with a ""0""
Files are organized as follows: Date (dd/mm/yyyy) | Open ($) | High ($) | Low ($) | Close ($) | 24-hrs Volume ($) | MarketCap ($)
This dataset includes (with starting date):
ardor.csv (23/07/2016) ark.csv (22/03/2017) attention-token-of-media.csv (04/10/2017) augur.csv (27/10/2015) basic-attention-token.csv (01/06/2017) binance-coin.csv (25/07/2017) bitcoin-cash.csv (23/07/2017) bitcoin.csv (28/04/2013) bitcoindark.csv (16/07/2014) bitconnect.csv (20/10/2017) bitshares.csv (21/07/2014) byteball.csv (27/12/2017) bytecoin-bcn.csv (17/06/2014) cardano.csv (01/10/2017) dash.csv (14/02/2014) decred.csv (10/02/2016) digixdao.csv (18/04/2016) dogecoin.csv (15/12/2013) eos.csv (01/07/2017) ethereum-classic.csv (24/07/2016) ethereum.csv (07/08/2015) factom.csv (06/10/2015) gas.csv (06/07/2017) golem.csv (19/10/2017) hshare.csv (20/08/2017) iota.csv (13/06/2017) komodo.csv (05/02/2017) kyber-network.csv (24/09/2017) lisk.csv (06/04/2016) litecoin.csv (28/04/2013) maidsafecoin.csv (28/04/2014) monacoin.csv (20/03/2014) monero.csv (21/05/2014) nem.csv (01/04/2015) neo.csv (09/09/2016) omisego.csv (14/07/2017) pivx.csv (13/02/2016) populous.csv (11/07/2017) qtum.csv (24/05/2017) ripple.csv (04/08/2013) salt.csv (29/09/2017) steem.csv (18/04/2016) stellar.csv (05/08/2014) stratis.csv (12/08/2016) tenx.csv (27/06/2017) tether.csv (25/02/2015) veritaseum.csv (08/06/2017) vertcoin.csv (20/01/2014) waves.csv (02/06/2016) zcash.csv (29/10/2016)
I will keep this collection up to date as much as I can. Please let me know if you are interested in additional cryptos.
Acknowledgements and Inspiration
If these files exist is thanks to CoinMarketCap (coinmarketcap.com) which is an awesome database of cryptocurrencies (and makes an awesome homepage).
Many have tried, few have succeeded. Can you predict tomorrow's price? What data patterns do you recognise? Can't wait to see what code/results you have to share! Comment below and please upvote this if you like it."
The Ultimate Halloween Candy Power Ranking,What’s the best Halloween candy?,FiveThirtyEight,16,"Version 1,2017-11-01","food and drink
popular culture",CSV,5 KB,Other,"3,015 views",536 downloads,8 kernels,0 topics,https://www.kaggle.com/fivethirtyeight/the-ultimate-halloween-candy-power-ranking,"Context
What’s the best (or at least the most popular) Halloween candy? That was the question this dataset was collected to answer. Data was collected by creating a website where participants were shown presenting two fun-sized candies and asked to click on the one they would prefer to receive. In total, more than 269 thousand votes were collected from 8,371 different IP addresses.
Content
candy-data.csv includes attributes for each candy along with its ranking. For binary variables, 1 means yes, 0 means no. The data contains the following fields:
chocolate: Does it contain chocolate?
fruity: Is it fruit flavored?
caramel: Is there caramel in the candy?
peanutalmondy: Does it contain peanuts, peanut butter or almonds?
nougat: Does it contain nougat?
crispedricewafer: Does it contain crisped rice, wafers, or a cookie component?
hard: Is it a hard candy?
bar: Is it a candy bar?
pluribus: Is it one of many candies in a bag or box?
sugarpercent: The percentile of sugar it falls under within the data set.
pricepercent: The unit price percentile compared to the rest of the set.
winpercent: The overall win percentage according to 269,000 matchups.
Acknowledgements:
This dataset is Copyright (c) 2014 ESPN Internet Ventures and distributed under an MIT license. Check out the analysis and write-up here: The Ultimate Halloween Candy Power Ranking. Thanks to Walt Hickey for making the data available.
Inspiration:
Which qualities are associated with higher rankings?
What’s the most popular candy? Least popular?
Can you recreate the 538 analysis of this dataset?"
Professional Hockey Database,"Data on hockey players, teams, and coaches from 1909 to 2011",Open Source Sports,16,"Version 1,2016-11-27",ice hockey,CSV,5 MB,Other,"8,881 views","1,250 downloads",10 kernels,0 topics,https://www.kaggle.com/open-source-sports/professional-hockey-database,"The Hockey Database is a collection of historical statistics from men's professional hockey teams in North America.
Note that as of v1, this dataset is missing a few files, due to Kaggle restrictions on the number of individual files that can be uploaded. The missing files will be noted in the description below.
The Data
The dataset contains the following tables (all are csv):
Master: Names and biographical information
Scoring: Scoring statistics
ScoringSup: Supplemental scoring statistics. Missing in v1
ScoringSC: Scoring for Stanley Cup finals, 1917-18 through 1925-26
ScoringShootout: Scoring statistics for shootouts
Goalies: Goaltending statistics
GoaliesSC: Goaltending for Stanley Cup finals, 1917-18 through 1925-26
GoaliesShootout: Goaltending statistics for shootouts
AwardsPlayers: Player awards, trophies, postseason all-star teams
AwardsCoaches: Coaches awards, trophies, postseason all-star teams
AwardsMisc: Miscellaneous awards. Missing in v1
Coaches: Coaching statistics
Teams: Team regular season statistics
TeamsPost: Team postseason statistics
TeamsSC: Team Stanley Cup finals statistics, 1917-18 through 1925-26
TeamsHalf: First half / second half standings, 1917-18 through 1920-21
TeamSplits: Team home/road and monthly splits
TeamVsTeam: Team vs. team results
SeriesPost: Postseason series
CombinedShutouts: List of combined shutouts.
abbrev: Abbreviations used in Teams and SeriesPost tables
HOF: Hall of Fame information
Descriptions of the individual fields in each file can be found in the file's description.
Copyright Notice
The Hockey Databank project allows for free usage of its data, including the production of a commercial product based upon the data, subject to the terms outlined below.
1) In exchange for any usage of data, in whole or in part, you agree to display the following statement prominently and in its entirety on your end product:
""The information used herein was obtained free of charge from and is copyrighted by the Hockey Databank project. For more information about the Hockey Databank project please visit http://sports.groups.yahoo.com/group/hockey-databank""
2) Your usage of the data constitutes your acknowledgment, acceptance, and agreement that the Hockey Databank project makes no guarantees regarding the accuracy of the data supplied, and will not be held responsible for any consequences arising from the use of the information presented.
Acknowledgments
This dataset was downloaded from the hockey database at Open Source Sports. The original acknowledgments are as follows:
A variety of sources were consulted while constructing this database. These are listed below in no particular order.
Books:
National Hockey League Guide (various years)
National Hockey League Official Record Book (1982-83 and 1983-84)
National Hockey League Official Guide & Record Book (1984-85 to present)
The Stanley Cup Records and Statistics (various years)
World Hockey Association Media Guide (various years)
WHA Schedule & Statistics (1974-75)
The Sporting News Hockey Guide (various years)
Official NHL Record Book 1917-64
The Complete Historical and Statistical Reference to the World Hockey Association 1972-1979, by Scott Surgent; Xaler Press (7th edition, 2004; 8th edition, 2008)
Total Hockey; Total Sports Publishing (1st edition, 1998; 2nd edition, 2000)
The Encyclopedia of Hockey, by Robert A. Styer; A.S. Barnes (2nd edition, 1973)
The Hockey Encyclopedia, by Stan Fischler and Shirley Walton Fischler; Macmillan (1983)
The Trail of the Stanley Cup (Vol. 1, 2, and 3), by Charles L. Coleman
Periodicals:
The Sporting News
On-line sources:
ESPN.com: http://www.espn.com/nhl/statistics
Find A Grave: http://www.findagrave.com
The Goaltender Home Page (Doug Norris): http://hockeygoalies.org
History Of NHL Trades: http://nhltradeshistory.blogspot.com
Hockey Research Association: http://www.hockeyresearch.com/stats
Hockey-Reference.com (Justin Kubatko): http://www.hockey-reference.com
Hockey Summary Project: http://sports.groups.yahoo.com/group/hockey_summary_project/, http://hsp.flyershistory.com (previously at http://www.shrpsports.com/hsp)
Internet Hockey Database (Ralph Slate): http://www.hockeydb.com
Legends of Hockey.net (Hockey Hall of Fame): http://www.legendsofhockey.net/html/search.htm
LostHockey.com: http://www.losthockey.com
National Hockey League: http://www.nhl.com
NHL Hockey Shootout Statistics: http://jeays.net/shootout/index.htm
NHL Shootouts: http://www.nhlshootouts.com
North American Pro Hockey: http://www.ottawavalleyonline.com/sites/tomking_01/index.html
Puckerings: http://www.puckerings.com
Society for International Hockey Research: http://www.sihrhockey.org
The Sports Network: http://www.sportsnetwork.com
USA Today hockey stats archive: http://www.usatoday.com/sports/hockey, http://www.usatoday.com/sports/hockey/archive.htm
Yahoo Sports: http://sports.yahoo.com/nhl
Thanks to the following individuals:
Ralph Dinger (NHL Publishing / Dan Diamond and Associates) has confirmed a number of corrections to errors found in the NHL's official statistics. Thanks also to Justin Kubatko of hockey-reference.com for a number of discussions in this area.
Morey Holzman provided information on Lloyd Cook's 1921-22 goaltending appearance.
Stu McMurray provided correct 1917-18 scoring statistics, including GWG.
Doug Norris provided corrected 1984-85 statistics for Rick St. Croix.
Paul Reeths created the Hall of Fame table, and provided updates for the Coaches table
Other contributors include Roger Brewer, Mike Burton, Eric Hornick, and Claude Paradis.
An acknowledgement is also given to the team led by Sean Forman and Sean Lahman that has developed and maintained the Lahman baseball database. This database follows the same general design."
City Payroll Data,Payroll information for all Los Angeles city departments since 2013,City of Los Angeles,16,"Version 1,2016-11-27","cities
income",CSV,89 MB,Other,"4,296 views",568 downloads,15 kernels,0 topics,https://www.kaggle.com/cityofLA/city-payroll-data,"Context
The Los Angeles City Controller Office releases payroll information for all city employees on a quarterly basis since 2013.
Content
Data includes department titles, job titles, projected annual salaries (with breakdowns of quarterly pay), bonuses, and benefits information.
Inspiration
How do benefits and salaries differ for employees across departments and titles? Are there any unusually large differences between lowest and highest employee salaries?
How have salaries changed over the past three years?
Have the costs of benefits changed dramatically since the passing of the Affordable Care Act?
What is the most common government role in Los Angeles?"
California Crime and Law Enforcement,Crime and law enforcement employment data from 2015,Federal Bureau of Investigation,16,"Version 1,2016-12-08","crime
law",CSV,98 KB,Other,"4,221 views",602 downloads,14 kernels,0 topics,https://www.kaggle.com/fbi-us/california-crime,"Context
The Uniform Crime Reporting (UCR) Program has been the starting place for law enforcement executives, students of criminal justice, researchers, members of the media, and the public at large seeking information on crime in the nation. The program was conceived in 1929 by the International Association of Chiefs of Police to meet the need for reliable uniform crime statistics for the nation. In 1930, the FBI was tasked with collecting, publishing, and archiving those statistics.
Today, four annual publications, Crime in the United States, National Incident-Based Reporting System, Law Enforcement Officers Killed and Assaulted, and Hate Crime Statistics are produced from data received from over 18,000 city, university/college, county, state, tribal, and federal law enforcement agencies voluntarily participating in the program. The crime data are submitted either through a state UCR Program or directly to the FBI’s UCR Program.
This dataset focuses on the crime rates and law enforcement employment data in the state of California.
Content
Crime and law enforcement employment rates are separated into individual files, focusing on offenses by enforcement agency, college/university campus, county, and city. Categories of crimes reported include violent crime, murder and nonnegligent manslaughter, rape, robbery, aggravated assault, property crime, burglary, larceny-theft, motor vehicle damage, and arson. In the case of rape, data is collected for both revised and legacy definitions. In some cases, a small number of enforcement agencies switched definition collection sometime within the same year.
Acknowledgements
This dataset originates from the FBI UCR project, and the complete dataset for all 2015 crime reports can be found here.
Inspiration
What are the most common types of crimes in California? Are there certain crimes that are more common in a particular place category, such as a college/university campus, compared to the rest of the state?
How does the number of law enforcement officers compare to the crime rates of a particular area? Is the ratio similar throughout the state, or do certain campuses, counties, or cities have a differing rate?
How does the legacy vs. refined definition of rape differ, and how do the rape counts compare? If you pulled the same data from FBI datasets for previous years, can you see a difference in rape rates over time?"
Roman emperors from 26 BC to 395 AD,"Life, death and reign of Roman emperors",LaurentBerder,15,"Version 2,2017-10-19|Version 1,2017-10-18","life
death
politicians
+ 2 more...",CSV,25 KB,ODbL,"1,820 views",157 downloads,,,https://www.kaggle.com/lberder/roman-emperors-from-26-bc-to-395-ad,"Context
We all know of the Roman empire, but what about its emperors specifically?
Content
Here, you will find information on each of the emperors of the Roman empire, which lasted between 26 BC and 395 AD. Specifically, you can use data on their:
Names
Date of birth
City and Province of birth
Date of death
Method of accession to power
Date of accession to power
Date of end of reign
Cause of death
Identity of killer
Dynasty
Era
Photo
Acknowledgements
This dataset was provided by Zonination, who made it available on Wikipedia. See his repository on Github
Inspiration
What kind of trend can you find in these emperors' lives and reigns? What aspects of them allowed them to live longer?"
Path of exile game statistic,"Statistic of 59000 players, lets analyze it!",Ggzet,15,"Version 1,2017-10-27",video games,CSV,9 MB,Other,"1,616 views",106 downloads,4 kernels,2 topics,https://www.kaggle.com/gagazet/path-of-exile-league-statistic,"Path of Exile League statistic
Data contains stats of 59000 players, from 4th August of 2017 and before now.
Content
One file with 12 data sections. One league - Harbinger, but 4 different types of divisions:
Harbinger
Hardcore Harbinger
SSF Harbinger
SSF Harbinger HC
Each division has own ladder with leaders
Acknowledgements
I found this data at the pathofstats.com as a JSON format and exported at CVS. Data have been collecting by this API, and free to use for interested people. If GGG or pathofstats.com don't want to share it here, please contact me and it will be removed.
As I could understand, it is simple data for people, who are new at data science and want to have practice.
Questions for participants
A total number of players in each division, usage of each class in descending order.
Some of the players streaming their game (twitch colum). Do they play better than people, who does not?
Predict chance to be at top 30 in each division, if we are Necromancer. With and without stream.
Average number of finished challanges for each division, show division with highest and lowest average challanges.
Show dependency between level and class of died characters. Only for HC divisions."
A Year of Pumpkin Prices,Pumpkin Prices in 13 US Cities: 2016-2017,US Department of Agriculture,15,"Version 2,2017-10-25|Version 1,2017-10-12","food and drink
united states
finance
agriculture",CSV,184 KB,CC0,"3,987 views",618 downloads,6 kernels,,https://www.kaggle.com/usda/a-year-of-pumpkin-prices,"Context:
Over 1.5 billions pounds of pumpkin are grown annually in the United States. Where are they sold, and for how much?
This dataset contains prices for which pumpkins were sold at selected U.S. cities’ terminal markets. Prices are differentiated by the commodities’ growing origin, variety, size, package and grade.
Content:
This dataset contains terminal market prices for different pumpkin crops in 13 cities in the United States from September 24, 2016 to September 30, 2017. In keeping with the structure of the original source data, information on each city has been uploaded as a separate file.
Atlanta, GA
Baltimore, MD
Boston, MA
Chicago, IL
Columbia, SC
Dallas, TX
Detroit, MI
Los Angeles, CA
Miami, FL
New York, NY
Philadelphia, PA
San Francisco, CA
Saint Louis, MO
Data for each city includes the following columns (although not all information is available for every city)
Commodity Name: Always pumpkin, since this is a pumpkin-only dataset
City Name: City where the pumpkin was sold
Type
Package
Variety
Sub Variety
Grade: In the US, usually only canned pumpkin is graded
Date: Date of sale (rounded up to the nearest Saturday)
Low Price
High Price
Mostly Low
Mostly High
Origin: Where the pumpkins were grown
Origin District
Item Size
Color
Environment
Unit of Sale
Quality
Condition
Appearance
Storage
Crop
Repack: Whether the pumpkin has been repackaged before sale
Trans Mode
Acknowledgements:
This dataset is based on Specialty Crops Terminal Markets Standard Reports distributed by the United States Department of Agriculture. Up-to-date reports can be generated here. This data is in the public domain.
Inspiration:
Which states produce the most pumpkin?
Where are pumpkin prices highest?
How does pumpkin size relate to price?
Which pumpkin variety is the most expensive? Least expensive?"
Crimes Committed in France,Monthly counts of crimes committed since 2000,Government of France,15,"Version 1,2017-09-30","europe
crime",CSV,96 KB,ODbL,"2,469 views",375 downloads,,0 topics,https://www.kaggle.com/government-of-france/crimes-in-france,"Context
This dataset is an aggregated count of all crimes committed in France, broken down by month and category.
Content
This data was aggregated by the French national government and published online on the French Open Data Portal. It is a combination of records kept by both local and national police forces. It's important to note that the name of the categories of crime are in French!
Acknowledgements
This data is a part of a larger group of Excel files published by the French Goverment on the French Open Data Portal. It has been converted to a single CSV file before uploading here.
Inspiration
This is a simple time series dataset that can be probed for trends in the underlying types of crimes committed. Is petty theft more or less popular today than it was ten years ago? How much variation is there in the amount of robberies year-to-year? Can you normalize the growth in the number of crimes against the growth in the number of people? How do crimes committed here differ from those committed in, say, Los Angeles?"
Sample Sales Data,"Denormalize Sales Data : Segmentation, Clustering, Shipping, etc.",Gus Segura,15,"Version 1,2016-11-24",,CSV,516 KB,CC0,"16,660 views","2,006 downloads",6 kernels,3 topics,https://www.kaggle.com/kyanyoga/sample-sales-data,"Sample Sales Data, Order Info, Sales, Customer, Shipping, etc., Used for Segmentation, Customer Analytics, Clustering and More. Inspired for retail analytics. This was originally used for Pentaho DI Kettle, But I found the set could be useful for Sales Simulation training.
Originally Written by María Carina Roldán, Pentaho Community Member, BI consultant (Assert Solutions), Argentina. This work is licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported License. Modified by Gus Segura June 2014."
Pricing Model,Used Car Loan Portfolio for past one year,Sumit,15,"Version 1,2016-08-19",,Other,4 MB,Other,"6,511 views",717 downloads,9 kernels,2 topics,https://www.kaggle.com/sumit9/pricing-model,"Build the proposed optimized pricing model: Determine the largest or key value driver from the data Build price segments using product characteristics, distribution channel, behavior and demographic customer characteristics. Proposed new Pricing strategy bases of customer segmentation. Identify segments where we can lower the rates and as well as highlight segments where it is underpriced without impacting the profitability"
"US Unemployment Rate by County, 1990-2016",Thanks to the US Department of Labor's Bureau of Labor Statistics,Jay Ravaliya,15,"Version 2,2017-05-23|Version 1,2017-03-07",employment,CSV,77 MB,CC0,"5,983 views",796 downloads,10 kernels,3 topics,https://www.kaggle.com/jayrav13/unemployment-by-county-us,"Context
This is a dataset that I built by scraping the United States Department of Labor's Bureau of Labor Statistics. I was looking for county-level unemployment data and realized that there was a data source for this, but the data set itself hadn't existed yet, so I decided to write a scraper and build it out myself.
Content
This data represents the Local Area Unemployment Statistics from 1990-2016, broken down by state and month. The data itself is pulled from this mapping site:
https://data.bls.gov/map/MapToolServlet?survey=la&map=county&seasonal=u
Further, the ever-evolving and ever-improving codebase that pulled this data is available here:
https://github.com/jayrav13/bls_local_area_unemployment
Acknowledgements
Of course, a huge shoutout to bls.gov and their open and transparent data. I've certainly been inspired to dive into US-related data recently and having this data open further enables my curiosities.
Inspiration
I was excited about building this data set out because I was pretty sure something similar didn't exist - curious to see what folks can do with it once they run with it! A curious question I had was surrounding Unemployment vs 2016 Presidential Election outcome down to the county level. A comparison can probably lead to interesting questions and discoveries such as trends in local elections that led to their most recent election outcome, etc.
Next Steps
Version 1 of this is as a massive JSON blob, normalized by year / month / state. I intend to transform this into a CSV in the future as well."
NBA Free Throws,Over 600k free throws between 2006 and 2016,Sebastian Mantey,15,"Version 1,2017-01-05",basketball,CSV,72 MB,Other,"5,858 views",676 downloads,22 kernels,0 topics,https://www.kaggle.com/sebastianmantey/nba-free-throws,"Context
The data set includes information when the free throw was taken during the game, who took the shot and if it went in or not.
Content
The data was scraped from ESPN.com. One example site is: http://www.espn.com/nba/playbyplay?gameId=261229030"
Weather Madrid 1997 - 2015,"Location: Barajas Airport, Madrid. Data: The Weather Company, LLC",Julian Simon de Castro,15,"Version 1,2016-09-06","climate
history",CSV,517 KB,Other,"6,722 views",907 downloads,15 kernels,0 topics,https://www.kaggle.com/juliansimon/weather_madrid_lemd_1997_2015.csv,"Weather data Barajas Airport, Madrid, between 1997 and 2015. Gathered web https://www.wunderground.com/ The Weather Company, LLC
Fields:
Max TemperatureC
Mean TemperatureC
Min TemperatureC
Dew PointC
MeanDew PointC
Min DewpointC
Max Humidity
Mean Humidity
Min Humidity
Max Sea Level PressurehPa
Mean Sea Level PressurehPa
Min Sea Level PressurehPa
Max VisibilityKm
Mean VisibilityKm
Min VisibilitykM
Max Wind SpeedKm/h
Mean Wind SpeedKm/h
Max Gust SpeedKm/h
Precipitationmm
CloudCover
Events
WindDirDegrees
Script for download the data:
#!/bin/bash

LOCAL='weather_madrid'
STATION='LEMD'
INI=1997
END=2015
FILE=${LOCAL}_${STATION}_${INI}_${END}.csv
SITE='airport'

echo ""CET,Max TemperatureC,Mean TemperatureC,Min TemperatureC,Dew PointC,MeanDew PointC,Min DewpointC,Max Humidity, Mean Humidity, Min Humidity, Max Sea Level PressurehPa, Mean Sea Level PressurehPa, Min Sea Level PressurehPa, Max VisibilityKm, Mean VisibilityKm, Min VisibilitykM, Max Wind SpeedKm/h, Mean Wind SpeedKm/h, Max Gust SpeedKm/h,Precipitationmm, CloudCover, Events,WindDirDegrees"" > ${FILE}

for YEAR in $(seq ${INI} ${END})
do
   echo ""Year $YEAR""
   wget ""https://www.wunderground.com/history/${SITE}/${STATION}/${YEAR}/1/1/CustomHistory.html?dayend=31&monthend=12&yearend=${YEAR}&req_city=&req_state=&req_statename=&reqdb.zip=&reqdb.magic=&reqdb.wmo=&format=1"" -O ""${LOCAL}_${YEAR}.csv""
   tail -n +3 ${LOCAL}_${YEAR}.csv > ${LOCAL}_${YEAR}_1.csv
   sed 's/<br\ \/>//g' ${LOCAL}_${YEAR}_1.csv >> ${FILE}
   rm ${LOCAL}_${YEAR}.csv ${LOCAL}_${YEAR}_1.csv
done"
World's Highest Mountains,Wikipedia's list of world's highest mountains,Alberto Barradas,15,,mountains,CSV,13 KB,Other,,,,,https://www.kaggle.com/abcsds/highest-mountains,
Quarterback Stats from 1996 - 2016,Over 5000 Regular Season Games,James Littiebrant,15,"Version 2,2017-01-12|Version 1,2017-01-11","american football
sports",CSV,1 MB,ODbL,"4,238 views",696 downloads,13 kernels,,https://www.kaggle.com/speckledpingu/nfl-qb-stats,"Why?
The NFL, ESPN, and many others have their own Quarterback rating system. Can you create your own? How many points does a QB contribute to a given game? And, with MVP trophy season coming up, who really stands out as an MVP and who is carried by their team?
QB Stats
This is scraped from footballdb.com using Pandas' read_html function. This dataset contains every regular season NFL game and every NFL passer (including non-quarterbacks) from 1996 to 2016. Individual years are available for the past 10 years, and all 21 years are in QBStats_all. In addition to the traditional stats, the total points from the game have been appended to the stats. Win/Loss is up and coming, but is not a priority at the moment since a QB cannot control how well the defense stops the opposing offense.
Content
Inside you'll find:
Quarterback Name (qb)
Attempts (att)
Completions (cmp)
Yards (yds)
Yards per Attempt (ypa)
Touchdowns (td)
Interceptions (int)
Longest Throw (lg)
Sacks (sack)
Loss of Yards (loss)
The NFL's Quarterback Rating for the game (rate)
Total points scored in the game (game_points)
Home or Away Game (home_away)
Year (year)
Important Note:
Because of the way that these were scraped, the the game week is not supplied. However, the games are all in oldest to most recent which would allow for some time-series analysis.
Additionally:
Feel free to make any requests for additional information. But due to the time that it takes to scrape 21 years of NFL stats, it will likely take a while before I finish updating the dataset.
Acknowledgements
I would very much like to thank footballdb.com for not blacklisting me after numerous scrapes and potential future scrapes for information on other positions."
Solar Flares from RHESSI Mission,Reuven Ramaty High Energy Solar Spectroscopic Imager,Kheirallah Samaha,15,"Version 1,2017-02-09","astronomy
space",CSV,10 MB,CC0,"3,146 views",231 downloads,13 kernels,,https://www.kaggle.com/khsamaha/solar-flares-rhessi,"Context
Reuven Ramaty High Energy Solar Spectroscopic Imager (RHESSI, originally High Energy Solar Spectroscopic Imager or HESSI) is a NASA solar flare observatory. It is the sixth mission in the Small Explorer program, selected in October 1997 and launched on 5 February 2002. Its primary mission is to explore the physics of particle acceleration and energy release in solar flares. HESSI was renamed to RHESSI on 29 March 2002 in honor of Reuven Ramaty, a pioneer in the area of high energy solar physics. RHESSI is the first space mission named after a NASA scientist. RHESSI was built by Spectrum Astro for Goddard Space Flight Center and is operated by the Space Sciences Laboratory in Berkeley, California. The principal investigator from 2002 to 2012 was Robert Lin, who was succeeded by Säm Krucker.
useful links: https://en.wikipedia.org/wiki/Reuven_Ramaty_High_Energy_Solar_Spectroscopic_Imager https://hesperia.gsfc.nasa.gov/hessi/objectives.htm
Content
Ramaty High Energy Solar Spectroscopic Imager (RHESSI)
Notes: Note that only events with non-zero position and energy range not equal to 3-6 keV are confirmed as solar sources. Events which have no position and show up mostly in the front detectors, but were not able to be imaged are flagged as ""PS"".
Events which do not have valid position are only confirmed to be non-solar if the NS flag is set.
Peak Rate: peak counts/second in energy range 6-12 keV, averaged over active collimators, including background.
Total Counts: counts in energy range 6-12 keV integrated over duration of flare summed over all subcollimators, including background.
Energy: the highest energy band in which the flare was observed. Electron Kev (kilo electron volt) https://en.wikipedia.org/wiki/Electronvolt
Radial Distance: distance from Sun center
Quality Codes: Qn, where n is the total number of data gap, SAA, particle, eclipse or decimation flags set for event. n ranges from 0 to 11. Use care when analyzing the data when the quality is not zero.
Active_Region: A number for the closest active region, if available
radial_offset: the offset of the flare position from the spin axis of the spacecraft in arcsec. This is used i spectroscopy.
peak_c/s: peak count rate in corrected counts.
Flare Flag Codes: a0 - In attenuator state 0 (None) sometime during flare a1 - In attenuator state 1 (Thin) sometime during flare a2 - In attenuator state 2 (Thick) sometime during flare a3 - In attenuator state 3 (Both) sometime during flare An - Attenuator state (0=None, 1=Thin, 2=Thick, 3=Both) at peak of flare DF - Front segment counts were decimated sometime during flare DR - Rear segment counts were decimated sometime during flare ED - Spacecraft eclipse (night) sometime during flare EE - Flare ended in spacecraft eclipse (night) ES - Flare started in spacecraft eclipse (night) FE - Flare ongoing at end of file FR - In Fast Rate Mode FS - Flare ongoing at start of file GD - Data gap during flare GE - Flare ended in data gap GS - Flare started in data gap MR - Spacecraft in high-latitude zone during flare NS - Non-solar event PE - Particle event: Particles are present PS - Possible Solar Flare; in front detectors, but no position Pn - Position Quality: P0 = Position is NOT valid, P1 = Position is valid Qn - Data Quality: Q0 = Highest Quality, Q11 = Lowest Quality SD - Spacecraft was in SAA sometime during flare SE - Flare ended when spacecraft was in SAA SS - Flare started when spacecraft was in SAA
Acknowledgements
What is a solar flare?
A Solar flare is the rapid release of a large amount of energy stored in the solar atmosphere. During a flare, gas is heated to 10 to 20 million degrees Kelvin (K) and radiates soft X rays and longer-wavelength emission. Unable to penetrate the Earth's atmosphere, the X rays can only be detected from space. Instruments on Skylab, SMM, the Japanese/US Yohkoh mission and other spacecraft have recorded many flares in X rays over the last twenty years or so. Ground-based observatories have recorded the visible and radio outputs. These data form the basis of our current understanding of a solar flare. But there are many possible mechanisms for heating the gas, and observations to date have not been able to differentiate between them.
HESSI's new approach
Researchers believe that much of the energy released during a flare is used to accelerate, to very high energies, electrons (emitting primarily X-rays) and protons and other ions (emitting primarily gamma rays). The new approach of the HESSI mission is to combine, for the first time, high-resolution imaging in hard X-rays and gamma rays with high-resolution spectroscopy, so that a detailed energy spectrum can be obtained at each point of the image.
This new approach will enable researchers to find out where these particles are accelerated and to what energies. Such information will advance understanding of the fundamental high-energy processes at the core of the solar flare problem. https://hesperia.gsfc.nasa.gov/hessi/objectives.htm
Inspiration
Explore,
Know something new,
Predict the solar flare,
Respect the Sun and value it and
Take care of the environments.
Thanks"
No Data Sources,The home of Kaggle kernels that do not use data,Kaggle,15,"Version 1,2017-04-13",,CSV,1 B,Other,"4,058 views",62 downloads,,,https://www.kaggle.com/kaggle/no-data-sources,"This isn't a dataset, it is a collection of kernels written on Kaggle that use no data at all."
2015 American Community Survey,Detailed information about the American people and workforce,US Census Bureau,15,"Version 3,2017-02-04|Version 2,2017-01-31|Version 1,2017-01-31","employment
demographics
sociology",Other,4 GB,CC0,"4,427 views",648 downloads,21 kernels,0 topics,https://www.kaggle.com/census/2015-american-community-survey,"The 2015 American Community Survey Public Use Microdata Sample
Context
The American Community Survey (ACS) is an ongoing survey that provides vital information on a yearly basis about our nation and its people. Information from the survey generates data that help determine how more than $400 billion in federal and state funds are distributed each year.
Frequency: Annual
Period: 2015
PWGTP (Weights)
Please note. Each record is weighted with PWGTP. For accurate analysis, these weights need to be applied. Reference Getting Started 'Python' for a simple kernel on how this field gets used. Or, click on the image below to see how this can be done in R (see code in this kernel).
The Data Dictionary can be found here, but you'll need to scroll down to the PERSON RECORD section.
Content
Through the ACS, we know more about jobs and occupations, educational attainment, veterans, whether people own or rent their home, and other topics. Public officials, planners, and entrepreneurs use this information to assess the past and plan the future. When you respond to the ACS, you are doing your part to help your community plan hospitals and schools, support school lunch programs, improve emergency services, build bridges, and inform businesses looking to add jobs and expand to new markets, and more. The data dictionary can be found here.
Inspiration
Kernels created using the 2014 ACS and 2013 ACS can serve as excellent starting points for working with the 2015 ACS. For example, the following analyses were created using ACS data:
Work arrival times and earnings in the USA
Inequality in STEM careers
Acknowledgements
The American Community Survey (ACS) is administered, processed, researched and disseminated by the U.S. Census Bureau within the U.S. Department of Commerce."
AI2 Science Questions,"2,707 multiple choice science questions from student assessments",Allen Institute for Artificial Intelligence,15,"Version 2,2016-12-14|Version 1,2016-12-13","linguistics
artificial intelligence",CSV,58 MB,CC4,"4,752 views",323 downloads,3 kernels,0 topics,https://www.kaggle.com/allenai/ai2-science-questions,"Context
Project Aristo at the Allen Institute for Artificial Intelligence (AI2) is focused on building a system that acquires and stores a vast amount of knowledge in computable form, then applies this knowledge to answer a variety of science questions from standardized exams for students in multiple grade levels. We are inviting the wider AI research community to work on this grand challenge with us by providing this dataset of student science assessment questions.
Content
These are English language questions that span several grade levels as indicated in the files. Each question is a 4-way multiple choice structure. Some of these questions include a diagram, either as part of the question text, as an answer option, or both. The diagrams are represented in the text with filenames that correspond to the diagram file itself in the companion folder. These questions come pre-split into Train, Development, and Test sets.
The data set includes the following fields:
questionID: a unique identifier for the question
originalQuestionID: the question number on the test
totalPossiblePoints: how many points the question is worth
AnswerKey: the correct answer option
isMultipleChoiceQuestion: 1 = multiple choice, 0 = other
includesDiagram: 1 = includes diagram, 0 = other
examName: the source of the exam
schoolGrade: grade level
year: year the source exam was published
question: the question itself
subject: Science
category: Test, Train, or Dev (data comes pre-split into these categories)
Evaluation
AI2 has made available Aristo mini, a light-weight question answering system that can quickly evaluate science questions with an evaluation web server and provided baseline solvers. You can extend the provided solvers with your own implementation to try out new approaches and compare results.
Acknowledgements
The Aristo project team at AI2 compiled this dataset and we use it actively in our research. For a description of the motivations and intention for this data, please see:
Clark, Peter. “Elementary School Science and Math Tests as a Driver for AI: Take the Aristo Challenge!” AAAI (2015)."
Eye Gaze,Simulated and Real datasets of eyes looking in different directions,4Quant,15,"Version 2,2017-05-02|Version 1,2017-05-02",psychometrics,Other,668 MB,CC4,"7,331 views",700 downloads,21 kernels,0 topics,https://www.kaggle.com/4quant/eye-gaze,"Context
The main reason for making this dataset is the publication of the paper: Learning from Simulated and Unsupervised Images through Adversarial Training and the idea of the SimGAN. The dataset and kernels should make it easier to get started making SimGAN networks and testing them out and comparing them to other approaches like KNN, GAN, InfoGAN and the like.
Source
The synthetic images were generated with the windows version of UnityEyes http://www.cl.cam.ac.uk/research/rainbow/projects/unityeyes/tutorial.html
The real images were taken from https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/gaze-based-human-computer-interaction/appearance-based-gaze-estimation-in-the-wild-mpiigaze/, which can be cited like this: Appearance-based Gaze Estimation in the Wild, X. Zhang, Y. Sugano, M. Fritz and A. Bulling, Proc. of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), June, p.4511-4520, (2015).
Challenges
Enhancement
One of the challenges (as covered in the paper) is enhancing the simulated images by using the real images. One possible approach is using the SimGAN which is implemented for reference in one of the notebooks. There are a number of other approaches (pix2pix, CycleGAN) which could have interesting results.
Gaze Detection
The synthetic dataset has the gaze information since it was generated by UnityEyes with a predefined look-vector. The overview notebook covers what this vector means and how each component can be interpreted. It would be very useful to have a simple, quick network for automatically generating this look vector from an image"
Congress Trump Score,How often do congresspeople vote with or against Trump?,FiveThirtyEight,15,"Version 4,2017-06-26|Version 3,2017-04-14|Version 2,2017-04-14|Version 1,2017-03-18",politics,CSV,2 MB,Other,"4,866 views",311 downloads,5 kernels,3 topics,https://www.kaggle.com/fivethirtyeight/trump-score,"Context
This is FiveThirtyEight's Congress Trump Score. As the website itself puts it, it's ""an updating tally of how often every member of the House and the Senate votes with or against the president"".
Content
There are two tables: cts and votes. The first one has summary information for every congressperson: their name, their state, their Trump Score, Trump's share of the votes in the election, etc. The second one has information about every vote each congressperson has cast: their vote, Trump's position on the issue, etc.
The data was extracted using R. The code is available as a package on github.
Acknowledgements
The data is 100% collected and maintained by FiveThirtyEight. They are awesome."
United States Droughts by County,Weekly data on extent and severity of drought in each US county (2000-present),United States Drought Monitor,15,"Version 1,2016-11-08","geography
climate",CSV,251 MB,CC0,"4,481 views",576 downloads,11 kernels,0 topics,https://www.kaggle.com/us-drought-monitor/united-states-droughts-by-county,"The United States Drought Monitor collects weekly data on drought conditions around the U.S.
Acknowledgements
All data was downloaded from the United States Drought Monitor webpage.
The U.S. Drought Monitor is jointly produced by the National Drought Mitigation Center at the University of Nebraska-Lincoln, the United States Department of Agriculture, and the National Oceanic and Atmospheric Administration. Map courtesy of NDMC-UNL.
The Data
The data contains weekly observations about the extent and severity of drought in each county of the United States. The dataset contains the following fields:
releaseDate: when this data was released on the USDM website
FIPS: the FIPS code for this county
county: the county name
state: the state the county is in
NONE: percentage of the county that is not in drought
D0: percentage of the county that is in abnormally dry conditions
D1: percentage of the county that is in moderate drought
D2: percentage of the county that is in severe drought
D3: percentage of the county that is in extreme drought
D4: percentage of the county that is in exceptional drought
validStart: the starting date of the week that these observations represent
validEnd: the ending date of the week that these observations represent
domStatisticFormatID: seems to always be 1
Note: the drought categories are cumulative: if an area is in D3, then it is also in D2, D1, and D0. This means that, for every observation, D4 <= D3 <= D2 <= D1 <= D0.
County Info
To make some analyses slightly easier, I've also included *county_info_2016.csv*, which contains physical size information about each county. This file contains the following fields:
USPS: United States Postal Service State Abbreviation
GEOID: FIPS code
ANSICODE: American National Standards Institute code
NAME: Name
ALAND: Land Area (square meters) - Created for statistical purposes only
AWATER: Water Area (square meters) - Created for statistical purposes only
ALAND_SQMI: Land Area (square miles) - Created for statistical purposes only
AWATER_SQMI: Water Area (square miles) - Created for statistical purposes only
INTPTLAT: Latitude (decimal degrees) First character is blank or ""-"" denoting North or South latitude respectively
INTPTLONG: Longitude (decimal degrees) First character is blank or ""-"" denoting East or West longitude respectively"
Baseball Databank,"Data on baseball players, teams, and games from 1871 to 2015",Open Source Sports,15,"Version 1,2016-11-14",baseball,CSV,24 MB,Other,"5,934 views","1,043 downloads",6 kernels,0 topics,https://www.kaggle.com/open-source-sports/baseball-databank,"Baseball Databank is a compilation of historical baseball data in a convenient, tidy format, distributed under Open Data terms.
This version of the Baseball databank was downloaded from Sean Lahman's website.
Note that as of v1, this dataset is missing a few tables because of a restriction on the number of individual files that can be added. This is in the process of being fixed. The missing tables are Parks, HomeGames, CollegePlaying, Schools, Appearances, and FieldingPost.
The Data
The design follows these general principles. Each player is assigned a unique number (playerID). All of the information relating to that player is tagged with his playerID. The playerIDs are linked to names and birthdates in the MASTER table.
The database is comprised of the following main tables:
MASTER - Player names, DOB, and biographical info
Batting - batting statistics
Pitching - pitching statistics
Fielding - fielding statistics
It is supplemented by these tables:
AllStarFull - All-Star appearances
HallofFame - Hall of Fame voting data
Managers - managerial statistics
Teams - yearly stats and standings
BattingPost - post-season batting statistics
PitchingPost - post-season pitching statistics
TeamFranchises - franchise information
FieldingOF - outfield position data
FieldingPost- post-season fielding data
ManagersHalf - split season data for managers
TeamsHalf - split season data for teams
Salaries - player salary data
SeriesPost - post-season series information
AwardsManagers - awards won by managers
AwardsPlayers - awards won by players
AwardsShareManagers - award voting for manager awards
AwardsSharePlayers - award voting for player awards
Appearances - details on the positions a player appeared at
Schools - list of colleges that players attended
CollegePlaying - list of players and the colleges they attended
Descriptions of each of these tables can be found attached to their associated files, below.
Acknowledgments
This work is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License. For details see: http://creativecommons.org/licenses/by-sa/3.0/
Person identification and demographics data are provided by Chadwick Baseball Bureau (http://www.chadwick-bureau.com), from its Register of baseball personnel.
Player performance data for 1871 through 2014 is based on the Lahman Baseball Database, version 2015-01-24, which is Copyright (C) 1996-2015 by Sean Lahman.
The tables Parks.csv and HomeGames.csv are based on the game logs and park code table published by Retrosheet. This information is available free of charge from and is copyrighted by Retrosheet. Interested parties may contact Retrosheet at http://www.retrosheet.org."
Online Auctions Dataset,Modeling Online Auctions Dataset from eBay,Modeling Online Auctions,15,"Version 1,2016-11-13","business
internet",CSV,1 MB,Other,"9,146 views",908 downloads,22 kernels,0 topics,https://www.kaggle.com/onlineauctions/online-auctions-dataset,"Context
The datasets are from a companion website for the book Modeling Online Auctions, by Wolfgang Jank and Galit Shmueli (Wiley and Sons, ISBN: 978-0-470-47565-2, July 2010).
Content
The datasets contain eBay auction information on Cartier wristwatches, Palm Pilot M515 PDAs, Xbox game consoles, and Swarowski beads.
auction.csv includes 9 variables:
auctionid: unique identifier of an auction
bid: the proxy bid placed by a bidder
bidtime: the time in days that the bid was placed, from the start of the auction
bidder: eBay username of the bidder
bidderrate: eBay feedback rating of the bidder
openbid: the opening bid set by the seller
price: the closing price that the item sold for (equivalent to the second highest bid + an increment)
item: auction item
auction_type
swarovski.csv includes 5 variables:
Seller
Bidder
Weight
Bidder.Volume
Seller.Volume
Acknowledgements
The original dataset can be found here.
Inspiration
Some ideas worth exploring:
For each item, what is the relationship between bids, bid time, and the closing price? Does this differ by length of the auction, opening bid, or by bidder rating?"
"The ""Trump Effect"" in Europe",A post-election survey about populism in the US and the EU-28,Dalia Research,15,"Version 3,2017-01-24|Version 2,2017-01-13|Version 1,2017-01-12","politics
international relations",Other,51 MB,CC4,"9,238 views","1,030 downloads",19 kernels,3 topics,https://www.kaggle.com/daliaresearch/trump-effect,"Context
The election of Donald Trump has taken the world by surprise and is fuelling populist movements in Europe, e.g. in Italy, Austria and France. Understanding populism and assessing the impact of the “Trump effect” on Europe is a tremendous challenge, and Dalia wants to help pool brainpower to find answers.
The goal is to find out where the next wave of populism could hit in Europe by comparing and contrasting US and EU voter profiles, opinions of Trump vs Clinton voters, Brexit vs. Bremain voters, and future expectations.
Content
Expanding Dalia’s quarterly ""EuroPulse"" omnibus survey to the USA, Dalia has conducted a representative survey with n=11.283 respondents across all 28 EU member countries and n=1.052 respondents from the United States of America. To find out where the next wave of populism could hit Europe, Dalia’s survey traces commonalities in social and political mindsets (like authoritarianism, prejudice, open-mindedness, xenophobia, etc.), voting behaviour and socio-demographic profiles on both sides of the Atlantic.
Inspiration
The sources of our inspirations are many, but to name a few who influenced the way we asked questions: we were very inspired by the 'angry voter' profile laid out by Douglas Rivers, the influence of political and moral attitudes pointed out by Jonathan Haidt and the profile of ""America's forgotten working class"" by J. D. Vance.
Researchers should apply the necessary logic, caution and diligence when analysing and interpreting the results."
2016 EU Referendum in the United Kingdom,How did population demographics impact the Brexit vote?,Electoral Commission,15,"Version 1,2017-01-18","politics
demographics",CSV,116 KB,CC0,"3,912 views",542 downloads,6 kernels,,https://www.kaggle.com/electoralcommission/brexit-results,"Context
A referendum was held on the 23 June 2016 to decide whether the United Kingdom should remain a member of the European Union or leave. Approximately 52%, or more than 17 million people, voted to leave the EU. The referendum turnout was 72.2%, with more than 33.5 million votes cast.
Content
The Electoral Commission published the results of the EU referendum by district and region after the vote. The Office of National Statistics provided the population demographics by district from the 2011 United Kingdom Census."
Pesticide Use in Agriculture,Which compounds are used most frequently in the United States?,US Geological Survey,15,"Version 1,2017-01-26",agriculture,CSV,24 MB,CC0,"4,307 views",490 downloads,4 kernels,0 topics,https://www.kaggle.com/usgs/pesticide-use,"Content
This dataset includes annual county-level pesticide use estimates for 423 pesticides (active ingredients) applied to agricultural crops grown in the contiguous United States. Two different methods were used to estimate a range of pesticide use for all states except California. Both low and high estimate methods incorporated proprietary surveyed rates for United States Department of Agriculture Crop Reporting Districts, but the estimates differed in how they treated situations when a district was surveyed and pesticide use was not reported. Low estimates assumed zero use in the district for that pesticide; however, high estimates treated the unreported use of pesticides as missing data and estimated the pesticide usage from neighboring locations within the same region.
Acknowledgements
Data for the state of California was provided by the 2014 Department of Pesticide Regulation Pesticide Use Report. The 2015 report is not yet available."
Firefighter Fatalities in the United States,"Name, rank, and cause of death for all firefighters killed since 2000",Federal Emergency Management Agency,15,"Version 1,2017-01-26",firefighting,CSV,272 KB,CC0,"2,079 views",224 downloads,9 kernels,0 topics,https://www.kaggle.com/fema/firefighter-fatalities,"Content
The U. S. Fire Administration tracks and collects information on the causes of on-duty firefighter fatalities that occur in the United States. We conduct an annual analysis to identify specific problems so that we may direct efforts toward finding solutions that will reduce firefighter fatalities in the future.
Acknowledgements
This study of firefighter fatalities would not have been possible without members of individual fire departments, chief fire officers, fire service organizations, the National Fire Protection Association, and the National Fallen Firefighters Foundation."
NFL Football Player Stats,"25k player's stats for over 1,000,000 games played",zackthoutt,15,"Version 1,2017-12-08","american football
sports
time series",{}JSON,33 MB,CC0,936 views,137 downloads,,,https://www.kaggle.com/zynicide/nfl-football-player-stats,"NFL Football Stats
My family has always been serious about fantasy football. I've managed my own team since elementary school. It's a fun reason to talk with each other on a weekly basis for almost half the year.
Ever since I was in 8th grade I've dreamed of building an AI that could draft players and choose lineups for me. I started off in Excel and have since worked my way up to more sophisticated machine learning. The one thing that I've been lacking is really good data, which is why I decided to scrape pro-football-reference.com for all recorded NFL player data.
From what I've been able to determine researching, this is the most complete public source of NFL player stats available online. I scraped every NFL player in their database going back to the 1940s. That's over 25,000 players who have played over 1,000,000 football games.
The scraper code can be found here. Feel free to user, alter, or contribute to the repository.
The data was scraped 12/1/17-12/4/17
I finished in last place this year in fantasy football, so hopefully this data will help me improve my performance next year. Only 8 months until draft day!
My ultimate goal is to create an AI that ranks players every week, which could be used to set lineups and draft players. I'm also interested in predicting the winners of games. If you have any ideas or would like to collaborate, please contact me!
The data is broken into two parts. There is a players table where each player has been asigned an ID and a game stats table that has one entry per game played. These tables can be linked together using the player ID.
Player Profile Fields
Player ID: The assigned ID for the player.
Name: The player's full name.
Position: The position the player played abbreviated to two characters. If the player played more than one position, the position field will be a comma-separated list of positions (i.e. ""hb,qb"").
Height: The height of the player in feet and inches. The data format is -. So 6-5 would be six feet and five inches tall.
Weight: The weight of the player in pounds.
Current Team: The three-letter code of the team the player plays for. This is null if they are not currently active.
Birth Date: The day, month, and year the player was born. This is null if unknown.
Birth Place: The city, state or city, country the player was born in. This is null if unknown.
Death Date: The day, month, and year the player died. This is null if they are still alive.
College: The name of the college they played football at. This is null if they did not play football in college.
High School: the city, state or city, country the player went to high school. This is null if the player didn't go to high school or if the school is unknown.
Draft Team: The three letter code of the team that drafted the player. This is null if the player was not drafted.
Draft Position: The draft position number the player was taken. Again, null if the player was not drafted.
Draft Round: The round of the draft the player was drafted in. Null if the player was not drafted.
Draft Position: The position the player was drafted at as a two-letter code. Null if the player was not drafted.
Draft Year: The year the player was drafted. Null if the player was not drafted.
Current Salary Cap Hit: The player's current salary hit for their current team. Null if the player is not currently active on a team.
Hall of Fame Induction Year: The year the player was inducted into the NFL Hall of Fame. Null if the player has not been inducted into the HOF yet.
Game Stats Fields
Note that if there are games missing in the season for a player (i.e. the player has logs for games 1, 2, 3, 5, 6,...), then they didn't play in game 4 because of injury, suspension, etc.
Game Info:
Player ID: The assigned ID for the player.
Year: The year the game took place.
Date: The date the game took place.
Game Number: The number of the game when all games in a season are numbered sequentially.
Age: The age of the player when the game was played. This is in the format -. So 22-344 would be 22 years and 344 days old.
Team: The three-letter code of the team the player played for.
Game Location: One of H, A, or N. H=Home, A=Away, and N=Neutral.
Opponent: The three-letter code of the team the game was played against.
Player Team Score: The score of the team the player played for.
Opponent Score: The score of the team the player played against. You can use this field and the last field to determine if the player's team won.
Passing Stats:
Passing Attempts: The number of passes thrown by the player.
Passing Completions: The number of completions thrown by the player.
Passing Yards: The number of passing yards thrown by the player.
Passing Rating: The NFL passer rating for the player in that game.
Passing Touchdowns: The number of passing touchdowns the player threw.
Passing Interceptions: The number of interceptions the player threw.
Passing Sacks: The number of times the player was sacked.
Passing Sacks Yards Lost: The cumulative yards lost from the player being sacked.
Rushing Stats:
Rushing Attempts: The number of times the the player attempted a rush.
Rushing Yards: The number of yards the player rushed for.
Rushing Touchdowns: The number of touchdowns the player rushed for.
Receiving Stats:
Receiving Targets: The number of times the player was thrown to.
Receiving Receptions: The number of times the player caught a pass thrown to them.
Receiving Yards: The number of yards the player gained through receiving.
Receiving Touchdowns: The number of touchdowns scored through receiving.
Kick/Punt Return Stats
Kick Return Attempts: The number of times the player attempted to return a kick.
Kick Return Yards: The cumulative number of yards the player returned kicks for.
Kick Return Touchdowns: The number of touchdowns the player scored through kick returns.
Punt Return Attempts: The number of times the player attempted to return a punt.
Punt Return Yards: The cumulative number of yards the player returned punts for.
Punt Return Touchdowns: The number of touchdowns the player scored through punt returns.
Kick/Punt Stats
Point After Attempts: The number of PAs the player attempted kicking.
Point After Makes: The number of PAs the player made.
Field Goal Attempts: The number of field goals the player attempted.
Field Goal Makes: The number of field goals the player made.
Defense Stats
Sacks: The number of sacks the player got.
Tackles: The number of tackles the player got.
Tackle Assists: The number of tackles the player assisted on.
Interceptions: The number of times the player intercepted the ball.
Interception Yards: The number of yards the player gained after interceptions.
Interception Touchdowns: The number of touchdowns the player scored after interceptions.
Safeties: The number of safeties the player caused.
Futute Improvements
Format data in an SQLite database
Extract Team IDs to make relating players across teams and games easier
Resolve Game IDs to make relating players in a given game easier
Scrape college data (there are links on the website that shouldn't be too difficult to scrape)
Figure out another method of scraping some additional data that isn't available on pro-football-reference.com, such as fumbles, passes defended, etc.
Resolve blocking stats back to lineman based on the team they played for and the QB's sack stats for that game.
Contributing
If you would like to contribute, please feel free to put up a PR or reach out to me with ideas. I would love to collaborate with some fellow football fans on this project.
If you're interested in collaborating, the repository can be found here.
Connect with me
If you'd like to collaborate on a project, learn more about me, or just say hi, feel free to contact me using any of the social channels listed below.
Personal Website
Email
LinkedIn
Twitter
Medium
Quora
HackerNews
Reddit
Kaggle
Instagram
500px"
Wikipedia Article Titles,All of the titles of articles on Wikipedia,Aleksey Bilogur,15,"Version 1,2017-09-22","encyclopedias
knowledge",Other,296 MB,CC4,"2,075 views",227 downloads,,0 topics,https://www.kaggle.com/residentmario/wikipedia-article-titles,"Context
Wikipedia, the world's largest encyclopedia, is a crowdsourced open knowledge project and website with millions of individual web pages. This dataset is a grab of the title of every article on Wikipedia as of September 20, 2017.
Content
This dataset is a simple newline (\n) delimited list of article titles. No distinction is made between redirects (like Schwarzenegger) and actual article pages (like Arnold Schwarzenegger).
Acknowledgements
This dataset was created by scraping Special:AllPages on Wikipedia. It was originally shared here.
Inspiration
What are common article title tokens? How do they compare against frequent words in the English language?
What is the longest article title? The shortest?
What countries are most popular within article titles?"
Mathematicians of Wikipedia,A Dataset of the World's Most Famous Mathematicians,Joe Philleo,15,"Version 2,2017-09-17|Version 1,2017-09-17","mathematics
people
demographics
internet",CSV,10 MB,CC0,"2,995 views",236 downloads,,,https://www.kaggle.com/joephilleo/mathematicians-on-wikipedia,"Context
What distinguishes the great from the good, the remembered from the accomplished, and the genius from the merely brilliant? Scrapping English Wikipedia, Joseph Philleo has cleaned and compiled a database of more than 8,500 famous mathematicians for the Kaggle data science community to analyze and better understand.
Inspiration
What are the common characteristics of famous mathematicians?
How old do they live, which fields do they work in, where are they born, and where do they live?
Can you predict which mathematicians will win a Fields Medal, join the Royal Society, or secure tenure at Harvard?"
Crime in Baltimore,Crime data for 2012-2017,Sohier Dane,15,"Version 1,2017-09-13",crime,CSV,39 MB,CC0,"5,216 views",866 downloads,3 kernels,,https://www.kaggle.com/sohier/crime-in-baltimore,"All BPD data on Open Baltimore is preliminary data and subject to change. The information presented through Open Baltimore represents Part I victim based crime data. The data do not represent statistics submitted to the FBI's Uniform Crime Report (UCR); therefore any comparisons are strictly prohibited. For further clarification of UCR data, please visit http://www.fbi.gov/about-us/cjis/ucr/ucr. Please note that this data is preliminary and subject to change. Prior month data is likely to show changes when it is refreshed on a monthly basis. All data is geocoded to the approximate latitude/longitude location of the incident and excludes those records for which an address could not be geocoded. Any attempt to match the approximate location of the incident to an exact address is strictly prohibited.
Acknowledgements
This dataset was kindly made available by the City of Baltimore. You can find the original dataset, which is updated regularly, here."
30 Years of European Wind Generation,Hourly energy potential for 1986-2015,Sohier Dane,15,"Version 2,2017-09-15|Version 1,2017-09-15",energy,CSV,710 MB,CC0,"2,341 views",309 downloads,,0 topics,https://www.kaggle.com/sohier/30-years-of-european-wind-generation,"This dataset contains hourly estimates of an area's energy potential for 1986-2015 as a percentage of a power plant's maximum output.
The overall scope of EMHIRES is to allow users to assess the impact of meteorological and climate variability on the generation of solar power in Europe and not to mime the actual evolution of wind power production in the latest decades. For this reason, the hourly wind power generation time series are released for meteorological conditions of the years 1986-2015 (30 years) without considering any changes in the wind installed capacity. Thus, the installed capacity considered is fixed as the one installed at the end of 2015. For this reason, data from EMHIRES should not be compared with actual power generation data other than referring to the reference year 2015.
Content
The data is available at both the national level and the NUTS 2 level. The NUTS 2 system divides the EU into 276 statistical units.
Please see the manual for the technical details of how these estimates were generated.
This product is intended for policy analysis over a wide area and is not the best for estimating the output from a single system. Please don't use it commercially.
Acknowledgements
This dataset was kindly made available by the European Commission's STETIS program. You can find the original dataset here.
Inspiration
How clean is the dataset?
What does a typical year look like? One common approach is to stitch together 12 months of raw data, using the 12 most typical months per this ISO standard.
Can you identify more useful geographical areas for this sort of analysis, such as valleys that would share similar wind patterns?
If you like
If you like this dataset, you might also enjoy: - 30 years of European solar - Google's Project Sunroof data"
Deceptive Opinion Spam Corpus,A corpus of truthful and deceptive hotel reviews,Rachael Tatman,15,"Version 2,2017-07-18|Version 1,2017-07-18","languages
linguistics",CSV,1 MB,CC4,"1,970 views",271 downloads,3 kernels,,https://www.kaggle.com/rtatman/deceptive-opinion-spam-corpus,"Context
This corpus consists of truthful and deceptive hotel reviews of 20 Chicago hotels. The data is described in two papers according to the sentiment of the review. In particular, we discuss positive sentiment reviews in [1] and negative sentiment reviews in [2]. While we have tried to maintain consistent data preprocessing procedures across the data, there are differences which are explained in more detail in the associated papers. Please see those papers for specific details.
Content
This corpus contains:
400 truthful positive reviews from TripAdvisor (described in [1])
400 deceptive positive reviews from Mechanical Turk (described in [1])
400 truthful negative reviews from Expedia, Hotels.com, Orbitz, Priceline, TripAdvisor and Yelp (described in [2])
400 deceptive negative reviews from Mechanical Turk (described in [2])
Each of the above datasets consist of 20 reviews for each of the 20 most popular Chicago hotels (see [1] for more details). The files are named according to the following conventions: Directories prefixed with fold correspond to a single fold from the cross-validation experiments reported in [1] and [2].
Hotels included in this dataset
affinia: Affinia Chicago (now MileNorth, A Chicago Hotel)
allegro: Hotel Allegro Chicago - a Kimpton Hotel
amalfi: Amalfi Hotel Chicago
ambassador: Ambassador East Hotel (now PUBLIC Chicago)
conrad: Conrad Chicago
fairmont: Fairmont Chicago Millennium Park
hardrock: Hard Rock Hotel Chicago
hilton: Hilton Chicago
homewood: Homewood Suites by Hilton Chicago Downtown
hyatt: Hyatt Regency Chicago
intercontinental: InterContinental Chicago
james: James Chicago
knickerbocker: Millennium Knickerbocker Hotel Chicago
monaco: Hotel Monaco Chicago - a Kimpton Hotel
omni: Omni Chicago Hotel
palmer: The Palmer House Hilton
sheraton: Sheraton Chicago Hotel and Towers
sofitel: Sofitel Chicago Water Tower
swissotel: Swissotel Chicago
talbott: The Talbott Hotel
References
[1] M. Ott, Y. Choi, C. Cardie, and J.T. Hancock. 2011. Finding Deceptive Opinion Spam by Any Stretch of the Imagination. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies.
[2] M. Ott, C. Cardie, and J.T. Hancock. 2013. Negative Deceptive Opinion Spam. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.
Acknowledgements
If you use any of this data in your work, please cite the appropriate associated paper (described above). Please direct questions to Myle Ott (myleott@cs.cornell.edu)."
Fraudulent E-mail Corpus,"CLAIR collection of ""Nigerian"" fraud emails",Rachael Tatman,15,"Version 1,2017-07-26","crime
linguistics
internet",Other,17 MB,CC4,"4,829 views",557 downloads,,,https://www.kaggle.com/rtatman/fraudulent-email-corpus,"Context:
Fraudulent e-mails contain criminally deceptive information, usually with the intent of convincing the recipient to give the sender a large amount of money. Perhaps the best known type of fraudulent e-mails is the Nigerian Letter or “419” Fraud.
Content:
This dataset is a collection of more than 2,500 ""Nigerian"" Fraud Letters, dating from 1998 to 2007.
These emails are in a single text file. Each e-mail has a header which includes the following information:
Return-Path: address the email was sent from
X-Sieve: the X-Sieve host (always cmu-sieve 2.0)
Message-Id: a unique identifier for each message
From: the message sender (sometimes blank)
Reply-To: the email address to which replies will be sent
To: the email address to which the e-mail was originally set (some are truncated for anonymity)
Date: Date e-mail was sent
Subject: Subject line of e-mail
X-Mailer: The platform the e-mail was sent from
MIME-Version: The Multipurpose Internet Mail Extension version
Content-Type: type of content & character encoding
Content-Transfer-Encoding: encoding in bits
X-MIME-Autoconverted: the type of autoconversion done
Status: r (read) and o (opened)
Acknowledgements:
If you use this collection of fraud email in your research, please include the following citation in any resulting papers:
Radev, D. (2008), CLAIR collection of fraud email, ACL Data and Code Repository, ADCR2008T001, http://aclweb.org/aclwiki
Inspiration:
This dataset contains fraudulent e-mails sent over a period of years. Has the language used in fraudulent E-mails changed over time?
Are there any words or phrases that are particularly common in this type of e-mail? (You might compare it with the Enron email corpus, linked below)
Related datasets:
https://www.kaggle.com/wcukierski/enron-email-dataset
https://www.kaggle.com/uciml/sms-spam-collection-dataset"
When do children learn words?,How common 731 Norwegian words are & when children learn them,Rachael Tatman,15,"Version 1,2017-07-28","languages
europe
linguistics",CSV,77 KB,CC0,"2,379 views",199 downloads,,,https://www.kaggle.com/rtatman/when-do-children-learn-words,"Context:
When children are born they don’t know any words. By the time they’re three, most children know 200 words or more. These words aren’t randomly selected from the language they’re learning, however. A two-year-old is much more likely to know the word “bottle” than the word “titration”. What words do children learn first, and what qualities do those words have? This dataset was collected to explore this question.
Content:
The main dataset includes information for 732 Norwegian words. A second table also includes measures of how frequently each word is used in Norwegian, both on the internet (as observed in the Norwegian Web as Corpus dataset) and when an adult is talking to a child. The latter is commonly called “child directed speech” and is abbreviated as “CDS”.
Main data
ID_CDI_I: Word ID from the Norwegian adaptation of the MacArthur-Bates Communicative Development Inventories, version 1
ID_CDI_II: Word ID from the Norwegian adaptation of the MacArthur-Bates Communicative Development Inventories, version 2
Word_NW: The word in Norwegian
Word_CDI: The form of the word found in the Norwegian adaptation of the MacArthur-Bates Communicative Development Inventories
Translation: the English translation of the Norwegian word
AoA: how old a child generally is was when they this this word, in months (Estimated from the MacArthur-Bates Communicative Development Inventories)
VSoA: how many other words a child generally knows when they learn this word (rounded up to the nearest 10)
Lex_cat: the specific part of speech of the word
Broad_lex: the broad part of speech of the word
Freq: a measure of how commonly this word occurs in Norwegian
CDS_Freq: a measure of how commonly this word occurs when a Norwegian adult is talking to a Norwegian child
Norwegian CDS Frequency
Word_CDI: The word from, as found in the Norwegian adaptation of the MacArthur-Bates Communicative Development Inventories
Translation: The English translation of the Norwegian word
Freq_NoWaC: How often this word is used on the internet
Freq_CDS: How often this word is used when talking to children (based on two Norwegian CHILDES corpora)
Acknowledgements:
This dataset was collected by Pernille Hansen. If you use this data, please cite the following paper:
Hansen (2016). What makes a word easy to acquire? The effects of word class, frequency, imageability and phonological neighbourhood density on lexical development. First Language. Advance online publication. doi: 10.1177/0142 723716679956 http://dx.doi.org/10.1177/0142723716679956
Inspiration:
How well can you predict which words a child will learn first?
Are some sounds or letters found more often than chance in words learned early?
Can you build topic models on earlier-acquired and later-acquired words? Which topics are over-represented in words learned very early?"
Pre-trained Word Vectors for Spanish,Over 1 million 300-dimensional word vectors for Spanish,Rachael Tatman,15,"Version 1,2017-08-10","languages
linguistics
artificial intelligence",Other,3 GB,CC4,"1,931 views",124 downloads,,0 topics,https://www.kaggle.com/rtatman/pretrained-word-vectors-for-spanish,"Context:
Word vectors, also called word embeddings, are a multi-dimensional representation of words based on which words are used in similar contexts. They can capture some elements of words’ meanings. For example, documents which use a lot of words that are clustered together in a vector space representation are more likely to be on similar topics.
Word vectors are very computationally intensive to train, and the vectors themselves will vary based on the documents or corpora they are trained on. For these reasons, it is often convenient to use word vectors which have been pre-trained rather than training them from scratch for each project.
Content:
This dataset contains 1,000,653 word embeddings of dimension 300 trained on the Spanish Billion Words Corpus. These embeddings were trained using word2vec.
Parameters for Embeddings Training:
Word embeddings were trained using the following parameters:
The selected algorithm was the skip-gram model with negative-sampling.
The minimum word frequency was 5.
The amount of “noise words” for the negative sampling was 20.
The 273 most common words were downsampled.
The dimension of the final word embedding was 300.
The original corpus had the following amount of data:
A total of 1420665810 raw words.
A total of 46925295 sentences.
A total of 3817833 unique tokens.
After the skip-gram model was applied, filtering of words with less than 5 occurrences as well as the downsample of the 273 most common words, the following values were obtained:
A total of 771508817 raw words.
A total of 1000653 unique tokens.
Acknowledgements:
This dataset was created by Cristian Cardellino. If you use this dataset in your work, please reference the following citation:
Cristian Cardellino: Spanish Billion Words Corpus and Embeddings (March 2016), http://crscardellino.me/SBWCE/
Inspiration:
Word vector representations are widely-used in natural language processing tasks.
Can you improve an existing part of speech tagger in Spanish by using word vectors? You might find it helpful to check out this paper.
Can you use these word embeddings to improve existing parsers for Spanish? This paper outlines some approaches for this.
There has been quite a bit of work recently on how word embeddings might encode implicit gender bias. For example, this paper show how embeddings can capture stereyoptyes about career fields and gender. However, this recent paper suggests that for languages with grammatical gender (like Spanish), grammatical gender is more influential than gender bias. Do these word embeddings support that claim?
You may also like:
GloVe: Global Vectors for Word Representation. Pre-trained English word vectors from Wikipedia 2014 + Gigaword 5
20 Million Word Spanish Corpus. The Spanish Language portion of the Wikicorpus (v 1.0)"
Data Lab,To use for various exercises including multivariate analysis,KrisMurphy,15,"Version 2,2017-08-19|Version 1,2017-08-03",,CSV,2 MB,CC0,"3,268 views",442 downloads,,3 topics,https://www.kaggle.com/krismurphy01/data-lab,"Context
We are building a data set that can be used for building useful reports, understanding the difference between data and information, and multivariate analysis. The data set we are building is similar to that used in several academic reports and what may be found in ERP HR subsystems.
We will update the sample data set as we gain a better understanding of the data elements using the calculations that exist in scholarly journals. Specifically, we will use the correlation tables to rebuild the data sets.
Content
The fields represent a fictitious data set where a survey was taken and actual employee metrics exist for a particular organization. None of this data is real.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Prabhjot Singh contributed a portion of the data (the columns on the right before the survey data was added). https://www.kaggle.com/prabhjotindia https://www.kaggle.com/prabhjotindia/visualizing-employee-data/data
About this Dataset Why are our best and most experienced employees leaving prematurely? Have fun with this database and try to predict which valuable employees will leave next. Fields in the dataset include:
Satisfaction Level Last evaluation Number of projects Average monthly hours Time spent at the company Whether they have had a work accident Whether they have had a promotion in the last 5 years Departments Salary
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
Daily and Intraday Stock Price Data,Daily and Intraday Price + Volume Data For All U.S. Stocks & ETFs,Boris Marjanovic,15,"Version 1,2017-12-09","business
finance
economics
artificial intelligence",Other,418 MB,CC0,393 views,74 downloads,,,https://www.kaggle.com/borismarjanovic/daily-and-intraday-stock-price-data,"Context
Stock market data -- and particularly intraday price data -- can be very expensive to buy. To help more people gain access to it, here I provide daily as well as intraday price and volume data for all U.S.-based stocks and ETFs trading on the NYSE, NASDAQ, and NYSE MKT.
Content
The dataset (last updated 12/06/2017) is presented in CSV format as follows:
Intraday data: Date,Time,Open,High,Low,Close,Volume,OpenInt
Daily data: Date,Open,High,Low,Close,Volume,OpenInt
Acknowledgements
The dataset belongs to me. I’m sharing it here for free. You may do with it as you wish.
Inspiration
Many have tried, but most have failed, to predict the stock market's ups and downs. Can you do any better?"
24 thousand tweets later,2017 tweets from incubators and accelerators,Derrick M,15,"Version 3,2018-01-07|Version 2,2018-01-06|Version 1,2017-12-29","twitter
companies",CSV,4 MB,GPL,"1,367 views",144 downloads,3 kernels,0 topics,https://www.kaggle.com/derrickmwiti/24-thousand-tweets-later,"Context
I collected this data from incubators and accelerators to find out what they have been talking about in 2017.
Content
The data contains the twitter usernames of various organizations and tweets for the year 2017 collected on 28th Dec 2017.
Acknowledgements
Much appreciation to @emmanuelkens for helping in thinking through this
Inspiration
I am very curious to find out what the various organizations have been talking about in 2017. I would also like to find out the most popular organization by tweets and engagement. I am also curious to find out if there is any relationship between the number of retweets a tweet gets and the time of day it was posted!"
Toxic Release Inventory,US EPA data on release of toxic chemicals for 1987-2016,US Environmental Protection Agency,15,"Version 1,2017-08-24","government agencies
pollution",Other,2 GB,Other,"1,771 views",226 downloads,,0 topics,https://www.kaggle.com/epa/toxic-release-inventory,"Context
This database is managed by the US Environmental Protection Agency and contains information reported annually by some industry groups as well as federal facilities. Each year, companies across a wide range of industries (including chemical, mining, paper, oil and gas industries) that produce more than 25,000 pounds or handle more than 10,000 pounds of a listed toxic chemical must report it to the TRI. The TRI threshold was initially set at 75,000 pounds annually. If the company treats, recycles, disposes, or releases more than 500 pounds of that chemical into the environment (as opposed to just handling it), then they must provide a detailed inventory of that chemical's inventory.
Content
There are roughly 100 columns in this dataset; please see the tri_basic_data_file_format_v15.pdf for details. You may also wish to consult factors_to_consider_6.15.15_final.pdf for general background about interpreting the data.
I've merged all of the TRI basic data files into a single large csv. You will probably need to process it in batches or use a tool like Dask to stay within kernel memory limits.
Please note that the 2016 data remains preliminary at the time of this release.
Acknowledgements
This dataset was released by the US EPA. You can find the original dataset, more detailed versions of the data, and a great deal of background information here: https://www.epa.gov/toxics-release-inventory-tri-program/tri-data-and-tools
Inspiration
The EPA runs an annual university contest. Their list of previous winners contains a lot of great ideas that people have had for this dataset in the past. The 2017 competition is already over, but you can find the rules here."
Article Titles from TechCrunch and VentureBeat,"Titles of 22,000+ article published on two of the top media websites",PromptCloud,15,"Version 1,2017-12-20","journalism
mass media
internet",CSV,1 MB,CC4,600 views,34 downloads,,0 topics,https://www.kaggle.com/PromptCloudHQ/titles-by-techcrunch-and-venturebeat-in-2017,"Context
Web data extraction or web scraping can be a great business tool for trend spotting via media monitoring. Essentially, leading media outlets can be tracked to unveil the top buzzwords and the number of mentions companies (including their products) garner over specific time period. We wanted to apply this method to understand the tech landscape and its coverage in 2017. Hence, we deployed PromptCloud’s in-house web crawler to extract the article titles from two popular outlets (TechCrunch and VentureBeat) and performed text mining on the dataset to uncover the top buzzwords, companies and products.
Content
The dataset contains following 3 fields:
URL
Title
Date of publication
Acknowledgements
This dataset was created by using PromptCloud's in-house web scraping service.
Inspiration
Initial Analysis can be found here. It includes the following findings:
Top companies/products that were covered by media over the year
Top tech trends over the year"
Freight Analysis Framework,Flows of goods among US regions for all modes of transportation,Department of Transportation,15,"Version 1,2017-08-09","industry
rail transport
shipping",Other,623 MB,Other,"2,331 views",307 downloads,2 kernels,0 topics,https://www.kaggle.com/usdot/freight-analysis-framework,"The Freight Analysis Framework (FAF) integrates data from a variety of sources to create a comprehensive picture of freight movement among states and major metropolitan areas by all modes of transportation. Starting with data from the 2012 Commodity Flow Survey (CFS) and international trade data from the Census Bureau, FAF incorporates data from agriculture, extraction, utility, construction, service, and other sectors. FAF version 4 (FAF4) provides estimates for tonnage (in thousand tons) and value (in million dollars) by regions of origin and destination, commodity type, and mode. Data are available for the base year of 2012, the recent years of 2013 - 2015, and forecasts from 2020 through 2045 in 5-year intervals.
Inspiration
This dataset should be great for map-based visualizations."
Armenian Online Job Postings,"19,000 online job postings from 2004 to 2015 from Armenia's CareerCenter",Udacity,15,"Version 1,2017-08-06",,Other,92 MB,Other,"1,755 views",630 downloads,,0 topics,https://www.kaggle.com/udacity/armenian-online-job-postings,"Context
The online job market is a good indicator of overall demand for labor in an economy. This dataset consists of 19,000 job postings from 2004 to 2015 posted on CareerCenter, an Armenian human resource portal. Since postings are text documents and tend to have similar structures, text mining can be used to extract features like posting date, job title, company name, job description, salary, and more. Postings that had no structure or were not job-related were removed. The data was originally scraped from a Yahoo! mailing group.
Inspiration
Students, job seekers, employers, career advisors, policymakers, and curriculum developers use online job postings to explore the nature of today's dynamic labor market. This dataset can be used to:
Understand the demand for certain professions, job titles, or industries
Identify skills that are most frequently required by employers, and how the distribution of necessary skills changes over time
Help education providers with curriculum development
Acknowledgements
The data collection and initial research were funded by the American University of Armenia’s research grant (2015).
Habet Madoyan, CEO at Datamotus, compiled this dataset and has granted us permission to republish. The republished dataset is identical to the original dataset, which can be found here. Datamotus also published a report detailing the text mining techniques used, plus analyses and visualizations of the data."
"UN General Assembly Votes, 1946-2015",Votes by member states on individual resolutions and specific issues,United Nations,15,"Version 1,2017-02-17","politics
international relations",CSV,36 MB,CC0,"4,799 views",371 downloads,7 kernels,,https://www.kaggle.com/unitednations/general-assembly,"Content
This dataset documents all United Nations General Assembly votes since its establishment in 1946. The data is broken into three different files: the first lists each UN resolution, subject, and vote records; the second records individual member state votes per resolution; and the third provides an annual summary of member state voting records with affinity scores and an ideal point estimate in relation to the United States.
Acknowledgements
The UN General Assembly voting data was compiled and published by Professor Erik Voeten of Georgetown University."
City Lines,Explore the transport systems of the world's cities,citylines.co,15,"Version 3,2017-08-05|Version 2,2017-06-20|Version 1,2017-06-08","cities
transport",CSV,2 MB,ODbL,"4,126 views",463 downloads,25 kernels,2 topics,https://www.kaggle.com/citylines/city-lines,"Context
What did the expansion of the London Underground, the world’s first underground railway which opened in 1863, look like? What about the transportation system in your home city? Citylines collects data on transportation lines across the world so you can answer questions like these and more.
Content
This dataset, originally shared and updated here, includes transportation line data from a number of cities from around the world including London, Berlin, Mexico City, Barcelona, Washington D.C., and others covering many thousands of kilometers of lines.
Inspiration
You can explore geometries to generate maps and even see how lines have changed over time based on historical records. Want to include shapefiles with your analysis? Simply publish a shapefile dataset here and then create a new kernel (R or Python script/notebook), adding your shapefile as an additional datasource."
Financial Statement Extracts,Data extracted from filings companies make to the SEC 2015-2017,Securities and Exchange Commission,15,"Version 2,2017-09-14|Version 1,2017-09-14",finance,{}JSON,3 GB,CC0,"3,586 views",526 downloads,,0 topics,https://www.kaggle.com/securities-exchange-commission/financial-statement-extracts,"The Financial Statement Data Sets below provide numeric information from the face financials of all financial statements. This data is extracted from exhibits to corporate financial reports filed with the Commission using eXtensible Business Reporting Language (XBRL). As compared to the more extensive Financial Statement and Notes Data Sets, which provide the numeric and narrative disclosures from all financial statements and their notes, the Financial Statement Data Sets are more compact.
The information is presented without change from the ""as filed"" financial reports submitted by each registrant. The data is presented in a flattened format to help users analyze and compare corporate disclosure information over time and across registrants. The data sets also contain additional fields including a company's Standard Industrial Classification to facilitate the data's use.
Content
Each quarter's data is stored as a json of the original text files. This was necessary to limit the overall number of files. The num.txt file will likely be of most interest.
Acknowledgements
This dataset was kindly made available by the SEC. You can find the original dataset, which is updated quarterly, here."
levin vehicle telematics,Vehicle and Driving Data,Yun Solutions,15,"Version 3,2018-02-10|Version 2,2018-02-10|Version 1,2018-01-22","india
business
internet
+ 2 more...",CSV,587 MB,CC4,"1,321 views",128 downloads,,2 topics,https://www.kaggle.com/yunlevin/levin-vehicle-telematics,"We are updating this dataset everymonth. Access updated data for every month here
Context
The dataset is proprietary data of Yun Solutions, collected from Beta Testing phase. The dataset contains sensor and OBD data for over 4 months and around 30 vehicles.
Content
The Dataset contains Vehicle telematics and Driving data. Metadata is explained in the file description on the data tab.
Acknowledgements
We look forward to analysis on the data.
Inspiration
We want to make this data accessible for learning and analysis."
Multidimensional Poverty Measures,Indexing different types of simultaneous deprivation,Oxford Poverty & Human Development Initiative,15,"Version 2,2018-02-17|Version 1,2018-02-16","government agencies
public health
finance
+ 2 more...",CSV,77 KB,CC0,593 views,123 downloads,,0 topics,https://www.kaggle.com/ophi/mpi,"Context
Most countries of the world define poverty as a lack of money. Yet poor people themselves consider their experience of poverty much more broadly. A person who is poor can suffer from multiple disadvantages at the same time – for example they may have poor health or malnutrition, a lack of clean water or electricity, poor quality of work or little schooling. Focusing on one factor alone, such as income, is not enough to capture the true reality of poverty.
Multidimensional poverty measures can be used to create a more comprehensive picture. They reveal who is poor and how they are poor – the range of different disadvantages they experience. As well as providing a headline measure of poverty, multidimensional measures can be broken down to reveal the poverty level in different areas of a country, and among different sub-groups of people.
Content
OPHI researchers apply the AF method and related multidimensional measures to a range of different countries and contexts. Their analyses span a number of different topics, such as changes in multidimensional poverty over time, comparisons in rural and urban poverty, and inequality among the poor. For more information on OPHI’s research, see our working paper series and research briefings.
OPHI also calculates the Global Multidimensional Poverty Index MPI, which has been published since 2010 in the United Nations Development Programme’s Human Development Report. The Global MPI is an internationally-comparable measure of acute poverty covering more than 100 developing countries. It is updated by OPHI twice a year and constructed using the AF method.
The Alkire Foster (AF) method is a way of measuring multidimensional poverty developed by OPHI’s Sabina Alkire and James Foster. Building on the Foster-Greer-Thorbecke poverty measures, it involves counting the different types of deprivation that individuals experience at the same time, such as a lack of education or employment, or poor health or living standards. These deprivation profiles are analysed to identify who is poor, and then used to construct a multidimensional index of poverty (MPI). For free online video guides on how to use the AF method, see OPHI’s online training portal.
To identify the poor, the AF method counts the overlapping or simultaneous deprivations that a person or household experiences in different indicators of poverty. The indicators may be equally weighted or take different weights. People are identified as multidimensionally poor if the weighted sum of their deprivations is greater than or equal to a poverty cut off – such as 20%, 30% or 50% of all deprivations.
It is a flexible approach which can be tailored to a variety of situations by selecting different dimensions (e.g. education), indicators of poverty within each dimension (e.g. how many years schooling a person has) and poverty cut offs (e.g. a person with fewer than five years of education is considered deprived).
The most common way of measuring poverty is to calculate the percentage of the population who are poor, known as the headcount ratio (H). Having identified who is poor, the AF method generates a unique class of poverty measures (Mα) that goes beyond the simple headcount ratio. Three measures in this class are of high importance:
Adjusted headcount ratio (M0), otherwise known as the MPI: This measure reflects both the incidence of poverty (the percentage of the population who are poor) and the intensity of poverty (the percentage of deprivations suffered by each person or household on average). M0 is calculated by multiplying the incidence (H) by the intensity (A). M0 = H x A.
Find out about other ways the AF method is used in research and policy.
Additional data here.
Acknowledgements
Alkire, S. and Robles, G. (2017). “Multidimensional Poverty Index Summer 2017: Brief methodological note and results.” OPHI Methodological Note 44, University of Oxford.
Alkire, S. and Santos, M. E. (2010). “Acute multidimensional poverty: A new index for developing countries.” OPHI Working Papers 38, University of Oxford.
Alkire, S. Jindra, C. Robles, G. and Vaz, A. (2017). ‘Multidimensional Poverty Index – Summer 2017: brief methodological note and results’. OPHI MPI Methodological Notes No. 44, Oxford Poverty and Human Development Initiative, University of Oxford.
Inspiration
Which countries exhibit the largest subnational disparities in MPI?
Which countries have high per-capita incomes yet still rank highly in MPI?"
Education in India,"District and state-wise primary & secondary school education data, 2015-16",Rajanand Ilangovan / இராஜ்ஆனந்த் இளங்கோவன்,14,"Version 1,2017-08-15","india
education",CSV,2 MB,CC4,"5,171 views",908 downloads,8 kernels,,https://www.kaggle.com/rajanand/education-in-india,"Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks.
Context
When India got independence from British in 1947 the literacy rate was 12.2% and as per the recent census 2011 it is 74.0%. Although it looks an accomplishment, still many people are there without access to education.
It would be interesting to know the current status of the Indian education system.
Content
This dataset contains district and state wise Indian primary and secondary school education data for 2015-16.
Granularity: Annual
List of files:
2015_16_Districtwise.csv ( 680 observations and 819 variables )
2015_16_Statewise_Elementary.csv ( 36 observations and 816 variables )
2015_16_Statewise_Secondary.csv ( 36 observations and 630 variables )
Acknowledgements
Ministry of Human Resource Development (DISE) has shared the dataset here and also published some reports.
Inspiration
This dataset provides the complete information about primary and secondary education. There are many inferences can be made from this dataset. There are few things I would like to understand from this dataset.
Drop out ratio in primary and secondary education. (Govt. has made law that every child under age 14 should get free compulsary education.)
Various factors affecting examination results of the students.
What are all the factors that makes the difference (in literacy rate) between Kerala and Bihar?
What could be done to improve the female literacy rate and literacy rate in rural area?"
US Casualties of the Vietnam War,Data from The U.S. National Archives and Records Administration,0rangutan,14,"Version 6,2017-02-17|Version 5,2017-02-16|Version 4,2017-02-15|Version 3,2017-02-10|Version 2,2017-02-10|Version 1,2017-02-09","history
war",Other,24 MB,Other,"3,915 views",410 downloads,19 kernels,,https://www.kaggle.com/orangutan/vietnamconfilct,"Context
Information reproduced from the National Archives:
""The Vietnam Conflict Extract Data File of the Defense Casualty Analysis System (DCAS) Extract Files contains records of 58,220 U.S. military fatal casualties of the Vietnam War. These records were transferred into the custody of the National Archives and Records Administration in 2008. The earliest casualty record contains a date of death of June 8, 1956, and the most recent casualty record contains a date of death of May 28, 2006. The Defense Casualty Analysis System Extract Files were created by the Defense Manpower Data Center (DMDC) of the Office of the Secretary of Defense. The records correspond to the Vietnam Conflict statistics on the DMDC web site, which is accessible online at https://www.dmdc.osd.mil/dcas/pages/main.xhtml .
A full series description for the Defense Casualty Analysis System (DCAS) Extract Files is accessible online via the National Archives Catalog under the National Archives Identifier 2163536. The Vietnam Conflict Extract Data File is also accessible for direct download via the National Archives Catalog file-level description, National Archives Identifier 2240992. ""
Content
The raw data files have been cleaned and labelled as best as I can with reference to the accompanying Supplemental Code Lists. Names and ID numbers have been removed out of respect and to provide anonymity.
Acknowledgements
Data provided by The U.S. National Archives and Records Administration.
Raw data can be accessed via the following link: https://catalog.archives.gov/id/2240992
Inspiration
By cleaning the data I hope to give wider access to this resource."
Rare Diseases on Facebook Groups,Help improve the quality of life of people with rare diseases,Natalia,14,"Version 5,2017-08-08|Version 4,2017-07-10|Version 3,2017-07-07|Version 2,2017-07-07|Version 1,2017-06-28","diseases
epidemiology
internet",CSV,2 MB,Other,"3,455 views",205 downloads,3 kernels,,https://www.kaggle.com/natt77/rare-diseases-on-facebook-groups,"Context
This dataset was obtained from Facebook groups as part of my postgraduate thesis. The objective of the thesis was to extract posts from groups related to rare diseases and compare them with the Spanish association of rare diseases (FEDER). If you want to use this open dataset or the code you should cite our paper:
Reguera, N., Subirats, L., Armayones, M. Mining Facebook data of people with rare diseases. IEEE Computer-Based Medical Systems (IEEE CBMS 2017), Thessaloniki, Greece, 22-24th June, 2017.
Content
The file contains information 3917 records from 5 Facebook groups and was extracted using Netvizz. The posts were generated since each group started (as far as 2009) until the 26/11/2016.
The content is as follows:
type: Facebook's post classification (e.g. photo, status, etc.)
by: either""post_page_pageid"" (post by page) or ""post_user_pageid"" (post by user);
post_id: id of the post;
post_link: direct link to the post;
post_message: text of the post;
picture: the picture scraped from any link included with the post;
full_picture: the picture scraped from any link included with the post (full size);
link: link URL (if the post points to external content);
link_domain: domain name of link;
post_published: publishing date
post_published_unix: publishing date as Unix timestamp (for easy conversion and ranking);
post_published_sql: publishing date in SQL format (some analysis tools prefer this);
likes_count_fb: Facebook provided like count for posts;
comments_count_fb: Facebook provided comment count for posts;
reactions_count_fb: Facebook provided reactions count for posts (includes likes);
shares_count_fb: Facebook provided share count for posts;
engagement_fb: sum of comment, reaction, and share counts;
Acknowledgements
I would like to thank my thesis directors, who guided me through all the process: Laia Subirats Maté and Manuel Armayones Ruiz.
Inspiration
During my research (code available on https://github.com/natt77/UOC---TFP) several insights were found on the relation between the information posted on the gropus and the information in FEDER. Text analytics was performed together with sentiment analysis. It would be interesting to deepen the analysis, create models to predict engagement or any other action that can help improving the life quality of people with rare diseases."
Crime in Vancouver,Data of crimes in Vancouver (Canada) from 2003 to 2017,Wilian Osaku,14,"Version 2,2017-11-07|Version 1,2017-08-13",crime,CSV,56 MB,ODbL,"4,030 views",594 downloads,6 kernels,2 topics,https://www.kaggle.com/wosaku/crime-in-vancouver,"Content
The data comes from the Vancouver Open Data Catalogue. It was extracted on 2017-07-18 and it contains 530,652 records from 2003-01-01 to 2017-07-13. The original data set contains coordinates in UTM Zone 10 (columns X and Y). I also included Latitude and Longitude, which I converted using this spreadsheet that can be found here.
There's also a Google Trends data that shows how often a search-term is entered relative to the total search-volume. From Google Trends:
""Numbers represent search interest relative to the highest point on the chart for the given region and time. A value of 100 is the peak popularity for the term. A value of 50 means that the term is half as popular. Likewise a score of 0 means the term was less than 1% as popular as the peak.""
Original data for search term ""crime"" location British Columbia: https://trends.google.com/trends/explore?date=2004-01-01%202017-06-30&geo=CA-BC&q=crime
Acknowledgements
Photo By Charles de Jesus [CC BY 3.0 (http://creativecommons.org/licenses/by/3.0)], via Wikimedia Commons"
Facial Expression of Emotion,Frame-by-frame analysis of facial expressions in a video interview,Javier Villanueva-Valle,14,"Version 1,2017-03-09","film
sociology",CSV,5 MB,ODbL,"8,103 views",841 downloads,3 kernels,,https://www.kaggle.com/sivlemx/facial-expression-of-emotion,"Context
An interview was videotaped to analyze the facial expressions of emotion. This interview lasts for 21:30 minutes.
Content
An interview with a duration of 21:30 minutes was videotaped. Emotional facial expressions were analyzed every 0.12 seconds with FaceReader software. Emotions were neutral, joy, fear, anger, surprise, fear and contemp, positive-negative valences, arousal, degrees of head direction, facial action units, among others.
Acknowledgements
Acknowledgements to The Clinic of Borderline Personality Disorder for contributing the material, also to the Laboratory of Chronoecology and Human Ethology.
Inspiration
How does the direction of the gaze relate to the expressions of emotions?
From what score of any emotion is considered open, closed and / or neutral in the right eye, left eye, right eyebrow and left eyebrow?
In addition to descriptive statistical analyzes, what other analyzes can be performed?"
Rio de Janeiro Crime Records,Crime records from Rio de Janeiro State,Daniel Esteves,14,"Version 1,2017-08-16",,CSV,4 MB,CC0,"1,459 views",192 downloads,,0 topics,https://www.kaggle.com/danielesteves/rio-police-records,"Context
Rio de Janeiro is one of the most beautiful and famous city in the world. Unfortunately, it's also one of the most dangerous. For the last years, in a scenario of economical and political crisis in Brazil, the State of Rio de Janeiro was one of the most affected. Since 2006, the Instituto de Segurança Pública do Rio de Janeiro (Institue of Public Security of Rio de Janeiro State) publishes reports of each police station.
Content
Three datasets are available: BaseDPEvolucaoMensalCisp - Monthly evolution of statistics by police station PopulacaoEvolucaoMensalCisp - Monthly evolution of population covered by police station delegacias - Info about each police station
Most of the data are in Brazilian Portuguese because it was extracted directly from government sites.
Acknowledgements
This dataset is provided by the Instituto de Segurança Pública. delegacias.csv was compiled by myself.
Inspiration
What is the most unsafe city in Rio de Janeiro State? And the safest? Which events can be correlated with the numbers in dataset? (Elections, crisis...) How crime correlates with population?"
Wordgame,0.3M word-word associations scraped from 10 internet forums,LouweAL,14,"Version 3,2017-07-21|Version 2,2017-06-28|Version 1,2017-06-23","linguistics
internet",CSV,8 MB,Other,"1,904 views",128 downloads,4 kernels,,https://www.kaggle.com/anneloes/wordgame,"This dataset was created to as part of an ongoing research on what information about the human mind could be possibly hidden in word-word associations.
Inspiration
Here are a few questions you might try to answer with this dataset:
How well can we classify word pairs as originating from a specific online community?
What are the properties of the graph/network of word associations?
What are frequently used words? Are there differences among communities?
Content
The data was scraped from 10 public internet forums. The data is anonymous (all usernames are converted to unique user IDs) and cleaned[script].
Most topics were still active at the time of scraping (june 2017), thus rescraping will result in a (slightly) bigger dataset.
The dataset contains 4 columns, {author, word1, word2, source}, where author is the person who wrote the second word as reaction to the first word. A word can also be a phrase or (in some cases) a sentence."
Mussel Watch,The longest running contaminant monitoring program in U.S. coastal waters,Sohier Dane,14,"Version 3,2017-09-18|Version 2,2017-08-29|Version 1,2017-08-29","ecology
pollution",CSV,156 MB,CC0,"1,268 views",229 downloads,5 kernels,,https://www.kaggle.com/sohier/mussel-watch,"The National Oceanic and Atmospheric Administration (NOAA) National Status and Trends (NS&T) Mussel Watch Program is a contaminant monitoring program that started in 1986. It is the longest running continuous contaminant monitoring program of its kind in the United States. Mussel Watch monitors the concentration of contaminants in bivalves (mussels and oysters) and sediments in the coastal waters of the U.S., including the Great Lakes, to monitor bivalve health and by extension the health of their local and regional environment.
Mussel Watch consults with experts to determine appropriate contaminants to monitor; these include dichlorodiphenyltrichloroethane (DDT), polycyclic aromatic hydrocarbons (PAHs), and polychlorinated biphenyls (PCBs). As of 2008, Mussel Watch monitors approximately 140 analytes. In addition to the effects of contaminants, Mussel Watch is able to assess the effects of natural disasters, such as the 2005 Hurricane Katrina, and environmental disasters, such as the 2010 Deepwater Horizon oil spill. Data collected by Mussel Watch can also be used to monitor the effectiveness of coastal remediation. The Mussel Watch Program utilized its 20 years of monitoring data to effectively analyze the impacts of Hurricane Katrina and has affected regulatory decisions based on the data it has collected on bivalve parasites.
You can find additional details about the history of the program here.
Data Notes
This version has been consolidated and lightly cleaned from its original format.
It was not possible to acquire data for all sites in mussel watch while preparing the dataset.
The pdf manuals are technically for specific sites and may not map perfectly to the data here. You can find manuals specific to each site here if need be.
Acknowledgements
This dataset is the result of the work of generations of scientists working for NOAA. You can find the original data here."
3 Million German Sentences,German language data from the Leipzig Corpus Collection,Rachael Tatman,14,"Version 1,2017-08-16","languages
linguistics",Other,382 MB,Other,"1,285 views",140 downloads,,0 topics,https://www.kaggle.com/rtatman/3-million-german-sentences,"Context
The Leipzig Corpora Collection presents corpora in different languages using the same format and comparable sources. All data are available as plain text files and can be imported into a MySQL database by using the provided import script. They are intended both for scientific use by corpus linguists as well as for applications such as knowledge extraction programs.
Content
This dataset contains 3 million sentences taken from newspaper texts in 2015. Non-sentences and foreign language material was removed. In addition to the sentences themselves, this dataset contains information on the frequency of each word. More information about the format and content of these files can be found here.
The corpora are automatically collected from carefully selected public sources without considering in detail the content of the contained text. No responsibility is taken for the content of the data. In particular, the views and opinions expressed in specific parts of the data remain exclusively with the authors.
Acknowledgements
This dataset is released under a CC-BY 4.0 license. If you use this dataset in your work, please cite the following paper:
D. Goldhahn, T. Eckart & U. Quasthoff: Building Large Monolingual Dictionaries at the Leipzig Corpora Collection: From 100 to 200 Languages. In: Proceedings of the 8th International Language Resources and Evaluation (LREC'12), 2012"
Every Pub in England,Every pub in the UK and its address,Rachael Tatman,14,"Version 1,2017-08-19","alcohol
europe",CSV,6 MB,Other,"2,240 views",256 downloads,5 kernels,4 topics,https://www.kaggle.com/rtatman/every-pub-in-england,"Context:
Pubs, or public houses, are popular traditional British gathering places where alcohol and food is served.
Content:
This dataset includes information on 51,566 pubs. This dataset contains the following columns:
fsa_id (int): Food Standard Agency's ID for this pub.
name (string)L Name of the pub
address (string): Address fields separated by commas.
postcode (string): Postcode of the pub.
easting (int)
northing (int)
latitude (decimal)
longitude (decimal)
local_authority (string): Local authority this pub falls under.
Acknowledgements:
The data was derived from the Food Standard Agency's Food Hygiene Ratings and the ONS Postcode Directory. The data is licensed under the Open Government Licence. (See the included .html file.)
Inspiration:
You could use this data as the basis for a real-life travelling salesman problem and plan the world’s longest pub crawl."
OpenCorpora: Russian,A Tagged 1.5 Million Word Corpus of Russian,Rachael Tatman,14,"Version 1,2017-09-13","languages
russia
linguistics",{}JSON,270 MB,CC4,"1,584 views",100 downloads,,0 topics,https://www.kaggle.com/rtatman/opencorpora-russian,"Context:
“Russian is an East Slavic language and an official language in Russia, Belarus, Kazakhstan, Kyrgyzstan and many minor or unrecognised territories. It is an unofficial but widely spoken language in Ukraine and Latvia, and to a lesser extent, the other countries that were once constituent republics of the Soviet Union and former participants of the Eastern Bloc.” -- “Russian Language” on Wikipedia
Russian has around 150 million native speakers and 110 million non-native speakers. Russian in written in Cyrillic script. This dataset is a morphologically, syntactically and semantically annotated corpus of texts in Russian, fully accessible to researchers and edited by users.
Content:
This dataset is encoded in UTF-8. There are two files included in this dataset: the corpus and the dictionary. The corpus is in .json format, while the dictionary is in plain text.
Dictionary
In the dictionary, each entry is a lemma, presented with all of its tagged derivations. The tags depend on the part of speech of the lemma. Some examples are:
Nouns: part of speech, animacy, gender & number, case
Verbs: Part of speech, aspect, transitivity, gender & number, person, tense, mood
Adjectives: part of speech (ADJF), gender, number, case
A Python script to convert the tags in this corpus to this set more commonly used in English-language linguistics can be found here.
Sample dictionary entries:
1
ЁЖ  NOUN,anim,masc sing,nomn
ЕЖА NOUN,anim,masc sing,gent
ЕЖУ NOUN,anim,masc sing,datv
ЕЖА NOUN,anim,masc sing,accs
ЕЖОМ    NOUN,anim,masc sing,ablt
ЕЖЕ NOUN,anim,masc sing,loct
ЕЖИ NOUN,anim,masc plur,nomn
ЕЖЕЙ    NOUN,anim,masc plur,gent
ЕЖАМ    NOUN,anim,masc plur,datv

41
ЁРНИЧАЮ VERB,impf,intr sing,1per,pres,indc
ЁРНИЧАЕМ    VERB,impf,intr plur,1per,pres,indc
ЁРНИЧАЕШЬ   VERB,impf,intr sing,2per,pres,indc
ЁРНИЧАЕТЕ   VERB,impf,intr plur,2per,pres,indc
ЁРНИЧАЕТ    VERB,impf,intr sing,3per,pres,indc
ЁРНИЧАЮТ    VERB,impf,intr plur,3per,pres,indc
ЁРНИЧАЛ VERB,impf,intr masc,sing,past,indc
ЁРНИЧАЛА    VERB,impf,intr femn,sing,past,indc
ЁРНИЧАЛО    VERB,impf,intr neut,sing,past,indc
ЁРНИЧАЛИ    VERB,impf,intr plur,past,indc
ЁРНИЧАЙ VERB,impf,intr sing,impr,excl
ЁРНИЧАЙТЕ   VERB,impf,intr plur,impr,excl
Corpous
In this corpus, each word has been grammatically tagged. You can access individual tokens using the following general path:
JSON > text > paragraphs > paragraph > [paragraph number] > sentence > [sentence number] > tokens > [token number]
Each token has: * A unique id number (@id) * The text of the token (@text) * information on the lemma (under “l”), including the id number of the lemma as found in the dictionary
You can see an example of the token portion of the .json structure below:
             {
                ""@id"": 1714292,
                ""@text"": ""сват"",
                ""tfr"": {
                  ""@t"": ""сват"",
                  ""@rev_id"": 3754311,
                  ""v"": {
                    ""l"": {
                      ""@id"": 314741,
                      ""@t"": ""сват"",
                      ""g"": [
                        {
                          ""@v"": ""NOUN""
                        },
                        {
                          ""@v"": ""anim""
                        },
                        {
                          ""@v"": ""masc""
                        },
                        {
                          ""@v"": ""sing""
                        },
                        {
                          ""@v"": ""nomn""
                        }
                      ]
                    }
                  }
            }
          }
Acknowledgements:
This dataset was collected and annotated by, among others, Svetlana Alekseeva, Anastasia Bodrova, Victor Bocharov, Dmitry Granovsky, Irina Krylova, Maria Nikolaeva, Catherine Protopopova, Alexander Chuchunkov, Anastasia Shimorina, Vasily Alekseev, Natalia Ostapuk, Maria Stepanova and Alexey Surikov. The code used to collect and clean this data is available online.
It is reproduced here under a CC-BY-SA license.
More information on this corpus and its most recent version can be found here (in Russian.)"
"US Traffic, 2015","7.1M Daily Traffic Volume Observations, By Hour and Direction",Jacob Boysen,14,"Version 1,2017-08-15",,Other,446 MB,CC0,"3,768 views",552 downloads,,0 topics,https://www.kaggle.com/jboysen/us-traffic-2015,"Context:
Traffic management is a critical concern for policymakers, and a fascinating data question. This ~2gb dataset contains daily volumes of traffic, binned by hour. Information on flow direction and sensor placement is also included.
Content:
Two datasets are included:
dot_traffic_2015.txt.gz
daily observation of traffic volume, divided into 24 hourly bins
station_id, location information (geographical place), traffic flow direction, and type of road
dot_traffic_stations_2015.txt.gz
deeper location and historical data on individual observation stations, cross-referenced by station_id
Acknowledgements:
This dataset was compiled by the US Department of Transportation and available on Google BigQuery
Inspiration:
Where are the heaviest traffic volumes? By time of day? By type of road?
Any interesting seasonal patterns to traffic volumes?"
Barcelona Accidents,List of people who have been involved in an accident in Barcelona (2010 - 2016),Marc Velmer,14,"Version 1,2017-09-15",road transport,CSV,17 MB,CC4,"2,159 views",246 downloads,,0 topics,https://www.kaggle.com/marcvelmer/barcelona-accidents,"Context
This dataset is a list of people who have been involved in an accident in the city of Barcelona (Spain) from year 2010 till 2016. This data is managed by the Police in the city of Barcelona and includes several information described below.
Content
This dataset is composed by 7 files, each one containing between 10k-12k lines.
Every row contains several information, like the type of injury (slightly wounded, serious injuries or death). It includes a description of the person (driver, passenger or pedestrian), sex, age, location, etc...
Important: This dataset is uploaded as it is, so it's possible that some data in some rows is missing/not correct.
Description of each column:
Número d'expedient: Case File Number
Codi districte: District code where the accident was. Barcelona is divided in several districts
Nom districte: Name of the district
Codi barri: Hood code where the accident was. Every district in Barcelona has several hoods
Nom barri: Name of the hood
Codi carrer: Street code (Every street has a code)
Nom carrer: Name of the street
Num postal caption: Postal number of the street
Descripció dia setmana: Day of the week in text (written in Catalan)
Dia setmana: Shortcode of the previous field (also in Catalan)
Descripció tipus dia: Description of the type of the day, it can be ""labor"" or ""festive"" (also in Catalan)
NK Any: Number of the year
Mes de any: Number of the month (1-12)
Nom mes: Name of the month (in Catalan)
Dia de mes: Day of the month
Descripció torn: Type of round of the police. It can be ""Matí"" (Morning), ""Tarda"" (Evening) or ""Nit"" (Night)
Hora de dia: Hour of the day (0-23)
Descripció causa vianant: Text in catalan. Describes the accident in case the victim is a pedestrian. If not, it says ""No és causa del vianant""
Desc. Tipus vehicle implicat: Type of vehicle in the accident. Also in Catalan.
Descripció sexe: Sex of the victim. ""Home"" means man, ""Dona"" means woman.
Descripció tipus persona: Type of role in the accident. It describes if the victim is the pilot (Conductor ), passenger (Passatger), pedestrian (Vianant)
Edat: Age of the victim
Descripció victimització: Type of injury in Catalan (slightly wounded (Ferit lleu), serious injuries (Ferit greu) or death (Mort))
Coordenada UTM (Y): UTM coordinate Y
Coordenada UTM (X): UTM coordinate X
As you can see, some columns could be removed and we wouldn't loose information. My experience working with these files tells me that some rows have no correct data or no data at all. So, be careful!
Acknowledgements
This data can be found in ""Open Data BCN - Barcelona's City Hall Open Data Service"", which is the owner of the CSV files.
Inspiration
I have uploaded this information here because I believe that data should be shared with everybody! So, do your own ""research"" and share it also! I'm always happy to get some feedback and help each other!"
Patent Litigations,"Detailed Patent Litigation Data on 74k Cases, 1963-2015",US Patent and Trademark Office,14,"Version 1,2017-07-13",law,CSV,2 GB,Other,"1,405 views",173 downloads,,0 topics,https://www.kaggle.com/uspto/patent-litigations,"Context
Achieving the appropriate balance of intellectual property (IP) protection through patent litigation is critical to economic growth. Examining the interplay between US patent law and economic effect is of great interest to many stakeholders. Published in March 2017, this dataset is the most comprehensive public body of information on USPTO patent litigation.
Content
The dataset covers over 74k cases across 52 years. Five different files (attorneys.csv, cases.csv, documents.csv, names.csv, pacer_cases.csv) detail the litigating parties, their attorneys, results, locations, and dates. The large documents.csv file covers more than 5 million relevant documents (a tool like split might be your friend here).
Acknowledgements
This data was collected by the Office of the Chief Economist at the USPTO. Data was collected from both the Public Access to Court Electronics Records (PACER), as well as RECAP, an independent PACER repository. Further documentation available via this paper.
Inspiration
Patent litigation is a tug of war between patent holders, competing parties using similar IP, and government policy. Which industries see the most litigation? Any notable changes over time? Is there a positive (or negative) correlation between litigation, and a company’s economic fortunes?
License
Public Domain Mark 1.0 Also see source."
US jobs on Monster.com,"22,000 US-based Job Listings",PromptCloud,14,"Version 1,2017-09-15",internet,CSV,65 MB,CC4,"4,475 views",619 downloads,2 kernels,,https://www.kaggle.com/PromptCloudHQ/us-jobs-on-monstercom,"Context
This is a pre-crawled dataset, taken as subset of a bigger dataset (more than 4.7 million job listings) that was created by extracting data from Monster.com, a leading job board.
Content
This dataset has following fields:
country
country_code
date_added
has_expired - Always false.
job_description - The primary field for this dataset, containing the bulk of the information on what the job is about.
job_title
job_type - The type of tasks and skills involved in the job. For example, ""management"".
location
organization
page_url
salary
sector - The industry sector the job is in. For example, ""Medical services"".
Acknowledgements
This dataset was created by PromptCloud's in-house web-crawling service.
Inspiration
What kinds of jobs titles correspond with what kinds of wages?
What can you learn about the Moster.com-based US job market based on analyzing the contents of the job descriptions?
How do job descriptions different between different industry sectors?"
Nutrition facts for Starbucks Menu,"Nutrition information for Starbucks menu items, including food and drinks",Starbucks,14,"Version 2,2017-07-21|Version 1,2017-07-19","food and drink
nutrition",CSV,44 KB,Other,"8,329 views","1,246 downloads",58 kernels,2 topics,https://www.kaggle.com/starbucks/starbucks-menu,"Context:
Starbucks is an American coffee chain founded in Seattle. It serves both beverages and food.
Content:
This dataset includes the nutritional information for Starbucks’ food and drink menu items. All nutritional information for drinks are for a 12oz serving size.
Acknowledgements:
Food composition data is in the public domain, but product names marked with ® or ™ remain the registered trademarks of Starbucks.
Inspiration:
Can you train a Markov Chain to generate new Starbucks drink or food items?
Can you design an easy-to-interpret visualization for the nutrition of each item?
How to Starbucks menu items compare to McDonald’s menu items (see link to dataset below) in terms of nutrition?
You may also like:
Nutrition Facts for McDonald's Menu
Starbucks Locations Worldwide"
Carbon Dioxide Levels in Atmosphere,Atmospheric carbon dioxide data from Mauna Loa Observatory since 1958,UC San Diego,14,"Version 1,2017-03-10","environment
climate",CSV,31 KB,Other,"4,797 views",731 downloads,9 kernels,,https://www.kaggle.com/ucsandiego/carbon-dioxide,"Context
The carbon dioxide record from Mauna Loa Observatory, known as the “Keeling Curve,” is the world’s longest unbroken record of atmospheric carbon dioxide concentrations. Scientists make atmospheric measurements in remote locations to sample air that is representative of a large volume of Earth’s atmosphere and relatively free from local influences.
Content
This dataset includes a monthly observation of atmospheric carbon dioxide (or CO2) concentrations from the Mauna Loa Observatory (Hawaii) at a latitude of 19.5, longitude of -155.6, and elevation of 3397 meters.
Columns 1-3: Provide the date in the following redundant formats: year, month and decimal date
Column 4: Monthly CO2 concentrations in parts per million (ppm) measured on the 08A calibration scale and collected at 24:00 hours on the fifteenth of each month.
Column 5: The fifth column provides the same data after a seasonal adjustment, which involves subtracting from the data a 4-harmonic fit with a linear gain factor to remove the seasonal cycle from carbon dioxide measurements
Column 6: The sixth column provides the data with noise removed, generated from a stiff cubic spline function plus 4-harmonic functions with linear gain
Column 7: The seventh column is the same data with the seasonal cycle removed.
Acknowledgements
The carbon dioxide data was collected and published by the University of California's Scripps Institution of Oceanography under the supervision of Charles David Keeling with support from the US Department of Energy, Earth Networks, and the National Science Foundation.
Inspiration
How have atmospheric carbon dioxide levels changed in the past sixty years? How do carbon dioxide concentrations change seasonally? What do you think causes this seasonal cycle? When will the carbon dioxide levels exceed 450 parts per million?"
Hospital ratings,The official dataset used on Medicare.gov for hospital quality comparison,Center for Medicare and Medicaid,14,"Version 1,2017-07-27","hospitals
public health
health",CSV,3 MB,CC0,"3,543 views",485 downloads,2 kernels,0 topics,https://www.kaggle.com/center-for-medicare-and-medicaid/hospital-ratings,"Context
This are the official datasets used on the Medicare.gov Hospital Compare Website provided by the Centers for Medicare & Medicaid Services. These data allow you to compare the quality of care at over 4,000 Medicare-certified hospitals across the country.
Content
Dataset fields:
Provider ID
Hospital Name
Address
City
State
ZIP Code
County Name
Phone Number
Hospital Type
Hospital Ownership
Emergency Services
Meets criteria for meaningful use of EHRs
Hospital overall rating
Hospital overall rating footnote
Mortality national comparison
Mortality national comparison footnote
Safety of care national comparison
Safety of care national comparison footnote
Readmission national comparison
Readmission national comparison footnote
Patient experience national comparison
Patient experience national comparison footnote
Effectiveness of care national comparison
Effectiveness of care national comparison footnote
Timeliness of care national comparison
Timeliness of care national comparison footnote
Efficient use of medical imaging national comparison
Efficient use of medical imaging national comparison
Acknowledgements
Dataset was downloaded from [https://data.medicare.gov/data/hospital-compare]
Inspiration
If you just broke your leg, you might need to use this dataset to find the best Hospital to get that fixed!"
Pulse of the Nation,Cards Against Humanity's Pulse of the Nation,Cards Against Humanity,14,"Version 1,2017-12-21","social sciences
demographics",CSV,478 KB,CC0,"1,216 views",259 downloads,2 kernels,0 topics,https://www.kaggle.com/cardsagainsthumanity/pulse-of-the-nation,"THE POLL
As part of Cards Against Humanity Saves America, this poll is funded for one year of monthly public opinion polls. Cards Against Humanity is asking the American people about their social and political views, what they think of the president, and their pee-pee habits.
To conduct their polls in a scientifically rigorous manner, they partnered with Survey Sampling International — a professional research firm — to contact a nationally representative sample of the American public. For the first three polls, they interrupted people’s dinners on both their cell phones and landlines, and a total of about 3,000 adults didn’t hang up immediately. They examined the data for statistically significant correlations which can be found here: https://thepulseofthenation.com/
Content
Polls are released each month (they are still polling so this will be updated each month)
Row one is the header and contains the questions
Each row is one respondent's answers
Questions in the Sep 2017 poll:
Income
Gender
Age
Age Range
Political Affiliation
Do you approve or disapprove of how Donald Trump is handling his job as president?
What is your highest level of education?
What is your race?
What is your marital status?
What would you say is the likelihood that your current job will be entirely performed by robots or computers within the next decade?
Do you believe that climate change is real and caused by people, real but not caused by people, or not real at all?""
How many Transformers movies have you seen?
Do you agree or disagree with the following statement: scientists are generally honest and are serving the public good.
Do you agree or disagree with the following statement: vaccines are safe and protect children from disease.
""How many books, if any have you read in the past year?""
Do you believe in ghosts?
What percentage of the federal budget would you estimate is spent on scientific research?
""Is federal funding of scientific research too high too low or about right?""
True or false: the earth is always farther away from the sun in the winter than in the summer.
""If you had to choose: would you rather be smart and sad or dumb and happy?""
Do you think it is acceptable or unacceptable to urinate in the shower?
Questions from Oct 2017 poll
Income
Gender
Age
Age Range
Political Affiliation
Do you approve or disapprove of how Donald Trump is handling his job as president?
What is your highest level of education?
What is your race?
From what you have heard or seen do you mostly agree or mostly disagree with the beliefs of White Nationalists?
If you had to guess what percentage of Republicans would say that they mostly agree with the beliefs of White Nationalists?
Would you say that you love America?
If you had to guess, what percentage of Democrats would say that they love America?
Do you think that government policies should help those who are poor and struggling in America?
If you had to guess, what percentage of Republicans would say yes to that question?
Do you think that most white people in America are racist?
If you had to guess, what percentage of Democrats would say yes to that question?
Have you lost any friendships or other relationships as a result of the 2016 presidential election?
Do you think it is likely or unlikely that there will be a Civil War in the United States within the next decade?
Have you ever gone hunting?
Have you ever eaten a kale salad?
If Dwayne ""The Rock"" Johnson ran for president as a candidate for your political party, would you vote for him?
Who would you prefer as president of the United States, Darth Vader or Donald Trump?
Questions from Nov 2017 poll
Income
Gender
Age
Age Range
In politics today, do you consider yourself a Democrat, a Republican or Independent?
Would you say you are liberal, conservative, or moderate?
What is your highest level of education? (High school or less, Some college, College degree, Graduate degree)
What is your race? (white, black, latino, asian, other)
Do you live in a city, suburb, or small town?
Do you approve, disapprove, or neither approve nor disapprove of how Donald Trump is handling his job as president?
Do you think federal funding for welfare programs in America should be increased, decreased, or kept the same?
Do you think poor black people are more likely to benefit from welfare programs than poor white people?
Do you think poor people in cities are more likely to benefit from welfare programs than poor people in small towns?
If you had to choose, would you rather live in a more equal society or a more unequal society?
Acknowledgements
These polls are from Cards Against Humanity Saves America and the raw data can be found here: https://thepulseofthenation.com/#future"
Sarcasm on Reddit,1.3 million labelled comments from Reddit,Dan Ofer,14,"Version 3,2018-02-13|Version 2,2017-05-24|Version 1,2017-05-24","humor
reddit
internet",CSV,209 MB,Other,"2,091 views",193 downloads,2 kernels,0 topics,https://www.kaggle.com/danofer/sarcasm,"Context
This dataset contains 1.3 million Sarcastic comments from the Internet commentary website Reddit. The dataset was generated by scraping comments from Reddit (not by me :)) containing the \s ( sarcasm) tag. This tag is often used by Redditors to indicate that their comment is in jest and not meant to be taken seriously, and is generally a reliable indicator of sarcastic comment content.
Content
Data has balanced and imbalanced (i.e true distribution) versions. (True ratio is about 1:100). The corpus has 1.3 million sarcastic statements, along with what they responded to as well as many non-sarcastic comments from the same source.
Labelled comments are in the train-balanced-sarcasm.csv file.
Acknowledgements
The data was gathered by: Mikhail Khodak and Nikunj Saunshi and Kiran Vodrahalli for their article ""A Large Self-Annotated Corpus for Sarcasm"". The data is hosted here.
Citation:
@unpublished{SARC,
  authors={Mikhail Khodak and Nikunj Saunshi and Kiran Vodrahalli},
  title={A Large Self-Annotated Corpus for Sarcasm},
  url={https://arxiv.org/abs/1704.05579},
  year=2017
}
Annotation of files in the original dataset: readme.txt.
Inspiration
Predicting sarcasm and relevant NLP features (e.g. subjective determinant, racism, conditionals, sentiment heavy words, ""Internet Slang"" and specific phrases).
Sarcasm vs Sentiment
Unusual linguistic features such as caps, italics, or elongated words. e.g., ""Yeahhh, I'm sure THAT is the right answer"".
Topics that people tend to react to sarcastically"
The Zurich Urban Micro Aerial Vehicle Dataset,"For appearance-based localization, visual odometry, and SLAM",Megan Risdal,14,"Version 2,2017-04-14|Version 1,2017-04-13","cities
vehicles",Other,383 MB,CC3,"2,610 views",199 downloads,4 kernels,0 topics,https://www.kaggle.com/mrisdal/zurich-urban-micro-aerial-vehicle,"Context
This dataset was originally published by the University of Zurich Robotics and Perception Group here. A sample of the data along with accompanying descriptions is provided here for research uses.
This presents the world's first dataset recorded on-board a camera equipped Micro Aerial Vehicle (MAV) flying within urban streets at low altitudes (i.e., 5-15 meters above the ground). The 2 km dataset consists of time synchronized aerial high-resolution images, GPS and IMU sensor data, ground-level street view images, and ground truth data. The dataset is ideal to evaluate and benchmark appearance-based topological localization, monocular visual odometry, simultaneous localization and mapping (SLAM), and online 3D reconstruction algorithms for MAV in urban environments.
Content
The entire dataset is roughly 28 gigabyte. We also provide a sample subset less than 200 megabyte, representing the first part of the dataset. You can download the entire dataset from this page.
The dataset contains time-synchronized high-resolution images (1920 x 1080 x 24 bits), GPS, IMU, and ground level Google-Street-View images. The high-resolution aerial images were captured with a rolling shutter GoPro Hero 4 camera that records each image frame line by line, from top to bottom with a readout time of 30 millisecond. A summary of the enclosed files is given below.
The data from the on-board barometric pressure sensor BarometricPressure.csv, accelerometer RawAccel.csv, gyroscope RawGyro.csv, GPS receiver OnbordGPS.csv, and pose estimation OnboardPose.csv is logged and timesynchronized using the clock of the PX4 autopilot board. The on-board sensor data was spatially and temporally aligned with the aerial images. The first column of every file contains the timestamp when the data was recorded expressed in microseconds. In the next columns the sensor readings are stored. The second column in OnbordGPS.csv encodes the identification number (ID) of every aerial image stored in the /MAV Images/ folder. The first column in GroundTruthAGL.csv is the ID of the aerial image, followed by the ground truth camera position of the MAV and the raw GPS data. The second column in GroundTruthAGM.csv is the ID of of the aerial image, followed by the ID of the first, second and third best match ground-level street view image in the /Street View Img/ folder.
Ground Truth
Two types of ground truth data are provided in order to evaluate and benchmark different vision-based localization algorithms. Firstly, appearance-based topological localization algorithms, that match aerial images to street level ones, can be evaluated in terms of precision rate and recall rate. Secondly, metric localization algorithms, that computed the ego-motion of the MAV using monocular visual SLAM tools, can be evaluated in terms of standard deviations from the ground truth path of the vehicle.
See more details here.
Past Research
The work listed below inspired the recording of this dataset. In these papers a much smaller dataset was used, that did not contain time synchronized GPS (except a small street segment ), IMU data and accurate metric ground truth. If you used this dataset, please send your paper to majdik (at) ifi (dot) uzh (dot) ch.
A.L. Majdik, D. Verda, Y. Albers-Schoenberg, D. Scaramuzza. Air-ground Matching: Appearance-based GPS-denied Urban Localization of Micro Aerial Vehicles Journal of Field Robotics, 2015.
A. L. Majdik, D. Verda, Y. Albers-Schoenberg, D. Scaramuzza Micro Air Vehicle Localization and Position Tracking from Textured 3D Cadastral Models IEEE International Conference on Robotics and Automation (ICRA), Hong Kong, 2014.
A. Majdik, Y. Albers-Schoenberg, D. Scaramuzza. MAV Urban Localization from Google Street View Data IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Tokyo, 2013.
License
These datasets are released under the Creative Commons license (CC BY-NC-SA 3.0), which is free for non-commercial use (including research).
Acknowledgements
This dataset was recorded with the help of Karl Schwabe, Mathieu Noirot-Cosson, and Yves Albers-Schoenberg. To record the dataset we used a Fotokite MAV offered to our disposal by Perspective Robotics AG—http://fotokite.com.
This work was supported by the National Centre of Competence in Research Robotics (NCCR) through the Swiss National Science Foundation and by the Hungarian Scientific Research Fund (No. OTKA/NKFIH 120499)."
Epileptic Seizure Recognition,Can you recognize if it is a seizure or not ?,Asma BELHAOUA,14,"Version 1,2017-10-29","neuroscience
health",CSV,7 MB,CC0,"2,550 views",281 downloads,2 kernels,,https://www.kaggle.com/younasm/epileptic-seizure-recognition,"Context
Epilepsy is a group of neurological disorders characterized by epileptic seizures. Epileptic seizures are episodes that can vary from brief and nearly undetectable to long periods of vigorous shaking.
Content
The original dataset consists of 5 different folders, each with 100 files, with each file representing a single subject/person. Each file is a recording of brain activity for 23.6 seconds. The corresponding time-series is sampled into 4097 data points. Each data point is the value of the EEG recording at a different point in time.
The column y contains the category of the 178-dimensional input vector. Specifically y in {1, 2, 3, 4, 5}
5- eyes open, means when they were recording the EEG signal of the brain the patient had their eyes open 4- eyes closed, means when they were recording the EEG signal the patient had their eyes closed 3- Yes they identify where the region of the tumor was in the brain and recording the EEG activity from the healthy brain area 2- They recorder the EEG from the area where the tumor was located
All subjects falling in classes 2, 3, 4, and 5 are subjects who did not have epileptic seizure
1- Recording of seizure activity
The Explanatory variables X1, X2, ..., X178
Each 178-dimensional vector contained in a row, represents a randomly selected 1-second long sample picked from the single file. Recall that each file is a recording of brain activity for 23.6 seconds. The corresponding time-series is sampled into 4097 data points.
Acknowledgements
Andrzejak RG, Lehnertz K, Rieke C, Mormann F, David P, Elger CE (2001) Indications of nonlinear deterministic and finite dimensional structures in time series of brain electrical activity: Dependence on recording region and brain state, Phys. Rev. E, 64, 061907
Inspiration
Can you help epileptic people live a better life ?"
Seattle Police Reports,Seattle Police Reports,Sam Harris,14,"Version 3,2016-10-25|Version 2,2016-10-25|Version 1,2016-10-25","cities
crime",CSV,96 MB,Other,"3,036 views",234 downloads,6 kernels,0 topics,https://www.kaggle.com/samharris/seattle-crime,All recorded police reports as taken from https://data.seattle.gov/Public-Safety/Seattle-Police-Department-Police-Report-Incident/7ais-f98f
Bug Triaging,"Bug triaging...contains details bug prioritization or assign, defect analysis.",Monika Munjal,14,"Version 2,2016-09-28|Version 1,2016-09-25",programming,CSV,3 MB,Other,"4,982 views",377 downloads,5 kernels,,https://www.kaggle.com/monika11/bug-triagingbug-assignment,Bug triage.
Raw Twitter Timelines w/ No Retweets,These are complete twitter timelines of various popular celebs with no retweets,James Littiebrant,14,"Version 2,2016-10-15|Version 1,2016-10-15","celebrity
linguistics
internet",CSV,35 MB,ODbL,"5,863 views",846 downloads,25 kernels,2 topics,https://www.kaggle.com/speckledpingu/RawTwitterFeeds,"This is a dataset of tweets from various active scientists and personalities ranging from Donald Trump and Hillary Clinton to Neil deGrasse Tyson. More are forthcoming.
They were obtained through javascript scraping of the browser twitter timeline rather than a Tweepy python API or the twitter timeline API.
The inspiration for this twitter dataset is comparing tweets in my own twitter analysis to find who tweets like whom, e.g. does Trump or Hillary tweet more like Kim Kardashian than one another?
Thus, this goes further back in time than anything directly available from Twitter.
The data is in JSON format rather than CSV, which will be forthcoming as well.
Kim Kardashian, Adam Savage, BillNye, Neil deGrasse Tyson, Donald Trump, and Hillary Clinton have been collected up to 2016-10-14 Richard Dawkins, Commander Scott Kelly, Barack Obama, NASA, and The Onion, tweets up to 2016-10-15.
For your own pleasure, with special thanks to the Trump Twitter Archive for providing some of the code, here is the JavaScript used to scrape tweets off of a timeline and output the results to the clipboard in JSON format:
1) Construct the query with from:TWITTERHANDLE since:DATE until:DATE
2) In the browser console set up automatic scrolling with: setInterval(function(){ scrollTo(0, document.body.scrollHeight) }, 2500)
3) Scrape the resulting timeline with: var allTweets = []; var tweetElements = document.querySelectorAll('li.stream-item');
for (var i = 0; i < tweetElements.length; i++) { try { var el = tweetElements[i]; var text = el.querySelector('.tweet-text').textContent; allTweets.push({ id: el.getAttribute('data-item-id'), date: el.querySelector('.time a').textContent, text: text, link: el.querySelector('div.tweet').getAttribute('data-permalink-path'), retweet: text.indexOf('\""@') == 0 && text.includes(':') ? true : false }); } catch(err) {} }; copy(allTweets);
Have fun!"
US Veteran Suicides,2005-2011 veteran deaths outside of combat by state,Aleksey Bilogur,14,"Version 1,2017-11-15","mental health
military",CSV,58 KB,CC4,"1,666 views",237 downloads,2 kernels,0 topics,https://www.kaggle.com/residentmario/us-veteran-suicides,"Context
There is a well-documented phenomenon of increased suicide rates among United States military veterans. One recent analysis, published in 2016, found the suicide rate amongst veterans to be around 20 per day. The widespread nature of the problem has resulted in efforts by and pressure on the United States military services to combat and address mental health issues in and after service in the country's armed forces.
In 2013 News21 published a sequence of reports on the phenomenon, aggregating and using data provided by individual states to typify the nationwide pattern. This dataset is the underlying data used in that report, as collected by the News21 team.
Content
The data consists of six files, one for each year between 2005 and 2011. Each year's worth of data includes the general population of each US state, a count of suicides, a count of state veterans, and a count of veteran suicides.
Acknowledgements
This data was originally published by News21. It has been converted from an XLS to a CSV format for publication on Kaggle. The original data, visualizations, and stories can be found at the source.
Inspiration
What is the geospatial pattern of veterans in the United States? How much more vulnerable is the average veteran to suicide than the average citizen? Is the problem increasing or decreasing over time?"
German Federal Elections 2017,Results in the different areas,Jens Laufer,14,"Version 3,2017-09-30|Version 2,2017-09-28|Version 1,2017-09-27",politics,Other,10 MB,Other,"2,462 views",306 downloads,4 kernels,3 topics,https://www.kaggle.com/jenslaufer/german-election-2017,"Context
The dataset reflects the votes for the different areas in Germany for the 2017 federal elections. See German Federal Election 2017 for more details
Content
The data was aquired from govdata.de, which is state website offering interesting datasets. The original dataset was not easy to use, therefore I did some reshaping without changing the data
Acknowledgements
The dataset is originally from https://www.govdata.de/web/guest/apps/-/details/bundestagswahl-2017
Inspiration
The data is interesting as it reflects the votes for all areas in Germany"
Comcast Consumer Complaints,Public complaints made about Comcast internet and television service.,Charlie H.,14,"Version 2,2016-11-29|Version 1,2016-11-28","business
internet
networks",CSV,11 MB,Other,"5,616 views",643 downloads,16 kernels,0 topics,https://www.kaggle.com/archaeocharlie/comcastcomplaints,"Comcast is notorious for terrible customer service and despite repeated promises to improve, they continue to fall short. Only last month (October 2016) the FCC fined them a cool $2.3 million after receiving over 1000 consumer complaints. After dealing with their customer service for hours yesterday, I wanted to find out more about others' experiences.
This will serve as a repository of public customer complaints filed against Comcast as I scrape them from the web. The data should not only provide a fun source for analysis, but it will help to pin down just what is wrong with Comcast's customer service."
Salt Lake City Crime Reports,"Includes latitude and longitudes, offence descriptions, and city council numbers",foenix,14,"Version 3,2017-08-19|Version 2,2017-01-10|Version 1,2016-12-20",crime,CSV,216 MB,CC0,"3,079 views",228 downloads,8 kernels,2 topics,https://www.kaggle.com/foenix/slc-crime,"Context
A collection of SLC End-of-Year Crime Reports geocoded to standard GPS coordinates.
Content
2016 Crime Statistics for Salt Lake City, UT. Includes:
Case Numbers
Offence Codes for categorization
Descriptions for context
IBR (National Incident-Based Reporting System Number)
Occurrence Date
Report Date
Day of the Week (1 = Monday, 7 = Sunday)
Location (Addresses in SLC)
City Council District
SLCPD Police Zones
SLCPD Grid
x_coordinate: note that this is based on epsg:32043 projections
y_coordinate: note that this is based on epsg:32043 projections
x_gps_coords (added by yours truly, converted to epsg:4326)
y_gps_coords (added by yours truly, converted to epsg:4326)
Data Accuracy Notes
Some data wrangling will still likely be required to clean up null columns.
I went ahead and lowercased column names (and corrected a spelling mistake in the y-coordinate column).
epsg:32043 projections were converted to epsg:4326 projections using pyplot with distances preserved.
Multiple year munging performed here: https://github.com/octaflop/slcpd/blob/master/develop/2017-08-16-crunch.ipynb
Still awaiting dataset owner clarification of Calls vs Cases
Acknowledgements
Taken from the SLC Open Data Web Site.
Thank you Dean Larson, the original dataset owner.
Thank you to the City of Salt Lake government and the Utah.gov catalog for providing this data for public use.
Thanks to the DAT Science EdEx course for inspiring me to take a look at my own city's crime stats.
Thank you to the SLCPD for keeping Salt Lake City citizens safe and enforcing an internal discipline of open data-collection.
Inspiration
Crime report locations by season?
Cross Reference of city council districts
Time of day
Offence descriptions
Moving centroids based on time of day / season?
Holiday rowdiness?
Coming Soon
Full 2016 reports (eta Spring 2017) ✔
3-year combined reports (eta Summer 2017) ✔
3-year combined cases vs calls (eta Summer 2017)
year-by-year files (eta Fall 2017)"
Marathon time Predictions,Predict Marathon Results from Athletes Open Data Sources,Andrea Girardi,14,"Version 2,2017-05-13|Version 1,2017-05-13",running,CSV,6 KB,CC0,"5,193 views",564 downloads,22 kernels,4 topics,https://www.kaggle.com/girardi69/marathon-time-predictions,"Context
Every Marathoner has a time goal in mind, and this is the result of all the training done in months of exercises. Long runs, Strides, Kilometers and phisical exercise, all add improvement to the result. Marathon time prediction is an art, generally guided by expert physiologists that prescribe the weekly exercises and the milestones to the marathon.
Unfortunately, Runners have a lot of distractions while preparing the marathon, work, family, illnes, and therefore each one of us arrives to the marathon with his own story. The ""simple"" approach is to look at data after the competition, the Leaderboard.
But what if we could link the Marathon result to the training history of the Athlete? Could we find that ""non orthodox"" training plans give good results?
The Athlete Training History
As a start, I'll take just two data from the Athlete History, easy to extract. Two meaningful data, the average km run during the 4 weeks before the marathon, and the average speed that the athlete has run these km.
Meaningful, because in the last month of the training I have the recap of all the previous months that brought me to the marathon.
Easy to extract, because I can go to Strava and I have a ""side-by-side"" comparison, myself and the reference athlete. I said easy, well, that's not so easy, since I have to search every athlete and write down those numbers, the exact day the marathon happened, otherwise I will put in the average the rest days after the marathon.
I've set my future work in extracting more data and build better algorithms. Thank you for helping me to understand or suggest.
Content
id:
simple counter
Marathon:
the Marathon name where the data were extracted. I use the data coming out from Strava ""Side by side comparison"" and the data coming from the final marathon result
Name:
The athlete's name, still some problems with UTF-8, I'll fix that soon
Category:
the sex and age group of a runner - MAM Male Athletes under 40 years - WAM Women under 40 Years - M40 Male Athletes between 40 and 45 years
km4week
This is the total number of kilometers run in the last 4 weeks before the marathon, marathon included. If, for example, the km4week is 100, the athlete has run 400 km in the four weeks before the marathon
sp4week
This is the average speed of the athlete in the last 4 training weeks. The average counts all the kilometers done, included the slow kilometers done before and after the training. A typic running session can be of 2km of slow running, then 12-14km of fast running, and finally other 2km of slow running. The average of the speed is this number, and with time this is one of the numbers that has to be refined
cross training:
If the runner is also a cyclist, or a triathlete, does it counts? Use this parameter to see if the athlete is also a cross trainer in other disciplines
Wall21: In decimal. The tricky field. To acknowledge a good performance, as a marathoner, I have to run the first half marathon with the same split of the second half. If, for example, I run the first half marathon in 1h30m, I must finish the marathon in 3h (for doing a good job). If I finish in 3h20m, I started too fast and I hit ""the wall"". My training history is, therefore, less valid, since I was not estimating my result
Marathon time:
In decimal. This is the final result. Based on my training history, I must predict my expected Marathon time
Category:
This is an ancillary field. It gives some direction, so feel free to use or discard it. It groups in:
- A results under 3h
- B results between 3h and 3h20m
- C results between 3h20m and 3h40m
- D results between 3h40 and 4h
Acknowledgements
Thank you to the main Athletes data sources, GARMIN and STRAVA
The Goal of this Competition:
Based on my training history, I must predict my expected Marathon time. Which other relevant data could help me to be more precise? Heart rate, cadence, speed training, what else? And how could I get those data?"
Who starts and who debunks rumors,Webpages cited by rumor trackers,Armineh Nourbakhsh,14,"Version 3,2017-03-27|Version 2,2017-03-22|Version 1,2017-03-21","linguistics
sociology",CSV,9 MB,CC0,"4,070 views",344 downloads,6 kernels,,https://www.kaggle.com/arminehn/rumor-citation,"Context
Emergent.info was a major rumor tracker, created by veteran journalist Craig Silverman. It has been defunct for a while, but its well-structured format and well-documented content provides an opportunity for analyzing rumors on the web.
Snopes.com is one of the oldest rumors trackers on the web. Originally launched by Barbara and David Mikkelson, it is now run by a team of editors who investigate urban legends, myths, viral rumors and fake news. The investigators try to provide a detailed explanation for why they have chosen to confirm or debunk a rumor, often citing several web pages and other external sources.
Politifact.com is a fact-checker that is focused on statements made by politicians and claims circulated by political campaigns, blogs and similar websites. Politifact's labels range from ""true,"" to ""pants on fire!""
Content
This dataset consists of three files. One file is a collection of all webpages cited in Emergent.info, and the second is a collection of webpages cited in Snopes.com, and the third is a similar collection from Politifact.com. The webpages were often cited because they had started a rumor, shared a rumor, or debunked a rumor.
Emergent.info
Emergent.info often provides a clean timeline of the rumor's propagation on the web, and identifies which page was for the rumor, which page was against it, and which page was simply observing it. Please refer to the image below to learn more about the fields in this dataset.
Snopes.com
The structure of posts on Snopes.com is not as well-defined. Please refer to the image below to learn more about the fields in the Snopes dataset.
Politifact.com
Similar to Emergent.info, Politifact.com follows a well-structured format in reporting and documenting rumors. There is a sidebar on the right side of each page that lists all of the sources cited within the page. The top link is the likeliest to be the original source of the rumor. For this link, page_is_first_citation is set to true.
Inspiration
I created this dataset in order to study domains that frequently start, propagate, or debunk rumors. By studying these domains and people who follow them, I hope to gain some insight into the dynamics of rumor propagation on the web, as well as social media.
Notes/Disclaimer
When using the Snopes dataset, please keep the following in mind:
In addition to debunking rumors, Snopes.com occasionally reports news and other types of content. This collection only includes data from ""Fact Check"" posts on Snopes.
Snopes.com was launched years ago. Some of the older posts on the website do not follow the current format of the site, therefore some of the fields might be missing.
Snopes.com used to use a service named ""DoNotLink.com"" for citation purposes. That service is no longer active and as a result some of the links are missing from older posts on Snopes.
In addition, some of the shortened links would time-out prior to resolution, in which case they would not be added to the dataset.
Occasionally, a website that has been cited has not maliciously started a rumor. For instance, Andy Borowitz is a humorist who writes for The New Yorker. His satirical column is sometimes mistaken for real news; as a result, The New Yorker may be cited as a source of fake news on Snopes.com. This does not mean that The New Yorker is a fake news website.
When using the Politifact dataset, please keep the following in mind:
The data included in this dataset are collected from the ""truth-o-meter"" page of Politifact.com.
Politifact often fact-checks statements made by politicians. Since this dataset is focused on websites, I have ignored all the posts in which the rumor was attributed to a person, a political party, a campaign, or an organization. Instead, I have only included rumors attributed explicitly to websites or blogs.
Useful Tips for Using the Snopes collection
As opposed to the Emergent collection where each page is flagged with whether it was for or against a rumor, no such information is available for the Snopes dataset. To avoid manually labeling the data, you may use the following heuristics to identify which page started a rumor:
Webpages that are cited in the ""Examples"" section of a post are often ""observing"" the rumor, i.e. they have not started it, but they are repeating it. In the snopes.csv file, these webpages have been flagged as ""page_is_example.""
Webpages that are cited in the ""Featured Image"" section of a post are often not related to the rumor. The editors on Snopes have simply extracted an image from those pages to embed in their posts. In the snopes.csv file, these webpages have been flagged as ""page_is_image_credit.""
Webpages that are cited through a secondary service (such as archive.is) are likelier to be rumor-propagators. Editors do not link to them directly so that a record of their page is available, even if it is later deleted.
If neither of these hints help, very often (but not always) the first link cited on the page (for which ""page_is_example"" and ""page_is_image_credit"" are false) is the link to a page that started the rumor. This link is identified by the ""page_is_first_citation"" field. Pages for which both ""page_is_first_citation"" and ""page_is_archived"" are true are very likely to be rumor propagators.
To identify satirical websites that are mistaken for real news, it's useful to inspect the way they are cited on Snopes. To demonstrate that a website contains satire or humor, Snopes writers often cite the ""about us"" page of the site. Therefore it's useful to see which domains often contain a URI to their ""about"" page (e.g. ""http://politicops.com/about-us/"")."
Planes in Satellite Imagery,Detect aircraft in Planet satellite image chips,rhammell,14,"Version 9,2018-01-22|Version 8,2017-09-11|Version 7,2017-07-29|Version 6,2017-06-29|Version 5,2017-06-20|Version 4,2017-05-26|Version 3,2017-05-19|Version 2,2017-05-15|Version 1,2017-05-02",,{}JSON,79 MB,CC4,"4,520 views",281 downloads,3 kernels,2 topics,https://www.kaggle.com/rhammell/planesnet,"Context
Satellite imagery provides unique insights into various markets, including agriculture, defense and intelligence, energy, and finance. New commercial imagery providers, such as Planet and BlackSky, are using constellations of small satellites to exponentially increase the amount of images of the earth captured every day.
This flood of new imagery is outgrowing the ability for organizations to manually look at each image that gets captured, and there is a need for machine learning and computer vision algorithms to help automate the analysis process.
The aim of this dataset is to help address the difficult task of detecting the location of airplanes in satellite images. Automating this process can be applied to many issues including monitoring airports for activity and traffic patterns, and defense intelligence.
Continusouly updates will be made to this dataset as new Planet imagery released. Current images were collected as late as July 2017.
Content
Provided is a zipped directory planesnet.zip that contains the entire dataset as .png image chips. Each individual image filename follows a specific format: {label} __ {scene id} __ {longitude} _ {latitude}.png
label: Valued 1 or 0, representing the ""plane"" class and ""no-plane"" class, respectively.
scene id: The unique identifier of the PlanetScope visual scene the image chip was extracted from. The scene id can be used with the Planet API to discover and download the entire scene.
longitude_latitude: The longitude and latitude coordinates of the image center point, with values separated by a single underscore.
The dataset is also distributed as a JSON formatted text file planesnet.json. The loaded object contains data, label, scene_ids, and location lists.
The pixel value data for each 20x20 RGB image is stored as a list of 1200 integers within the data list. The first 400 entries contain the red channel values, the next 400 the green, and the final 400 the blue. The image is stored in row-major order, so that the first 20 entries of the array are the red channel values of the first row of the image.
The list values at index i in labels, scene_ids, and locations each correspond to the i-th image in the data list.
Class Labels
The ""plane"" class includes 8000 images. Images in this class are near-centered on the body of a single airplane, with the majority of the plane's wings, tail, and nose also visible. Examples of different aircraft sizes, orientations, and atmospheric collection conditions are included. Example images from this class are shown below.
The ""no-plane"" class includes 24000 images. A third of these are a random sampling of different landcover features - water, vegetion, bare earth, buildings, etc. - that do not include any portion of an airplane. The next third are ""partial planes"" that contain a portion of an airplane, but not enough to meet the full definition of the ""plane"" class. The last third are ""confusers"" - chips with bright objects or strong linear features that resemble a plane - that have previously been mislabeled by machine learning models. Example images from this class are shown below.
Acknowledgements
Satellite imagery used to build PlanesNet is made available through Planet's Open California dataset, which is openly licensed. As such, this dataset is also available under the same CC-BY-SA license. Users can sign up for a free Planet account to search, view, and download thier imagery and gain access to their API."
US Candy Production by Month,From January 1972 to August 2017,Rachael Tatman,14,"Version 1,2017-10-14","food and drink
time series
product
+ 2 more...",CSV,10 KB,CC0,"4,780 views",732 downloads,4 kernels,,https://www.kaggle.com/rtatman/us-candy-production-by-month,"Context:
Halloween begins frenetic candy consumption that continues into the Christmas holidays and New Year’s Day, when people often make (usually short-lived) resolutions to lose weight. But all this consumption first needs production. The graph shows the relevant data from the industrial production index and its stunning seasonality
Content:
The industrial production (IP) index measures the real output of all relevant establishments located in the United States, regardless of their ownership, but not those located in U.S. territories. This dataset tracks industrial production every month from January 1972 to August 2017.
Acknowledgements:
Board of Governors of the Federal Reserve System (US), Industrial Production: Nondurable Goods: Sugar and confectionery product [IPG3113N], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/IPG3113N, October 13, 2017.
Inspiration:
Can you correct for the seasonality in this data?
Which months have the highest candy production?
Can you predict production for September through December 2017?"
Canadian Disaster Database,Over 1000 Disasters Affecting Canadians At-Home or Abroad Since 1900,criticalhits,14,"Version 4,2017-09-30|Version 3,2017-09-30|Version 2,2017-09-30|Version 1,2017-09-30","north america
time series
geography",CSV,2 MB,Other,"2,114 views",284 downloads,,,https://www.kaggle.com/criticalhits/canadian-disaster-database,"Context
The Canadian Disaster Database The Canadian Disaster Database (CDD) contains detailed disaster information on more than 1000 natural, technological and conflict events (excluding war) that have happened since 1900 at home or abroad and that have directly affected Canadians.
Content
Data description copied from: https://www.publicsafety.gc.ca/cnt/rsrcs/cndn-dsstr-dtbs/index-en.aspx
Dataset date range: 1900 - present
The CDD tracks ""significant disaster events"" which conform to the Emergency Management Framework for Canada definition of a ""disaster"" and meet one or more of the following criteria:
10 or more people killed
100 or more people affected/injured/infected/evacuated or homeless
an appeal for national/international assistance
historical significance
significant damage/interruption of normal processes such that the community affected cannot recover on its own
The database describes where and when a disaster occurred, the number of injuries, evacuations, and fatalities, as well as a rough estimate of the costs. As much as possible, the CDD contains primary data that is valid, current and supported by reliable and traceable sources, including federal institutions, provincial/territorial governments, non-governmental organizations and media sources.
Data is updated and reviewed on a semi-annual basis.
Data Field Description
Disaster Type The type of disaster (e.g. flood, earthquake, etc.) that occurred.
Date of Event The date a specific event took place.
Specific Location The city, town or region where a specific event took place.
Description of Event A brief description of a specific event, including pertinent details that may not be captured in other data fields (e.g. amount of precipitation, temperatures, neighbourhoods, etc.)
Fatalities The number of people killed due to a specific event.
Injured/Infected The number of people injured or infected due to a specific event.
Evacuees The number of individuals evacuated by the government of Canada due to a specific event.
Latitude & Longitude The exact geographic location of a specific event.
Province/Territory The province or territory where a specific event took place.
Estimated Total Cost A roll-up of all the costs listed within the financial data fields for a specific event.
DFAA Payments The amount, in dollars, paid out by Disaster Financial Assistance Arrangements (Public Safety Canada) due to a specific event.
Insurance Payments The amount, in dollars, paid out by insurance companies due to a specific event.
Provincial/Territorial Costs/Payments The amount, in dollars, paid out by a Province or Territory due to a specific event.
Utility Costs/Losses The amount of people whose utility services (power, water, etc.) were interrupted/affected by a specific event.
Magnitude A measure of the size of an earthquake, related to the amount of energy released.
Other Federal Institution Costs The amount, in dollars, paid out by other federal institutions.
Acknowledgements
Data gathered from: http://cdd.publicsafety.gc.ca Terms of use for commercial and non-comerical reproduction: https://www.publicsafety.gc.ca/cnt/ntcs/trms-en.aspx
Inspiration
This dataset provides valuable insight to natural and non-natrual disasters which have affected Canada.
Possible explorations: * Where do different types of disasters occur more frequently? * Which Province / Location in Canada has been hit the hardest in terms of fatalities, number of injuries, estimated total cost, etc.?
Spatial-temporal correlations between natural/artifical distasters *
I think that this can be used to produce some interesting data visualizations. Some of the questions I look forward to answering include:
Can any spatial-temporal correlations between disasters be found in this dataset?
Which locations in Canada have been hit the hardest, in terms of people injured, fatalities, financial impact, etc."
Movie Industry,Three decades of movies,Daniel Grijalva,14,"Version 2,2017-10-05|Version 1,2017-09-29","film
entertainment",CSV,953 KB,CC0,"2,924 views",427 downloads,5 kernels,0 topics,https://www.kaggle.com/danielgrijalvas/movies,"Context
Is the movie industry dying? is Netflix the new entertainment king? Those were the first questions that lead me to create a dataset focused on movie revenue and analyze it over the last decades. But, why stop there? There are more factors that intervene in this kind of thing, like actors, genres, user ratings and more. And now, anyone with experience (you) can ask specific questions about the movie industry, and get answers.
Content
There are 6820 movies in the dataset (220 movies per year, 1986-2016). Each movie has the following attributes:
budget: the budget of a movie. Some movies don't have this, so it appears as 0
company: the production company
country: country of origin
director: the director
genre: main genre of the movie.
gross: revenue of the movie
name: name of the movie
rating: rating of the movie (R, PG, etc.)
released: release date (YYYY-MM-DD)
runtime: duration of the movie
score: IMDb user rating
votes: number of user votes
star: main actor/actress
writer: writer of the movie
year: year of release
Acknowledgements
This data was scraped from IMDb.
Contribute
You can contribute via GitHub."
Open Sprayer images,A collection of broad leaved dock images for weed sprayer,GavinArmstrong,14,"Version 1,2017-11-10","agriculture
agronomy
surveillance
image data",Other,148 MB,CC0,912 views,80 downloads,,,https://www.kaggle.com/gavinarmstrong/open-sprayer-images,"OpenSprayer.com
Open Sprayer will hopefully be an open sourced autonomous land drone that will propel itself across the fields spraying weeds it can see with its mounted cameras. The project should involve a mix of mechanical engineering, classical software design and machine learning to achieve its goal. The project is meant to be a DIY effort to compete with the big companies like John Deere currently developing similar tech. The benefit of an open design is cheaper capital and maintenance cost. The ability to fix, update and repair your own sprayer would offer a great alternative to the usual high running costs of branded machines.
The data set includes around 150 photos with annotations (Bounding box coordinates) locating the broad leaved docks. 70% were taken by me with the remaining being collected on google. I plan to update the images to better reflect the images that the sprayer drone will produce when operating. When the drone is running photos will most likely be taken from a height looking straight down at the ground, therefore the google images may be useless or not? Give me feedback and I can take more pictures to improve the dataset."
2014 American Community Survey,Detailed information about the American people and workforce,US Census Bureau,14,"Version 4,2016-10-31|Version 3,2016-10-28|Version 2,2016-10-25|Version 1,2016-10-20","demographics
sociology",Other,3 GB,CC0,"7,765 views",804 downloads,27 kernels,0 topics,https://www.kaggle.com/census/2014-american-community-survey,"The 2014 American Community Survey Public Use Microdata Sample
Context
The American Community Survey (ACS) is an ongoing survey that provides vital information on a yearly basis about our nation and its people. Information from the survey generates data that help determine how more than $400 billion in federal and state funds are distributed each year.
Frequency: Annual
Period: 2014
Content
Through the ACS, we know more about jobs and occupations, educational attainment, veterans, whether people own or rent their home, and other topics. Public officials, planners, and entrepreneurs use this information to assess the past and plan the future. When you respond to the ACS, you are doing your part to help your community plan hospitals and schools, support school lunch programs, improve emergency services, build bridges, and inform businesses looking to add jobs and expand to new markets, and more. The data dictionary can be found here.
Inspiration
Kernels created using the 2013 ACS can serve as excellent starting points for working with the 2014 ACS. For example, the following analyses were created using ACS data:
Work arrival times and earnings in the USA
Inequality in STEM careers
Acknowledgements
The American Community Survey (ACS) is administered, processed, researched and disseminated by the U.S. Census Bureau within the U.S. Department of Commerce."
Questions from Cross Validated Stack Exchange,"Full text of Q&A from Cross Validated, the Stack Exchange statistics site",Stack Overflow,14,"Version 1,2016-10-21","statistics
internet",CSV,453 MB,Other,"4,252 views",276 downloads,10 kernels,2 topics,https://www.kaggle.com/stackoverflow/statsquestions,"Full text of questions and answers from Cross Validated, the statistics and machine learning Q&A site from the Stack Exchange network.
This is organized as three tables:
Questions contains the title, body, creation date, score, and owner ID for each question.
Answers contains the body, creation date, score, and owner ID for each of the answers to these questions. The ParentId column links back to the Questions table.
Tags contains the tags on each question
For space reasons only non-deleted and non-closed content are included in the dataset. The dataset contains questions up to 19 October 2016 (UTC).
License
All Stack Exchange user contributions are licensed under CC-BY-SA 3.0 with attribution required."
"Parking Violations, December 2015",Parking citation locations in the District of Columbia,ArcGIS Open Data,14,"Version 1,2016-11-30","automobiles
road transport",CSV,25 MB,Other,"3,024 views",242 downloads,12 kernels,0 topics,https://www.kaggle.com/arcgisopendata/dc-parking-violations,"Context
The Vision Zero data contained in this layer pertain to parking violations issued by the District of Columbia's Metropolitan Police Department (MPD) and partner agencies with the authority. Parking violation locations are summarized ticket counts based on time of day, week of year, year, and category of violation. Data was originally downloaded from the District Department of Motor Vehicle's eTIMS meter work order management system.
Content
This dataset contains 132,850 rows of:
OBJECTID (unique ID)
ROWID_ DAY_OF_WEEK (text)
HOLIDAY (number)
WEEK_OF_YEAR (number)
MONTH_OF_YEAR (number)
ISSUE_TIME (number)
VIOLATION_CODE (text)
VIOLATION_DESCRIPTION (text)
LOCATION (text)
RP_PLATE_STATE (text)
BODY_STYLE (text)
ADDRESS_ID (number)
STREETSEGID (number)
XCOORD (number)
YCOORD (number)
TICKET_ISSUE_DATE (date or time)
X
Y
Acknowledgements
The dataset is shared by DCGISopendata, and the original dataset and metadata can be found here.
Inspiration
Can you use the dataset to determine:
Which area has the highest concentration of parking violations? How about by type of violation?
Do areas with high concentration of parking violations change throughout the month of December? Can you identify any trends?"
International Greenhouse Gas Emissions,A global GHG inventory from 1990-2017,United Nations,14,"Version 1,2017-11-17","earth sciences
climate",CSV,989 KB,Other,"1,634 views",265 downloads,2 kernels,0 topics,https://www.kaggle.com/unitednations/international-greenhouse-gas-emissions,"The Greenhouse Gas (GHG) Inventory Data contains the most recently submitted information, covering the period from 1990 to the latest available year, to the extent the data have been provided. The GHG data contain information on anthropogenic emissions by sources and removals by sinks of the following GHGs (carbon dioxide (CO2), methane (CH4), nitrous oxide (N2O), hydrofluorocarbons (HFCs), perfluorocarbons (PFCs), unspecified mix of HFCs and PFCs, sulphur hexafluoride (SF6) and nitrogen triflouride (NF3)) that are not controlled by the Montreal Protocol.
GHG emission inventories are developed by Parties to the Convention using scientific and methodological guidance from the Intergovernmental Panel on Climate Change (IPCC), such as 2006 IPCC Guidelines for National Greenhouse Gas Inventories, Revised Guidelines for National Greenhouse Gas Inventories (1996), IPCC Good Practice Guidance and Uncertainty Management in National Greenhouse Gas Inventories (2000) and IPCC Good Practice Guidance on Land Use, Land-use Change and Forestry (2003). Last update in UNdata: 23 Mar 2017 with data released in Nov 2016.
Acknowledgements
This dataset was kindly published by the United Nation on the UNData site. You can find the original dataset here.
License
Per the UNData terms of use: all data and metadata provided on UNdata’s website are available free of charge and may be copied freely, duplicated and further distributed provided that UNdata is cited as the reference."
Retrosheet events 1970 - 2015,A granular history of baseball,Ben Dilday,13,"Version 3,2016-08-21|Version 2,2016-08-21|Version 1,2016-08-21","baseball
history",CSV,960 MB,CC0,"4,345 views",351 downloads,20 kernels,2 topics,https://www.kaggle.com/bdilday/retrosheet-events-1970-2015,"This data set comprises events for major league baseball, provided by http://retrosheet.org.
 The information used here was obtained free of
 charge from and is copyrighted by Retrosheet.  Interested
 parties may contact Retrosheet at ""www.retrosheet.org"".
Roughly speaking an event is an outcome in a baseball game. This includes the end result of a plate appearance (strikeout, out in the field, hit, base on balls), events that occur within a plate appearance (stolen bases, caught stealing), and rare other occurrences. The retrosheet event data prior to 1955 are not complete. The data subsequent to 1988 include pitch counts while the data prior do not. The data here cover the years 1970-2015, in three divisions (1970-1992, 1993-2004, 2005-2015) that correspond, roughly, to distinct eras with different run-scoring environments. These data have specifically been obtained with a mix of the data dumps available at baseball heatmaps and with the py-retrosheet Python package, available on github.
I have augmented the data provided by retrosheet with some additional fields. Most substantively the rows include the wOBA value of the event, in the field woba_pts, and an estimated time stamp, in units of seconds since Jan. 1, 1900 (time_since_1900).
The conversion from retrosheet files to sql and csv is done by the chadwick software. A detailed description of all of the fields is available on the documentation for chadwick, http://chadwick.sourceforge.net/doc/cwevent.html. In order to keep the file sizes down, I have limited the fields in this data set to a subset of the fields described in the chadwick documentation.
The master.csv file is a subset of the Baseball Databank data and is released under a Creative Commons Attribution-ShareAlike 3.0 Unported License.https://creativecommons.org/licenses/by-sa/3.0/. More details are available on the Baseball Databank github https://github.com/chadwickbureau/baseballdatabank"
United States crime rates by county,County-level crime data of the United States,Mike Johnson Jr,13,"Version 1,2016-12-28",,CSV,337 KB,CC0,"4,691 views",403 downloads,,,https://www.kaggle.com/mikejohnsonjr/united-states-crime-rates-by-county,"Content
Data Sources : - Crime (2016): https://www.icpsr.umich.edu/icpsrweb/ - Population (2013): https://census.gov"
Housing Prices Dataset,Copied from UCLA this data set contains information related to housing prices.,Apratim Bhattacharya,13,"Version 1,2016-10-23",,CSV,34 KB,Other,"12,015 views","1,438 downloads",19 kernels,0 topics,https://www.kaggle.com/apratim87/housingdata,UCLA Housing dataset
Dataset malware/beningn permissions Android,Dataset malware/beningn permissions Android,Christian Urcuqui,13,"Version 1,2016-10-24","crime
computer security",CSV,270 KB,Other,"6,027 views",544 downloads,4 kernels,2 topics,https://www.kaggle.com/xwolf12/datasetandroidpermissions,"This dataset is a result of my research production in machine learning and android security. The data were obtained by a process that consisted to create a binary vector of permissions used for each application analyzed {1=used, 0=no used}. Moreover, the samples of malware/benign were devided by ""Type""; 1 malware and 0 non-malware.
One important topic to work is to create a good set of malware, because it is difficult to find one updated and with a research work to support it .
If your papers or other works use our dataset, please cite our COLCOM 2016 paper as follows. Urcuqui, C., & Navarro, A. (2016, April). Machine learning classifiers for android malware analysis. In Communications and Computing (COLCOM), 2016 IEEE Colombian Conference on (pp. 1-6). IEEE.
If you need an article in english, you can download another of my works: Urcuqui, Christian., & Navarro, Andres. (2016). Framework for malware analysis in Android. Sistemas & Telemática, 14(37), 45-56.
ccurcuqui@icesi.edu.co"
1000 parallel sentences,"1000 parallel sentences of Korean, English, Japanese, Spanish, and Indonesian",Kyubyong Park,13,"Version 1,2016-12-25","languages
linguistics",CSV,270 KB,CC4,"1,950 views",115 downloads,,0 topics,https://www.kaggle.com/bryanpark/parallelsents,"Context
A few years ago, I investigated a Korean corpus in order to find the most frequent 1000 words. Subsequently, I asked native speakers to translate those words and their example sentences into English, Japanese, Spanish, and Indonesian. I've totally forgotten this data since then, but it flashed on me this might be helpful for some people. Undoubtedly, 1000 sentences are a pretty small corpus, but it is also true that parallel corpora are hard to get.
Content
It's a csv file. As you expect, the first line is the heading.
ID: Id of the headword. It is arranged by alphabetical order.
HEADWORD: 1000 most frequent Korean words.
POS: Part of speech.
ENGLISH: English meaning or equivalent.
JAPANESE: Japanese meaning or equivalent.
SPANISH: Spanish meaning or equivalent.
INDONESIAN: Indonesian meaning or equivalent.
EXAMPLE (KO): An example sentence
EXAMPLE (EN): English translation
EXAMPLE (JA): Japanese translation
EXAMPLE (ES): Spanish translation
EXAMPLE (ID): Indonesian translation
Inspiration
For now, I'm not sure how this small corpus can be used. Hopefully this will be helpful for some pilot linguistic project."
Museum Reviews Collected from TripAdvisor,"Museum name, type, location, review, rating, rank, etc.",annecool37,13,"Version 1,2016-09-20",museums,{}JSON,10 MB,Other,"5,516 views",737 downloads,2 kernels,3 topics,https://www.kaggle.com/annecool37/museum-data,"Visit my GitHub for detailed description and code.
Also, here's an App, a Recommendation System for Museum Selection, I created with this data set: http://216.230.228.88:3838/bootcamp006_project/Project5-Capstone/Museo/app/
Inspiration for Further Analysis
Predict if a museum will be featured or not
Discover the important factors that results in a higher rating
Apply natural language processing to discover insights from review/quote/tag data
Cluster museums based on review/quote polarity or subjectivity (sentiment analysis)
Apply association rules to uncover relationship of different combinations of review tags
Processed and Merged Data
tripadvisor_merged.csv: A file containing museum data collected from TripAdvisor including tag/ type/ review/ quote features
Raw Data Scraped from TripAdvisor (US/World)
tripadvisor_museum: general museum data scraped from TripAdvisor
traveler_type: {'museum': ['Families','Couples','Solo','Business','Friends']}
traveler_rating: {'museum': ['Excellent','Very good','Average','Poor','Terrible']}
tag_clouds: {'museum': ['tag 1', 'tag 2', 'tag 3' ...]}
review_quote: {'museum': ['quote 1', 'quote 2', ... ,'quote 10']}
review_content: {'museum': ['review 1', 'review 2', ... ,'review 10']}
museum_categories: {'museum': ['museum type 1','museum type 2', ...]}"
Dengue Cases in the Philippines,"Monthly and Regional Cases of Dengue per 100,000 Population from 2008 to 2016",Francis Paul Flores,13,"Version 1,2017-10-30","healthcare
diseases
public health
demographics",CSV,51 KB,CC0,"1,920 views",275 downloads,3 kernels,,https://www.kaggle.com/grosvenpaul/dengue-cases-in-the-philippines,"Context
Data set contains the recorded number of dengue cases per 100,000 population per region of the Philippines from 2008 to 2016
Content
This is a small data set that is a good starting point for beginners that wants to play around with small scale temporal and spatial data set
Acknowledgements
Publisher would like to thank the Department of Health of the Philippines for providing the raw data
Inspiration
What is the trend of dengue cases in the Philippines? What region/s recorded the highest prevalence of dengue cases? In what specific years do we observe the highest dengue cases? When and where will a possible dengue outbreak occur?"
Christmas Tweets,"50,000 scraped tweet metadata from this 2k16 Christmas",DhruvMangtani,13,"Version 1,2016-12-26","faith and traditions
internet",CSV,70 MB,ODbL,"4,761 views",345 downloads,4 kernels,,https://www.kaggle.com/dhruvm/christmastwitterdata,"Context
This dataset contains the metadata of over 50,000 tweets from Christmas Eve and Christmas. We are hoping the data science and research community can use this to develop new and informative conclusions about this holiday season.
Content
We acquired this data through a web crawler written in Java. The first field is the id of the tweet, and the second is the HTML metadata. We recommend using BeautifulSoup or another library to parse this data and extract information from each tweet.
Inspiration
We would especially like to see research on the use of emojis in tweets, the type of sentiment there is on Christmas (Maybe determine how grateful each country is), or some kind of demographic on the age or nationality of active Twitter users during Christmas."
2016 US Presidential Primary Debates,Full transcripts of the debates among all the 2016 contenders for US President,Ed King,13,"Version 2,2016-10-19|Version 1,2016-10-18",politics,CSV,4 MB,CC0,"6,557 views",592 downloads,34 kernels,,https://www.kaggle.com/kinguistics/2016-us-presidential-primary-debates,"Overview
Only two major-party candidates are competing in the 2016 US presidential election, but there was tough competition to get to the general election. This dataset contains transcripts of every Democratic, Republican, and Republican Undercard debate held during the 2016 primary season.
This dataset is meant to be a complement to Megan Risdal's transcripts of the 2016 US Presidential (General Election) Debates.
So you can now take all of the questions (who talks the most? who has a wider vocabulary?) that you answered in the general election debates, and apply the same procedures to see how your favorite (or least favorite) candidate has changed over time.
The column names (and order) in this dataset are a superset of those found in the general election dataset, and non-speech annotations (such as ""(applause)"") in this dataset are also a superset. Kernels uploaded for the general election dataset should be compatible with this dataset as well; please let me know if you have any compatibility issues.
What in the world is an ""Undercard"" debate?
The field of Republicans running for President in the primaries was (yuuuuge!) pretty big: 17 candidates threw their hat in the ring at one point or another. Debate organizers realized that having 17 people on stage (each with a set amount of time to answer a question / respond to a criticism / interrupt each other) would, in the best case, lead to a three-to-four-hour-long debate (and, in the worst case, lead to a chaotic shout-fest as candidates tried to talk over each other for three to four hours).
To alleviate this issue, many of the Republican debate nights were split into two separate debates: the main debate with the top party contenders, aired live during primetime; and the Undercard debate, which typically aired a few hours earlier than the main debate.
The criteria for a candidate to be allowed into the main debate (rather than the ""kids' table"" debate, as some pundits derisively called the Undercard event) varied a bit by event. Typically a poll showing of 1% in one of several specified polls was sufficient to gain admission to the Undercard. To get into the main debate, candidates had to either (a) be polling above a different, higher, percentage in specific polls, or (b) be among the top n Republican candidates in said polls.
The details get a little thorny (certain debates had multiple criteria, of which candidates had to meet at least one), so I refer questions to the individual Undercard debate pages at the American Presidency Project for detailed criteria.
In this dataset, the split between Republican debates is made in the Party column: Republican is used for the primetime main events, and Republican Undercard is used for the Undercard.
Acknowledgements
All transcripts were scraped from the presidential debates page of the American Presidency Project at the University of California, Santa Barbara. Individual lines in the dataset contain links to the particular page for that debate.
Updates
v2 contains a few minor changes related to normalizing parenthesized elements (non-speech) within the Text field, and adding a few lines of interruptions that persisted in the Text field
The Data
The fields are described more fully in the file description. This section describes the particular elements that can appear in the Speaker and Text fields.
Who's Who?
(aka, the Speaker column)
The primary debates had a ton of participants. This dataset contains utterances from 22 politicians and 49 moderators, not to mention the occasional audience laughter or 2-minute timer.
Almost all Speaker columns contain either a single title-case name (listed below in the Participants and Moderators subsections) or a single upper-case word indicating non-speaker noise (listed below in the None-speaker Turns subsection); the exceptions to this are cases where a name is concatenated with a space and a parenthesized tag, as follows:
spkr (VIDEO): transcriptions of pre-recorded material of any of the candidates or moderators
spkr (TRANSLATED): in the Univision/Telemundo debate, some questions and answers are translated into English from the original Spanish; the transcript reflects the translations as spoken by a translator
Non-speaker Turns
The non-speaker turns in the Speaker column are:
AUDIENCE: any laughter, booing, applause, etc. from the audience
CANDIDATES: cross-talk between candidates
OTHER: non-speaker, non-audience noise (such as commercial break, timer bell, national anthem, etc.)
QUESTION: a question from an audience member (or a prerecorded question)
UNKNOWN: cases where the transcriber could hear a phrase but could not determine who said it
Here are the various speakers who appear in the dataset:
Candidates
Democratic
Chafee: Former Governor Lincoln Chafee (RI)
Clinton: Former Secretary of State Hillary Clinton
O'Malley: Former Governor Martin O'Malley (MD)
Sanders: Senator Bernie Sanders (VT)
Webb: Former Senator Jim Webb (VA)
Republican
Bush: Former Governor Jeb Bush (FL)
Carson: Ben Carson
Cruz: Senator Ted Cruz (TX)
Kasich: Governor John Kasich (OH)
Paul: Senator Rand Paul (KY)
Rubio: Senator Marco Rubio (FL)
Trump: Donald Trump
Walker: Governor Scott Walker (WI)
Republican (Undercard ONLY)
Gilmore: Former Governor Jim Gilmore (VA)
Graham: Senator Lindsey Graham (SC)
Jindal: Governor Bobby Jindal (LA)
Pataki: Former Governor George Pataki (NY)
Perry: Former Governor Rick Perry (TX)
Santorum: Former Senator Rick Santorum (PA)
Republican (Main AND Undercard)
Christie: Governor Chris Christie (NJ)
Fiorina: Carly Fiorina
Huckabee: Former Governor Mike Huckabee (AR)
All candidates in a Python list for easy copy/paste
['Bush', 'Carson', 'Chafee', 'Christie', 'Clinton', 'Cruz', 'Fiorina', 'Gilmore', 'Graham', 'Huckabee', 'Jindal', 'Kasich', ""O'Malley"", 'Pataki', 'Paul', 'Perry', 'Rubio', 'Sanders', 'Santorum', 'Trump', 'Walker', 'Webb']
Moderators
NOTE: Some moderators are seen across various debates; in particular, the Republican main debates and undercard debates on a given day tend to share the same moderators. Some moderators are public figures who are seen only in videos (with the (VIDEO) tag).
Moderators
Arrarás: María Celeste Arrarás (Telemundo)
Baier: Bret Baier (Fox News)
Baker: Gerard Baker (The Wall Street Journal)
Bartiromo: Maria Bartiromo (Fox Business Network)
Bash: Dana Bash (CNN)
Blitzer: Wolf Blitzer (CNN)
Cavuto: Neil Cavuto (Fox Business Network)
Cooney: Kevin Cooney (CBS News)
Cooper: Anderson Cooper (CNN)
Cordes: Nancy Cordes (CBS News)
Cramer: Jim Cramer (CNBC)
Cuomo: Governor Andrew Cuomo (NY)
Dickerson: John Dickerson (CBS News)
Dinan: Stephen Dinan (Washington Times)
Epperson: Sharon Epperson (CNBC)
Garrett: Major Garrett (CBS News)
Ham: Mary Katharine Ham (Hot Air)
Hannity: Sean Hannity (Fox News)
Harwood: John Harwood (CNBC)
Hemmer: Bill Hemmer (Fox News)
Hewitt: Hugh Hewitt (Salem Radio Network)
Holt: Lester Holt (NBC News)
Ifill: Gwen Ifill (PBS)
Kelly: Megyn Kelly (Fox News)
Lemon: Don Lemon (CNN)
Levesque: Neil Levesque (New Hampshire Institute of Politics)
Lopez: Juan Carlos Lopez (CNN en Espanol)
Louis: Errol Louis (NY1)
MacCallum: Martha MacCallum (Fox News)
Maddow: Rachel Maddow (MSNBC)
Mcelveen: Josh McElveen (WMUR-TV)
Mitchell: Andrea Mitchell (NBC News)
Muir: David Muir (ABC News)
O'Reilly: Bill O'Reilly (Fox News)
Obradovich: Kathie Obradovich (The Des Moines Register)
Quick: Becky Quick (CNBC)
Quintanilla: Carl Quintanilla (CNBC)
Raddatz: Martha Raddatz (ABC News)
Ramos: Jorge Ramos (Univision)
Regan: Trish Regan (Fox Business Network)
Salinas: María Elena Salinas (Univision)
Santelli: Rick Santelli (CNBC)
Seib: Gerald Seib (The Wall Street Journal)
Strassel: Kimberly Strassel (The Wall Street Journal)
Tapper: Jake Tapper (CNN)
Todd: Chuck Todd (MSNBC)
Tumulty: Karen Tumulty (The Washington Post)
Wallace: Chris Wallace (Fox News)
Woodruff: Judy Woodruff (PBS)
All moderators in a Python list for easy copy/paste
['Arrarás', 'Baier', 'Baker', 'Bartiromo', 'Bash', 'Blitzer', 'Cavuto', 'Cooney', 'Cooper', 'Cordes', 'Cramer', 'Cuomo', 'Dickerson', 'Dinan', 'Epperson', 'Garrett', 'Ham', 'Hannity', 'Harwood', 'Hemmer', 'Hewitt', 'Holt', 'Ifill', 'Kelly', 'Lemon', 'Levesque', 'Lopez', 'Louis', 'MacCallum', 'Maddow', 'Mcelveen', 'Mitchell', 'Muir', ""O'Reilly"", 'Obradovich', 'Quick', 'Quintanilla', 'Raddatz', 'Ramos', 'Regan', 'Salinas', 'Santelli', 'Seib', 'Strassel', 'Tapper', 'Todd', 'Tumulty', 'Wallace', 'Woodruff']
What's What?
(aka, the Text column)
In general, the Text column contains fully punctuated and appropriately capitalized speech transcriptions. Most parenthesized elements are non-speech elements, with the following exceptions:
(c) and (4): are spoken in reference to 501(c)(4)s (tax-exempt lobbying groups)
(k): spoken in reference to 401(k)s (individual pension accounts)
The non-speech elements that can be found in parentheses in the Text column are:
(ANTHEM): the national anthem is played
(APPLAUSE): the audience expresses approval
(BELL): a bell or buzzer indicating that a candidate's time has expired
(BOOING): the audience expresses disapproval
(COMMERCIAL): the televised debate breaks for a commercial advertisement
(CROSSTALK): more than one candidate or moderator are speaking at the same time
(LAUGHTER): the audience expresses a sense of humor
(MOMENT.OF.SILENCE): the debate pauses for a moment of silence
(SPANISH): the utterance is in Spanish (for the Democrats' Univision-hosted debate on 3/9/16 in Miami)
(VIDEO.END): a video clip ends
(VIDEO.START): a video clip begins
(inaudible): the utterance was inaudible, off-mike, or too indecipherable to transcribe"
Movebank: Animal Tracking,Analyzing migratory patterns of animals,PULKIT KHANDELWAL,13,"Version 1,2017-01-09",animals,CSV,21 MB,Other,"2,854 views",202 downloads,8 kernels,2 topics,https://www.kaggle.com/pulkit8595/movebank-animal-tracking,"Context
Movebank is a free, online database of animal tracking data hosted by the Max Planck Institute for Ornithology. It is designed to help animal tracking researchers to manage, share, protect, analyze, and archive their data. Movebank is an international project with over 11,000 users, including people from research and conservation groups around the world. The animal tracking data accessible through Movebank belongs to researchers all over the world. These researchers can choose to make part or all of their study information and animal tracks visible to other registered users, or to the public.
Animal tracking
Animal tracking data helps us understand how individuals and populations move within local areas, migrate across oceans and continents, and evolve through millennia. This information is being used to address environmental challenges such as climate and land use change, biodiversity loss, invasive species, and the spread of infectious diseases. Read more."
NYC Government Building Energy Usage,Energy usage for New York City owned office buildings,Aleksey Bilogur,13,"Version 1,2017-10-23","government
real estate
energy",CSV,10 KB,CC0,"1,869 views",205 downloads,2 kernels,0 topics,https://www.kaggle.com/residentmario/nyc-building-energy-usage,"Context
This dataset contains energy usage information for every building owned and managed by NYC DCAS. DCAS, or the Department of Citywide Administrative Services, is the arm of the New York City municipal government which handles ownership and management of the city's office facilities and real estate inventory. The organization voluntarily publicly discloses self-measured information about the energy use of its buildings.
Content
This data contains information on the name, address, location, and 2015 financial cycle energy usage of every building managed at that time by DCAS.
Acknowledgements
This dataset is published as-is by of the City of New York (here).
Inspiration
By combining this dataset with the New York City Buildings Database, what can you learn about the energy usage of buildings in New York City?
Can you use this data to model the energy consumption for city's office space at large?"
Short Track Speed Skating Database,Detailed lap data of each game in the last 5 seasons,xWang,13,"Version 3,2016-12-29|Version 2,2016-12-10|Version 1,2016-12-09",sports,CSV,840 KB,Other,"2,269 views",178 downloads,5 kernels,0 topics,https://www.kaggle.com/seniorwx/shorttrack,"Short Track Speed Skating Database for Sports Data Analysis
What is short track speed skating?
Maybe some people have never heard of this sport. Short track is a competitive and strategic game in which skaters race on ice. Sometimes the smartest or the luckiest guy, rather than the strongest, wins the game (for example).
What's in the data?
The database covers all the international short track games in the last 5 years. Currently it contains only men's 500m, but I will keep updating it.
Detailed lap data including personal time and ranking in each game from seasons 2012/2013 to present .
The final time results, ranking, starting position, qualified or penalized information of each athlete in each game.
All series of World Cup, World Championship, European Championship and Olympic Games.
Original data source
The data is collected from the ISU's (International Skating Union) official website. I have already done the cleaning procedure.
Please make sure that the data are only for personal and non-commercial use.
Explore the data
Interesting questions may be like:
What will happen in a game when there are more than one athlete from the same team? Are there performance all improved?
How does the performance of athletes change within a season and over seasons?
Do some athletes have special patterns in terms of time allocation and surpassing opportunity?
What is the influence of the implementation of ""no toe starts"" rules on athletes since July 2015?
Is there also home advantage like in other sports?
Who are the most ""dangerous"" guys that always get penalty?"
BIXI Montreal (public bicycle sharing system),Data on North America's first large-scale bike sharing system,Aubert Sigouin,13,"Version 3,2017-12-02|Version 2,2017-11-09|Version 1,2017-11-07","cycling
road transport",CSV,166 MB,CC4,"1,638 views",145 downloads,5 kernels,,https://www.kaggle.com/aubertsigouin/biximtl,"Context
« BIXI Montréal is a public bicycle sharing system serving Montréal, Quebec, Canada.
Launched in May 2009, it is North America's first large-scale bike sharing system and the original BIXI brand of systems.
The location of a BIXI bike station is determined by several parameters, including population density, points of interest and activities (universities, bike paths, other transportation networks, and data on travel patterns of the general public. In 2009, 5,000 bikes were deployed in Montreal through a network of pay stations located mainly in the boroughs of Rosemont–La Petite-Patrie, the Plateau-Mont-Royal and Ville-Marie, spilling over into parts of Outremont and the South West. As of 2011, the system has spread to Hochelaga-Maisonneuve, Villeray–Saint-Michel–Parc-Extension, Ahuntsic, Côte-des-Neiges–Notre-Dame-de-Grâce, Westmount and Verdun. » [1]
Content
1. BIXI - Movements history
Datasets containing the details of the travels made via the BIXI Montréal self-service bike network. Each year is a .zip file (ex. BixiMontrealRentals2014.zip) containing several .CSV files (ex. OD_2014-04.csv) for each months.

Version 2 : All .csv are merged per year (OD-year.csv).

The data is extracted from the BIXI Montréal station and bike management system. Trips of less than 1 minute or more than 2 hours are excluded. The station identifiers used correspond to those of the station status data set
Data Sructure
start_date: Date and time of the start of the trip ( AAAA-MM-JJ hh:mm )
start_station_code: Start station ID
end_date: Date and time of the start of the trip ( AAAA-MM-JJ hh:mm )
end_station_code : End station ID
is_member : Type users. (1 : Suscriber, 0 : Non-suscriber)
duration_sec: Total travel time in seconds

2. BIXI - The condition of stations
This dataset presents the list of stations in the BIXI Montréal self-service bicycle network, including the geographic position, the number of bicycles available and the number of terminals available. It is a .json file, which corresponds to a dictionary.
The data is produced by the BIXI Montréal station management system with a refresh rate of 5 minutes. Station locations are generally stable over time, but may be subject to change during the season, particularly when the City of Montreal is carrying out work or as part of special events. Temporary storage stations are not included in station status. The data is automatically generated on the BIXI servers, so the date of last update of this dataset does not represent the actual date of update. Information about bikes and bad terminals is available in the JSON format file.
Data Sructure
id: Unique station ID
s: Name of the station
n: Station terminal ID
st: Station status
b: Boolean value (true or false) specifying whether the station is blocked
su: Boolean value (true or false) specifying whether the station is suspended
m: Boolean value (true or false) specifying whether the station is displayed as out of service
read: timestamp of the last update of the data in milliseconds since January 1, 1970.
lc: timestamp of the last communication with the server in milliseconds since January 1, 1970.
bk: (For future use)
bl: (For future use)
la: latitude of the station according to the geodesic datum WGS84
lo: longitude of the station according to the WGS84 datum
da: Number of available terminals at this station
dx: Number of unavailable terminals at this station
ba: Number of available bikes at this station
bx: Number of unavailable bicycles at this station
3. Geographical boundaries of Montreal (Borough and related city)
This dataset is optional and will be useful mostly for ploting data and doing some choropleth maps.
Acknowledgements
Creative Commons Attribution 4.0 International
For more details :
http://donnees.ville.montreal.qc.ca/dataset/bixi-historique-des-deplacements
http://donnees.ville.montreal.qc.ca/dataset/bixi-etat-des-stations
http://donnees.ville.montreal.qc.ca/dataset/polygones-arrondissements
Inspiration
Can you find pattern in the behavior of Bixi users?
Are there any inefficient stations ?
What insights can we use from this data for decision making ?
[1] https://en.wikipedia.org/wiki/BIXI_Montr%C3%A9al"
Elon Musk's Tweets,Tweets by @elonmusk from 2012 to 2017,Kaan Ulgen,13,"Version 1,2017-10-12",,CSV,442 KB,CC0,"2,961 views",254 downloads,7 kernels,,https://www.kaggle.com/kulgen/elon-musks-tweets,"Context
Elon Musk is an American business magnate. He was one of the founders of PayPal in the past, and the founder and/or cofounder and/or CEO of SpaceX, Tesla, SolarCity, OpenAI, Neuralink, and The Boring Company in the present. He is known as much for his extremely forward-thinking ideas and huge media presence as he is for his extremely business savvy.
Musk is famously active on Twitter. This dataset contains all tweets made by @elonmusk, his official Twitter handle, between November 16, 2012 and September 29, 2017.
Content
This dataset includes the body of the tweet and the time it was made, as well as who it was re-tweeted from (if it is a retweet).
Inspiration
Can you figure out Elon Musk's opinions on various things by studying his Twitter statements?
How Elon Musk's post rate increased, decreased, or stayed about the same over time?"
Workers Browser Activity in CrowdFlower Tasks,In-page behaviour of crowdworkers performing tasks on CrowdFlower,Human Computation,13,"Version 6,2017-10-20|Version 5,2017-09-29|Version 4,2017-09-29|Version 3,2017-09-28|Version 2,2017-09-28|Version 1,2017-09-27",web sites,CSV,10 MB,CC4,"1,931 views",82 downloads,,3 topics,https://www.kaggle.com/humancomp/worker-activity-crowdflower,"Context
Data Scientists often use crowdsourcing platforms, such as Amazon Mechanical Turk or CrowdFlower to collect labels for their data. Controlling high quality and timeless execution of tasks is an important part of such collection process. It is not possible (or not efficient) to manually check every worker assignment. There is an intuition that there quality could be predicted based on workers task browser behaviour (e.g. key presses, scrolling, mouse clicks, tab switching). In this dataset there are assignment results for 3 different crowdsourcing tasks launched on CrowdFlower, along with associated workers behaviour.
Content
We collected data running 3 tasks:
Image labelling,
Receipt Transcription,
Business Search.
Tasks are described in tasks.csv. Results for corresponding tasks are given in files: results_{task_id}.csv. Workers's activity could be found in the following files:
activity_keyboard.csv - timestamps of keyboard keys pressed
activity_mouse.csv - timestamps of mouse clicks with associated HTML elements
activity_tab.csv - timestamps of event task browser tab changes (opened, active, hidden, closed)
activity_page.csv - a summary of events happened in the task page every 2 seconds (boolean keyboard activity, boolean mouse movement activity, boolean scrolling activity, the position of the screen, boolean if text was selected)
Result files have a similar structure to the original one given by CrowdFlower:
_unit_id: A unique ID number created by the system for each row
_created_at: The time the contributor submitted the judgement
_golden: This will be ""true"" if this is a test question, otherwise it is ""false""
_id: A unique ID number generated for this specific judgment
_missed: This will be ""true"" if the row is an incorrect judgment on a test question.
_started_at: The time at which the contributor started working on the judgement
_tainted: This will be ""true"" if the contributor has been flagged for falling below the required accuracy. This judgment will not be used in the aggregation.
_channel: The work channel that the contributor accessed the job through
_trust: The contributor's accuracy. Learn more about trust here
_worker_id: A unique ID number assigned to the contributor (in the current dataset MD5 value is given)
_country: The country the contributor is from
_region: A region code for the area the contributor is from
_city: The city the contributor is from
_ip: The IP address for the contributor (in the current dataset MD5 value is given)
{{field}}: There will be a column for each field in the job, with a header equal to the field's name.
{{field}}_gold: The correct answer for the test question
Acknowledgements
We thank crowd workers who accomplished our not always exciting tasks on CrowdFlower."
Risk of being drawn into online sex work,Detecting individuals at risk using semi-supervised learning,Panos Kostakos,13,"Version 2,2017-11-28|Version 1,2017-11-25",,CSV,475 KB,CC0,"2,737 views",332 downloads,2 kernels,2 topics,https://www.kaggle.com/panoskostakos/online-sex-work,"Context
This database was used in the paper: ""Covert online ethnography and machine learning for detecting individuals at risk of being drawn into online sex work"". https://www.flinders.edu.au/centre-crime-policy-research/illicit-networks-workshop
Content
The database includes data scraped from a European online adult forum. Using covert online ethnography we interviewed a small number of participants and determined their risk to either supply or demand sex services through that forum. This is a great dataset for semi-supervised learning.
Inspiration
How can we identify individuals at risk of being drawn into online sex work? The spread of online social media enables a greater number of people to be involved into online sex trade; however, detecting deviant behaviors online is limited by the low available of data. To overcome this challenge, we combine covert online ethnography with semi-supervised learning using data from a popular European adult forum."
Car Features and MSRP,"Includes features such as make, model, year, and engine type to predict price",CooperUnion,13,"Version 1,2016-12-22",,CSV,1 MB,Other,"4,756 views",680 downloads,6 kernels,2 topics,https://www.kaggle.com/CooperUnion/cardataset,"Context
Cars dataset with features including make, model, year, engine, and other properties of the car used to predict its price.
Content
Scraped from Edmunds and Twitter.
Acknowledgements
Edmunds and Twitter for providing info Sam Keene
Inspiration
Effects of features on the price How does the brand affect the price? What cars can be consider overpriced? Price VS. popularity?"
2016 Presidential Campaign Finance,How did presidential candidates spend their campaign funds?,Federal Election Commission,13,"Version 1,2017-01-18","finance
politics",Other,9 MB,CC0,"3,107 views",460 downloads,3 kernels,0 topics,https://www.kaggle.com/fec/presidential-campaign-finance,"Context
The Federal Election Commission (FEC) is an independent regulatory agency established in 1975 to administer and enforce the Federal Election Campaign Act (FECA), which requires public disclosure of campaign finance information. The FEC publishes campaign finance reports for presidential and legislative election campaign candidates on the Campaign Finance Disclosure Portal.
Content
The finance summary report contains one record for each financial report (Form 3P) filed by the presidential campaign committees during the 2016 primary and general election campaigns. Presidential committees file quarterly prior to the election year and monthly during the election year. The campaign expenditures file contains individual operating expenditures made by the campaign committee and reported on Form 3P Line 23 during the same period. Operating expenditures consist of the routine costs of campaigning for president, which include staffing, travel, advertising, voter outreach, and other activities."
What's On The Menu?,"Dataset on historical menus, dishes, and dish prices",New York Public Library,13,"Version 1,2016-11-06",food and drink,CSV,146 MB,CC0,"8,325 views",998 downloads,5 kernels,0 topics,https://www.kaggle.com/nypl/whats-on-the-menu,"The New York Public Library is digitizing and transcribing its collection of historical menus. The collection includes about 45,000 menus from the 1840s to the present, and the goal of the digitization project is to transcribe each page of each menu, creating an enormous database of dishes, prices, locations, and so on. As of early November, 2016, the transcribed database contains 1,332,279 dishes from 17,545 menus.
The Data
This dataset is split into four files to minimize the amount of redundant information contained in each (and thus, the size of each file). The four data files are Menu, MenuPage, MenuItem, and Dish. These four files are described briefly here, and in detail in their individual file descriptions below.
Menu
The core element of the dataset. Each Menu has a unique identifier and associated data, including data on the venue and/or event that the menu was created for; the location that the menu was used; the currency in use on the menu; and various other fields.
Each menu is associated with some number of MenuPage values.
MenuPage
Each MenuPage refers to the Menu it comes from, via the menu_id variable (corresponding to Menu:id). Each MenuPage also has a unique identifier of its own. Associated MenuPage data includes the page number of this MenuPage, an identifier for the scanned image of the page, and the dimensions of the page.
Each MenuPage is associated with some number of MenuItem values.
MenuItem
Each MenuItem refers to both the MenuPage it is found on -- via the menu_page_id variable -- and the Dish that it represents -- via the dish_id variable. Each MenuItem also has a unique identifier of its own. Other associated data includes the price of the item and the dates when the item was created or modified in the database.
Dish
A Dish is a broad category that covers some number of MenuItems. Each dish has a unique id, to which it is referred by its affiliated MenuItems. Each dish also has a name, a description, a number of menus it appears on, and both date and price ranges.
Inspiration
What are some things we can look at with this dataset?
How has the median price of restaurant dishes changed over time? Are there particular types of dishes (alcoholic beverages, seafood, breakfast food) whose price changes have been greater than or less than the average change over time?
Can we predict anything about a dish's price based on its name or description?
-- There's been some work on how the words used in advertisements for potato chips are reflective of their price; is that also true of the words used in the name of the food?
-- Are, for example, French or Italian words more likely to predict a more expensive dish?
Acknowledgments
This dataset was downloaded from the New York Public Library's What's on the menu? page. The What's on the menu? data files are updated twice monthly, so expect this dataset to go through multiple versions."
Indoor Positioning,Dataset of bluetooth beacons readings indoor,liwste,13,"Version 2,2017-03-13|Version 1,2017-03-07",networks,CSV,22 KB,CC4,"5,098 views",303 downloads,13 kernels,5 topics,https://www.kaggle.com/liwste/indoor-positioning,"Context
I am interested in indoor positioning technology and had been playing with blue-tooth beacons. Typically blue-tooth beacons have accuracy of a range between 1 to 4 meters. I am thinking of maybe using machine learning can produce results of greater accuracy compared to using traditional filtering methods e.g. kalman filters, particle filters.
Content
3 Kontakt blue-tooth beacons are mounted in a 2.74 meters wide x 4.38 meters long (width x length) room. The 3 beacons are transmitting at a transmit power of -12dbm. A Sony Xperia E3 smartphone with blue-tooth turned on is used as a receiver to the record the data. Recordings are done on several positions in the room of an interval of 30-60 seconds in the same position.
Beacons location # Beacon X(meters) Y(meters) BeaconA 0.10 1.80 BeaconB 2.74 2.66 BeaconC 1.22 4.54
Fields:
The distance in meters between beacon A and the device calculated by using the rssi of this blue-tooth beacon.
The distance in meters between beacon B and the device calculated by using the rssi of this blue-tooth beacon.
The distance in meters between beacon C and the device calculated by using the rssi of this blue-tooth beacon.
X coordinate in centimeters rounded to the nearest centimeter measured using a measuring tape with +/-1cm accuracy.
Y coordinate in centimeters rounded to the nearest centimeter measured using a measuring tape with +/-1cm accuracy.
Date and time of the recording.
Acknowledgements
The data is collected solely by me.
Inspiration
To try and improve on the accuracy of indoor position using blue-tooth beacons which typically have accuracy of range between 1 meters to 4 meters."
"Taxi Routes of Mexico City, Quito and more","Data collected from Taxi, Cabify and Uber trips, using EC Taximeter",Mario Navas,13,"Version 3,2017-08-03|Version 2,2017-08-02|Version 1,2017-07-21","telecommunications
road transport",CSV,20 MB,CC4,"2,408 views",306 downloads,3 kernels,2 topics,https://www.kaggle.com/mnavas/taxi-routes-for-mexico-city-and-quito,"Context
This dataset was collected using our App EC Taximeter.
An easy to use tool developed to compare fees, giving the user an accurate fee based on GPS to calculate a cost of the taxi ride. Due to the ability to verify that you are charged fairly, our App is very popular in several cities. We encourage our users to send us URLs with the taxi/transportation fees in their cities to keep growing our database.
★ Our App gets the available fares for your location based on your GPS, perfect when traveling and not getting scammed.
★ Users can start a taximeter in their own phone and check they are charged fairly
★ Several useful information is displayed to the user during the ride: Speed, Wait time, Distance, GPS update, GPS precision, Range of error.
★ Each fare has information available for reference like: Schedule, Minimum fee, Source, Last update.
★ It’s possible to surf through several cities and countries which fares are available for use. If a fare is not in the app, now it’s easier than ever to let us know thanks to Questbee Apps.
We invite users to contribute to our project and expect this data set to be useful, please don't hesitate to contact us to info@ashkadata.com to add your city or to contribute with this project.
Content
The data is collected from June 2016 until July 20th 2017. The data is not completely clean, many users forget to turn off the taximeter when done with the route. Hence, we encourage data scientist to explore it and trim the data a little bit
Acknowledgements
We have to acknowledge the valuable help of our users, who have contributed to generate this dataset and have push our growth by mouth to mouth recommendation.
Inspiration
Our first inspiration for the App was after being scammed in our home city Quito. We started it as a tool for people to be fairly charged when riding a taxi. Currently with other transportation options available, we also help user to compare fares in their cities or the cities which they are visiting.
File descriptions
mex_clean.csv - the dataset contains information of routes in Mexico City
uio_clean.csv - the dataset contains information of routes in Quito Ecuador
bog_clean.csv - the dataset contains information of routes in Bogota
all-data_clean.csv - the dataset contains information of routes in different cities"
"French Presidential Election, 2017",Results of both rounds at polling station level,GrishaSizov,13,"Version 1,2017-05-26",politics,CSV,228 MB,Other,"2,294 views",286 downloads,20 kernels,,https://www.kaggle.com/grishasizov/frenchpresidentialelection2017,"Presidential elections have just finished in France, with two rounds on April 23rd and May 7th 2017. The results of the elections are available online for each ~67000 polling stations around the country. This data can be used to gain insights about both the electorate and the candidates. Examples of basic questions worth looking at are:
How are candidates' scores correlated with each other? Can you infer 'political affinity' of the candidates just looking at the data, without any other a priori knowledge?
How are scores of various candidates correlated with the turnout? Supporters of which candidate are the most 'participative'?
How did the votes get redistributed from the first to the second round? Can you build an a posteriori predictive model of the second round results taking as an input the results of the first round ?
The data provides coordinates of each polling station. Can you gain any insight from the geography?
Some preliminary analysis is described in two blog posts here and here. This dataset is inspired by an analogous one for the 2016 US elections by Ben Hamner."
Predict Molecular Properties,Help computational alchemy with machine learning!,BurakH,13,"Version 2,2017-08-15|Version 1,2017-08-11","physical sciences
chemistry
physics",{}JSON,1 GB,CC0,"3,278 views",255 downloads,3 kernels,,https://www.kaggle.com/burakhmmtgl/predict-molecular-properties,"Predict molecular properties
Context
This dataset contains molecular properties scraped from the PubChem database. Each file contains properties for thousands of molecules , made up of the elements H, C, N, O, F, Si, P, S, Cl, Br, and I. The dataset is related to a previous one which had fewer number of molecules, where the features were preconstructed.
Instead, this dataset is a challenging case for feature engineering and is subject of active research (see references below).
Data Description
The utilities used to download and process the data can be accessed from my Github repo.
Each JSON file contains a list of molecular data. An example molecule is given below:
{
'En': 37.801,
'atoms': [
{'type': 'O', 'xyz': [0.3387, 0.9262, 0.46]},
{'type': 'O', 'xyz': [3.4786, -1.7069, -0.3119]},
{'type': 'O', 'xyz': [1.8428, -1.4073, 1.2523]},
{'type': 'O', 'xyz': [0.4166, 2.5213, -1.2091]},
{'type': 'N', 'xyz': [-2.2359, -0.7251, 0.027]},
{'type': 'C', 'xyz': [-0.7783, -1.1579, 0.0914]},
{'type': 'C', 'xyz': [0.1368, -0.0961, -0.5161]},
{'type': 'C', 'xyz': [-3.1119, -1.7972, 0.659]},
{'type': 'C', 'xyz': [-2.4103, 0.5837, 0.784]},
{'type': 'C', 'xyz': [-2.6433, -0.5289, -1.426]},
{'type': 'C', 'xyz': [1.4879, -0.6438, -0.9795]},
{'type': 'C', 'xyz': [2.3478, -1.3163, 0.1002]},
{'type': 'C', 'xyz': [0.4627, 2.1935, -0.0312]},
{'type': 'C', 'xyz': [0.6678, 3.1549, 1.1001]},
{'type': 'H', 'xyz': [-0.7073, -2.1051, -0.4563]},
{'type': 'H', 'xyz': [-0.5669, -1.3392, 1.1503]},
{'type': 'H', 'xyz': [-0.3089, 0.3239, -1.4193]},
{'type': 'H', 'xyz': [-2.9705, -2.7295, 0.1044]},
{'type': 'H', 'xyz': [-2.8083, -1.921, 1.7028]},
{'type': 'H', 'xyz': [-4.1563, -1.4762, 0.6031]},
{'type': 'H', 'xyz': [-2.0398, 1.417, 0.1863]},
{'type': 'H', 'xyz': [-3.4837, 0.7378, 0.9384]},
{'type': 'H', 'xyz': [-1.9129, 0.5071, 1.7551]},
{'type': 'H', 'xyz': [-2.245, 0.4089, -1.819]},
{'type': 'H', 'xyz': [-2.3, -1.3879, -2.01]},
{'type': 'H', 'xyz': [-3.7365, -0.4723, -1.463]},
{'type': 'H', 'xyz': [1.3299, -1.3744, -1.7823]},
{'type': 'H', 'xyz': [2.09, 0.1756, -1.3923]},
{'type': 'H', 'xyz': [-0.1953, 3.128, 1.7699]},
{'type': 'H', 'xyz': [0.7681, 4.1684, 0.7012]},
{'type': 'H', 'xyz': [1.5832, 2.901, 1.6404]}
],
'id': 1,
'shapeM': [259.66, 4.28, 3.04, 1.21, 1.75, 2.55, 0.16, -3.13, -0.22, -2.18, -0.56, 0.21, 0.17, 0.09]
}
En: This field is the molecular energy calculated using a force-field method. See references [1,2] for details. This is the target variable which is being predicted.
atoms: This field contains the name of the element and the position (x,y,z coordinates) and needs to be used for feature engineering.
id : This field is the PubChem Id
shapeM : This field contains the shape multipoles and can be used for feature engineering. For definition of shape multipoles, see reference [3].
Notice that each molecule contains different number and types of atoms, so it is challenging to come up with features that can describe every molecule in a unique way. There are several approaches taken in the literature (see the references), one of which is to use the Coulomb Matrix for a given molecule defined by
CIJ=
ZIZJ
|RI−RJ|
,(I≠J)CIJ=Z
2.4
I
,(I=J)
where $Z_I$ are atomic numbers (can be looked up from the periodic table for each element), and ${\vert R_I - R_J \vert}$ is the distance between two atoms I and J. The previous dataset used these features for a subset of molecules given here, where the maximum number of elements in a given molecules was limited by 50. In this case, each molecule has a 50x50 Coulomb matrix where zeroes were used as padding when the molecule had smaller than 50 number of atoms.
There are around 100,000,000 molecules in the whole database. As more files are scraped, new data will be added in time.
Note: In the previous dataset, the molecular energies were computed by quantum mechanical simulations. Here, the given energies are computed using another method, so their values are different.
Inspiration
Simulations of molecular properties are computationally expensive. The purpose of this project is to use machine learning methods to come up with a model that can predict molecular properties from a database. In the PubChem database, there are around 100,000,000 molecules. It could take years to do simulations on all of these molecules, however machine learning can be used to predict their properties much faster. As a result, this could open up many possibilities in computational design and discovery of molecules, compounds and new drugs.
This is a regression problem so mean squared error is minimized during training.
I am looking for Kagglers to find the best model and reduce mean squared error as much as possible!
References
[1] Halgren TA. Merck Molecular Force Field: I. Basis, Form, Scope, Parameterization and Performance of MMFF94. J. Comp. Chem. 1996;17:490-519.
[2] Halgren TA. Merck Molecular Force Field: VI. MMFF94s Option for Energy Minimization Studies. J. Comp. Chem. 1999;20:720-729.
[3] Kim, Sunghwan, Evan E Bolton, and Stephen H Bryant. “PubChem3D: Shape Compatibility Filtering Using Molecular Shape Quadrupoles.” J. Cheminf. 3 (2011): 25.
[4] Himmetoglu B.: Tree based machine learning framework for predicting ground state energies of molecules, J. Chem. Phys 145, 134101 (2016)
[5] Rupp M., Ramakrishnan R., von Lilienfeld OA.: Machine Learning for Quantum Mechanical Properties of Atoms in Molecules, J. Phys. Chem. Lett. , 6(16): 3309–3313 (2015)
[6] Montavon G., Rupp M., Gobre V., Vazquez-Mayagoitia A., Hansen K., Tkatchenko A., Müller K-R., von Lilienfeld OA.: Machine learning of molecular electronic properties in chemical compound space, New J. Phys., 15(9): 095003 (2013)
[7] Hansen K., Montavon G., Biegler F., Fazli S., Rupp M., Scheffler M., von Lilienfeld OA., Tkatchenko A., Müller K-P.: Assessment and Validation of Machine Learning Methods for Predicting Molecular Atomization Energies, J. Chem. Theory Comput. , 9(8): 3543–3556 (2013)"
Chinese Characters Generator,"Chinese fonts dataset, which can be used for Chinese text OCR",Dylan,13,"Version 3,2017-07-14|Version 2,2017-07-14|Version 1,2017-07-12","writing
linguistics",Other,199 MB,Other,"3,322 views",231 downloads,2 kernels,2 topics,https://www.kaggle.com/dylanli/chinesecharacter,"About This Dataset
You can use this fonts file to generate some Chinese character. Use this image can train a machine learning model to recognize text.
Dataset is updating
Tell me if you have other font file or anything related to this topic."
Marvel Characters and Universes,All distinct Marvel characters and every universe where they appear,Yan Ramos da Silva,13,"Version 1,2017-01-31",comics,CSV,285 MB,CC0,"3,485 views",351 downloads,3 kernels,0 topics,https://www.kaggle.com/yrdasilva/marvel-characters-and-universes,"Context
I have been a comic book fan for many years and, when I started writing web scrapers for practice, it was only natural that I did one inspired by my passion for Marvel.
Content
This dataset has 27.290 rows, each one representing a distinct Marvel character, such as Peter Parker, Tony Stark or Jean Grey. As for columns, there are 1822 of them, one for each Marvel universe. All the cells contain a boolean value: true if there is a version of that character from that universe or false otherwise.
Acknowledgements
I would like to thank the Marvel Wikia for its amazing amount of information, as well as very practical API, and Marvel for having such a huge and diverse multiverse that inspires many possibilities of analysis.
Inspiration
This was my first attempt at data science. It was challenging but very fun and rewarding. I would really appreciate any feedback or suggestions for next works."
Symptom Disease sorting,make a symptomsorter,Paul Larmuseau,13,"Version 4,2017-03-10|Version 3,2017-03-10|Version 2,2017-03-04|Version 1,2017-03-04",,CSV,286 KB,Other,"6,461 views","1,051 downloads",5 kernels,3 topics,https://www.kaggle.com/plarmuseau/sdsort,"Context
Can you create a python symptom sorter, in bot style with these data ?
Symptom 1: i have an eye problem > the set selects actually all the diseases and symptoms related to the eyes OK
then comes a question: is your nose congested ? and do you have red eye ? Symptom 2: congested nose Yes, red eye No OK
then comes a second question do you cough ? and do you have chills ? Symptom 3: Cough Yes, Chills No OK
the answer: with these symptoms i think you have...
The primer shows the idea.
primer to start with If you want a reduced data input, you download the US_df from the primer So the challenge i have here is, what classification method gives the best results for making a decision tree
Content
its one of my databases."
Titanic,Suited for binary logistic regression,Khashayar Baghizadeh Hosseini,13,"Version 1,2017-05-16",,CSV,82 KB,ODbL,"5,495 views","1,021 downloads",11 kernels,0 topics,https://www.kaggle.com/heptapod/titanic,"Overview
This is the original data from Titanic competition plus some changes that I applied to it to be better suited for binary logistic regression:
Merged the train and test data.
Removed the 'ticket' and 'cabin' attributes.
Moved the 'Survived' attribute to the last column.
Added extra zero columns for categorical inputs to be better suited for One-Hot-Encoding.
Substituted the values of 'Sex' and 'Embarked' attributes with binary and categorical values respectively.
Filled the missing values in 'Age' and 'Fare' attributes with the median of the data."
Texas Death Row Executions Info and Last Words,Information and last words of Texas death row executions between 1982 and 2017,ianmobbs,13,"Version 3,2017-06-09|Version 2,2017-06-01|Version 1,2017-06-01","languages
death
law
linguistics",CSV,277 KB,CC4,"2,213 views",238 downloads,4 kernels,2 topics,https://www.kaggle.com/ianmobbs/texas-death-row-executions-info-and-last-words,"Context
The death penalty was authorized by 32 states, the Federal Government, and the U.S. Military. While Connecticut, Maryland, and New Mexico no longer have death penalty statutes, they do currently incarcerate death-sentenced offenders. Texas leads the nation in the number of executions since the death penalty was reinstated in 1976. California, Florida, Texas, and Pennsylvania have the largest death row populations.
The following crimes are Capital Murder in Texas:
murder of a peace officer or fireman who is acting in the lawful discharge of an official duty and who the person knows is a peace officer or fireman;
murder during the commission or attempted commission of kidnapping, burglary, robbery, aggravated sexual assault, arson, obstruction or retaliation, or terroristic threat;
murder for remuneration or promise of remuneration or employs another to commit murder for remuneration or promise of remuneration;
murder during escape or attempted escape from a penal institution;
murder, while incarcerated in a penal institution, of a correctional employee or with the intent to establish, maintain, or participate in a combination or in the profits of a combination;
murder while incarcerated in a penal institution for a conviction of murder or capital murder;
murder while incarcerated in a penal institution serving a life sentence or a 99 year sentence for a conviction of aggravated kidnapping, aggravated sexual assault, or aggravated robbery;
murder of more than one person during the same criminal transaction or during different criminal transactions but the murders are committed pursuant to the same scheme or course of conduct;
murder of an individual under ten years of age; or
murder in retaliation for or on account of the service or status of the other person as a judge or justice of the supreme court, the court of criminal appeals, a court of appeals, a district court, a criminal district court, a constitutional county court, a statutory county court, a justice court, or a municipal court.
Content
The Texas Department of Criminal Justice publishes various details, including the last words, of every inmate on death row they execute. This dataset includes information on the name, age, race, county, date, and last words of Texas death row inmates from 1982 to 2017.
Acknowledgments
This dataset on last statements by executed offenders was obtained here: https://www.tdcj.state.tx.us/death_row/dr_executed_offenders.html
Start a new kernel"
Trump Financial Disclosure 2017,Donald Trump's 2017 Form 278e,Sohier Dane,13,"Version 1,2017-07-11","finance
politics",CSV,125 KB,Other,"2,537 views",129 downloads,,0 topics,https://www.kaggle.com/sohier/trump-financial-disclosure-2017,"Context
This is a general disclosure of Donald Trump's assets, debts, and sources of income. Because the forms are only meant to reveal on potential conflicts of interests a filer might have, they provide far less specificity than tax returns would. This is an unpacked version of the pdf of Trump's form that was made available by the Federal Election Commission June 16th, 2017. It contains some information about his financial interests, but not enough to paint a complete picture of his net worth. It may be possible to use some of these forms to identify his foreign business partners.
Acknowledgements
This dataset was kindly extracted from the original PDF and made publicly available by the Center for Responsive Politics.
Inspiration
Previous work on similar disclosures by Trump has often focused on identifying his foreign business ties. Are you able to find any new ones?
You might also like
Trump's 2016 Financial Disclosure
Trump's World
Trump's Tweets
Trump Campaign Expenditures"
London Police Records,"A snapshot of crime, outcome, and stop and search data",Sohier Dane,13,"Version 1,2017-08-15","crime
government",CSV,1 GB,Other,"2,516 views",402 downloads,,0 topics,https://www.kaggle.com/sohier/london-police-records,"This dataset provides a complete snapshot of crime, outcome, and stop and search data, as held by the Home Office from late 2014 through mid 2017 for London, both the greater metro and the city.
Content
The core fields are as follows:
Reported by: The force that provided the data about the crime.
Falls within: At present, also the force that provided the data about the crime. This is currently being looked into and is likely to change in the near future.
Longitude and Latitude: The anonymised coordinates of the crime.
LSOA code and LSOA name: References to the Lower Layer Super Output Area that the anonymised point falls into, according to the LSOA boundaries provided by the Office for National Statistics.
Crime type: One of the crime types listed in the Police.UK FAQ.
Last outcome category: A reference to whichever of the outcomes associated with the crime occurred most recently. For example, this crime's 'Last outcome category' would be 'Offender fined'.
Context: A field provided for forces to provide additional human-readable data about individual crimes. Currently, for newly added CSVs, this is always empty.
For additional details, including the steps taken to anonymize the data, please see https://data.police.uk/about/#provenance.
Acknowledgements
This dataset was kindly released by the British Home Office under the Open Government License 3.0 at https://data.police.uk/data/. If you are looking for more data, they cover much more than London! All major cities in England and Wales are available, adding up to roughly 2gb of new data per month."
Japanese lemma frequency,"A list of the 15,000 most common word forms in Japanese",Rachael Tatman,13,"Version 1,2017-07-25","languages
linguistics",CSV,281 KB,Other,"1,206 views",79 downloads,2 kernels,,https://www.kaggle.com/rtatman/japanese-lemma-frequency,"Context:
A lemma is the uninflected form of a word. So while “tree” and “trees” are two words, they are the same lemma: “tree”. Similarly, “go”, “went” and “going” are all forms of the underlying lemma “to go”. This dataset contains the most common lemmas in Japanese.
Content:
This dataset contains the most common Japanese lemmas from the Internet Corpus, as tagged by the ChaSen morphological tagger for Japanese (http://chasen.naist.jp/hiki/ChaSen/). For each lemma, both the frequency (number of times it occurs in the corpus) and its relative rank to other lemmas is provided.
The total corpus size is 253,071,774 tokens, with a lexicon of 451,963 types.
Acknowledgements:
This dataset was developed at the University of Leeds by Centre for Translation Studies(more information: http://corpus.leeds.ac.uk/list.html), and is distributed under a CC-BY license.
Inspiration:
This dataset is an especially helpful resource for work on Japanese texts.
What is the distribution of hiragana, katakana and kanji characters among common lemmas?
Can you use machine translation to find the equivalent lemmas and their frequency in other languages? Is there a lot of cross-linguistic difference between what concepts are the most frequent?
Which parts of speech are the most common in Japanese? Are these different across languages?"
Ubuntu Dialogue Corpus,26 million turns from natural two-person dialogues,Rachael Tatman,13,"Version 2,2017-08-17|Version 1,2017-08-17","languages
linguistics
computing and society",CSV,3 GB,Other,"4,519 views",365 downloads,,0 topics,https://www.kaggle.com/rtatman/ubuntu-dialogue-corpus,"Context:
Building dialogue systems, where a human can have a natural-feeling conversation with a virtual agent, is a difficult task in Natural Language Processing and the focus of much ongoing research. Some of the challenges include linking references to the same entity over time, tracking what’s happened in the conversation previously, and generating appropriate responses. This corpus of naturally-occurring dialogues can be helpful for building and evaluating dialogue systems.
Content:
The new Ubuntu Dialogue Corpus consists of almost one million two-person conversations extracted from the Ubuntu chat logs, used to receive technical support for various Ubuntu-related problems. The conversations have an average of 8 turns each, with a minimum of 3 turns. All conversations are carried out in text form (not audio).
The full dataset contains 930,000 dialogues and over 100,000,000 words and is available here. This dataset contains a sample of this dataset spread across .csv files. This dataset contains more than 269 million words of text, spread out over 26 million turns.
folder: The folder that a dialogue comes from. Each file contains dialogues from one folder .
dialogueID: An ID number for a specific dialogue. Dialogue ID’s are reused across folders.
date: A timestamp of the time this line of dialogue was sent.
from: The user who sent that line of dialogue.
to: The user to whom they were replying. On the first turn of a dialogue, this field is blank.
text: The text of that turn of dialogue, separated by double quotes (“). Line breaks (\n) have been removed.
Acknowledgements:
This dataset was collected by Ryan Lowe, Nissan Pow , Iulian V. Serban† and Joelle Pineau. It is made available here under the Apache License, 2.0. If you use this data in your work, please include the following citation:
Ryan Lowe, Nissan Pow, Iulian V. Serban and Joelle Pineau, ""The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems"", SIGDial 2015. URL: http://www.sigdial.org/workshops/conference16/proceedings/pdf/SIGDIAL40.pdf
Inspiration:
Can you use these chat logs to build a chatbot that offers help with Ubuntu?"
Malarial Mosquito Database,Geo-coded Inventory of Anophelines in the Sub-Saharan Africa: 1898-2016,Jacob Boysen,13,"Version 2,2017-08-28|Version 1,2017-08-23","healthcare
public health",Other,6 MB,CC4,"3,182 views",433 downloads,,,https://www.kaggle.com/jboysen/malaria-mosquito,"Context:
Understanding the distribution of anopheline vectors of malaria is an important prelude to the design of national malaria control and elimination programmes. A single, geo-coded continental inventory of anophelines using all available published and unpublished data has not been undertaken since the 1960s. We present the largest ever geo-coded database of anophelines in Africa representing a legacy dataset for future updating and identification of knowledge gaps at national levels. The geo-coded and referenced database is made available with the related publication as a reference source for African national malaria control programmes planning their future control and elimination strategies. Information about the underlying research studies can be found at http://kemri-wellcome.org/programme/population-health/.
Content:
Geocoded info on anopheline inventory. See key below.
Acknowledgements:
KEMRI-Wellcome Trust assembled the data and distributed it on Dataverse.
Inspiration:
Where have malarial mosquito populations grown or decreased?
Can you predict mosquito population growth trends?
Do you seen any correlation between mosquito populations and malaria deaths from this dataset?
Is the banner image mosquito capable of carrying malaria?"
CAT Scan Localization,384 features extracted from CT images,UCI Machine Learning,13,"Version 1,2017-09-07","healthcare
biology
health
+ 2 more...",CSV,78 MB,CC0,"2,199 views",101 downloads,,,https://www.kaggle.com/uciml/ct-slice-localization,"Context
This dataset consists of 384 features extracted from CT images. The class variable is numeric and denotes the relative location of the CT slice on the axial axis of the human body. The data was retrieved from a set of 53500 CT images from 74 different patients (43 male, 31 female).
Content
Each CT slice is described by two histograms in polar space. The first histogram describes the location of bone structures in the image, the second the location of air inclusions inside of the body. Both histograms are concatenated to form the final feature vector. Bins that are outside of the image are marked with the value -0.25.
The class variable (relative location of an image on the axial axis) was constructed by manually annotating up to 10 different distinct landmarks in each CT Volume with known location. The location of slices in between landmarks was interpolated.
Field Descriptions:
patientId: Each ID identifies a different patient
value[1-241]: Histogram describing bone structures
value[242 - 385]: Histogram describing air inclusions
386: Relative location of the image on the axial axis (class value).
Values are in the range [0; 180] where 0 denotes the top of the head and 180 the soles of the feet.
Acknowledgements
Original dataset was downloaded from UCI Machine learning Repository
Lichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.
Banner image acknowledgement:
Self Portre on cat scan, 1997
Title: ""Soon I will be there""
Date: 8 April 1997
Author: Sérgio Valle Duarte
License: CC BY 3.0
Source: Wikipedia
Inspiration
Predict the relative location of CT slices on the axial axis"
World Glacier Inventory,"Name, location, altitude, and area of every glacier on the planet",National Snow and Ice Data Center,13,"Version 1,2017-02-09","environment
climate",CSV,16 MB,CC0,"2,638 views",264 downloads,2 kernels,,https://www.kaggle.com/nsidcorg/glacier-inventory,"Content
The World Glacier Inventory contains information for over 130,000 glaciers. Inventory parameters include geographic location, area, length, orientation, elevation, and classification. The WGI is based primarily on aerial photographs and maps with most glaciers having one data entry only. The data set can be viewed as a snapshot of the glacier distribution in the second half of the twentieth century. It was founded on the original WGI from the World Glacier Monitoring Service.
Acknowledgements
The National Snow & Ice Data Center continues to work with the World Glacier Monitoring Service to update the glacier inventory database."
Corruption Perceptions Index,Perceived level of political corruption for every country in the world,Transparency International,13,"Version 1,2017-01-27","crime
politics",CSV,23 KB,CC4,"4,255 views",595 downloads,8 kernels,,https://www.kaggle.com/transparencyint/corruption-index,"Content
The Corruption Perceptions Index scores and ranks countries/territories based on how corrupt a country’s public sector is perceived to be. It is a composite index, a combination of surveys and assessments of corruption, collected by a variety of reputable institutions. The CPI is the most widely used indicator of corruption worldwide.
Corruption generally comprises illegal activities, which are deliberately hidden and only come to light through scandals, investigations or prosecutions. There is no meaningful way to assess absolute levels of corruption in countries or territories on the basis of hard empirical data. Possible attempts to do so, such as by comparing bribes reported, the number of prosecutions brought or studying court cases directly linked to corruption, cannot be taken as definitive indicators of corruption levels. Instead, they show how effective prosecutors, the courts or the media are in investigating and exposing corruption. Capturing perceptions of corruption of those in a position to offer assessments of public sector corruption is the most reliable method of comparing relative corruption levels across countries.
Acknowledgements
The data sources used to calculate the Corruption Perceptions Index scores and ranks were provided by the African Development Bank, Bertelsmann Stiftung Foundation, The Economist, Freedom House, IHS Markit, IMD Business School, Political and Economic Risk Consultancy, Political Risk Services, World Bank, World Economic Forum, World Justice Project, and Varieties of Democracy Project."
2016 Advanced Placement Exam Scores,What is the relationship between student demographics and exam subjects/scores?,College Board,13,"Version 1,2017-02-03",education,CSV,25 KB,Other,"4,972 views",768 downloads,22 kernels,0 topics,https://www.kaggle.com/collegeboard/ap-scores,"Content
The Advanced Placement Exam scores for the class of 2016, highlighted in this dataset, show that students continue to demonstrate college-level skills and knowledge in increasing numbers. Even as AP teachers deliver rigor to an ever-diversifying population of students, participation and performance continue to improve. Behind and within these data are the daily sacrifices of AP students and teachers, including the late nights that students put in diligently studying and the weekends that teachers give up to help their students succeed. Their hard work and effort are worth celebrating.
Acknowledgements
This data was collected and released by the College Board after the May 2016 exam administration."
"Nobel Laureates, 1901-Present",Which country has won the most prizes in each category?,The Nobel Foundation,13,"Version 1,2017-02-16",,CSV,283 KB,Other,"5,709 views",575 downloads,23 kernels,,https://www.kaggle.com/nobelfoundation/nobel-laureates,"Context
Between 1901 and 2016, the Nobel Prizes and the Prize in Economic Sciences were awarded 579 times to 911 people and organizations. The Nobel Prize is an international award administered by the Nobel Foundation in Stockholm, Sweden, and based on the fortune of Alfred Nobel, Swedish inventor and entrepreneur. In 1968, Sveriges Riksbank established The Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel, founder of the Nobel Prize. Each Prize consists of a medal, a personal diploma, and a cash award.
A person or organization awarded the Nobel Prize is called Nobel Laureate. The word ""laureate"" refers to being signified by the laurel wreath. In ancient Greece, laurel wreaths were awarded to victors as a sign of honor.
Content
This dataset includes a record for every individual or organization that was awarded the Nobel Prize since 1901.
Acknowledgements
The Nobel laureate data was acquired from the Nobel Prize API.
Inspiration
Which country has won the most prizes in each category? What words are most frequently written in the prize motivation? Can you predict the age, gender, and nationality of next year's Nobel laureates?"
Adverse Food Events,~90k FDA Recorded Medical Events,Food and Drug Administration,13,"Version 1,2017-09-08","government agencies
food and drink
healthcare
+ 2 more...",CSV,19 MB,CC0,"2,445 views",263 downloads,16 kernels,0 topics,https://www.kaggle.com/fda/adverse-food-events,"Context:
The CFSAN Adverse Event Reporting System (CAERS) is a database that contains information on adverse event and product complaint reports submitted to FDA for foods, dietary supplements, and cosmetics. The database is designed to support CFSAN's safety surveillance program. Adverse events are coded to terms in the Medical Dictionary for Regulatory Activities (MedDRA) terminology.
Content:
See the metadata description in the accompanying README.pdf below or here. Approximately 90k reactions are recorded from 2004-mid 2017, with 12 columns of information regarding type of reaction and related event details.
Acknowledgements:
This dataset is collected by the US Food and Drug Administration.
Inspiration:
What are the most commonly reported foodstuffs?
What are the most commonly reported medical reactions to foods?
Where do people in the US most commonly report food-related conditions?"
Tappy Keystroke Data with Parkinson's Patients,Raw data used to predict the onset of Parkinson from typing tendencies,Patrick DeKelly,13,"Version 1,2018-02-04","research
diseases
neurology
+ 2 more...",Other,85 MB,CC0,736 views,70 downloads,,0 topics,https://www.kaggle.com/valkling/tappy-keystroke-data-with-parkinsons-patients,"Introduction
This is the keystroke dataset for the study titled 'High-accuracy detection of early Parkinson's Disease using multiple characteristics of finger movement while typing'. This research report is currently under review for publication by PLOS ONE.
The dataset contains keystroke logs collected from over 200 subjects, with and without Parkinson's Disease (PD), as they typed normally on their own computer (without any supervision) over a period of weeks or months (having initially installed a custom keystroke recording app, Tappy). This dataset has been collected and analyzed in order to indicate that the routine interaction with computer keyboards can be used to detect changes in the characteristics of finger movement in the early stages of PD.
Data
The participants, from the U.S., Canada, UK and Australia, had visited the project website and agreed to participate in the study. The research was approved by the Human Research Ethics Committee at Charles Sturt University, Australia, protocol number H17013.
Each data file collected includes the timing information from typing activity as the participants used their various Windows applications (such as email, word processing, web searches and the like). The keystroke acquisition software ('Tappy') provided timing accuracy of key press and release timestamps to within several milliseconds.
The data files comprise two Zip archives, one with the participant detail files and the other with the keystroke data files for each user.
Acknowledgements
This dataset is from the research article ""High-accuracy detection of early Parkinson's Disease using multiple characteristics of finger movement while typing"" by Warwick R. Adams. Read the article here: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0188226#sec008
Inspiration
While this is a difficult dataset to work with, there is a rich trove of information. It is a great set to practice preprocessing, attempt to replicate the results of the article, or do your own analysis of keystroke data."
DeepSat (SAT-4) Airborne Dataset,"500,000 image patches covering four broad land cover classes",Chris Crawford,13,"Version 2,2018-01-18|Version 1,2018-01-17",machine learning,CSV,3 GB,CC0,940 views,53 downloads,3 kernels,0 topics,https://www.kaggle.com/crawford/deepsat-sat4,"DeepSat SAT-4


Originally, images were extracted from the National Agriculture Imagery Program (NAIP) dataset. The NAIP dataset consists of a total of 330,000 scenes spanning the whole of the Continental United States (CONUS). The authors used the uncompressed digital Ortho quarter quad tiles (DOQQs) which are GeoTIFF images and the area corresponds to the United States Geological Survey (USGS) topographic quadrangles. The average image tiles are ~6000 pixels in width and ~7000 pixels in height, measuring around 200 megabytes each. The entire NAIP dataset for CONUS is ~65 terabytes. The imagery is acquired at a 1-m ground sample distance (GSD) with a horizontal accuracy that lies within six meters of photo-identifiable ground control points.
The images consist of 4 bands - red, green, blue and Near Infrared (NIR). In order to maintain the high variance inherent in the entire NAIP dataset, we sample image patches from a multitude of scenes (a total of 1500 image tiles) covering different landscapes like rural areas, urban areas, densely forested, mountainous terrain, small to large water bodies, agricultural areas, etc. covering the whole state of California. An image labeling tool developed as part of this study was used to manually label uniform image patches belonging to a particular landcover class.
Once labeled, 28x28 non-overlapping sliding window blocks were extracted from the uniform image patch and saved to the dataset with the corresponding label. We chose 28x28 as the window size to maintain a significantly bigger context, and at the same time not to make it as big as to drop the relative statistical properties of the target class conditional distributions within the contextual window. Care was taken to avoid interclass overlaps within a selected and labeled image patch.
Content
Each sample image is 28x28 pixels and consists of 4 bands - red, green, blue and near infrared.
The training and test labels are one-hot encoded 1x4 vectors
The four classes represent the four broad land covers which include barren land, trees, grassland and a class that consists of all land cover classes other than the above three.
Training and test datasets belong to disjoint set of image tiles.
Each image patch is size normalized to 28x28 pixels.
Once generated, both the training and testing datasets were randomized using a pseudo-random number generator.
CSV files
X_train_sat4.csv: 400,000 training images, 28x28 images each with 4 channels
y_train_sat4.csv: 400,000 training labels, 1x4 one-hot encoded vectors
X_test_sat4.csv: 100,000 training images, 28x28 images each with 4 channels
y_test_sat4.csv: 100,000 training labels, 1x4 one-hot encoded vectors
The original MAT file
train_x: 28x28x4x400000 uint8 (containing 400000 training samples of 28x28 images each with 4 channels)
train_y: 400000x4 uint8 (containing 4x1 vectors having labels for the 400000 training samples)
test_x: 28x28x4x100000 uint8 (containing 100000 test samples of 28x28 images each with 4 channels)
test_y: 100000x4 uint8 (containing 4x1 vectors having labels for the 100000 test samples)
Acknowledgements
The original MATLAB file was converted to multiple CSV files
The original SAT-4 and SAT-6 airborne datasets can be found here:
http://csc.lsu.edu/~saikat/deepsat/
Thanks to:
Saikat Basu, Robert DiBiano, Manohar Karki and Supratik Mukhopadhyay, Louisiana State University Sangram Ganguly, Bay Area Environmental Research Institute/NASA Ames Research Center Ramakrishna R. Nemani, NASA Advanced Supercomputing Division, NASA Ames Research Center"
Women's E-Commerce Clothing Reviews,"23,000 Customer Reviews and Ratings",Nick Brooks,13,"Version 1,2018-02-04","business
internet
nlp
text mining",CSV,3 MB,CC0,"1,901 views",285 downloads,3 kernels,0 topics,https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews,"Context
Welcome. This is a Women’s Clothing E-Commerce dataset revolving around the reviews written by customers. Its nine supportive features offer a great environment to parse out the text through its multiple dimensions. Because this is real commercial data, it has been anonymized, and references to the company in the review text and body have been replaced with “retailer”.
Content
This dataset includes 23486 rows and 10 feature variables. Each row corresponds to a customer review, and includes the variables:
Clothing ID: Integer Categorical variable that refers to the specific piece being reviewed.
Age: Positive Integer variable of the reviewers age.
Title: String variable for the title of the review.
Review Text: String variable for the review body.
Rating: Positive Ordinal Integer variable for the product score granted by the customer from 1 Worst, to 5 Best.
Recommended IND: Binary variable stating where the customer recommends the product where 1 is recommended, 0 is not recommended.
Positive Feedback Count: Positive Integer documenting the number of other customers who found this review positive.
Division Name: Categorical name of the product high level division.
Department Name: Categorical name of the product department name.
Class Name: Categorical name of the product class name.
Acknowledgements
Anonymous
Inspiration
I look forward to come quality NLP! There is also some great opportunities for feature engineering, and multivariate analysis."
Flipkart Products,"20,000 products on Flipkart",PromptCloud,13,"Version 1,2017-09-15",internet,CSV,36 MB,CC4,"6,046 views",977 downloads,2 kernels,0 topics,https://www.kaggle.com/PromptCloudHQ/flipkart-products,"Context
This is a pre-crawled dataset, taken as subset of a bigger dataset (more than 5.8 million products) that was created by extracting data from Flipkart.com, a leading Indian eCommerce store.
Content
This dataset has following fields:
product_url
product_name
product_category_tree
pid
retail_price
discounted_price
image
is_FK_Advantage_product
description
product_rating
overall_rating
brand
product_specifications
Acknowledgements
This dataset was created by PromptCloud's in-house web-crawling service.
Inspiration
Analyses of the pricing, product specification and brand can be performed."
Jobs on Naukri.com,"22,000 job listings on Naukri.com",PromptCloud,13,"Version 1,2017-09-16",,CSV,50 MB,CC4,"3,128 views",481 downloads,2 kernels,,https://www.kaggle.com/PromptCloudHQ/jobs-on-naukricom,"Context
This is a pre-crawled dataset, taken as subset of a bigger dataset (more than 9.4 million job listings) that was created by extracting data from Naukri.com, a leading job board.
Content
This dataset has following fields:
company
education
experience
industry
job description
jobid
joblocation_address
job title
number of positions
pay rate
postdate
site_name
skills
Acknowledgements
This dataset was created by PromptCloud's in-house web-crawling service.
Inspiration
Analyses of the pay rate, job title, industry and experience can be performed to name a few starting points."
London-based restaurants' reviews on TripAdvisor,"20,000 restaurant's reviews on TripAdvisor.co.uk",PromptCloud,13,"Version 1,2017-09-16",internet,CSV,15 MB,CC4,"2,707 views",419 downloads,2 kernels,0 topics,https://www.kaggle.com/PromptCloudHQ/londonbased-restaurants-reviews-on-tripadvisor,"Context
This is a pre-crawled dataset, taken as subset of a bigger dataset (more than 1.8 million restaurants) that was created by extracting data from Tripadvisor.co.uk.
Content
This dataset has following fields:
uniq_id
url
restaurant_id
restaurant_location
name
category
title
review_date
review_text
author
author_url
location
rating
food
value
service
visited_on
Acknowledgements
This dataset was created by PromptCloud's in-house web-crawling service.
Inspiration
Analyses of the restaurant reviews and ratings can be performed."
TMY3 Solar,One Year of Typical Hourly Solar & Weather Data for +1000 US Locations,US Department of Energy,13,"Version 2,2017-09-16|Version 1,2017-09-16",energy,CSV,2 GB,CC0,"1,587 views",220 downloads,,0 topics,https://www.kaggle.com/us-doe/tmy3-solar,"The TMY3s are data sets of hourly values of solar radiation and meteorological elements for a 1-year period. Their intended use is for computer simulations of solar energy conversion systems and building systems to facilitate performance comparisons of different system types, configurations, and locations in the United States and its territories. Because they represent typical rather than extreme conditions, they are not suited for designing systems to meet the worst-case conditions occurring at a location.
Please note that TMY3 is NOT the state of the art solar data. It was used as a key component of investment analyses for several years, but NREL has released a more recent version based on satellite data and updated meteorological models that provides coverage for the entire United States. That dataset is much too large to publish here, but is highly recommended if you need the best information.
Content
Please see the pdf manual for full details of each field; there are several dozen of them.
It's important to know that nearly all of the solar data is modeled based on estimates of cloud cover; less than 1% of the stations directly measured sunlight.
This data is not appropriate for time series analysis. A typical meteorological year is literally twelve months of real data from twelve different years. Please see the manual for further details.
Acknowledgements
This dataset was made available by the National Renewable Energy Laboratory. You can find the original dataset here.
If you like
If you liked this dataset, you might also enjoy:
Google Project Sunroof
30 Years of European Wind Generation
30 Years of European Solar Generation"
Weekly Dairy Product Prices,USDA data on bulk dairy production since 2010,Sohier Dane,12,"Version 1,2017-08-30","finance
agriculture",CSV,349 KB,CC0,"2,352 views",332 downloads,3 kernels,,https://www.kaggle.com/sohier/weekly-dairy-product-prices,"We don't always think about industrial scale food, but cheese blocks the size of a small car are important.
The Mandatory Price Reporting Act of 2010 (pdf) was passed on September 27, 2010, the act required USDA to release dairy product sales information on or before Wednesday at 3:00 pm EST (unless affected by a Federal Holiday). The act also required the establishment of an electronic mandatory price reporting system for dairy products reported under Public Law 106-532. These dairy statistics will continue to be collected on a weekly basis, AMS-Dairy Programs will collect, analyze, aggregate, and publish dairy product sales information for selected dairy commodities.
Acknowledgements
This data is released by the US Department of Agriculture. You can find the original dataset here.
Inspiration
Can you predict changes in moisture content for 40 pound blocks of cheese?"
Salem Witchcraft Dataset,Explore and visualize the Salem witchcraft trials,Rachael Tatman,12,"Version 1,2017-09-02","united states
history
crime
violence",CSV,32 KB,CC4,"1,920 views",237 downloads,,0 topics,https://www.kaggle.com/rtatman/salem-witchcraft-dataset,"Context:
Few events in American history are better known than the Salem witchcraft trials of 1692. Its popularity is doubtless attributable to a number of things: a persistent fascination with the occult; a perverse pleasure to expose the underbelly of an American culture that boasts of toleration, social harmony, and progress; and an appreciation for a compelling, dramatic narrative replete with heroes and villains. Skeptics, like the preeminent twentieth-century historian Perry Miller, question whether the Salem trials constituted anything more than an inconsequential episode in colonial history. But most historians consider Salem worthy of continuing investigation even if it was less than a major turning point in history. Indeed, Salem has been an unusually fertile field for historical research because it readily lends itself to new approaches, insights, and methodologies. To understand what happened in Salem, historians have profitably applied the perspectives of politics, anthropology, economic and social analysis, religion, social psychology, and demography. If the ultimate meaning of Salem is still elusive, these investigations have broadened and deepened our understanding of the 1692 witchcraft outbreak.
Content:
The Salem Witchcraft Website contains eight data sets. They provide only a small portion of the historical record about Salem. They do not contain transcripts of examinations or trials or contemporary narrative accounts, for example. Instead, they provide information, primarily of a quantitative nature, about three major aspects of the outbreak: its chronology, its geographic spread, and the social and economic divisions in Salem Village that shaped events. The data were derived primarily from four published sources: Paul Boyer and Stephen Nissenbaum's three-volume transcription of the legal records of the witchcraft trials, The Salem Witchcraft Papers; the new and now authoritative Records of the Salem Witch-Hunt, edited by Bernard Rosenthal, et. al.; Boyer and Nissenbaum's edited collection of documents, Salem-Village Witchcraft; and Salem Village's Book of Record, which contain tax records and other information relating to Salem Village. Photocopies of the original Salem Village record book and church records were examined at the Danvers Archival Center.
The Accused Witches Data Set contains information about those who were formally accused of witchcraft during the Salem episode. This means that there exists evidence of some form of direct legal involvement, such as a complaint made before civil officials, an arrest warrant, an examination, or court record. Accused witches were almost always detained in jail to await further action by a grand jury, which had the authority to indict and hold the accused for trial. Trials by a special Court of Oyer and Terminer began in June 1692. In October 1692, this court was discontinued due to mounting criticism of its methods. It was replaced by another court, the Superior Court of Judicature, which held trials from January to May 1693.
The ""Accused Witch"" column records the names of the 152 people mentioned in legal records as having been formally accused of witchcraft. Their names are alphabetically arranged. Spelling generally follows that of Paul Boyer and Stephen Nissenbaum, Salem Witchcraft Papers but has been sometimes changed in accordance with the newer Records of the Salem Witch-Hunt and other sources.
""Residence"" identifies the community in which the accused person was living when accused of witchcraft. In a few cases, the residence of an accused witch is problematic. For Elizabeth How, for example, some records cite Ipswich while others name Topsfield as her home. In such cases, the most likely residence has been used. In a few instances, the residence entry does not reflect the actual geographic relationship of the accused with the trials. George Burroughs was living in Wells, Maine, in 1692, but he had been a controversial minister in Salem Village in the early 1680s.
""Month of Accusation"" numerically expresses the month of the year in which an alleged witch was accused: ""1""=January 1692; ""6""=June 1692; and ""13""=January 1693. A negative 1 (-1) indicates that the actual month of accusation is not known with sufficient certainty to be included. Some of these ""unknowns"" can be approximated from available records, and users may choose to substitute their estimate. Users should also recognize an artificial quality to this data: those accused in one month, May (5), for example, may have been charged only a day or two before someone in June (6).
""Month of Execution"" numerically expresses the month in which an alleged witch was executed as a result of the legal process. The data do not include entries for those who died as a result of their incarceration. In one case, Giles Corey, the month of execution does record the month in which he was pressed to death for refusing to plead to the charges against him.
The Towns Data Set provides a convenient way to construct histograms of the communities whose residents were charged with witchcraft in 1692. It contains 25 columns:
Twenty-four columns record each town for which at least one formal accusation occurred (Salem Village and Salem Town are listed separately). Each cell lists the month of an accusation, numerically expressed: 1=January 1692, 2=February 1692, and so forth. The negative number, -1, indicates that the month of accusation is unknown.
A ""Bin"" column contains the range of months of witchcraft accusations, from 1 (January 1692) to 12 (December 1692), with -1 for unknown months of accusation.
Both the Pro-Parris and Anti-Parris data sets contain the same four columns:
""Name"" identifies each signer of the pro-Parris petition.
""Identification"" indicates the category in the petition under which the signer was placed.
""Sex"" indicates whether the signer was male or female.
""Sort"" locates each signer in the data set so that it can be returned to its original order.
To compare the social make-up of Salem Village's pro- and anti-Parris factions to the village's general population, download the Salem Village Data Set. The data set contains four columns:
""Name"" lists every person in Salem Village who appeared on any village tax assessments for 1690, 1695, and 1697. The 137 names are a good, though imperfect, indicator of the village's adult male population in the period of the witch hunt. Only a few women, all widows, appear. Young men not yet independent or paying taxes do not appear.
""Petition"" notes whether the taxpayer signed either the pro- or anti-Parris petition in 1695. ""NoS"" (no signature) means that the person did not sign either petition.
""Church to 1696"" indicates whether a person was a church member though 1696. No distinction is made as to whether a person was a member of the Salem Village church or another church. The list is compiled from the pro- and anti-Parris petitions as well as from the records of the Salem Village church as recorded by Samuel Parris. Additional information came from the published records of the First Church in Salem Town.
""Sort"" permits data to be easily restored to their original order after a statistical manipulation.
The Committee Yearly Data Set contains information about Salem Village's committees from 1685 to 1698, a period that covers the last years Deodat Lawson's ministry and the entire tenure of Samuel Parris. The data set contains three columns of information for each committee:
""Committee"" lists the names of committeemen for a particular year (in 1688, only four men were elected).
""Petition"" indicates whether the committeeman signed either the pro- or anti-Parris petition in 1695. ""NoS"" (no signature) means that this committeeman did not sign either petition. Signing a petition strongly suggests but does not conclusively establish a petitioner's earlier position regarding Parris or the witchcraft outbreak.
""Social"" indicates whether the committeeman was a church member or a householder. Three committeemen (William Sibley, James Smith, and Jacob Fuller) have been listed as householders in the absence of information linking them to a church.)
The Committee List Data Set provides information about all Salem Village committee members who held office from 1685 to 1698. The data set contains 18 columns:
""Committee Members"" records the names of the thirty-one villagers who held committee office from 1685-1698. They are listed in the order in which they first appeared in the village's Book of Record.
""Petition"" notes whether the committeeman signed either the pro- or anti-Parris petition in 1695. ""NoS"" (no signature) means that this committeeman did not sign either petition.
""Social"" indicates whether the committeeman was a church member or a householder. (Three committeemen, William Sibley, James Smith, and Jacob Fuller, have been listed as householders in the absence of information linking them to a church; the three did not sign either petition.)
Columns 4-17 indicate committee membership for each year.
""Sort"" permits data to be easily restored to their original order.
The Tax Comparison Data Set was compiled by listing all Salem Village taxpayers who were assessed rates in the period between 1681 and 1700. The rates are recorded in Salem Village's Book of Record (see Bibliography).
""Name"" lists in alphabetical order all assessments on Salem Village's tax lists for 1681, 1690, 1694, 1695, 1697, and 1700.
""Tax"" records the taxpayer's assessment in shillings. Since the village's revenue needs changed, the total assessment (and individual allocations) changed accordingly.
""Petition"" indicates whether the taxpayer signed either the pro- or anti-Parris petition in 1695. ""NoS"" (no signature) means that this taxpayer did not sign either petition.
""Sort"" permits data to be easily restored to their original order after a statistical manipulation.
Acknowledgements:
Users who copy, share, adapt, and re-publish any of the content in Salem Witchcraft Dataset should credit Professor Richard Latner of Tulane University for making this material available. More information and guided exercises can be found on this website.
Inspiration:
What was the relationship between economic success and support for Parris?
Can you split the list of accused witches and predict who would be accused based on other acquisitions?"
English Word Frequency,⅓ Million Most Frequent English Words on the Web,Rachael Tatman,12,"Version 1,2017-09-07","languages
linguistics
internet",CSV,5 MB,Other,"2,251 views",277 downloads,2 kernels,0 topics,https://www.kaggle.com/rtatman/english-word-frequency,"Context:
How frequently a word occurs in a language is an important piece of information for natural language processing and linguists. In natural language processing, very frequent words tend to be less informative than less frequent one and are often removed during preprocessing. Human language users are also sensitive to word frequency. How often a word is used affects language processing in humans. For example, very frequent words are read and understood more quickly and can be understood more easily in background noise.
Content:
This dataset contains the counts of the 333,333 most commonly-used single words on the English language web, as derived from the Google Web Trillion Word Corpus.
Acknowledgements:
Data files were derived from the Google Web Trillion Word Corpus (as described by Thorsten Brants and Alex Franz, and distributed by the Linguistic Data Consortium) by Peter Norvig. You can find more information on these files and the code used to generate them here.
The code used to generate this dataset is distributed under the MIT License.
Inspiration:
Can you tag the part of speech of these words? Which parts of speech are most frequent? Is this similar to other languages, like Japanese?
What differences are there between the very frequent words in this dataset, and the the frequent words in other corpora, such as the Brown Corpus or the TIMIT corpus? What might these differences tell us about how language is used?"
Portland Oregon Crime Data,"By Year, Event, and Location",katzwigmore,12,"Version 2,2017-09-25|Version 1,2017-09-17",crime,CSV,131 MB,CC0,"1,841 views",219 downloads,2 kernels,,https://www.kaggle.com/katzwigmore/portland-oregon-crime-data,"Content
The contents of this data set comes from public data available on the city of Portland website. Each individual crime reported is lists the location, time and date of the incident as well as a the neighborhood in which the event occurred.
All data prior to 2015 has the same general format but the newer 2015-17 data needs to be reformatted for easier comparison since it does not match the older organizational scheme. To this end I will be adding new .csv with 2015 , 2016, and 2017 YTD data broken out. Coordinate data will also be added to make the data sets more easily comparable and mappable.
Update: I created new .csv for each year 2015-2017 changing the formatting from the Portland Police Department's tab separated values to the standard comma separated values. The pre-2015 data still isn't comparable because of the differences in the crime categorization but I will work creating some sort of key so that the full data set can be analyzed as a single batch of information.
Acknowledgements
Banner image by Zack Spear on Unsplash.
All data gathered from portlandoregon.gov and civicapps.org"
Monthly Salary of Public Worker in Brazil,Salary of Public Worker in Brazil,Luís Gustavo Modelli,12,"Version 1,2018-01-13","government agencies
brazil
finance
+ 2 more...",CSV,18 MB,CC0,"1,983 views",303 downloads,2 kernels,0 topics,https://www.kaggle.com/gustavomodelli/monthly-salary-of-public-worker-in-brazil,"Context
The monthly salary of the public workers of the State of São Paulo in Brazil is a Public data available in the transparency portal of the state government at: http://www.transparencia.sp.gov.br/buscaRemunera.html
Content
The data is about the salary for all worker in the State for the month of October 2017. There are just over one million records. The names of the employee are anonymous represented by the variable id.
Inspiration
This database may reveal:
Higher salaries
The contribution of extra remuneration to higher salaries
By the rules of the government no employee could receive more than the state governor salary: R$ 21,631.05"
Analysis about crypto currencies and Stock Index,Relation and patterns between movements of stock exchange indexes and cryptocurrency,Albert Costas,12,"Version 2,2017-12-14|Version 1,2017-12-13","time series
money
economics",CSV,665 KB,ODbL,"1,620 views",143 downloads,,0 topics,https://www.kaggle.com/acostasg/cryptocurrenciesvsstockindex,"«Datasets per la comparació de moviments i patrons entre els principals índexs borsatils espanyols i les crypto-monedes»
Context
En aquest cas el context és detectar o preveure els diferents moviments que es produeixen per una serie factors, tant de moviment interns (compra-venda), com externs (moviments polítics, econòmics, etc...), en els principals índexs borsatils espanyols i de les crypto-monedes.
Hem seleccionat diferents fonts de dades per generar fitxers «csv», guardar diferents valors en el mateix període de temps. És important destacar que ens interessa més les tendències alcistes o baixes, que podem calcular o recuperar en aquests períodes de temps.
Content
En aquest cas el contingut està format per diferents csv, especialment tenim els fitxers de moviments de cryptomoneda, els quals s’ha generat un fitxer per dia del període de temps estudiat.
Pel que fa als moviments del principals índexs borsatils s’ha generat una carpeta per dia del període, en cada directori un fitxer amb cadascun del noms dels índexs. Degut això s’han comprimit aquests últims abans de publicar-los en el directori de «open data» kaggle.com.
Pel que fa als camps, ens interessà detectar els moviments alcistes i baixistes, o almenys aquelles que tenen un patró similar en les cryptomonedes i els índexs. Els camps especialment destacats són:
• Data: Data de la observació
• Nom: Nom empresa o cryptomoneda, per identificar de quina moneda o index estem representant.
• Símbol: Símbol de la moneda o del index borsatil, per realitzar gràfic posteriorment d’una forma mes senzilla que el nom.
• Preu: Valor en euros d’una acció o una cryptomoneda (transformarem la moneda a euros en el cas de estigui en dòlars amb l'última cotització (un dollar a 0,8501 euro)
• Tipus_cotitzacio: Valor nou que agregarem per discretitzar entre la cotització: baix (0 i 1), normal (1 i 100), alt (100 i 1000), molt_alt (&gt;1000)
Script R
Anàlisis de les observacions i el domini de les dades
Anàlisis en especial de Bitcoin i la IOTA.
Test de Levene per veure la homogeneitat
Kmeans per creació de cluster per veure la homegeneitat
Freqüències de les distribucions
Test de contrast d'hipòtesis de variables dependents (Wilcoxon)
Test de Shapiro-Wilk per veure la normalitat de les dades, per normalitzar-les o no
Correlació d'índexs borsatils, per eliminar complexitat dels índexs amb grau més alt de correlació
Iteració de Regressions lineals per obtenir el model amb més qualitat, observa'n el p-valor i l'índex de correlació
Validació de la qualitat del model
Representació grafica
Acknowledgements
En aquest cas les fonts de dades que s’han utilitzat per a la realització dels datasets corresponent a:
http://www.eleconomista.es
https://coinmarketcap.com
Per aquest fet, les dades de borsa i crypto-moneda estan en última instància sota llicència de les webs respectivament. Pel que fa a la terminologia financera podem veure vocabulari en renta4banco.
[https://www.r4.com/que-necesitas/formacion/diccionario]
Inspiration
Hi ha un estudi anterior on poder tenir primícies de com han enfocat els algoritmes:
https://arxiv.org/pdf/1410.1231v1.pdf
En aquest cas el «trading» en cryptomoneda és relativament nou, força popular per la seva formulació com a mitja digital d’intercanvi, utilitzant un protocol que garanteix la seguretat, integritat i equilibri del seu estat de compte per mitjà d’un entramat d’agents.
La comunitat podrà respondre, entre altres preguntes, a:
Està afectant o hi ha patrons comuns en les cotitzacions de cryptomonedes i el mercat de valors principals del país d'Espanya?
Els efectes o agents externs afecten per igual a les accions o cryptomonedes?
Hi ha relacions cause efecte entre les acciones i cryptomonedes?
Project repository
https://github.com/acostasg/scraping
Datasets
Els fitxers csv generats que componen el dataset s’han publicat en el repositori kaggle.com:
https://www.kaggle.com/acostasg/stock-index/
https://www.kaggle.com/acostasg/crypto-currencies
Per una banda, els fitxers els «stock-index» estan comprimits per carpetes amb la data d’extracció i cada fitxer amb el nom dels índexs borsatil. De forma diferent, les cryptomonedes aquestes estan dividides per fitxer on són totes les monedes amb la data d’extracció."
Weather archive Jena,"Air temperature, atmospheric pressure, humidity, etc recorded over seven years",Kris,12,"Version 1,2018-01-21","climate
weather",CSV,13 MB,Other,"1,297 views",152 downloads,,0 topics,https://www.kaggle.com/pankrzysiu/weather-archive-jena,"Context
The Dataset is used by ""A temperature-forecasting problem"" from the ""Deep Learning with Python"" book
Content
The data was downloaded from: https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip
It represents time period between 2009 and 2016
Acknowledgements
The dataset recorded at the Weather Station at the Max Planck Institute for Biogeochemistry in Jena, Germany. https://www.bgc-jena.mpg.de/wetter/
It was reassembled by François Chollet, the author of the ""Deep Learning with Python"" book
Inspiration
The main purpose of this dataset is to perform RNN exercise (6.3.1 A temperature-forecasting problem) from the ""Deep Learning with Python"" book."
Medicare's Doctor Comparison Scores,The 2017 Physican Compare Database,Centers for Medicare & Medicaid Services,12,"Version 1,2017-08-30","healthcare
public health",CSV,689 MB,CC0,"2,360 views",307 downloads,,0 topics,https://www.kaggle.com/cms/medicares-doctor-comparison-scores,"The Physician Compare website was created by the Centers for Medicare & Medicaid Services (CMS) in December 2010 as required by the Affordable Care Act (ACA) of 2010 to help patients assess and find doctors and hospitals. This dataset contains the information supplied to patients via that website, including patient satisfaction surveys and performance scores across over 100 metrics.
Acknowledgements
This dataset was kindly released by the Centers for Medicare & Medicaid Services. You can find the original copy of the dataset here."
U.S. Technology Jobs on Dice.com,"22,000 US-based Technology Job Listings",PromptCloud,12,"Version 1,2017-09-15",internet,CSV,58 MB,CC4,"2,939 views",394 downloads,,0 topics,https://www.kaggle.com/PromptCloudHQ/us-technology-jobs-on-dicecom,"Context
This is a pre-crawled dataset, taken as subset of a bigger dataset (more than 4.6 million job listings) that was created by extracting data from Dice.com, a prominent US-based technology job board.
Content
This dataset has following fields:
advertiserurl
company
employmenttype_jobstatus
jobdescription
joblocation_address
jobtitle
postdate
shift
skills
Acknowledgements
This dataset was created by PromptCloud's in-house web-crawling service.
Inspiration
Analyses of the job description with respect to the job title and skills can be performed."
New York City - Buildings Database,The PLUTO master record of buildings in New York City.,City of New York,12,"Version 2,2017-09-01|Version 1,2017-09-01","cities
civil engineering",CSV,290 MB,CC0,"2,503 views",353 downloads,4 kernels,,https://www.kaggle.com/new-york-city/nyc-buildings,"Context:
PLUTO is a master record of the locations and characteristics of buildings in New York City. It’s published by the New York City Department of City Planning on an approximately quarterly-to-half-yearly basis, and is one of the more important datasets for civic analysis in New York City.
Content:
PLUTO includes information on building height, square footage, location, type, landmark status, number of units, owner, year of construction, and other related fields.
Acknowledgements:
This dataset is published as-is by the New York City Department of Planning.
Inspiration:
What is the distribution of the heights of buildings in New York City? The age?
Can you define neighborhoods by clustering similar buildings within them?
What (and where) is the split between commercial, residential, and office space in New York City?"
New York City - Citywide Payroll Data,Salaries paid to New York City employees over four years,City of New York,12,"Version 1,2017-09-06","cities
money",CSV,395 MB,CC0,"3,459 views",584 downloads,2 kernels,,https://www.kaggle.com/new-york-city/nyc-citywide-payroll-data,"Context
This dataset contains the salary, pay rate, and total compensation of every New York City employee. In this dataset this information is provided for the 2014, 2015, 2016, and 2017 fiscal years, and provides a transparent lens into who gets paid how much and for what.
Note that fiscal years in the New York City budget cycle start on July 1st and end on June 30th (see here). That means that this dataset contains, in its sum, compensation information for all City of New York employees for the period July 1, 2014 to June 30, 2017.
Content
This dataset provides columns for fiscal year, employee name, the city department they work for, their job title, and various fields describing their compensation. The most important of these fields is ""Regular Gross Pay"", which provides that employee's total compensation.
Acknowledgements
This information was published as-is by the City of New York.
Inspiration
How many people do the various city agencies employ, and how much does each department spend on salary in total?
What are the most numerous job titles in civic government employment?
Where does overtime pay seem to be especially common? How much of it is there?
How do New York City employee salaries compare against salaries of city employees in Chicago? Is the difference more or less than the difference in cost of living between the two cities?"
New York City - East River Bicycle Crossings,Daily bicycle counts for major bridges in NYC,City of New York,12,"Version 1,2017-09-06","cities
road transport",CSV,18 KB,CC0,"2,440 views",650 downloads,222 kernels,0 topics,https://www.kaggle.com/new-york-city/nyc-east-river-bicycle-crossings,"Context
The New York City Department of Transportation collects daily data about the number of bicycles going over bridges in New York City. This data is used to measure bike utilization as a part of transportation planning. This dataset is a daily record of the number of bicycles crossing into or out of Manhattan via one of the East River bridges (that is, excluding Bronx thruways and the non-bikeable Hudson River tunnels) for a stretch of 9 months.
Content
A count of the number of bicycles on each of the bridges in question is provided on a day-by-day basis, along with information on maximum and minimum temperature and precipitation.
Acknowledgements
This data is published in an Excel format by the City of New York (here). It has been processed into a CSV file for use on Kaggle.
Inspiration
In this dataset, how many bicycles cross into and out of Manhattan per day?
How strongly do weather conditions affect bike volumes?
What is the top bridge in terms of bike load?"
NYC Rat Sightings,~102k Observations Around New York,City of New York,12,"Version 1,2017-09-18","government agencies
animals
government",CSV,52 MB,CC0,"2,171 views",268 downloads,,,https://www.kaggle.com/new-york-city/nyc-rat-sightings,"Context:
Rats in New York City are prevalent, as in many densely populated areas. For a long time, the exact number of rats in New York City was unknown, and a common urban legend was that there were up to four times as many rats as people. In 2014, however, scientists more accurately measured the entire city's rat population to be approximately only 25% of the number of humans; i.e., there were approximately 2 million rats to New York's 8.4 million people at the time of the study.[1][2]
Content:
New York City rodent complaints can be made online, or by dialing 3-1-1, and the New York City guide Preventing Rats on Your Property discusses how the New York City Health Department inspects private and public properties for rats. Property owners that fail inspections receive a Commissioner's Order and have five days to correct the problem. If after five days the property fails a second inspection, the owner receives a Notice of Violation and can be fined. The property owner is billed for any clean-up or extermination carried out by the Health Department.
Data is from 2010-Sept 16th, 2017 and includes date, location (lat/lon), type of structure, borough, and community board.
Acknowledgements:
Data was produced by the City of New York via their 311 portal.
Inspiration:
Where and when are rats most seen?
Can you predict rat sightings from previous data?
Are there any trends in rat sightings?"
Dogs of Zurich,"Data about Dog Owners in Zurich, Switzerland",Kevin Mader,12,"Version 2,2017-03-08|Version 1,2017-02-23","animals
sociology",CSV,1 MB,CC0,"3,376 views",302 downloads,39 kernels,2 topics,https://www.kaggle.com/kmader/dogs-of-zurich,"All the data was taken from Open Data Zurich (https://data.stadt-zuerich.ch/dataset/pd-stapo-hundebestand) with the idea of making a useful few Kernel demos from it and let people look at information about the dogs that live here.
German
Since German is the official language of Zurich most of the columns are in German but the translations to English aren't too tricky
ALTER -> Age
GESCHLECHT -> Gender
STADTKREIS -> City Quarter or District
RASSE1 -> Dog's Primary Breed
RASSE2 -> Dog's Secondary Breed
GEBURTSJAHR_HUND -> Dog's Year of Birth
GESCHLECHT_HUND -> Dog's Gender
HUNDEFARBE -> Dog's Color
Utility
It might also help people trying to find apartments in areas with the right kind of dogs
Could be used to look at how dog trends have changed in time (by looking at the numbers by birth year)
Helpful for picking the right kind of dog to get your 90 year old grandmother (what kind of dogs do other 90 year old women have)"
Foreign Direct Investment in India,Sector & Financial year wise time series data from 2000-2016.,Rajanand Ilangovan / இராஜ்ஆனந்த் இளங்கோவன்,12,"Version 1,2017-08-18","india
finance",CSV,8 KB,CC4,"1,766 views",253 downloads,,0 topics,https://www.kaggle.com/rajanand/fdi-in-india,"Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks.
Context
To understand the Foreign direct investment in India for the last 17 years from 2000-01 to 2016-17.
Content
This dataset contains sector and financial year wise data of FDI in India.
Acknowledgements
Ministry of Commerce and Industry has published Financial Year wise FDI Equity Inflows from 2000-01 to 2016-17 dataset in Open Government Data Platform India under Govt. Open Data License - India.
Inspiration
How much FDI has changed over the year?
How much has varied since 2014 after Narendra Modi become PM of India?"
U.S. Major League Soccer Salaries,Salaries from 2007 to 2017,Chris Crawford,12,"Version 1,2017-07-14","association football
income",CSV,202 KB,Other,"2,582 views",458 downloads,3 kernels,,https://www.kaggle.com/crawford/us-major-league-soccer-salaries,"Context
The Major League Soccer Union releases the salaries of every MLS player each year. This is a collection of salaries from 2007 to 2017.
Content
Each file contains the following fields:
club: Team abbreviation
last_name: Player last name
first_name: Player first name
position: Position abbreviation
base_salary: Base salary
guaranteed_compensation: Guaranteed compensation
Acknowledgements
Jeremy Singer-Vine over at Data is Plural scraped the PDF's released by the MLS Union and put the data in a nice little package of CSV files for everyone.
I downloaded this dataset from: https://github.com/data-is-plural/mls-salaries MIT License
Inspiration
Who in the MLS makes the most money? Are they worth it? I make about $900 bazillion each year, can I afford a soccer team?"
20 Newsgroups,"A collection of ~18,000 newsgroup documents from 20 different newsgroups",Chris Crawford,12,"Version 1,2017-07-27","linguistics
internet",Other,69 MB,Other,"3,093 views",249 downloads,,0 topics,https://www.kaggle.com/crawford/20-newsgroups,"Context
This dataset is a collection newsgroup documents. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.
Content
There is file (list.csv) that contains a reference to the document_id number and the newsgroup it is associated with. There are also 20 files that contain all of the documents, one document per newsgroup.
In this dataset, duplicate messages have been removed and the original messages only contain ""From"" and ""Subject"" headers (18828 messages total).
Each new message in the bundled file begins with these four headers:
Newsgroup: alt.newsgroup
Document_id: xxxxxx
From: Cat
Subject: Meow Meow Meow
The Newsgroup and Document_id can be referenced against list.csv
Organization - Each newsgroup file in the bundle represents a single newsgroup - Each message in a file is the text of some newsgroup document that was posted to that newsgroup.
This is a list of the 20 newsgroups:
comp.graphics
comp.os.ms-windows.misc
comp.sys.ibm.pc.hardware
comp.sys.mac.hardware
comp.windows.x rec.autos
rec.motorcycles
rec.sport.baseball
rec.sport.hockey sci.crypt
sci.electronics
sci.med
sci.space
misc.forsale talk.politics.misc
talk.politics.guns
talk.politics.mideast talk.religion.misc
alt.atheism
soc.religion.christian
Acknowledgements
Ken Lang is credited by the source for collecting this data. The source of the data files is here:
http://qwone.com/~jason/20Newsgroups/
Inspiration
This dataset text can be used to classify text documents"
Fact-Checking Facebook Politics Pages,Hyperpartisan Facebook pages and misleading information during the 2016 election,Megan Risdal,12,"Version 1,2017-06-06","news agencies
politics
political science
internet",CSV,356 KB,Other,"2,369 views",217 downloads,8 kernels,,https://www.kaggle.com/mrisdal/fact-checking-facebook-politics-pages,"Context
During the 2016 US presidential election, the phrase “fake news” found its way to the forefront in news articles, tweets, and fiery online debates the world over after misleading and untrue stories proliferated rapidly. BuzzFeed News analyzed over 1,000 stories from hyperpartisan political Facebook pages selected from the right, left, and mainstream media to determine the nature and popularity of false or misleading information they shared.
Content
This dataset supports the original story “Hyperpartisan Facebook Pages Are Publishing False And Misleading Information At An Alarming Rate” published October 20th, 2016. Here are more details on the methodology used for collecting and labeling the dataset (reproduced from the story):
More on Our Methodology and Data Limitations
“Each of our raters was given a rotating selection of pages from each category on different days. In some cases, we found that pages would repost the same link or video within 24 hours, which caused Facebook to assign it the same URL. When this occurred, we did not log or rate the repeat post and instead kept the original date and rating. Each rater was given the same guide for how to review posts:
“Mostly True: The post and any related link or image are based on factual information and portray it accurately. This lets them interpret the event/info in their own way, so long as they do not misrepresent events, numbers, quotes, reactions, etc., or make information up. This rating does not allow for unsupported speculation or claims.
“Mixture of True and False: Some elements of the information are factually accurate, but some elements or claims are not. This rating should be used when speculation or unfounded claims are mixed with real events, numbers, quotes, etc., or when the headline of the link being shared makes a false claim but the text of the story is largely accurate. It should also only be used when the unsupported or false information is roughly equal to the accurate information in the post or link. Finally, use this rating for news articles that are based on unconfirmed information.
“Mostly False: Most or all of the information in the post or in the link being shared is inaccurate. This should also be used when the central claim being made is false.
“No Factual Content: This rating is used for posts that are pure opinion, comics, satire, or any other posts that do not make a factual claim. This is also the category to use for posts that are of the “Like this if you think...” variety.
“In gathering the Facebook engagement data, the API did not return results for some posts. It did not return reaction count data for two posts, and two posts also did not return comment count data. There were 70 posts for which the API did not return share count data. We also used CrowdTangle's API to check that we had entered all posts from all nine pages on the assigned days. In some cases, the API returned URLs that were no longer active. We were unable to rate these posts and are unsure if they were subsequently removed by the pages or if the URLs were returned in error.”
Acknowledgements
This dataset was originally published on GitHub by BuzzFeed News here: https://github.com/BuzzFeedNews/2016-10-facebook-fact-check
Inspiration
Here are some ideas for exploring the hyperpartisan echo chambers on Facebook:
How do left, mainstream, and right categories of Facebook pages differ in the stories they share?
Which types of stories receive the most engagement from their Facebook followers? Are videos or links more effective for engagement?
Can you replicate BuzzFeed’s findings that “the least accurate pages generated some of the highest numbers of shares, reactions, and comments on Facebook”?
Start a new kernel"
JACS Papers 1996 - 2016,All papers published in J. Am. Chem. Soc. between 1996 and 2016,Mathew Savage,12,"Version 4,2017-12-22|Version 3,2017-02-02|Version 2,2017-01-31|Version 1,2017-01-30","research
chemistry",SQLite,39 MB,Other,"2,801 views",206 downloads,17 kernels,,https://www.kaggle.com/mathewsavage/jacs,"Context
The Journal of the American Chemical Society is the premier journal published by the American Chemical Society and one of the highest ranking journals in all of chemistry. With almost 60,000 papers and over 120,000 authors this collections of papers published between 1996 and 2016, represents the current state of chemistry research.
Content
This dataset is presented in 3 database tables, one of published articles, and one of all authors, with a further table relating authors to the journal articles they have published.
Update 21-12-2017: The previous data was collected by a top level scrape of the table of contents pages from the journal.
A couple of months ago I performed a page-level scrape and then forgot about it, but I had some positive reactions to the data-set this week, so I have processed some of the data (although more remains, the raw output from the scrape is an 18 GB csv file).
This new data updates the Articles table to contain many more data fields, including the paper abstract, number of citations and page views (page views are a relatively new feature, so is probably not for the lifetime of some of the older papers).
One interesting project for this data would be to look at the term frequencies in the abstracts of the papers, and use that to see how the focus of chemistry research has changed over the years.
If it is interesting to people, I have the following unprocessed data: - Articles citing papers in this database inc, Title, Journal, Year and Author list - Institutions of authors for each papers (this data is very complicated and requires some difficult parsing)
Acknowledgements
This data was scraped from the Table of Contents section of the JACS website and is available online publicly.
Inspiration
This data could be used to determine the average number of authors per paper, or the connections between authors, to determine if specific research fields can be grouped by the associated authors
Also see if you can find the two papers published by me in this year, and see who my co-authors were!"
Metal Bands by Nation,The data sets within contain information on metal bands and world population,mrpantherson,12,"Version 1,2017-02-16","music
international relations",CSV,380 KB,CC0,"2,856 views",349 downloads,20 kernels,0 topics,https://www.kaggle.com/mrpantherson/metal-by-nation,"Context
The story behind this data set and analysis is just a combination of my interest in data science and metal music. I was looking for interesting data that I could analyze and this happen to be one of the first I started exploring.
Content
The world population information was a direct download so I did not have to do any work to get it. This data set consists of population information for countries on earth from the years 1960 to 2015.
The metal band information was scraped from the website http://metalstorm.net/ and consists of the following - band name - how many fans the band has on the website - when the band formed - when the band split - the country of origin on the band - the styles of the band
Acknowledgements
The metal information was compiled from information found on http://metalstorm.net/, the world population information is from http://www.worldbank.org/.
Inspiration
There has already been some great analysis of metal bands on Kaggle, I wanted to contribute to the discussion by adding some new data and looking at it from a slightly different angle. Also I thought it might be useful to share the process by which I came up with the visualizations and data management since the community has been a big help to me."
Periodic Table of the Elements,"Elements, activity series, and electromotive potentials",Jake Waitze,12,"Version 1,2017-02-16","chemistry
physics",CSV,701 KB,CC0,"4,105 views",353 downloads,3 kernels,,https://www.kaggle.com/jwaitze/tablesoftheelements,"Context
Observations of particles much smaller than us, and various understandings of those particles, have propelled mankind forward in ways once impossible to imagine. ""The elements"" are what we call the sequential patterns in which some of these particles manifest themselves.
As a chemistry student and a coder, I wanted to do what came naturally to me and make my class a bit easier by coding/automating my way around some of the tedious work involved with calculations. Unfortunately, it seems that chemical-related datasets are not yet a thing which have been conveniently formatted into downloadable databases (as far as my research went). I decided that the elements would be a good place to start data collection, so I did that, and I'd like to see if this is useful to others as well.
Other related data sets I'd like to coalesce are some large amount of standard entropies and enthalpies of various compounds, and many of the data sets from the CRC Handbook of Chemistry and Physics. I also think as many diagrams as possible should be documented in a way that can be manipulated and read via code.
Content
Included here are three data sets. Each data set I have included is in three different formats (CSV, JSON, Excel), for a total of nine files.
Table of the Elements:
This is the primary data set.
118 elements in sequential order
72 features
Reactivity Series:
33 rows (in order of reactivity - most reactive at the top)
3 features (symbol, name, ion)
Electromotive Potentials:
284 rows (in order from most negative potential to most positive)
3 features (oxidant, reductant, potential)
Acknowledgements
All of the data was scraped from 120 pages on Wikipedia using scripts. The links to those scripts are available in the dataset descriptions.
Extra
If you are interested in trying the chemistry calculations code I made for completing some of my repetitive class work, it's publicly available on my GitHub. (Chemistry Calculations Repository) I plan to continue updating that as time goes on."
Women's Tennis Association Matches,WTA matches from 2000 to 2016,GMAdevs,12,"Version 1,2017-02-05",tennis,CSV,9 MB,CC4,"2,761 views",656 downloads,8 kernels,0 topics,https://www.kaggle.com/gmadevs/wta-matches,"Context
A dataset of WTA matches including individual statistics.
Content
In these datasets there are individual csv files for WTA tournament from 2000 to 2016.
The numbers in the last columns are absolute values, using them you can calculate percentages.
Acknowledgement
Thanks to Jeff Sackmann for the excellent work. Be sure to visit his github profile
https://github.com/JeffSackmann/tennis_wta
Inspiration
This dataset would be likely used to develop predictive modeling of tennis matches and to do statistic research. I'm planning to add historical odds and injuries data as soon as I have the time to get them."
New York City Census Data,"Demographic, Economic, and Location Data for Census Tracts in NYC",MuonNeutrino,12,"Version 2,2017-08-04|Version 1,2017-07-23","united states
demographics",CSV,2 MB,CC0,"2,653 views",454 downloads,66 kernels,0 topics,https://www.kaggle.com/muonneutrino/new-york-city-census-data,"Context
There are a number of Kaggle datasets that provide spatial data around New York City. For many of these, it may be quite interesting to relate the data to the demographic and economic characteristics of nearby neighborhoods. I hope this data set will allow for making these comparisons without too much difficulty.
Exploring the data and making maps could be quite interesting as well.
Content
This dataset contains two CSV files:
nyc_census_tracts.csv
This file contains a selection of census data taken from the ACS DP03 and DP05 tables. Things like total population, racial/ethnic demographic information, employment and commuting characteristics, and more are contained here. There is a great deal of additional data in the raw tables retrieved from the US Census Bureau website, so I could easily add more fields if there is enough interest.
I obtained data for individual census tracts, which typically contain several thousand residents.
census_block_loc.csv
For this file, I used an online FCC census block lookup tool to retrieve the census block code for a 200 x 200 grid containing New York City and a bit of the surrounding area. This file contains the coordinates and associated census block codes along
with the state and county names to make things a bit more readable to users.
Each census tract is split into a number of blocks, so one must extract the census tract code from the block code.
Acknowledgements
The data here was taken from the American Community Survey 2015 5-year estimates (https://factfinder.census.gov/faces/nav/jsf/pages/index.xhtml).
The census block coordinate data was taken from the FCC Census Block Conversions API (https://www.fcc.gov/general/census-block-conversions-api)
As public data from the US government, this is not subject to copyright within the US and should be considered public domain."
New York City Crimes,2014-2015 Crimes reported in all 5 boroughs of New York City,def love(x):,12,"Version 1,2017-08-11",crime,CSV,253 MB,CC0,"4,033 views",687 downloads,2 kernels,0 topics,https://www.kaggle.com/adamschroeder/crimes-new-york-city,"Context
With this dataset I hope to raise awareness on the trends in crime.
Content
For NYPD Complaint Data, each row represents a crime. For information on the columns, please see the attached csv, ""Crime_Column_Description"". Reported crime go back 5 years but I only attached reported crime from 2014-2015 due to file size. The full report can be found at NYC Open Data (https://data.cityofnewyork.us/Public-Safety/NYPD-Complaint-Data-Historic/qgea-i56i)
Acknowledgements
I would like to thank NYC Open Data for the dataset.
Inspiration
Additional things I would like to better understand: 1. Differences in crime that exist between the 5 boroughs 2. A mapping of the crimes per borough 3. Where do the most dangerous crimes happen and what time?"
Top 100 Chess Players Historical,The Top 100 ranked players in Chess between July 2000 and June 2017,Kelvin Wellington,12,"Version 2,2017-06-28|Version 1,2017-06-17",board games,CSV,567 KB,Other,"2,616 views",212 downloads,5 kernels,,https://www.kaggle.com/odartey/top-chess-players,"Context
Rankings are a constant phenomenon in society, with a persistent interest in the stratification of items in a set across various disciplines. In sports, rankings are a direct representation of the performance of a team or player over a certain period. Given the straightforward nature of rankings in sports (points based system) there is the opportunity to statistically explore rankings of sports disciplines.
Content
The dataset comprises monthly rankings data of the Top 100 Chess players between July 2000 and June 2017 . The data is housed in a single csv file.
Acknowledgements
Data was sourced from the official site of the World Chess Federation: fide.com
Inspiration
This dataset could be of use to anyone interested in the distribution of rankings in competitive events."
State of the Union Corpus (1989 - 2017),Full text of the State of the Union address between 1989 and 2017,Rachael Tatman,12,"Version 1,2017-07-21","presidents
politics
linguistics",Other,995 KB,CC4,"2,044 views","1,232 downloads",4 kernels,0 topics,https://www.kaggle.com/rtatman/state-of-the-union-corpus-1989-2017,"Context:
The State of the Union is an annual address by the President of the United States before a joint session of congress. In it, the President reviews the previous year and lays out his legislative agenda for the coming year.
Content:
This dataset contains the full text of the State of the Union address from 1989 (Regan) to 2017 (Trump).
Inspiration:
This is a nice, clean set of texts perfect for exploring Natural Language Processing techniques
Topic modelling: Which topics have become more popular over time? Which have become less popular?
Sentiment analysis: Are there differences in tone between different Presidents? Presidents from different parties?
Parsing: Can you train implement a parser to automatically extract the syntactic relationships between words?
Authorship identification: Can you correctly identify the author of a previously unseen address?"
Color terms dataset,Literal & figurative use of color terms and the colors of objects,Rachael Tatman,12,"Version 1,2017-07-26","visual arts
linguistics
artificial intelligence",CSV,5 KB,CC4,"2,011 views",173 downloads,,0 topics,https://www.kaggle.com/rtatman/color-terms-dataset,"Context:
Color terms are interesting in natural language processing because it’s an area where it’s possible to link distributional semantics (models of word meanings based on which words are used together in texts) to things in the world. This dataset was created to help link semantic models to images.
Content:
This dataset is made up of two smaller files, but were both presented and discussed in the same paper (see Acknowledgements). All data in this dataset is in English.
Concrete color terms
This dataset contains a list of common items manually labeled with one of the 11 colors from the set: black, blue, brown, green, grey, orange, pink, purple, red, white, yellow.
Literal vs. nonliteral colors
This dataset is made up of color adjective-noun phrases, randomly drawn from the most frequent 8K nouns and 4K adjectives in the concatenated ukWaC, Wackypedia, and BNC corpora. These were tagged by consensus by two human judges as literal (white towel, black feather) or nonliteral (white wine, white musician, green future). Some phrases had both literal and nonliteral uses, such as blue book in “book that is blue” vs. “automobile price guide”. In these cases, only the most common sense (according to the judges) was taken into account for the present experiment. The dataset consists of 370 phrases.
Acknowledgements:
If you use these datasets, please cite:
Bruni, E., G. Boleda, M. Baroni, N. K. Tran. 2012. Distributional semantics in technicolor. Proceedings of ACL 2012, pages 136-145, Jeju Island, Korea.
Inspiration:
Are some colors used more often in a literal sense?
Is there a relationship between how many objects are a given color and how often that color is used in a literal sense?
Can you use the color of concrete and an image database of those objects to create an automatic color labeller?"
120 Million Word Spanish Corpus,The Spanish Language portion of the Wikicorpus (v 1.0),Rachael Tatman,12,"Version 1,2017-08-09","languages
europe
south america
linguistics",Other,646 MB,CC3,"1,502 views",127 downloads,,0 topics,https://www.kaggle.com/rtatman/120-million-word-spanish-corpus,"Context:
Spanish is the second most widely-spoken language on Earth; over one in 20 humans alive today is a native speaker of Spanish. This medium-sized corpus contains 120 million words of modern Spanish taken from the Spanish-Language Wikipedia in 2010.
Content:
This dataset is made up of 57 text files. Each contains multiple Wikipedia articles in an XML format. The text of each article is surrounded by tags. The initial tag also contains metadata about the article, including the article’s id and the title of the article. The text “ENDOFARTICLE.” appears at the end of each article, before the closing tag.
Acknowledgements:
This dataset was collected by Samuel Reese, Gemma Boleda, Montse Cuadros, Lluís Padró and German Rigau. If you use it in your work, please cite the following paper:
Samuel Reese, Gemma Boleda, Montse Cuadros, Lluís Padró, German Rigau. Wikicorpus: A Word-Sense Disambiguated Multilingual Wikipedia Corpus. In Proceedings of 7th Language Resources and Evaluation Conference (LREC'10), La Valleta, Malta. May, 2010.
Inspiration:
Can you create a stop-word list for Spanish based on this corpus? How does it compare to the one in this dataset?
Can you build a topic model to cluster together articles on similar topics?
You may also like:
Brazilian Portuguese Literature Corpus: 3.7 million word corpus of Brazilian literature published between 1840-1908
Colonia Corpus of Historical Portuguese: A 5.1 million word corpus of historical Portuguese
The National University of Singapore SMS Corpus: A corpus of more than 67,000 SMS messages in Singapore English & Mandarin"
Deep Sea Corals,Coral Records from NOAA’s Deep-Sea Coral Research and Technology Program,NOAA,12,"Version 1,2017-08-29","science and culture
fishing
oceans
+ 2 more...",CSV,139 MB,CC0,"1,944 views",158 downloads,,,https://www.kaggle.com/noaa/deep-sea-corals,"Context
This dataset contains information about deep sea corals and sponges collected by NOAA and NOAA’s partners. Amongst the data are geo locations of deep sea corals and sponges and the whole thing is tailored to the occurrences of azooxanthellates - a subset of all corals and all sponge species (i.e. they don't have symbiotic relationships with certain microbes). Additionally, these records only consists of observations deeper than 50 meters to truly focus on the deep sea corals and sponges.
Content:
Column descriptions:
CatalogNumber: Unique record identifier assigned by the Deep-Sea Coral Research and Technology Program.
DataProvider: The institution, publication, or individual who ultimately deserves credit for acquiring or aggregating the data and making it available.
ScientificName: Taxonomic identification of the sample as a Latin binomial.
VernacularNameCategory: Common (vernacular) name category of the organism.
TaxonRank: Identifies the level in the taxonomic hierarchy of the ScientificName term.
ObservationDate: Time as hh:mm:ss when the sample/observation occurred (UTC).
Latitude (degrees North): Latitude in decimal degrees where the sample or observation was collected.
Longitude (degrees East): Longitude in decimal degrees where the sample or observation was collected.
DepthInMeters: Best single depth value for sample as a positive value in meters.
DepthMethod: Method by which best singular depth in meters (DepthInMeters) was determined. ""Averaged"" when start and stop depths were averaged. ""Assigned"" when depth was derived from bathymetry at the location. ""Reported"" when depth was reported based on instrumentation or described in literature.
Locality: A specific named place or named feature of origin for the specimen or observation (e.g., Dixon Entrance, Diaphus Bank, or Sur Ridge). Multiple locality names can be separated by a semicolon, arranged in a list from largest to smallest area (e.g., Gulf of Mexico; West Florida Shelf, Pulley Ridge).
IdentificationQualifier: Taxonomic identification method and level of expertise. Examples: “genetic ID”; “morphological ID from sample by taxonomic expert”; “ID by expert from image”; “ID by non-expert from video”; etc.
SamplingEquipment: Method of data collection. Examples: ROV, submersible, towed camera, SCUBA, etc.
RecordType: Denotes the origin and type of record. published literature (""literature""); a collected specimen (""specimen""); observation from a still image (""still image""); observation from video (""video observation""); notation without a specimen or image (""notation""); or observation from trawl surveys, longline surveys, and/or observer records (""catch record"").
Acknowledgements
Big shout out to NOAA and it's partners. Thank you for being scientists! The original and probably more up-to-date dataset can be found here: https://deepseacoraldata.noaa.gov/website/AGSViewers/DeepSeaCorals/mapSites.htm
This dataset hasn't been changed in anyway.
NOAA (2015) National Database for Deep-Sea Corals and Sponges (version 20170324-0). https://deepseacoraldata.noaa.gov/; NOAA Deep Sea Coral Research & Technology Program.
Inspiration
Who doesn't love coral and sponges?! I challenge you to find the best algorithm that successfully SAVES the world's corals 100% of the time!"
"Vehicle Fuel Economy Estimates, 1984-2017",Which makes and models have the highest city and highway MPG?,US Environmental Protection Agency,12,"Version 1,2017-01-31",vehicles,CSV,11 MB,CC0,"4,716 views",664 downloads,9 kernels,0 topics,https://www.kaggle.com/epa/fuel-economy,"Content
The purpose of EPA’s fuel economy estimates is to provide a reliable basis for comparing vehicles. Most vehicles in the database (other than plug-in hybrids) have three fuel economy estimates: a “city” estimate that represents urban driving, in which a vehicle is started in the morning (after being parked all night) and driven in stop-and-go traffic; a “highway” estimate that represents a mixture of rural and interstate highway driving in a warmed-up vehicle, typical of longer trips in free-flowing traffic; and a “combined” estimate that represents a combination of city driving (55%) and highway driving (45%). Estimates for all vehicles are based on laboratory testing under standardized conditions to allow for fair comparisons.
The database provides annual fuel cost estimates, rounded to the nearest $50, for each vehicle. The estimates are based on the assumptions that you travel 15,000 miles per year (55% under city driving conditions and 45% under highway conditions) and that fuel costs $2.33/gallon for regular unleaded gasoline, $2.58/gallon for mid-grade unleaded gasoline, and $2.82/gallon for premium.
EPA’s fuel economy values are good estimates of the fuel economy a typical driver will achieve under average driving conditions and provide a good basis to compare one vehicle to another. However, your fuel economy may be slightly higher or lower than EPA’s estimates. Fuel economy varies, sometimes significantly, based on driving conditions, driving style, and other factors.
Acknowledgements
Fuel economy data are produced during vehicle testing at the Environmental Protection Agency's National Vehicle and Fuel Emissions Laboratory in Ann Arbor, Michigan, and by vehicle manufacturers with EPA oversight."
Human Rights Project: Country Profiles by Year,Human rights data from US State Department and Amnesty International,University of Connecticut,12,"Version 1,2017-01-31","crime
war
international relations",CSV,526 KB,Other,"2,139 views",237 downloads,4 kernels,0 topics,https://www.kaggle.com/uconn/human-rights,"Content
The Cingranelli-Richards human rights database contains quantitative information on government recognition of 15 internationally recognized human rights in more than 200 countries from 1981-2011. It includes measures of the practices of governments that allow or impede citizens who wish to exercise their physical integrity rights like the rights not to be tortured, summarily executed, disappeared, or imprisoned for political beliefs; civil liberties such as free speech, freedom of association and assembly, freedom of movement, freedom of religion, and the right to participate in the selection of government leaders; employment rights; and rights of women to equal treatment politically, economically, and socially. The database is designed for use by scholars and students who seek to test theories about the causes and consequences of human rights violations, as well as policy makers and analysts who seek to estimate the human rights effects of a wide variety of institutional changes and public policies including democratization, economic aid, military aid, structural adjustment, and humanitarian intervention.
The primary source of information about human rights practices is obtained from the annual United States Department of State’s Country Reports on Human Rights Practices. Coders are instructed to use this source for all variables. For a group of four rights known as ""Physical Integrity Rights"" (the rights to freedom from extrajudicial killing, disappearance, torture, and political imprisonment), coders use Amnesty International’s Annual Report in addition the Department of State reports. If discrepancies exist between the two sources, coders are instructed to treat the Amnesty International report as authoritative; some scholars believe that this step is necessary to remove a potential bias in favor of American allies.
Acknowledgements
The human rights database was developed, updated, and published by Professor David Cingranelli of Binghamton University, SUNY, Professor David Richards of the University of Connecticut's Human Rights Institute, and Professor K. Chad Clay of the University of Georgia."
"Terrorism in America, 2001-Present",Terrorist activity in the United States and by Americans overseas since 9/11,New America,12,"Version 1,2017-02-01","history
crime",CSV,82 KB,CC0,"3,490 views",519 downloads,9 kernels,,https://www.kaggle.com/newamerica/terrorist-activity,"Content
The data in this report consists of individuals accused of terrorism and related crimes since September 11, 2001, who are either American citizens or who engaged in terrorist activity within the United States. The data includes some individuals who died before being charged with a crime, but were widely reported to have engaged in terrorist activity.
Acknowledgements
This report was produced by the International Security Program at New America."
Insect Light Trap,The University of Copenhagen’s Zoological Museum zapped insects for 18 years,University of Copenhagen,12,"Version 1,2017-07-12","biology
environment",CSV,3 MB,CC0,"1,451 views",217 downloads,3 kernels,0 topics,https://www.kaggle.com/University-of-Copenhagen/insect-light-trap,"Context
The University of Copenhagen’s Zoological Museum placed a light trap on their roof and for 18 years they documented the types of insects being caught. The data was collected as part of a study to determine insect responses to recent climate change.
Content
This file contains the raw data from the light trapping study ordered by: Order (Lepidoptera/Coleoptera), family, name (species), year, date1 (start), date2 (end) and number of individuals
Acknowledgements
Original publication: Thomsen PF, Jørgensen PS, Bruun HH, Pedersen J, Riis-Nielsen T, Jonko K, Słowińska I, Rahbek C, Karsholt O (2016) Resource specialists lead local insect community turnover associated with temperature – analysis of an 18-year full-seasonal record of moths and beetles. Journal of Animal Ecology 85(1): 251–261. http://dx.doi.org/10.1111/1365-2656.12452
The original dataset can be found at http://datadryad.org/resource/doi:10.5061/dryad.s4945/1
Inspiration
Climate change is on everyone's mind for one reason or another and insects are susceptible to climate change just like humans. Using these data, can you determine which species have become more or less prevalent over the 18 years of collection?"
School fires in Sweden 1998-2014,"Cases reported by municipality and year. Also, KPIs for each municipality.",brontosaur,12,"Version 6,2016-08-31|Version 5,2016-08-29|Version 4,2016-08-25|Version 3,2016-08-22|Version 2,2016-08-22|Version 1,2016-08-22",firefighting,CSV,2 GB,CC0,"7,149 views",574 downloads,26 kernels,4 topics,https://www.kaggle.com/mikaelhuss/swedish-school-fires,"Sweden has a surprisingly large number of school fires for a small country (< 10M inhabitants), and many of these fires are due to arson. For instance, according to the Division of Fire Safety Engineering at Lund University, ""Almost every day between one and two school fires occur in Sweden. In most cases arson is the cause of the fire."" The associated costs can be up to a billion SEK (around 120 million USD) per year.
It is hard to say why these fires are so common in Sweden compared to other countries, and this dataset doesn't address that question - but could it be possible, within a Swedish context, to find out which properties and indicators of Swedish towns (municipalities, to be exact) might be related to a high frequency of school fires?
I have collected data on school fire cases in Sweden between 1998 and 2014 through a web site with official statistics from the Swedish Civil Contingencies Agency (https://ida.msb.se/ida2#page=a0087). At least at the time when I collected the data, there was no API to allow easy access to schools fire data, so I had to collect them using a quasi-manual process, downloading XLSX report generated from the database year by year, after which I joined these with an R script into a single table of school fire cases where the suspected reason was arson. (Full details on the data acquisition process are available.)
The number of such cases is reported for each municipality (of which there are currently 290) and year (i e each row is a unique municipality/year combination). The population at the time is also reported.
As a complement to these data, I provide a list of municipal KPI:s (key performance indicators) from 1998 to 2014. There are thousands of these KPI:s, and it would be a futile task for me to try to translate the descriptions from Swedish to English, although I might take a stab at translating a small subset of them at some point. These KPIs were extracted from Kolada (a database of Swedish municipality and county council statistics) by repeatedly querying its API (https://github.com/Hypergene/kolada).
I'd be very interested to hear if anyone finds some interesting correlations between schools fire cases and municipality indicators!"
Pretrained PyTorch models,This dataset helps to use pretrained PyTorch models in Kernels.,Pedro Lima,12,"Version 7,2017-10-13|Version 6,2017-10-13|Version 5,2017-10-07|Version 4,2017-10-07|Version 3,2017-10-07|Version 2,2017-10-07|Version 1,2017-10-07",artificial intelligence,Other,366 MB,CC4,"1,214 views",72 downloads,2 kernels,0 topics,https://www.kaggle.com/pvlima/pretrained-pytorch-models,"Context
Experiment to apply same strategy from Beluga's Keras dataset with PyTorch models. This dataset has the weights for several models included in PyTorch. To use these weights they need to be copied when the kernel runs, like in this example.
Content
PyTorch models included:
DenseNet-161
Inception-V3
ResNet18
ResNet50
SqueezeNet 1.0
SqueezeNet 1.1
Acknowledgements
Beluga's Keras dataset
PyTorch"
Cuneiform Digital Library Initiative,Explore thousands of ancient tablet transliterations,Myles O'Neill,12,"Version 1,2017-05-10","languages
history
linguistics",CSV,192 MB,CC4,"1,574 views",97 downloads,4 kernels,,https://www.kaggle.com/mylesoneill/cuneiform-digital-library-initiative,"What is CDLI?
The Cuneiform Digital Library Initiative (CDLI) is an international digital library project aimed at putting text and images of an estimated 500,000 recovered cuneiform tablets created from between roughly 3350 BC and the end of the pre-Christian era online. The initiative is a joint project of the University of California, Los Angeles, the University of Oxford, and the Max Planck Institute for the History of Science, Berlin.
This dataset includes the full CDLI catalogue (metadata), transliterations of tablets in the catalogue, and word/sign lists from old akkadian and Ur III. This data was downloaded on the 9th of May 2017.
Transliterations are in .atf format, find out more about this format here: http://oracc.museum.upenn.edu/doc/help/editinginatf/cdliatf/index.html
Find more about CDLI here: http://cdli.ucla.edu/
What is Cuneiform?
Cuneiform script, one of the earliest systems of writing, was invented by the Sumerians. It is distinguished by its wedge-shaped marks on clay tablets, made by means of a blunt reed for a stylus. The name cuneiform itself simply means ""wedge shaped"".
Cuneiform is not a language, nor is it an alphabet. Cuneiform uses between 600-1000 characters to write words or syllables. It has been used by many different cultural groups to represent many different languages, but it was primarily used to write Sumerian and Akkadian. Deciphering cuneiform is very difficult to this day, though the difficulty varies depending on the language.
https://en.wikipedia.org/wiki/Cuneiform_script
What is Assyriology?
Assyriology is the study of the languages, history, and culture of the people who used the ancient writing system called cuneiform. Cuneiform was used primarily in an area called the Near East, centred on Mesopotamia (modern Iraq and eastern Syria) where cuneiform was invented, but including the Northern Levant (Western Syria and Lebanon), parts of Anatolia, and western Iran. The sources for Assyriology are all archaeological, and include both inscribed and uninscribed objects. Most Assyriologists focus on the rich textual record from the ancient Near East, and specialise in either the study of language, literature, or history of the ancient Near East.
Assyriology began as an academic discipline with the recovery of the monuments of ancient Assyria, and the decipherment of cuneiform, in the middle of the 19th century. Large numbers of archaeological objects, including texts, were brought to museums in Europe and later the US, following the early excavations of Nineveh, Kalhu, Babylon, Girsu, Assur and so forth. Today Assyriology is studied in universities across the globe, both as an undergraduate and a graduate subject, and knowledge from the ancient Near East informs students of numerous other disciplines such as the History of Science, Archaeology, Classics, Biblical studies and more."
Good Morning Tweets,Tweets captured over ~24 hours with the text 'good morning' in them,Rob Harrand,12,"Version 1,2016-12-09","linguistics
sociology
internet",CSV,3 MB,Other,"3,740 views",373 downloads,11 kernels,0 topics,https://www.kaggle.com/tentotheminus9/good-morning-tweets,"Context
It's possible, using R (and no doubt Python), to 'listen' to Twitter and capture tweets that match a certain description. I decided to test this out by grabbing tweets with the text 'good morning' in them over a 24 hours period, to see if you could see the world waking up from the location information and time-stamp. The main R package used was streamR
Content
The tweets have been tidied up quite a bit. First, I've removed re-tweets, second, I've removed duplicates (not sure why Twitter gave me them in the first place), third, I've made sure the tweet contained the words 'good morning' (some tweets were returned that didn't have the text in for some reason) and fourth, I've removed all the tweets that didn't have a longitude and latitude included. This latter step removed the vast majority. What's left are various aspects of just under 5000 tweets. The columns are,
text
retweet_count
favorited
truncated
id_str
in_reply_to_screen_name
source
retweeted
created_at
in_reply_to_status_id_str
in_reply_to_user_id_str
lang
listed_count
verified
location
user_id_str
description
geo_enabled
user_created_at
statuses_count
followers_count
favourites_count
protected
user_url
name
time_zone
user_lang
utc_offset
friends_count
screen_name
country_code
country
place_type
full_name
place_name
place_id
place_lat
place_lon
lat
lon
expanded_url
url
Acknowledgements
I used a few blog posts to get the code up and running, including this one
Code
The R code I used to get the tweets is as follows (note, I haven't includes the code to set up the connection to Twitter. See the streamR PFD and the link above for that. You need a Twitter account),
i = 1

while (i <= 280) {

filterStream(""tw_gm.json"", timeout = 300, oauth = my_oauth, track = 'good morning', language = 'en')
tweets_gm = parseTweets(""tw_gm.json"")

ex = grepl('RT', tweets_gm$text, ignore.case = FALSE) #Remove the RTs
tweets_gm = tweets_gm[!ex,]

ex = grepl('good morning', tweets_gm$text, ignore.case = TRUE) #Remove anything without good morning in the main tweet text
tweets_gm = tweets_gm[ex,]

ex = is.na(tweets_gm$place_lat) #Remove any with missing place_latitude information
tweets_gm = tweets_gm[!ex,]

tweets.all = rbind(tweets.all, tweets_gm) #Add to the collection

i=i+1

Sys.sleep(5)

}"
Keystroke Dynamics,Typing patterns for keystroke authentication,Kumar Nityan Suman,12,"Version 1,2017-03-11",computer security,Other,4 MB,CC0,"4,277 views",732 downloads,10 kernels,,https://www.kaggle.com/knityansuman/keystroke-dynamics,"Context
Keystroke dynamics is the study of whether people can be distinguished by their typing rhythms, much like handwriting is used to identify the author of a written text. Possible applications include acting as an electronic fingerprint, or in an access-control mechanism. A digital fingerprint would tie a person to a computer-based crime in the same manner that a physical fingerprint ties a person to the scene of a physical crime. Access control could incorporate keystroke dynamics both by requiring a legitimate user to type a password with the correct rhythm, and by continually authenticating that user while they type on the keyboard.
Content
The data are arranged as a table with 34 columns. Each row of data corresponds to the timing information for a single repetition of the password by a single subject. The first column, subject, is a unique identifier for each subject (e.g., s002 or s057). Even though the data set contains 51 subjects, the identifiers do not range from s001 to s051; subjects have been assigned unique IDs across a range of keystroke experiments, and not every subject participated in every experiment. For instance, Subject 1 did not perform the password typing task and so s001 does not appear in the data set. The second column, sessionIndex, is the session in which the password was typed (ranging from 1 to 8). The third column, rep, is the repetition of the password within the session (ranging from 1 to 50).
The remaining 31 columns present the timing information for the password. The name of the column encodes the type of timing information. Column names of the form H.key designate a hold time for the named key (i.e., the time from when key was pressed to when it was released). Column names of the form DD.key1.key2 designate a key down-key down time for the named digraph (i.e., the time from when key1 was pressed to when key2 was pressed). Column names of the form UD.key1.key2 designate a key up-key down time for the named digraph (i.e., the time from when key1 was released to when key2 was pressed). Note that UD times can be negative, and that H times and UD times add up to DD times.
Consider the following one-line example of what you will see in the data:
subject sessionIndex rep H.period DD.period.t UD.period.t ... s002 1 1 0.1491 0.3979 0.2488 ...
The example presents typing data for subject 2, session 1, repetition 1. The period key was held down for 0.1491 seconds (149.1 milliseconds); the time between pressing the period key and the t key (key down-key down time) was 0.3979 seconds; the time between releasing the period and pressing the t key (key up-key down time) was 0.2488 seconds; and so on
Acknowledgements
Kevin S. Killourhy and Roy A. Maxion
Inspiration
To make measurable progress in the field of keystroke dynamics, i shared data. The anomaly-detection task was to discriminate between the typing of a genuine user trying to gain legitimate access to his or her account, and the typing of an impostor trying to gain access illegitimately to that same account. Our intent with this is to share our resources—the typing data, with the research community, and to answer questions that they (or you) might have.
For starters: 1. The typing patterns of different users. 2. The changing typing styles of a user over different attempts. 3. The difference in typing of left-side keys and right-side keys on the keyboard.
and so on . . .
*TEMPORARY NOTE* Some people are having problem with the main download button, please try downloading from the bottom of the page rather than the main button if issues observed.
Happy Machine Learning!"
"ATP Matches, 1968 to 2017",Details of the ATP matches since 1968,Sijo VM,12,"Version 1,2017-03-30","tennis
sports",CSV,31 MB,Other,"4,062 views",801 downloads,11 kernels,,https://www.kaggle.com/sijovm/atpdata,"The data set contains the details about all the ATP matches played since 1968. The data set has a lot of missing values, especially for the period between 1968 - 1991.
Thanks to Xiaming Chen for making the data available to the online community.
Primarily, I would like to understand how tennis matches/players have evolved over time and any other insights."
SP1 factor binding sites on Chromosome1,SP1 factor binding and non-binding sites on Ch1 for classification tasks,Hossein Banki Koshki,12,"Version 2,2016-11-12|Version 1,2016-11-11",human genetics,CSV,211 KB,Other,"2,896 views",267 downloads,6 kernels,,https://www.kaggle.com/hobako1993/sp1-factor-binding-sites-on-chromosome1,"This dataset includes SP1 transcription factor binding and non-binding sites on human chromosome1. It can be used for binary classification tasks in bioinformatics. There are 1200 sequences for binding sites (BS) and 1200 sequences for non-biding sites (nBS) We have labeled sequences with 1 for BS and 0 for nBS. Each sequence is 14 nucleobase length, which is converted to numeric string using codes below, assigned to each nucleobase 00 for A 01 for T 10 for C 11 for G"
Haberman's Survival Data Set,Survival of patients who had undergone surgery for breast cancer,GilSousa,12,"Version 1,2016-12-01",,CSV,3 KB,Other,"4,927 views",540 downloads,16 kernels,0 topics,https://www.kaggle.com/gilsousa/habermans-survival-data-set,The dataset contains cases from a study that was conducted between 1958 and 1970 at the University of Chicago's Billings Hospital on the survival of patients who had undergone surgery for breast cancer.
Software Architectural Styles,The pattern analysis of software development by statistical/datamining methods,QadeemKhan,12,"Version 2,2017-03-27|Version 1,2016-12-14","computer architecture
programming",CSV,179 KB,CC0,"3,766 views",242 downloads,3 kernels,,https://www.kaggle.com/qadeemkhan/dataset-of-software-architectural-styles,"Context
Software systems are composed of one or more software architectural styles. These styles define the usage patterns of a programmer in order to develop a complex project. These architectural styles are required to analyze for pattern similarity in the structure of multiple groups of projects. The researcher can apply different types of data mining algorithms to analyze the software projects through architectural styles used. The dataset is obtained from an online questionnaire delivered to the world 's best academic and software industry.
Content
The content of this dataset are multiple architectural styles utilized by the system. He attributes are Repository, Client Server, Abstract Machine,Object Oriented,Function Oriented,Event Driven,Layered, Pipes & Filters, Data centeric, Blackboard, Rule Based, Publish Subscribe, Asynchronous Messaging, Plug-ins, Microkernel, Peer-to-Peer, Domain Driven, Shared Nothing.
Acknowledgements
Thanks to my honorable teacher Prof.Dr Usman Qamar for guiding me to accomplish this wonderful task.
Inspiration
The dataset is capable of updating and refinements.Any researcher ,who want to contribute ,plz feel free to ask."
"State Energy System Data, 1960-2014",State Energy Data Systems (SEDS) data for all US states including DC,Nathan,12,"Version 3,2017-03-14|Version 2,2017-03-13|Version 1,2017-03-12",energy,CSV,26 MB,CC0,"2,969 views",238 downloads,4 kernels,,https://www.kaggle.com/nathanto/seds-1960-2014F,"State Energy Data Systems (SEDS) data for all US states, including DC, from 1960 to 2014F
Context
This dataset is derived from my general interest in energy systems. It was originally composed for this exercise, as part of this Coursera/John Hopkins Data Science Specilisation.
The code that produced this dataset is in https://www.kaggle.com/nathanto/d/nathanto/seds-1960-2014F/data-wrangling-code-for-seds-1960-2014f
Content
The data is a composition of the State Energy Data Systems (SEDS) data for all US states, including DC, from 2016 to 2014F, for data released June 29, 2016. It has been tidied from a wide format to a long format, and includes unit codes for the values associated with the observations for each MSN code for each state for each year.
The ""F"" in the final year number indicates that these are the final observations. There is a lag of some 18 months after year end and final readings.
The columns are:
state - State postal code, composed from the function states.abb and including ""DC"".
msn - A mnemonic series name identifying the value being observed.
year - Year of the observation.
value - Of the observation.
units_code, representing the units of the value, e.g. BBtu is Billion British Thermal Units.
Note that the units_codes are mostly my own invention, based on the EIA Writng Style Guide.
Acknowledgements
Thank you to the US Energy Information Administration for making the data available.
Special thanks to Yvonne Taylor for guidance on style for the codes.
Inspiration
The first goal for this data was to support some plotting and forecast testing exercises, which is a work in progress. To what extent do past observations predict future observations? Since the data is readily available, and consistent, within limits, over a long period, this format is a good basis for experimenting with techniques in that space."
Deep Learning A-Z - ANN dataset,"Kirill Eremenko ""Deep Learning A-Z™: Hands-On Artificial Neural Networks"" course",Filippo,12,"Version 1,2017-05-16",artificial intelligence,CSV,669 KB,Other,"7,648 views",556 downloads,20 kernels,,https://www.kaggle.com/filippoo/deep-learning-az-ann,"Context
This is the dataset used in the section ""ANN (Artificial Neural Networks)"" of the Udemy course from Kirill Eremenko (Data Scientist & Forex Systems Expert) and Hadelin de Ponteves (Data Scientist), called Deep Learning A-Z™: Hands-On Artificial Neural Networks. The dataset is very useful for beginners of Machine Learning, and a simple playground where to compare several techniques/skills.
It can be freely downloaded here: https://www.superdatascience.com/deep-learning/
The story: A bank is investigating a very high rate of customer leaving the bank. Here is a 10.000 records dataset to investigate and predict which of the customers are more likely to leave the bank soon.
The story of the story: I'd like to compare several techniques (better if not alone, and with the experience of several Kaggle users) to improve my basic knowledge on Machine Learning.
Content
I will write more later, but the columns names are very self-explaining.
Acknowledgements
Udemy instructors Kirill Eremenko (Data Scientist & Forex Systems Expert) and Hadelin de Ponteves (Data Scientist), and their efforts to provide this dataset to their students.
Inspiration
Which methods score best with this dataset? Which are fastest (or, executable in a decent time)? Which are the basic steps with such a simple dataset, very useful to beginners?"
NBER Macrohistory Database,Western economic history data spanning 1785-1974,Sohier Dane,12,"Version 3,2017-10-12|Version 2,2017-10-12|Version 1,2017-10-12","history
economics",CSV,31 MB,CC0,984 views,90 downloads,2 kernels,0 topics,https://www.kaggle.com/sohier/nber-macrohistory-database,"Context
This data set covers all aspects of the pre-WWI and interwar economies, including production, construction, employment, money, prices, asset market transactions, foreign trade, and government activity. Many series are highly disaggregated, and many exist at the monthly or quarterly frequency. The data set has some coverage of the United Kingdom, France and Germany, although it predominantly covers the United States. For information see:
Improving the Accessibility of the NBER's Historical Data , by Daniel Feenberg and Jeff Miron. (NBER Working Paper #5186). Published in the Journal of Business and Economic Statistics, Volume 15 Number 3 (July 1997) pages 293-299.
Information about seasonal adjustments is available, but in most cases only unadjusted series have been made available here.
Content
The data.csv is organized in a long format with columns for the date, variable, and value. The dates are always the beginning of period date for whatever period existed in the original data. This means that '1920' was converted to January 1st, 1920 while Q2 1920 was converted to April 1, 1920. This is intended as a convenience to make it easier to work with multiple time series from the original mixed frequency data.
The data is currently organized into 16 chapters:
Chapter1: Production of Commodities
Chapter2: Construction
Chapter3: Transportation and Public Utilities
Chapter4: Prices
Chapter5: Stocks of Commodities
Chapter6: Distribution of Commodities
Chapter7: Foreign Trade
Chapter8: Income and Employment
Chapter9: Financial Status of Business
Chapter10: Savings and Investment
Chapter11: Security Markets
Chapter12: Volume of Transactions
Chapter13: Interest Rates
Chapter14: Money and Banking
Chapter15: Government and Finance
Chapter16: Leading Indicators
The dataset has been transformed from its original format. You can find the data preparation code here.
Acknowledgements
This dataset was kindly made available by the National Bureau of Economic Research (NBER). You can find the original dataset here.
Inspiration
Which major historical events can you detect from the data?
With roughly 3,500 time series in the dataset, finding relevant information can be challenging. Can you find a better way of organizing or indexing the data?"
Crypto Currencies,Cryptocurrency Market Capitalizations,Albert Costas,12,"Version 8,2017-12-04|Version 7,2017-11-08|Version 6,2017-10-29|Version 5,2017-10-27|Version 4,2017-10-25|Version 3,2017-10-23|Version 2,2017-10-16|Version 1,2017-10-13","finance
money",CSV,3 MB,ODbL,"4,363 views",285 downloads,2 kernels,0 topics,https://www.kaggle.com/acostasg/crypto-currencies,"«Datasets per la comparació de moviments i patrons entre els principals índexs borsatils espanyols i les crypto-monedes»
Context
En aquest cas el context és detectar o preveure els diferents moviments que es produeixen per una serie factors, tant de moviment interns (compra-venda), com externs (moviments polítics, econòmics, etc...), en els principals índexs borsatils espanyols i de les crypto-monedes.
Hem seleccionat diferents fonts de dades per generar fitxers «csv», guardar diferents valors en el mateix període de temps. És important destacar que ens interessa més les tendències alcistes o baixes, que podem calcular o recuperar en aquests períodes de temps.
Content
En aquest cas el contingut està format per diferents csv, especialment tenim els fitxers de moviments de cryptomoneda, els quals s’ha generat un fitxer per dia del període de temps estudiat.
Pel que fa als moviments del principals índexs borsatils s’ha generat una carpeta per dia del període, en cada directori un fitxer amb cadascun del noms dels índexs. Degut això s’han comprimit aquests últims abans de publicar-los en el directori de «open data» kaggle.com.
Pel que fa als camps, ens interessà detectar els moviments alcistes i baixistes, o almenys aquelles que tenen un patró similar en les cryptomonedes i els índexs. Els camps especialment destacats són:
• Nom: Nom empresa o cryptomoneda;
• Preu: Valor en euros d’una acció o una cryptomoneda;
• Volum: En euros/volum 24 hores,acumulat de les transaccions diàries en milions d’euros
• Simbol: Símbol o acrònim de la moneda
• Cap de mercat: Valor total de totes les monedes en el moment actual
• Oferta circulant: Valor en oportunitat de negoci
• % 1h, % 2h i %7d, tant per cent del valor la moneda en 1h, 2h o 7d sobre la resta de cyprtomonedes.
Acknowledgements
En aquest cas les fonts de dades que s’han utilitzat per a la realització dels datasets corresponent a:
http://www.eleconomista.es
https://coinmarketcap.com
Per aquest fet, les dades de borsa i crypto-moneda estan en última instància sota llicència de les webs respectivament. Pel que fa a la terminologia financera podem veure vocabulari en renta4banco.
[https://www.r4.com/que-necesitas/formacion/diccionario]
Inspiration
Hi ha un estudi anterior on poder tenir primícies de com han enfocat els algoritmes:
https://arxiv.org/pdf/1410.1231v1.pdf
En aquest cas el «trading» en cryptomoneda és relativament nou, força popular per la seva formulació com a mitja digital d’intercanvi, utilitzant un protocol que garanteix la seguretat, integritat i equilibri del seu estat de compte per mitjà d’un entramat d’agents.
La comunitat podrà respondre, entre altres preguntes, a:
Està afectant o hi ha patrons comuns en les cotitzacions de cryptomonedes i el mercat de valors principals del país d'Espanya?
Els efectes o agents externs afecten per igual a les accions o cryptomonedes?
Hi ha relacions cause efecte entre les acciones i cryptomonedes?
Project repository
https://github.com/acostasg/scraping
Datasets
Els fitxers csv generats que componen el dataset s’han publicat en el repositori kaggle.com:
https://www.kaggle.com/acostasg/stock-index/
https://www.kaggle.com/acostasg/crypto-currencies
Per una banda, els fitxers els «stock-index» estan comprimits per carpetes amb la data d’extracció i cada fitxer amb el nom dels índexs borsatil. De forma diferent, les cryptomonedes aquestes estan dividides per fitxer on són totes les monedes amb la data d’extracció."
Political Social Media Posts,"Classify partisan bias, audience, and goal based on politicians' social media",Crowdflower,12,"Version 1,2016-11-21","politics
internet",CSV,4 MB,CC0,"4,935 views",504 downloads,5 kernels,0 topics,https://www.kaggle.com/crowdflower/political-social-media-posts,"This dataset, from Crowdflower's Data For Everyone Library, provides text of 5000 messages from politicians' social media accounts, along with human judgments about the purpose, partisanship, and audience of the messages.
How was it collected?
Contributors looked at thousands of social media messages from US Senators and other American politicians to classify their content. Messages were broken down into audience (national or the tweeter’s constituency), bias (neutral/bipartisan, or biased/partisan), and finally tagged as the actual substance of the message itself (options ranged from informational, announcement of a media appearance, an attack on another candidate, etc.)
Acknowledgments
Data was provided by the Data For Everyone Library on Crowdflower.
Our Data for Everyone library is a collection of our favorite open data jobs that have come through our platform. They're available free of charge for the community, forever.
Inspiration
Here are a couple of questions you can explore with this dataset:
what words predict partisan v. neutral messages?
what words predict support messages v. attack messages?
do politicians use Twitter and Facebook for different purposes? (e.g., Twitter for attack messages, Facebook for policy messages)?
The Data
The dataset contains one file, with the following fields:
_unit_id: a unique id for the message
_golden: always FALSE; (presumably whether the message was in Crowdflower's gold standard)
_unit_state: always ""finalized""
_trusted_judgments: the number of trusted human judgments that were entered for this message; an integer between 1 and 3
_last_judgment_at: when the final judgment was collected
audience: one of national or constituency
audience:confidence: a measure of confidence in the audience judgment; a float between 0.5 and 1
bias: one of neutral or partisan
bias:confidence: a measure of confidence in the bias judgment; a float between 0.5 and 1
message: the aim of the message. one of: -- attack: the message attacks another politician
-- constituency: the message discusses the politician's constituency
-- information: an informational message about news in government or the wider U.S.
-- media: a message about interaction with the media
-- mobilization: a message intended to mobilize supporters
-- other: a catch-all category for messages that don't fit into the other
-- personal: a personal message, usually expressing sympathy, support or condolences, or other personal opinions
-- policy: a message about political policy
-- support: a message of political support
message:confidence: a measure of confidence in the message judgment; a float between 0.5 and 1
orig__golden: always empty; presumably whether some portion of the message was in the gold standard
audience_gold: always empty; presumably whether the audience response was in the gold standard
bias_gold: always empty; presumably whether the bias response was in the gold standard
bioid: a unique id for the politician
embed: HTML code to embed this message
id: unique id for the message WITHIN whichever social media site it was pulled from
label: a string of the form ""From: firstname lastname (position from state)""
message_gold: always blank; presumably whether the message response was in the gold standard
source: where the message was posted; one of ""facebook"" or ""twitter""
text: the text of the message"
"Landslides After Rainfall, 2007-2016",Location and cause of landslide events around the world,NASA,12,"Version 1,2017-01-19","mountains
geology
climate",CSV,431 KB,CC0,"3,535 views",366 downloads,4 kernels,2 topics,https://www.kaggle.com/nasa/landslide-events,"Context
Landslides are one of the most pervasive hazards in the world, causing more than 11,500 fatalities in 70 countries since 2007. Saturating the soil on vulnerable slopes, intense and prolonged rainfall is the most frequent landslide trigger.
Content
The Global Landslide Catalog (GLC) was developed with the goal of identifying rainfall-triggered landslide events around the world, regardless of size, impacts, or location. The GLC considers all types of mass movements triggered by rainfall, which have been reported in the media, disaster databases, scientific reports, or other sources.
Acknowledgements
The GLC has been compiled since 2007 at NASA Goddard Space Flight Center."
Independent Political Ad Spending (2004-2016),Spending on political ads by independent (non-candidate) groups,Federal Election Commission,12,"Version 1,2016-11-01","finance
politics",CSV,172 MB,CC0,"2,581 views",219 downloads,8 kernels,0 topics,https://www.kaggle.com/fec/independent-political-ad-spending,"What is an Independent Expenditure?
Independent expenditures are what some refer to as ""hard money"" in politics -- spending on ads that specifically mention a candidate (either supporting or opposing). The money for these ads must come from PACs that are independent of the candidate and campaign, and the PACs cannot coordinate with the candidate.
The Federal Election Commission (FEC) collects information on independent expenditures to ensure payers' independence from candidates.
What can we look at?
I'm super interested to see how much spending has increased over the years. The FEC data only goes back to 2004, and it may be the case that the older data is spotty, but I don't doubt that political spending has gone up in the past few years (the 2016 Presidential campaign reportedly involved the most political money since the 1970s).
What does the data look like?
This dataset includes a ton of information from the independent expenditure reports:
committee_id : unique id of the PAC that made the payment
committee_name : name of the PAC that made the payment
report_year : the year the report was file
report_type : one of 24 or 48; whether this is a 24-hour report or a 48-hour report
image_number : unique id of the scanned image of the report
line_number : line number in the report
file_number : unique id of the report
payee_name : who got paid
payee_first_name : if an individual payee, their first name
payee_middle_name : if an individual payee, their middle name
payee_last_name : if an individual payee, their last name
payee_street_1 : payee street address (1 of 2)
payee_street_2 : payee street address (2 of 2)
payee_city : payee city
payee_state : payee state
payee_zip : payee ZIP code
expenditure_description : a string describing the expenditure
expenditure_date : when was this expenditure made?
dissemination_date : when was the advertisement disseminated?
expenditure_amount : how much was spent?
office_total_ytd : how much has this PAC spent on this office, year-to-date?
category_code : category of the expenditure (need to find categories!)
category_code_full : category of the expenditure (need to find categories!)
support_oppose_indicator : one of S or O; whether the ad is in support of or opposition to the candidate
memo_code :
memo_code_full :
candidate_id : unique id of the candidate
candidate_name : name of the candidate
candidate_prefix : title or prefix of the candidate
candidate_first_name : first name of the candidate
candidate_middle_name : middle name of the candidate
candidate_last_name : last name of the candidate
candidate_suffix : suffix of the candidate's name
candidate_office : office that the candidate is running for -- one of P (President), S (Senate), or H (House)
cand_office_state : if House or Senate race, in what state?
cand_office_district : if House or Senate race, in what district?
conduit_committee_id :
conduit_committee_name :
conduit_committee_street1 :
conduit_committee_street2 :
conduit_committee_city :
conduit_committee_state :
conduit_committee_zip :
election_type : one of P (primary) or G (general)
election_type_full : an id comprising the election type and the year, with no delimiter
independent_sign_name :
independent_sign_date :
notary_sign_name :
notary_sign_date :
notary_commission_expiration_date :
back_reference_transaction_id :
back_reference_schedule_name :
filer_first_name :
filer_middle_name :
filer_last_name :
transaction_id : unique id identifying the transaction
original_sub_id :
action_code :
action_code_full :
schedule_type_full :
filing_form :
link_id :
sub_id :
payee_prefix :
payee_suffix :
is_notice :
memo_text :
filer_prefix :
filer_suffix :
schedule_type :
pdf_url : link to the scanned form"
Ae. aegypti and Ae. albopictus occurrences,Ocurrences of the mosquitos that transmit Zika,Dryad Digital Repository,12,"Version 2,2016-10-15|Version 1,2016-10-06","diseases
epidemiology
animals",CSV,3 MB,CC0,"3,474 views",401 downloads,7 kernels,,https://www.kaggle.com/dryad/ae-aegypti-and-ae-albopictus-occurrences,"Aedes aegypti and Ae. albopictus are the main vectors transmitting dengue and chikungunya viruses. Despite being pathogens of global public health importance, knowledge of their vectors’ global distribution remains patchy and sparse.
A global geographic database of known occurrences of Ae. aegypti and Ae. albopictus between 1960 and 2014 was compiled. The database, which comprises occurrence data linked to point or polygon locations, was derived from peer-reviewed literature and unpublished studies including national entomological surveys and expert networks. The authors describe all data collection processes, as well as geo-positioning methods, database management and quality-control procedures in their 2015 paper cited below.
This is the first comprehensive global database of Ae. aegypti and Ae. albopictus occurrence, consisting of 19,930 and 22,137 geo-positioned occurrence records respectively. The dataset can be used for a variety of mapping and spatial analyses of the vectors and, by inference, the diseases they transmit.
Citations
Kraemer MUG, Sinka ME, Duda KA, Mylne A, Shearer FM, Brady OJ, Messina JP, Barker CM, Moore CG, Carvalho RG, Coelho GE, Van Bortel W, Hendrickx G, Schaffner F, Wint GRW, Elyazar IRF, Teng H, Hay SI (2015) The global compendium of Aedes aegypti and Ae. albopictus occurrence. Scientific Data 2(7): 150035. http://dx.doi.org/10.1038/sdata.2015.35
Kraemer MUG, Sinka ME, Duda KA, Mylne A, Shearer FM, Brady OJ, Messina JP, Barker CM, Moore CG, Carvalho RG, Coelho GE, Van Bortel W, Hendrickx G, Schaffner F, Wint GRW, Elyazar IRF, Teng H, Hay SI (2015) Data from: The global compendium of Aedes aegypti and Ae. albopictus occurrence. Dryad Digital Repository. http://dx.doi.org/10.5061/dryad.47v3c"
Pizza Restaurants and the Pizza They Sell,"A list of pizza restaurants, 3,500 pizzas, and their menu prices.",Datafiniti,12,"Version 1,2017-09-30","databases
food and drink
business
internet",CSV,1 MB,CC4,"4,445 views",739 downloads,10 kernels,0 topics,https://www.kaggle.com/datafiniti/pizza-restaurants-and-the-pizza-they-sell,"About this Data
This is a list of over 3,500 pizzas from multiple restaurants provided by Datafiniti's Business Database. The dataset includes the category, name, address, city, state, menu information, price range, and more for each pizza restaurant.
What You Can Do with this Data
You can use this data to discover how much you can expect to pay for pizza across the country. E.g.:
What are the least and most expensive cities for pizza?
What is the number of restaurants serving pizza per capita (100,000 residents) across the U.S.?
What is the median price of a large plain pizza across the U.S.?
Which cities have the most restaurants serving pizza per capita (100,000 residents)?
Data Schema
A full schema for the data is available in our support documentation.
About Datafiniti
Datafiniti provides instant access to web data. We compile data from thousands of websites to create standardized databases of business, product, and property information. Learn more.
Want More?
You can get more data like this by joining Datafiniti or requesting a demo."
Nursing Home Compare,"Comparing the quality of care of over 15,000 nursing homes in the U.S.",Medicare,12,"Version 1,2016-11-28","healthcare
gerontology",CSV,318 MB,Other,"2,771 views",313 downloads,6 kernels,0 topics,https://www.kaggle.com/medicare/nursing-home-compare,"Context
This official dataset from the Medicare.gov Nursing Home Compare website allows for comparison of over 15,000 Medicare and Medicaid-certified nursing homes in the country.
Content
Separate data collections include:
Deficiencies, including fire safety, health, and inspection cycle types
Ownership details, including ownership percentage
Penalties, including filing date, fee, and payment date
Provider details, including non or for profit status, staff ratings, and survey scores
Quality MSR (Minimum Savings Rate) claims, including adjusted and observed scores
MDS (Minimum Data Set) quality measures, scored on a quarterly basis
State averages, including total number of quarterly deficiencies, and nurse staffing hours
Survey summaries for each nursing home
Inspiration
How would you determine what the top ten best nursing homes in the country are? The least?
Which states have the best level of nursing home care? The least?
In general, what are the most common types of complaints and deficiencies?
Acknowledgements
This dataset was collected by Medicare.gov, and the original files can be accessed here."
Average SAT Scores for NYC Public Schools,"Name, location, enrollment, and scores for 2014-2015 school year",NYC Open Data,12,"Version 1,2017-03-08",education,CSV,79 KB,CC0,"4,723 views",663 downloads,10 kernels,0 topics,https://www.kaggle.com/nycopendata/high-schools,"Content
This dataset consists of a row for every accredited high school in New York City with its department ID number, school name, borough, building code, street address, latitude/longitude coordinates, phone number, start and end times, student enrollment with race breakdown, and average scores on each SAT test section for the 2014-2015 school year.
Acknowledgements
The high school data was compiled and published by the New York City Department of Education, and the SAT score averages and testing rates were provided by the College Board.
Inspiration
Which public high school's students received the highest overall SAT score? Highest score for each section? Which borough has the highest performing schools? Do schools with lower student enrollment perform better?"
Urdu Speech Dataset,"2,500 Urdu audio samples",Hazrat Ali,12,"Version 1,2017-11-15",linguistics,Other,40 MB,CC4,"1,084 views",55 downloads,,,https://www.kaggle.com/hazrat/urdu-speech-dataset,"Context
This dataset presents speech files recorded for isolated words of Urdu. Language resources for Urdu language are not well developed. In this work, we summarize our work on the development of Urdu speech corpus for isolated words. The Corpus comprises of 250 isolated words of Urdu recorded by ten individuals. The speakers include both native and non-native, male and female individuals. The corpus can be used for both speech and speaker recognition tasks. The sampling frequency is 16000 Hz.
Content
Each folder name refers to a single speaker. The folder name gives information about the characteristics of each speaker. Each folder contains 250 isolated files i.e. 250 isolated words.
Speaker Name AA AB AC . . . AK
Gender M for male F for female
Native /Non-Native Y for Native N for Non-Native
Age Group G1, G2, G3, G4
Example: AAMNG1 Speaker Name = AA Gender = Male N = Non-Native G1 = Age Group G1
Acknowledgements
All the volunteers community who recorded for us."
Hadith Project,Chain of Narrators - Deep Learning for Authentication,Zeeshan-ul-hassan Usmani,12,"Version 1,2017-11-10","faith and traditions
islam",Other,9 MB,CC0,"1,417 views",62 downloads,,2 topics,https://www.kaggle.com/zusmani/hadithsahibukhari,"Context
A Hadith is a report describing the words, actions, intentions or habits of the last Prophet and Messenger Muhammed (Peace Be Upon Him). The term literally means report, account or narrative.
Ṣaḥīḥ al-Bukhārī (صحيح البخاري), is one of the six major hadith collections books. It was collected by a Muslim scholar Imam Muhammad al-Bukhari, after being transmitted orally for generations. There are 7,658 full hadiths in this collection narrated by 1,755 narrators/transmitters.
Imam Bukhari finished his work around 846 A.D.
Content
The two main sources of data regarding hadith are works of hadith and works containing biographical information about hadith narrators. The dataset contains 7,658 hadiths in Arabic and the names of 1,755 transmitters. Imam Bukhari followed the following criterion to include a hadith in this book.
Quality and soundness of chain of narrators - the lifetime of a narrator should overlap with the lifetime of the authority from whom he narrates.
Verifiable - it should be verifiable that narrators have met with their source persons. They should also expressly state that they obtained the narrative from these authorities.
Piousness – he only accepted the narratives from only those who, according to his knowledge, not only believed in Islam but practiced its teachings.
Acknowledgements
More information on Hadith and Sahih Bukhari can be found from this link - Hadith Books
Inspiration
Here are some ideas worth exploring:
The traditional criteria for determining if a hadith is Sahih (authentic) requires that there should be an uninterrupted chain of narrators; that all those narrators should be highly reliable and there should not be any hidden defects. Can we make a social network graph of all the narrators and then timestamp it with their age and era to see who overlaps who?
The name of a transmitter mentioned in a given hadith is not the full name, and many transmitters have similar names. So identifying who is the transmitter of a given hadith based on the names mentioned in the text might be a good problem to tackle
Can we analyze the chains of transmitters for entire collections using Neo4j or some other graph database
There exist different chains that reports the same hadith with little variation of words, can you identify those
Can you link the text with other external data sources?
Can we produce the word cloud for each chapter of the book?
Can we train a neural network to authenticate if the hadith is real or not?
Can we find out the specific style or vocabulary of each narrator?
Can we develop a system for comparing variant wordings for the same hadith to identify how reliable a given transmitter is.
Please also help me extend this dataset. If you have any other hadith book in CSV or text format, please send me a message and I will add."
Weekly Corn Price,Weekly corn close price from 2015 to 2017 (2017-10-01),Nick Wong,12,"Version 3,2017-10-10|Version 2,2017-10-06|Version 1,2017-10-04","finance
agriculture",Other,20 KB,CC0,"2,768 views",324 downloads,7 kernels,,https://www.kaggle.com/nickwong64/corn2015-2017,"Context
As I am trying to learn and build an LSTM prediction model for equity prices, I have tried Gold and then want to try crops which may have strong trends in times, so I prepared the dataset for the weekly corn prices.
Content
The file composed of simply 2 columns. One is the date (weekend) and the other is corn close price. The period is from 2015-01-04 to 2017-10-01. The original data is downloaded from Quantopian corn futures price.
Acknowledgements
Thanks to Jason of his tutorial about LSTM forecast: https://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/
Inspiration
William Gann: Time is the most important factor in determining market movements and by studying past price records you will be able to prove to yourself history does repeat and by knowing the past you can tell the future. There is a definite relation between price and time."
News of the Brazilian Newspaper,167.053 news of the site Folha de São Paulo (Brazilian Newspaper),Marlesson,12,"Version 1,2017-10-30","journalism
languages
brazil",CSV,480 MB,CC0,917 views,121 downloads,2 kernels,0 topics,https://www.kaggle.com/marlesson/news-of-the-site-folhauol,"Content
The dataset consists of 167.053 examples and contains Headlines, Url of Article, Complete Article and Category. I gathered the summarized news from Inshorts and only scraped the news articles from Folha de São Paulo - http://www.folha.uol.com.br/ (Brazilian Newspaper). Time period ranges is between January 2015 and September 2017."
ODI Cricket Matches,Dataset of all the One Day International cricket matches from year 1971-2017,Jalaz Kumar,12,"Version 4,2017-11-24|Version 3,2017-11-19|Version 2,2017-11-17|Version 1,2017-11-10",cricket,CSV,1 MB,CC4,"1,059 views",256 downloads,,0 topics,https://www.kaggle.com/jaykay12/odi-cricket-matches-19712017,"Context
Dataset was created by me for the Major Project of my Dual Degree Course Curriculum.
Content
Dataset comprises of all the ODI Cricket Matches in the interval 1971-2017
It consists of these files:
- originalDataset.csv : Raw Dataset File which I scraped using Pandas
- CategorialDataset.csv : Categorial Features suitable for models like MLPClassifier & DTClassifier
- ContinousDataset.csv : Purely for Experimental Purposes
- LabelledDataset.csv : Suitable for Support Vector Machines
Acknowledgements
ESPN CricInfo"
Tusbic Santander,Santander bike sharing system,Álvaro López García,12,"Version 6,2017-11-17|Version 5,2017-11-17|Version 4,2017-11-16|Version 3,2017-11-10|Version 2,2017-11-10|Version 1,2017-11-06","cities
cycling",CSV,2 MB,Other,950 views,67 downloads,6 kernels,2 topics,https://www.kaggle.com/alvarolopez/tusbic,"Description
This is a collection of the Santander (Spain) bike-sharing open data facility, named Tusbic, operated by JCDecaux.
I will be updating this dataset form time to time, added new data as I collect it.
Format
Bike sharing system data
The bikes.csv file contains the information from the bike sharing system. Data is structured as follows:
number number of the station
contract_name name of the contract of the station
name name of the station
address address of the station (raw)
lat latitude of the station in WGS84 format
lng latitude of the station in WGS84 format
banking indicates whether this station has a payment terminal
bonus indicates whether this is a bonus station
status indicates whether this station is CLOSEDor OPEN
bike_stands the number of operational bike stands at this station
available_bike_stands the number of available bike stands at this station
available_bikes the number of available and operational bikes at this station
last_update timestamp indicating the last update time in milliseconds since Epoch
Bike lane geometries
The bike_lanes.csv file contains the geometries of bike lanes in Santander city, as published by the Santander City Council in its open data platform.
ayto:WKT contains the geometry in WKT format, using ED50 UTM coordinates (zone 30N).
wkt_wsg84 contains the geometry in WKT format, using WGS84 coordinates.
ayto:Estado shows the status of the bike lane. EJECUTADO means that is has been built and it is operative.
License
The bike sharing data is being collected from the JCDecaux Developer Open Data platform and is licensed under the Etalab Open License, compatbile with the standards of Open Data licenses (ODC-BY, CC-BY 2.0).
The bike lane geometry is being collected form the Santander Open Data Platform and is licensed under a CC BY 4.0 license.
Dataste Kaggle logo is a photo licensed under a CC-BY-SA 3.0, authored by Tiia Monto."
Gender Info 2007,Global gender statistics,United Nations,12,"Version 1,2017-11-15",gender,CSV,2 MB,Other,"3,269 views",424 downloads,,0 topics,https://www.kaggle.com/unitednations/gender-info-2007,"Gender Info 2007 is a global database of gender statistics and indicators on a wide range of policy areas, including: population, families, health, education, work, and political participation. It can be used by governments, international organizations, advocacy groups, researchers and others in need of statistics for planning, analysis, advocacy and awareness-raising. Users will find in Gender Info an easy-to-use tool to shed light on gender issues through customizable tables, graphs and maps. It is an initiative of the United Nations Statistics Division, produced in collaboration with the United Nations Children’s Fund (UNICEF) and the United Nations Population Fund (UNFPA).
This dataset was last updated in 2008. If you need a more current version of the data please visit http://unstats.un.org/unsd/gender/data.html for other Gender Statistics.
Acknowledgements
This dataset was kindly published by the United Nations on the UNData site. You can find the original dataset here.
License
Per the UNData terms of use: all data and metadata provided on UNdata’s website are available free of charge and may be copied freely, duplicated and further distributed provided that UNdata is cited as the reference."
Diabetes 130 US hospitals for years 1999-2008,Diabetes - readmission,Humberto Brandão,11,"Version 1,2017-10-31","healthcare
health",CSV,20 MB,CC0,"1,849 views",275 downloads,5 kernels,0 topics,https://www.kaggle.com/brandao/diabetes,"Basic Explanation
It is important to know if a patient will be readmitted in some hospital. The reason is that you can change the treatment, in order to avoid a readmission.
In this database, you have 3 different outputs:
No readmission;
A readmission in less than 30 days (this situation is not good, because maybe your treatment was not appropriate);
A readmission in more than 30 days (this one is not so good as well the last one, however, the reason can be the state of the patient.
In this context, you can see different objective functions for the problem. You can try to figure out situations where the patient will not be readmitted, or if their are going to be readmitted in less than 30 days (because the problem can the the treatment), etc... Make your choice and let's help them creating new approaches for the problem.
Content
""The data set represents 10 years (1999-2008) of clinical care at 130 US hospitals and integrated delivery networks. It includes over 50 features representing patient and hospital outcomes. Information was extracted from the database for encounters that satisfied the following criteria.
It is an inpatient encounter (a hospital admission).
It is a diabetic encounter, that is, one during which any kind of diabetes was entered to the system as a diagnosis.
The length of stay was at least 1 day and at most 14 days.
Laboratory tests were performed during the encounter.
Medications were administered during the encounter.
The data contains such attributes as patient number, race, gender, age, admission type, time in hospital, medical specialty of admitting physician, number of lab test performed, HbA1c test result, diagnosis, number of medication, diabetic medications, number of outpatient, inpatient, and emergency visits in the year before the hospitalization, etc.""
https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008
Source
The data are submitted on behalf of the Center for Clinical and Translational Research, Virginia Commonwealth University, a recipient of NIH CTSA grant UL1 TR00058 and a recipient of the CERNER data. John Clore (jclore '@' vcu.edu), Krzysztof J. Cios (kcios '@' vcu.edu), Jon DeShazo (jpdeshazo '@' vcu.edu), and Beata Strack (strackb '@' vcu.edu). This data is a de-identified abstract of the Health Facts database (Cerner Corporation, Kansas City, MO).
Original source of the data set
https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008"
Union Membership & Coverage,The state of unions in the United States (1983 to 2015),Megan Risdal,11,"Version 1,2016-09-05","government
economics",CSV,440 KB,Other,"2,880 views",205 downloads,13 kernels,0 topics,https://www.kaggle.com/mrisdal/union-membership-coverage,"The United States Department of Labor tells us that ""Labor Day, the first Monday in September, is a creation of the labor movement and is dedicated to the social and economic achievements of American workers. It constitutes a yearly national tribute to the contributions workers have made to the strength, prosperity, and well-being of our country.""
This database of state-level union membership and coverage from 1983 to 2015 was originally compiled by Barry Hirsch (Andrew Young School of Policy Studies, Georgia State University) and David Macpherson (Department of Economics, Trinity University). The database, available at unionstats.com provides private and public sector labor union membership, coverage, and density estimates compiled from the monthly household Current Population Survey (CPS) using BLS methods.
Use of this data requires citation of the following paper which also includes a description of how the database was created: Barry T. Hirsch and David A. Macpherson, ""Union Membership and Coverage Database from the Current Population Survey: Note,"" Industrial and Labor Relations Review, Vol. 56, No. 2, January 2003, pp. 349-54. (PDF)."
Mujhe Kiyun Nikala,Why You Kicked Me Out?,Zeeshan-ul-hassan Usmani,11,"Version 1,2017-11-10",politics,CSV,1 MB,CC0,"1,544 views",28 downloads,,,https://www.kaggle.com/zusmani/mujhe-kiyun-nikala,"Context
The Panama Paper Case is the most publicized case in the history of Pakistan. It was heard before the Supreme Court of Pakistan between November 1st 2016 to February 23, 2017. It alleges the corruption, money laundering and wrong doing by the Prime Minister of Pakistan, Nawaz Sharif.
After subsequent formation of Joint Investigation Committee and its recommendations, court finally disqualified the prime minister on July 28th 2017. Since then, there is a hue and cry in the media by the incumbent political party and one question the ex-Prime Minister is keep asking is ""Mujhe Kiyun Nikala (Why They Kicked Me Out). It seems the poor man has no idea what happened and why.
Pakistan is a country of 207 million people according to the recent census and no one is giving him the answer of his question. I feel pity for the ex-Prime Minister, and thought to launch this dataset to call my fellow data scientists to run the kernels, dig out the hidden meanings, find patterns and linkages with off-shore companies (I've posted the complete Panama Papers in another dataset as well) to help him understand ""Unhen Kiyun Nikala - Why did he kicked out?""
Here is a good Wikipedia article with further details. Please do contribute more datasets regarding this case through discussion forums and I will keep updating it as the case/dataset progresses.
Please help me answer the most publicized question in the history of Pakistan -
Mujhe Kiyun Nikala.
Content
Full text of Panama Case Verdict is available through CSV file in the Data section. The original and subsequent review decisions by the Supreme Court of Pakistan can be found through these links:
Civil Review Petition No. 297 of 2017 in Const. Petition No. 29 of 2016
Civil Appeals No. 1406 and 1407 of 2016
Constitution Petition No. 29 of 2016
Review Decision
Full Verdict in Urdu
Full Verdict in English
Acknowledgements
Supreme Court of Pakistan
Inspiration
Some ideas worth exploring:
Who are the lawyers from both sides?
What charges were filled against the ex-Prime Minister
What charges were found Not True
What charges were found True
What other references court has given that resembles this case
How we can join this dataset with Panama dataset
How we can visually present the data
How data science can answer the question - Mujhe Kiyun Nikala"
Dota 2 Matches Dataset,Partial rows from the data dump of parsed matches from opendota.com,Joe Ramir,11,"Version 1,2016-09-13",,Other,13 MB,CC4,"8,902 views",603 downloads,4 kernels,,https://www.kaggle.com/jraramirez/dota-2-matches-dataset,"Context
The first 100 rows of the data dump of parsed matches from opendota.com (formerly yasp.co) as of mid December 2015.
Content
Columns:
match_id - INTEGER, unique match id
match_seq_num - INTEGER, match sequence number
radiant_win - STRING, boolean variable than indicates if radiant won or not in the match
start_time - INTEGER, start time of the match
duration - INTEGER, duration of the match
tower_status_radiant - INTEGER, remaining health of the towers of the radiant side
tower_status_dire - INTEGER, remaining health of the towers of the dire side
barracks_status_radiant - INTEGER, remaining health of the barracks of the radiant side
barracks_status_dire - INTEGER , remaining health of the towers of the direside
cluster - INTEGER,
first_blood_time - INTEGER, time when the first blood occured in the match
lobby_type - INTEGER, type of the looby of the match
human_players - INTEGER, number of human players in the match leagueid - INTEGER, league id
positive_votes - INTEGER, number of positive votes
negative_votes - INTEGER, number of negative votes
game_mode - INTEGER, game mode
engine - INTEGER, engine
picks_bans - STRING, picks and bans
parse_status - INTEGER, parse status
item - STRING, a complex JSON that also include all the columns mentioned but may need more processing since the more interesting data are found here (e.g. chats, teamfights, purchase logs, etc. )
Past Research
There are already several sites (see dotabuff and onedota) that analyze dota 2 matches.
Inspiration
The data set could be used by Dota 2 players but are data science enthusiasts as well.
There are lots of questions that can be formulated in this dataset.
It will be helpful if somebody can produce a better friendlier version of the dataset.
Acknowledgements
Citation:
Source: https://yasp.co/blog/33
Readme: https://github.com/yasp-dota/yasp/issues/924
Type: Dataset
Tags: Dota 2, Games, MOBA
Abstract: This is a data dump of all the parsed matches from yasp.co (as of mid December 2015). This is about 3.5 million matches.
License: CC BY-SA 4.0
Terms: We ask that you attribute yasp.co if you create or publish anything related to our data. Also, please seed for as long as possible.
https://www.opendota.com/
https://bigquery.cloud.google.com/table/fh-bigquery:public_dump.dota2_yasp_v1
How the dataset was compiled
The details of how the dataset was complied is detailed here: https://github.com/odota/core/issues/924
Suggested Way of Reading the File:
In R, the best way to read the file is through the use of read.delim() function since the values are tab separated.
example: dota = read.delim(""dota2_yasp_v1.txt"", sep=""\t"")
Image Source:
http://media.steampowered.com/apps/dota2/images/blogfiles/keyart_ezalor.jpg"
"The State of JavaScript, 2016",Responses to the State of JavaScript survey,姜上（Integ）,11,"Version 3,2016-12-05|Version 2,2016-12-02|Version 1,2016-12-02","programming languages
programming",CSV,20 MB,CC0,"2,712 views",140 downloads,3 kernels,2 topics,https://www.kaggle.com/integjs/state-of-javascript-2016,"Context
Over nine thousand developers took part in the first edition of the State Of JavaScript survey.
They answered questions on topics ranging from front-end frameworks and state management, to build tools and testing libraries.
You'll find out which libraries developers most want to learn next, and which have the highest satisfaction ratings. And hopefully, this data will help you make sense of the ever-changing JavaScript ecosystem.
Content
http://stateofjs.com/2016/introduction/
Acknowledgements
Thanks to http://stateofjs.com/ open the data for download."
Consumer Business Complaints in Brazil,Consumer complaints about issues with business in Brazil,Luiz Gerosa,11,"Version 3,2017-10-12|Version 2,2017-10-12|Version 1,2017-10-12","brazil
business",CSV,406 MB,CC4,"2,262 views",196 downloads,3 kernels,,https://www.kaggle.com/gerosa/procon,"Context
When Brazilian consumers need to resolve a dispute with business the first step is to go to a local Procon (Consumer Protection Agency) and file a complaint. The Procon assists the consumer and intermediates the resolution with the company.
Content
This dataset contains information about complaints filed in Procons between 2012 and 2016. This data was download from official Brazilian government open data website"
British Birdsong Dataset,264 recordings from 88 species,Rachael Tatman,11,"Version 2,2017-11-17|Version 1,2017-11-17","animals
acoustics",CSV,633 MB,Other,787 views,76 downloads,,0 topics,https://www.kaggle.com/rtatman/british-birdsong-dataset,"Context:
Birds use songs and calls of varying length and complexity to attract mates, warn of nearby danger and mark their territory. This dataset contains a recordings of different birdsongs from bird species that can be found in Britain (although the recordings themselves are from many different locations).
Content:
This is a dataset of bird sound recordings, a specific subset gathered from the Xeno Canto collection to form a balanced dataset across 88 species commonly heard in the United Kingdom.
The copyright in each audio file is owned by the user who donated the file to Xeno Canto. Please see ""birdsong_metadata.tsv"" for the full listing, which gives the authors' names and the CC licences applicable for each file. The audio files are encoded as .flac files.
Acknowledgements:
These recordings were collected by 68 separate birding enthusiasts and uploaded to and stored by xeno-canto: www.xeno-canto.org. If you make use of these recordings in your work, please cite the specific recording and include acknowledgement of and a link to the xeno-canto website.
Inspiration:
Can you build a classifier to identify birds based on their songs?
Can you visualize the songs of specific birds?
Can you generate new birdsongs based on this data?"
The Smell of Fear,Identification of markers for human emotions in breath,Joerg Simon Wicker,11,"Version 4,2017-11-28|Version 3,2017-11-28|Version 2,2017-11-22|Version 1,2017-11-22","film
time series
atmospheric sciences
+ 2 more...",CSV,94 MB,CC4,"1,821 views",105 downloads,,2 topics,https://www.kaggle.com/jswicker/the-smell-of-fear,"Context
While the physiological response of humans to emotional events or stimuli is well-investigated for many modalities (like EEG, skin resistance, ...), surprisingly little is known about the exhalation of so-called Volatile Organic Compounds (VOCs) at quite low concentrations in response to such stimuli. VOCs are molecules of relatively small mass that quickly evaporate or sublimate and can be detected in the air that surrounds us. The project introduces a new field of application for data mining, where trace gas responses of people reacting on-line to films shown in cinemas (or movie theaters) are related to the semantic content of the films themselves. To do so, we measured the VOCs from a movie theater over a whole month in intervals of thirty seconds, and annotated the screened films by a controlled vocabulary compiled from multiple sources.
Content
The data set consists of two parts, first the measured VOCs, and second the information about the movies. The VOCs are given in the file TOF_CO2_data_30sec.arff which is simply the time of the measurement in the first column, then all measured 400+ VOCs in the other columns. Roughly one measurement was carried out every 30 seconds. The information which movies were shown is given in the file screenings.csv. It gives start time, end time, movie title and how many visitors were in the screening. Additionally, the folder labels_aggregated give a consensus labelling of multiple annotators for the movies. The labels describe the scenes, each label represented by a row, then each column showing if the label is active (1) or not (0). This is available for 6 movies in the data set.
The goal of our initial analysis was the identification of markers, that is, finding certain VOCs that have a relation to certain labels and therefore emotions. For example, given the scene label blood, is there any increase or decrease in the concentration of a specific VOC?
Further information is available in our publications https://doi.org/10.1145/2783258.2783404 and https://doi.org/10.1038/srep25464
Acknowledgements
If you use this data set, please cite:
Jörg Wicker, Nicolas Krauter, Bettina Derstorff, Christof Stönner, Efstratios Bourtsoukidis, Thomas Klüpfel, Jonathan Williams, and Stefan Kramer. 2015. Cinema Data Mining: The Smell of Fear. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '15). ACM, New York, NY, USA, 1295-1304. DOI: https://doi.org/10.1145/2783258.2783404
Inspiration
While the first analysis gave already interesting results, we believe that this data set has a high potential for further analysis. We are currently working on increasing the size of the data. Additionally, multiple follow-up publications are being prepared. There are many posssible tasks, we focus mainly on the identification of markers in the VOC data, but there are many potential interesting findings in the data set. Are movies related based on the VOCs? Could we identify similar scenes based on the VOCs?"
Household Electric Power Consumption,time series analysis- regression / clustering,UCI Machine Learning,11,"Version 1,2016-08-24",,Other,127 MB,ODbL,"6,146 views",533 downloads,12 kernels,,https://www.kaggle.com/uciml/electric-power-consumption-data-set,"I need help to analyze this data set with R code, if someone can help me I'd appreciate a lot and I'd send some money for his kindness. I really need how to do a regression and clustering manipulating this data. Sorry about the format, it's in text file. Thanks in advance :)
Context: Measurements of electric power consumption in one household with a one-minute sampling rate over a period of almost 4 years. Different electrical quantities and some sub-metering values are available.
Data Set Characteristics:
Multivariate, Time-Series
Associated Tasks: Regression, Clustering
Data Set Information:
This archive contains 2075259 measurements gathered between December 2006 and November 2010 (47 months). Notes: 1.(global_active_power*1000/60 - sub_metering_1 - sub_metering_2 - sub_metering_3) represents the active energy consumed every minute (in watt hour) in the household by electrical equipment not measured in sub-meterings 1, 2 and 3.
2.The dataset contains some missing values in the measurements (nearly 1,25% of the rows). All calendar timestamps are present in the dataset but for some timestamps, the measurement values are missing: a missing value is represented by the absence of value between two consecutive semi-colon attribute separators. For instance, the dataset shows missing values on April 28, 2007.
Attribute Information: 1.date: Date in format dd/mm/yyyy
2.time: time in format hh:mm:ss
3.global_active_power: household global minute-averaged active power (in kilowatt)
4.global_reactive_power: household global minute-averaged reactive power (in kilowatt)
5.voltage: minute-averaged voltage (in volt)
6.global_intensity: household global minute-averaged current intensity (in ampere)
7.sub_metering_1: energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered).
8.sub_metering_2: energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light.
9.sub_metering_3: energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an electric water-heater and an air-conditioner."
Patent Assignment Daily,Contains daily patent assignment text for 10/18/2016,US Patent and Trademark Office,11,"Version 1,2016-10-20",law,Other,273 MB,Other,"3,064 views",146 downloads,9 kernels,0 topics,https://www.kaggle.com/uspto/patent-assignment-daily,"Patent Assignment Daily XML (PADX)
This dataset contains daily patent assignment (ownership) text (no drawings/images) for 10/18/2016 derived from patent assignment recordations made at the USPTO.
Context
Each day, the US Patent and Trademark Office (USPTO) records patent assignments (changes in ownership). These assignments can be used to track chain-of-ownership for patents and patent applications. For more information about Ownership/Assignability of Patents and Applications, please see the USPTO Manual of Patent Examining Procedure (MPEP), Section 301.
Frequency: Weekly (MON-SUN)
Period: 10/18/2016
Inspiration
This dataset provides insight into where new knowledge and technological advances originate in the United States. To get started working with XML files, fork the kernel Exploring Daily Patent Assignment Files. You can use this dataset to understand what sector is currently up-and-coming or which companies are filing a lot of patents, which can proxy their level of innovation, a corporate strength, or a focus of new research and development.
Acknowledgements
The USPTO owns the dataset. These files are extracted nightly from the Assignment Historical Database (AHD).
License
Creative Commons - Public Domain Mark 1.0"
Worldwide Economic Remittances,Money sent home to family by workers abroad,World Bank,11,"Version 1,2017-11-07","countries
globalization
demographics
+ 2 more...",CSV,503 KB,Other,"1,452 views",194 downloads,2 kernels,0 topics,https://www.kaggle.com/theworldbank/worldwide-economic-remittances,"Context
In 2013 alone, international migrants sent $413 billion home to families and friends. This money is known as ""remittance money"", and the total is more than three times that afforded by total global foreign aid ($135 billion). Remittances are traditionally associated with poor migrants moving outside of their home country to find work, supporting their families back home on their foreign wages; as a result, they make up a significant part of the economic picture for many developing countries in the world.
This dataset, published by the World Bank, provides estimates of 2016 remittance movements between various countries. It also provides historical data on the flow of such money going back to 1970.
For a look at how remittances play into the global economy, watch ""The hidden force in global economics: sending money home"".
Content
This dataset contains three files:
bilateral-remittance.csv --- Estimated remittances between world countries in the year 2016.
remittance-inflow.csv --- Historical remittance money inflow into world countries since 1970. Typically high in developing nations.
remittance-outflow.csv --- Historical remittance money outflow from world countries since 1970. Typically high in more developed nations.
All monetary values are in terms of millions of US dollars.
For more information on how this data was generated and calculated, refer to the World Bank Remittance Data FAQ.
Acknowledgements
This dataset is a republished version of three of the tables published by the World Bank which has been slightly cleaned up for use on Kaggle. For the original source, and other complimentary materials, check out the dataset home page.
Inspiration
What is the historical trend in remittance inflows and outflows for various countries? How does this relate to the developmental character of the countries in question?
What countries send to most money abroad? What countries receive the most money from abroad? Try combining this dataset with a demographics dataset to see what countries are most and least reliant on income from abroad.
How far do workers migrate for a job? Are they staying near home, or going half the world away? Are there any surprising facts about who send money to who?"
SNAP Memetracker,Tracking high-frequency phrases across internet news,Stanford Network Analysis Project,11,"Version 1,2016-11-21","linguistics
internet",SQLite,3 GB,CC0,"2,876 views",156 downloads,2 kernels,0 topics,https://www.kaggle.com/snap/snap-memetracker,"This database contains a subset of the Memetracker dataset collected by SNAP.
The full Memetracker dataset has observations broken into months. Because of size considerations, however, this version consists of one-half of a month: the first 15 days of Memetracker observations from November 2008.
About
Memetracker tracks the quotes and phrases that appear most frequently over time across the entire online news spectrum. This makes it possible to see how different stories compete for news and blog coverage each day, and how certain stories persist while others fade quickly.
Overall Memetracker tracks more than 17 million different phrases and about 54% of the total phrase/quote mentions appear on blogs and 46% in news media.
Acknowledgments
This dataset was collected by the Stanford Network Analysis Project. Detailed information about the data and its analysis can be found at the website here.
An analysis of this dataset was published here:
J. Leskovec, L. Backstrom, J. Kleinberg. Meme-tracking and the Dynamics of the News Cycle. ACM SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining, 2009.
The Data
The SQLite database contains three tables:
articles: 4,542,920 records, with the following fields:
article_id: a unique id for the article (int)
url: the URL of the article (text)
date: the date of the article (text), in the strptime format '%Y-%m-%d %H:%M:%S'
quotes: 7,956,125 records, with the following fields:
article_id: unique id for the article that this quote was found in (int)
phrase: the high-frequency phrase found in the article (text)
links: 16,727,125 records, with the following fields:
article_id: unique id for the article that this link was found in (int)
link_out: the URL of the link out (text)
link_out_id: unique id for the target article (int), if it exists; else NULL"
Top Trending How Tos on Google,"The top trending ""how to"" related searches on Google in the past 5 years",Google News Lab,11,"Version 1,2017-11-01",,CSV,3 KB,CC4,"1,401 views",159 downloads,,0 topics,https://www.kaggle.com/GoogleNewsLab/top-trending-how-tos-on-google,"These are the top trending ""How to"" searches on Google, ranked by their spike value. Trending searches are searches with the biggest increase in search interest since the previous time period. Data covers the past 5 years."
Devanagari Character Dataset,Devanagari (Nepali) Handwritten Character Dataset,Ashok Kumar Pant,11,"Version 2,2017-06-23|Version 1,2017-06-22",,CSV,9 MB,ODbL,"2,256 views",245 downloads,,,https://www.kaggle.com/ashokpant/devanagari-character-dataset,"Context
This dataset is created as a part of my dissertation work for the fulfillment of the Master's degree in Computer Science (Tribhuvan University, Nepal, 2012).
Content
The dataset contains three individual categories. Samples are collected from 40 individuals (persons) from different fields and cropped for character boundary.
Numerals (288 samples per class, 10 classes)
Vowels (221 samples per class, 12 classes)
Consonants (205 samples per class, 36 classes)
Citation
Please cite in your publications if it helps your research:
@inproceedings{pant2012off,
  title={Off-line Nepali handwritten character recognition using Multilayer Perceptron and Radial Basis Function neural networks},
  author={Pant, Ashok Kumar and Panday, Sanjeeb Prasad and Joshi, Shashidhar Ram},
  booktitle={2012 Third Asian Himalayas International Conference on Internet},
  pages={1--5},
  year={2012},
  organization={IEEE}
}"
"Subreddit Interactions for 25,000 Users",Modeling Reddit users from their metadata,colemaclean,11,"Version 1,2017-02-19","sociology
internet",CSV,484 MB,Other,"2,837 views",180 downloads,4 kernels,0 topics,https://www.kaggle.com/colemaclean/subreddit-interactions,"Context
The dataset is a csv file compiled using a python scrapper developed using Reddit's PRAW API. The raw data is a list of 3-tuples of [username,subreddit,utc timestamp]. Each row represents a single comment made by the user, representing about 5 days worth of Reddit data. Note that the actual comment text is not included, only the user, subreddit and comment timestamp of the users comment. The goal of the dataset is to provide a lens in discovering user patterns from reddit meta-data alone. The original use case was to compile a dataset suitable for training a neural network in developing a subreddit recommender system. That final system can be found here
A very unpolished EDA for the dataset can be found here. Note the published dataset is only half of the one used in the EDA and recommender system, to meet kaggle's 500MB size limitation.
Content
user - The username of the person submitting the comment
subreddit - The title of the subreddit the user made the comment in
utc_stamp - the utc timestamp of when the user made the comment
Acknowledgements
The dataset was compiled as part of a school project. The final project report, with my collaborators, can be found here
Inspiration
We were able to build a pretty cool subreddit recommender with the dataset. A blog post for it can be found here, and the stand alone jupyter notebook for it here. Our final model is very undertuned, so there's definitely improvements to be made there, but I think there are many other cool data projects and visualizations that could be built from this dataset. One example would be to analyze the spread of users through the Reddit ecosystem, whether the average user clusters in close communities, or traverses wide and far to different corners. If you do end up building something on this, please share! And have fun!
Released under Reddit's API licence"
Audio Features for Playlist Creation,"Playlist is based on themes like dinner, sleep, party and workout",Aniruddha Achar,11,"Version 2,2017-03-03|Version 1,2017-02-24","music
acoustics
sound technology",CSV,668 KB,CC4,"5,074 views",414 downloads,5 kernels,,https://www.kaggle.com/aniruddhaachar/audio-features,"Context
This data was compiled as part of our undergrad project that used machine learning to classify songs based on themes or activities songs are associated with. For the project we, four activities were choose.
Dinner: Songs that sound good when played in a dinner setting or at a restaurant.
Sleep: Songs that promote sleep when they are played.
Party: Songs that sound good when played at a party.
Workout: Songs that sound good when one is exercising/ working out.
The collection of data started with collecting playlist details form Spotify. Spotify web API was used for the collection of the playlist of each category. Track title, album name and artist names were used to extract low level and high level Audio features like MFCC, Spectral centroid, Spectral Roll-off, Spectral Bandwidth, Tempo, Spectral Contrast and Root Mean Square Energy of the songs. For ease of computation, the mean of the values were calculated and added to the tables.
Data was also curated using Spotify's audio analysis API. A larger set of songs is part of this data set.
Content
The data set has eight tables.
Four tables with names playlist_audio_features have the signal processing features like MFCC, spectral centroid etc.
Four more tables with names playlist_spotify_features have the data extracted from Spotify's audio feature API. These tables have larger number of features. The data set size is quite large.
Description of the ""playlist""_audio_features columns:
The first column has the simple integer id if the track. (This id is local to that file).
The second column has the name of the track.
The third column name mfcc has the mean of the calculated MFCC for that track. 20 MFC coefficients were extracted from one frame of the track.
The forth column is named scem: This is the mean of Spectral centroid. Spectral centroid was calculated for each frame.
The fifth column is named scom: This is the mean of Spectral contrast. Spectral contrast was calculated for each frame.
The sixth column is named srom: This is the mean of Spectral Roll-off. Spectral roll-off was calculated for each frame.
The seventh column is named sbwm: This is the mean of Spectral Bandwidth. Spectral Bandwidth was calculated for each frame.
The eight column is name tempo: This is the estimated tempo of the track.
The ninth column is name rmse: This is the mean of the RSME was calculated for each frame.
Description of the _spotify_features columns:
id: This is the Spotify id of the track.
name: This is the name of the track.
url: This is a Spotify uri of the track.
artist: This is a one or more artists who worked on the track. 5-13: Description of each of the column can be found at https://developer.spotify.com/web-api/get-audio-features/
Acknowledgements
We would like to thank Librosa an opensource audio feature extraction library in python for developing a great tool. We would also thank the large research done on music genre classification using audio feature which helped us in developing this data set as well as the classification. A special thanks to Spotify"
MEDLINE and MeSH,Biomedical bibliometric data and paper classification,verginer,11,"Version 1,2017-05-26","research
healthcare
pharmaceutical industry",CSV,4 GB,Other,"2,771 views",317 downloads,,0 topics,https://www.kaggle.com/alucaria/medline,"Content and Context
This dataset is a collection of biomedicine and life science bibliometric data obtained from MEDLINE.
The data covers 26,759,425 papers available thought MEDLINE from 1946 through 2016.
This dataset has been created by processing the publicly available data dump at nih.gov. The data dump consists of ca 23 Million xml articles. From these xml files I have extracted some meta data into more accessible csv files. The processed csvs contain both very basic metadata (i.e. creation date, number of authors, and ids) and the ""Medical Subject Headings"" MeSH classification.
The dataset is divided in two files,
paper_details.csv
mesh.csv
The paper_details.csv file contains the following columns:
pmid: this is the unique article identified within the dataset, it is also the official identifier used on PubMed
doi: digital object identifier, this id allows to uniquely identify a paper through the widely used DOI scheme
num_authors: number of authors on the paper
year: year the document has been created
month: month the document has been created
day: day the document has been created
title: title of the document
issn: the ISSN number of the journal the document has been published in
issn_type: printed or electronic
volume: volume of the journal the document has been published in
issue: issue of the journal the document has been published in
journal_title: Name of Journal the document has been published in
journal_title_iso: ISO abbreviation of the journal title.
5 GB, and 26,759,425 rows
In the mesh.csv file the following fields are available:
pmid: this is the unique article identified within the dataset, it is also the official identifier used on PubMed
descriptor_id: the id of the MeSH descriptor for the given document
descriptor_name: name of the MeSH descriptor
descriptor_major_topic: Y/N, indicate if the descriptor is a major topic in MeSH
qualifiers: a list of MeSH qualifiers (aka subheadings). Subheadings are attached to MeSH headings to describe a specific aspect of a concept.
NB: a document can and has more often then not more then one MeSH descriptor associated to it.
11 GB, and 247,415,857 rows
Other Resources
An introduction to MeSH can be found here and short tutorials from the U.S. National Library of Medicine on how to use MeSH can be found here.
Factsheets describing MeSH and MEDLINE:
https://www.nlm.nih.gov/pubs/factsheets/mesh.html
https://www.nlm.nih.gov/pubs/factsheets/medline.html
Further datasets, which might be useful to work with MeSH data:
https://mbr.nlm.nih.gov/Downloads.shtml
Acknowledgements
The data is freely available for download from MEDLINE. Specifically through their ftp service at ftp://ftp.ncbi.nlm.nih.gov/pub/.
Read the disclaimer from the U.S. National Library of Medicine regarding public use and redistribution of the data here
The data extraction would not have been possible without access to the computing facilities of IMT School for Advanced Studies Lucca.
Inspiration
Questions which one might want to look into using this dataset could be:
Prediction of Mesh Headings
Descriptive statistics on the rise and fall of MeSH descriptors over time.
Which Journals ˙have had the most meteoric rise?
Compute the co-occurrence probability of a given MeSH descriptor pair over time
How to open the files:
Due to the limit on Kaggle for files to be at most 500MB in size, the files have been split. More specifically the two files have been compressed with zip and split. To recombine them do the following. If you are on Linux/MacOS enter the following commands in the terminal
cat paper_details.csv.zip.part-* > papers_details.csv.zip
unzip papers_details.csv.zip
For Windows machines, this Stock Overflow Answer will help.
Image Credit
The beautiful cover image has been made by olsztyn-poland over at unsplash.com.
Link: https://unsplash.com/collections/610433/medical?photo=nss2eRzQwgw"
Poems from poetryfoundation.org,Modern and Renaissance poetry for classification exercises,ultra-jack,11,"Version 1,2017-06-27","poetry
linguistics",CSV,592 KB,CC0,"1,950 views",186 downloads,7 kernels,,https://www.kaggle.com/ultrajack/modern-renaissance-poetry,"Context
Study for poem classification. Trying to classified poems with targets age and type. I use two Xgboost predictors to predict target and type separately.
Content
Please refer to the website https://www.poetryfoundation.org/ For now I only crawl the data of
renaissance love
modern love
renaissance nature
modern nature
renaissance mythology & folklore
modern mythology & folklore
Some have copyrights. I only use for studying :)
Acknowledgements
https://www.poetryfoundation.org/ has the copyright
Inspiration
classification is fun!!"
Indian Premier League SQLite Database,"577 matches, players & teams attributes for Indian Premier League Database",HarshaVardhan,11,"Version 2,2016-12-30|Version 1,2016-12-17",cricket,SQLite,12 MB,ODbL,"6,050 views",524 downloads,6 kernels,2 topics,https://www.kaggle.com/harsha547/ipldatabase,"Indian Premier League Database for Data Analysis.
CSV Version for the same Dataset
Context
► 577 matches
► 469 Players
► Seasons 2008 to 2016
► Detailed Player Details (Player_DOB, Player_Batting_Style,Player_Bowling_Style, etc...) for 469 players.
► Detailed match events (Toss,Stadium, Date of the match, Who captained the match, who was the keeper, Extras, etc...) for 577 matches.
Example SQL Queries on IPL Database
Microsoft SQL Server Version
Database Diagram
Content :
Dataset includes 21 includes as mentioned below. Some tables created intentionally to practice SQL Queries. You can use Boolean values in some situations instead of joining tables like Toss_Table
Acknowledgements :
Thanks to stackoverflow community
Inspiration
Aim is to provide the database for all the Indian Premier League matches. where people can run their own queries to analyze the data.
Some of the analysis we most often heard in TV commentary are below.
Dhoni's Strike rate against Left-ball spinners.
What will be the result percentage when team lost more than three wickets in powerplay.
Below are the some websites where you can find inspiration for your analysis.
Knoema Cricket Analysis
Pandimi Cricket Analysis"
Party strength in each US state,From 1980 to present; a collection of political office compositions per state,GeneBurin,11,"Version 3,2017-01-13|Version 2,2017-01-12|Version 1,2017-01-11",politics,CSV,123 KB,CC0,"2,070 views",142 downloads,4 kernels,,https://www.kaggle.com/kiwiphrases/partystrengthbystate,"Data on party strength in each US state
The repository contains data on party strength for each state as shown on each state's corresponding party strength Wikipedia page (for example, here is Virginia )
Each state has a table of a detailed summary of the state of its governing and representing bodies on Wikipedia but there is no data set that collates these entries. I scraped each state's Wikipedia table and collated the entries into a single dataset. The data are stored in the state_party_strength.csv and state_party_strength_cleaned.csv. The code that generated the file can be found in corresponding Python notebooks.
Data contents:
The data contain information from 1980 on each state's: 1. governor and party 2. state house and senate composition 3. state representative composition in congress 4. electoral votes
Clean Version
Data in the clean version has been cleaned and processed substantially. Namely: - all columns now contain homogenous data within the column - names and Wiki-citations have been removed - only the party counts and party identification have been left The notebook that created this file is here
Uncleaned Data Version
The data contained herein have not been altered from their Wikipedia tables except in two instances: - Forced column names to be in accord across states - Any needed data modifications (ie concatenated string columns) to retain information when combining columns
To use the data:
Please note that the right encoding for the dataset is ""ISO-8859-1"", not 'utf-8' though in future versions I will try to fix that to make it more accessible.
This means that you will likely have to perform further data wrangling prior to doing any substantive analysis. The notebook that has been used to create this data file is located here
Raw scraped data
The raw scraped data can be found in the pickle. This file contains a Python dictionary where each key is a US state name and each element is the raw scraped table in Pandas DataFrame format.
Hope it proves as useful to you in analyzing/using political patterns at the state level in the US for political and policy research."
2017 Military Strength Ranking,The complete Global Firepower list for 2017,Blitzer,11,"Version 2,2017-07-14|Version 1,2017-07-14","military
international relations",CSV,59 KB,Other,"5,827 views",448 downloads,4 kernels,,https://www.kaggle.com/blitzr/gfp2017,"Context
The complete Global Firepower list for 2017 puts the military powers of the world into full perspective.
Content
GlobalFirePower.csv Contains all columns without categories. GlobalFirePower_multiindex.csv has 2 level columns, see the kernel for a parsing example.
Fields list
Manpower
Total Populations
Available Manpower
Manpower Fit-for-Service
Manpower Reaching Military Age Annually
Active Military Manpower
Active Reserve Military Manpower
Air Power
Total Aircraft Strength
Fighters & Interceptors
Attack Aircraft
Transports
Trainers
Total Helicopters
Attack Helicopters
Serviceable Airports
Army Strengths
Combat Tanks
Armored Fighting Vehicles
Self-Propelled Artillery
Towed Artillery
Rocket Projectors
Naval Power
Total Naval Strength
Aircraft Carriers
Frigates
Destroyers
Corvettes
Submarines
Patrol Craft
Mine Warfare
Financial Resources
Annual Defense Budgets
External Debt
Reserves of Foreign Exchange and Gold
Purchasing Power Parity
Logistical Resources
Labor Force Strength
Merchant Marine Strength
Major Ports & Terminals
Roadway Coverage
Railway Coverage
Natural Resources
Oil Production
Oil Consumption
Proven Oil Reserves
Geography
Square Land Areas
Coastline
Shared Borders
Waterway Coverage
The finalized Global Firepower ranking relies on over 50 factors to determine a given nation's PowerIndex ('PwrIndx') score. Our formula allows smaller, though more technologically-advanced, nations to compete with larger, lesser-developed ones. Modifiers (in the form of bonuses and penalties) are added to further refine the list. Some items to observe in regards to the finalized ranking:
Ranking does not simply rely on the total number of weapons available to any one country but rather focuses on weapon diversity within the number totals to provide a better balance of firepower available (i.e. fielding 100 minesweepers does not equal the strategic and tactical value of fielding 10 aircraft carriers).
Nuclear stockpiles are NOT taken into account but recognized / suspected nuclear powers receive a bonus.
Geographical factors, logistical flexibility, natural resources and local industry influence the final ranking.
Available manpower is a key consideration; nations with large populations tend to rank higher.
Land-locked nations are NOT penalized for lack of a navy; naval powers ARE penalized for lack of diversity in available assets.
NATO allies receive a slight bonus due to the theoretical sharing of resources.
Current political / military leadership is NOT taken into account.
For 2017 there are a total of 133 countries included in the GFP database.
Acknowledgements
The CSV files were parsed from http://www.globalfirepower.com/countries-listing.asp .
©2017 www.GlobalFirepower.com • Content ©2003-2017 GlobalFirepower.com • All Rights Reserved • The GlobalFirepower.com logo are trademarks and protected by all applicable domestic and international intellectual property laws. All material presented on this site is ""as is"" without any guarantee. Part of the MilitaryFactory.com network of sites."
News Articles,This dataset include articles from 2015 till date,AsadMahmood,11,"Version 1,2017-04-30","news agencies
journalism
linguistics",CSV,5 MB,CC0,"4,660 views",649 downloads,8 kernels,,https://www.kaggle.com/asad1m9a9h6mood/news-articles,"Content
This Dataset is scraped from https://www.thenews.com.pk website. It has news articles from 2015 till date related to business and sports. It Contains the Heading of the particular Article, Its content and its date. The content also contains the place from where the statement or Article was published.
Importance
This dataset can be used to detect main patterns between writing pattern of different types of articles. One more thing that can be extracted from it is that we could also detect the main locations from where the different types of articles originate.
Improvements
Some Data Cleaning could still be done specially in the content area of the dataset. One more thing that could be done is that we could extract the locations from the content and make a separated table for it.
Acknowledgements
I'd like to thanks developer of Selenium Library. That helped a lot in retrieving the data."
Largest Dog Breed Dataset,"Dog breed, tags, type, color, registered or not, etc.",LiamLarsen,11,"Version 1,2017-03-31",animals,CSV,26 MB,Other,"6,193 views",717 downloads,12 kernels,2 topics,https://www.kaggle.com/kingburrito666/largest-dog-breed-data-set,"Content
Dog: LicenseType, Breed, Color, DogName, OwnerZip, ExpYear, ValidDate
I've included all the data from 2007 to 2017
Inspiration
I love dogs To see trends in dog breeds, why are some more popular than others?"
"Elon Musk Tweets, 2010 to 2017",All Elon Musk Tweets from 2010 to 2017,LiamLarsen,11,"Version 1,2017-04-23","celebrity
technology forecasting
internet",CSV,393 KB,Other,"2,726 views",207 downloads,12 kernels,0 topics,https://www.kaggle.com/kingburrito666/elon-musk-tweets,"Content
tweet id, contains tweet-stamp
date + time, date and time of day (24hr)
tweet text, text of tweet, remove 'b'
usage
What's someone going to do with a bunch of tweets?
Maybe someone would want to generate text using this dataset
or do sentiment analysis
Or find out the most likely time of day Elon would tweet.
pie his tweets per month, ITS DATA!!
Either way its up to you!
Inspiration:"
Disaster/Accident Sources,"A list of Twitter users who report on disasters, accidents & crime",Armineh Nourbakhsh,11,"Version 1,2017-05-27","crime
internet",CSV,2 MB,CC0,"2,275 views",227 downloads,2 kernels,0 topics,https://www.kaggle.com/arminehn/disasteraccident-sources,"Content
Attached is a list of Twitter users who regularly report on natural and man-made disasters, violence or crime. The accounts may belong to journalists, news media, local fire or police departments, other local authorities, or disaster monitors. Disaster reporting may not be the primary function of the accounts, nevertheless they are a prolific source of disaster/accident reporting, especially at the location they are associated with.
Background
Details of the curation of this dataset, once published, will be added to this entry.
Disclaimer
The dataset does not include a measure of credibility for the users. The stories reported by them may or may not be true. Further vetting and verification is required to confirm if the stories that they report are credible."
Front Door Motion & Brightness,Can you tell when the newspaper is delivered?,Frank,11,"Version 2,2017-05-27|Version 1,2017-05-10","home
artificial intelligence",CSV,1001 KB,Other,"1,370 views",101 downloads,3 kernels,,https://www.kaggle.com/fdraeger/frontdoormotionbrightness,"Context
These data are from a couple of sensors of my dad's house.
Content
The data are from motion sensors at the front door, which also regularly logs brightness.
The front door motion detection data also includes motions of three cats.
Data structure is pretty self explanatory. Load the csv files.
Data Files
I already added day-of-year, year, weekday and time of day to the data for easier handling.
Brightness
These time-stamped values resemble the brightness readings. They range from dark (low numbers) to bright day light (high numbers).
The brightness nightly mean values differ. The reasons are: Christmas decoration during the winter, and solar lights being set up some time in mid April this year.
Contacts
These data indicate door and window openings. They are time-stamped. The boolean value isClosed indicates wether the contact has been closed.
Motion
There is a motion sensor at the front door which indicates movement. Movement detections also are time-stamped data.
Questions
At what time of the day the newspaper is delivered?
How can I tell from the brightness value, what kind of weather it was on this day?
Additional information
Newspaper is not delivered on Sundays
Newspaper is in the mailbox by 6:30 AM
Newspaper is usually taken out of the mailbox around 7:00 AM (not on Saturdays)
There is regular front door activity - someon leaving the house - at 7:30 (not on Saturdays and Sundays)
There are three cats living at the house"
"Football Manager Data (150,000+ players)",Includes information from football manager game.,Ajinkya Jumbad,11,"Version 2,2017-07-17|Version 1,2017-07-17",,CSV,37 MB,Other,"1,095 views",157 downloads,,,https://www.kaggle.com/ajinkyablaze/football-manager-data,"If you play the game, you might understand the columns by themselves, otherwise sometime in this week, ill update the information and provide all details. (Note: The encoding for names is in UTF-8).
If you want to contribute, send me a message.
All Data is collected from the game : Football Manager 2017.
Try to predict the next messi/ronaldo ?"
Ironic Corpus,1950 sentences labeled for ironic content,Rachael Tatman,11,"Version 1,2017-07-25","languages
linguistics",CSV,472 KB,CC4,"1,579 views",115 downloads,,0 topics,https://www.kaggle.com/rtatman/ironic-corpus,"Context:
Irony in language is when a statement is produced with one meaning but the intended meaning is exactly the opposite. For instance, someone who has burned toast might serve it and say ironically “it’s a little underdone”. Automatically detecting when language is ironic is an especially difficult task in Natural Language Processing.
Content:
This dataset contains 1950 comments, which have been labeled as ironic (1) or not ironic (-1) by human annotators. The text was taken from Reddit comments.
Acknowledgements:
This dataset and analysis of it is presented in the following paper.
Wallace, B. C., Do Kook Choe, L. K., Kertz, L., & Charniak, E. (2014, April). Humans Require Context to Infer Ironic Intent (so Computers Probably do, too). In ACL (2) (pp. 512-516). Url: http://www.byronwallace.com/static/articles/wallace-irony-acl-2014.pdf
Made possible by support from the Army Research Office (ARO), grant 64481-MA / W9111F-13-1-0406 ""Sociolinguistically Informed Natural Language Processing: Automating Irony Detection""
Inspiration:
Is irony more likely when discussing certain topics?
Does ironic text tend to have more positive or more negative sentiment?
What novel features can you develop to help detect irony?"
Noun Compositionality Judgements,Is a flea market a market for fleas?,Rachael Tatman,11,"Version 1,2017-07-27","languages
linguistics",CSV,484 KB,CC4,999 views,50 downloads,,0 topics,https://www.kaggle.com/rtatman/noun-compositionality-judgements,"Context:
“Compositionality” is a concept from linguistics where the meaning of a phrase is made up of the meaning of each of its individual words. So “the red apple” refers, literally, to an apple that is red. Sometimes when you combine words, however, the meaning of the phrase isn’t the same as the combined meanings of the individual words. “The Big Apple”, for example, means “New York City”, not a literal large apple.
This dataset contains human judgements of the compositionality of common English phrases with two nouns.
Content:
This dataset contains two files: summary data of the human judgements and the annotations by individual judges. All judgements are on a scale of 0 to 5, with 0 being “not literal” and 5 being “literal”.
Acknowledgements:
This dataset was collected by Siva Reddy, Diana McCarthy and Suresh Manandhar. If you use this dataset in your work, please cite the following paper:
Reddy, S., McCarthy, D., & Manandhar, S. (2011, November). An Empirical Study on Compositionality in Compound Nouns. In IJCNLP (pp. 210-218).
Inspiration:
Is there a relationship between how frequent a word is and how often it’s used literally? (You can estimate word frequency using the corpora included in the Natural Language Toolkit, which is already ready to run in any Python kernel!)
Given this dataset, can you predict whether other compound nouns will be literal or not?
Which phrases are the most literal? Which are the least literal? Are there any patterns you notice?"
San Francisco Crime Classification,12 Years of Crime Reports Across San Francisco,Kaggle,11,"Version 1,2017-01-13",,CSV,208 MB,Other,"2,091 views",174 downloads,2 kernels,0 topics,https://www.kaggle.com/kaggle/san-francisco-crime-classification,"Context
From 1934 to 1963, San Francisco was infamous for housing some of the world's most notorious criminals on the inescapable island of Alcatraz. Today, the city is known more for its tech scene than its criminal past. But, with rising wealth inequality, housing shortages, and a proliferation of expensive digital toys riding BART to work, there is no scarcity of crime in the city by the bay. From Sunset to SOMA, and Marina to Excelsior, this dataset provides nearly 12 years of crime reports from across all of San Francisco's neighborhoods.
This dataset was featured in our completed playground competition entitled San Francisco Crime Classification. The goals of the competition were to:
predict the category of crime that occurred, given the time and location
visualize the city and crimes (see Mapping and Visualizing Violent Crime for inspiration)
Content
This dataset contains incidents derived from SFPD Crime Incident Reporting system. The data ranges from 1/1/2003 to 5/13/2015. The training set and test set rotate every week, meaning week 1,3,5,7... belong to test set, week 2,4,6,8 belong to training set. There are 9 variables:
Dates - timestamp of the crime incident
Category - category of the crime incident (only in train.csv).
Descript - detailed description of the crime incident (only in train.csv)
DayOfWeek - the day of the week
PdDistrict - name of the Police Department District
Resolution - how the crime incident was resolved (only in train.csv)
Address - the approximate street address of the crime incident
X - Longitude
Y - Latitude
Acknowledgements
This dataset is part of our completed playground competition entitled San Francisco Crime Classification. Visit the competition page if you are interested in checking out past discussions, competition leaderboard, or more details regarding the competition. If you are curious to see how your results rank compared to others', you can still make a submission at the competition submission page!
The original dataset is from SF OpenData, the central clearinghouse for data published by the City and County of San Francisco."
Electoral Integrity in 2016 US Election,How did 700+ experts perceive the integrity of the presidential election?,Harvard University,11,"Version 1,2017-02-09",politics,CSV,571 KB,CC0,"1,777 views",198 downloads,3 kernels,0 topics,https://www.kaggle.com/harvard-university/electoral-integrity,"Context
Electoral integrity refers to international standards and global norms governing the appropriate conduct of elections. These standards have been endorsed in a series of authoritative conventions, treaties, protocols, and guidelines by agencies of the international community and apply universally to all countries throughout the electoral cycle, including during the pre-electoral period, the campaign, on polling day, and in its aftermath.
Content
The Perceptions of Electoral Integrity (PEI) survey asks experts to evaluate elections according to 49 indicators, grouped into eleven categories reflecting the whole electoral cycle. The PEI dataset is designed to provide a comprehensive, systematic and reliable way to monitor the quality of elections worldwide. It includes disaggregated scores for each of the individual indicators, summary indices for the eleven dimensions of electoral integrity, and a PEI index score out of 100 to summarize the overall integrity of the election.
Acknowledgements
This study was conducted by Pippa Norris, Alessandro Nai, and Max Grömping for Harvard University's Electoral Integrity Project."
"Presidential Pardons, 1900-2017",Petitions for executive clemency granted and denied by American presidents,Department of Justice,11,"Version 2,2017-01-20|Version 1,2017-01-19",,CSV,8 KB,CC0,"2,695 views",217 downloads,12 kernels,,https://www.kaggle.com/doj/presidential-pardons,"Context
On April 23, 2014, the Department of Justice, at the behest of President Obama, announced the Clemency Initiative, inviting petitions for commutation of sentence from nonviolent offenders who, among other criteria, likely would have received substantially lower sentences if convicted of the same offenses today. As expected, the announcement resulted in a record number of petitions – including thousands of petitions involving crimes not included in the initiative, such as terrorism, murder, sex crimes, public corruption, and financial fraud.
In the federal system, commutation of sentence and pardon are different forms of executive clemency, which is a broad term that applies to the President’s constitutional power to exercise leniency toward persons who have committed federal crimes.
A commutation of sentence reduces a sentence, either totally or partially, that is then being served, but it does not change the conviction, signify innocence, or remove civil disabilities from the criminal conviction. A commutation may include remission (or release) of the financial obligations that are imposed as part of a sentence, such as payment of a fine or restitution; a remission applies only to the part of the financial obligation that has not already been paid. To be eligible to apply for commutation of sentence, a person must have reported to prison to begin serving his sentence and may not be challenging his conviction in the courts.
A pardon is an expression of the President’s forgiveness and is granted in recognition of the applicant’s acceptance of responsibility for the crime and established good conduct for a significant period of time after conviction or completion of sentence. It does not signify innocence. It does, however, remove civil disabilities – such as restrictions on the right to vote, hold state or local office, or sit on a jury – imposed because of the conviction. A person is not eligible to apply for a presidential pardon until a minimum of five years has elapsed since his release from any form of confinement imposed upon him as part of a sentence for his most recent criminal conviction.
Acknowledgements
The data was compiled and published by the Office of the Pardon Attorney. The Office of the Pardon Attorney receives and reviews petitions for all forms of executive clemency, including pardon, commutation (reduction) of sentence, remission of fine or restitution, and reprieve, initiates the necessary investigations of clemency requests, and prepares the report and recommendation of the Attorney General to the President."
"Federal Emergencies and Disasters, 1953-Present",Has the number of emergencies declared by the president risen over time?,Federal Emergency Management Agency,11,"Version 1,2017-02-19","ecology
history",CSV,6 MB,CC0,"2,310 views",311 downloads,3 kernels,0 topics,https://www.kaggle.com/fema/federal-disasters,"Context
The president can declare an emergency for any occasion or instance when the President determines federal assistance is needed. Emergency declarations supplement State and local or Indian tribal government efforts in providing emergency services, such as the protection of lives, property, public health, and safety, or to lessen or avert the threat of a catastrophe in any part of the United States. The total amount of assistance provided for in a single emergency may not exceed $5 million.
The president can declare a major disaster for any natural event, including any hurricane, tornado, storm, high water, wind-driven water, tidal wave, tsunami, earthquake, volcanic eruption, landslide, mudslide, snowstorm, or drought, or, regardless of cause, fire, flood, or explosion, that the President determines has caused damage of such severity that it is beyond the combined capabilities of state and local governments to respond. A major disaster declaration provides a wide range of federal assistance programs for individuals and public infrastructure, including funds for both emergency and permanent work.
Content
This dataset includes a record for every federal emergency or disaster declared by the President of the United States since 1953.
Acknowledgements
The disaster database was published by the Federal Emergency Management Agency with data from the National Emergency Management Information System.
Inspiration
What type of disaster is the most commonly declared by FEMA? Which disasters or emergencies have lasted the longest? What disaster was declared in the most counties or states? Has the number of disasters declared by FEMA risen or fallen over time?"
Sentiment140 dataset with 1.6 million tweets,Sentiment analysis with tweets,Μαριος Μιχαηλιδης KazAnova,11,"Version 2,2017-09-14|Version 1,2017-09-14","languages
linguistics
internet",CSV,228 MB,Other,"1,586 views",197 downloads,3 kernels,0 topics,https://www.kaggle.com/kazanova/sentiment140,"Context
This is the sentiment140 dataset. It contains 1,600,000 tweets extracted using the twitter api . The tweets have been annotated (0 = negative, 2 = neutral, 4 = positive) and they can be used to detect sentiment .
Content
It contains the following 6 fields:
target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)
ids: The id of the tweet ( 2087)
date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)
flag: The query (lyx). If there is no query, then this value is NO_QUERY.
user: the user that tweeted (robotickilldozr)
text: the text of the tweet (Lyx is cool)
Acknowledgements
The official link regarding the dataset with resources about how it was generated is here The official paper detailing the approach is here
Citation: Go, A., Bhayani, R. and Huang, L., 2009. Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford, 1(2009), p.12.
Inspiration
To detect severity from tweets. You may have a look at this."
The fight against malaria,Who is dying and being saved from this destructive disease?,toby jolly,11,"Version 1,2017-08-22",public health,CSV,7 MB,CC0,"2,443 views",361 downloads,2 kernels,0 topics,https://www.kaggle.com/teajay/the-fight-against-malaria,"Context
The data here is from the Global Health Observatory (GHO) who provide data on malaria incidence, death and prevention from around the world. I have also included malaria net distribution data the Against Malaria Foundation (AMF). The AMF has consistently been ranked as the most cost effective charity by charity evaluators Give Well - http://www.givewell.org/charities/top-charities
Content
GHO data is all in narrow format, with variables for a country in a given year being found on different rows.
GHO data (there are a number or superfluous columns):
GHO (CODE)
GHO (DISPLAY) - this is the variable being measured
GHO (URL)
PUBLISHSTATE (CODE)
PUBLISHSTATE (DISPLAY)
PUBLISHSTATE (URL)
YEAR (CODE)
YEAR (DISPLAY)
YEAR (URL)
REGION (CODE)
REGION (DISPLAY)
REGION (URL)
COUNTRY (CODE) - can be used to join this data with the AMF data
COUNTRY (DISPLAY)
COUNTRY (URL)
Display Value - this is the measured value
Low - lower confidence interval
High - higher confidence interval
Comments
AMF distribution data:
#_llins - total number of malaria nets distributed
location - the specific area that received the nets, within the target country
country - the country in which the nets were distributed
when - the period the distribution
by_whom - the organisation(s) which partnered with the AMF to perform the distribution
country_code - the country's GHO country code (this will allow joining with the GHO data)
For the current version all data was downloaded 20-08-17 The GHO data covers the years from 2000 to 2015 (not all files have data in all years) The AMF data runs from 2006 - the present.
The GHO data is taken as is from the csv (lists) available here: http://apps.who.int/gho/data/node.main.A1362?lang=en The source of the AMF's distribution data is here: https://www.againstmalaria.com/distributions.aspx - it was assembled into a single csv using Excel (mea culpa)
Inspiration
Malaria is one of the world's most devastating diseases, not least because it largely affects some of the poorest people. Over the past 15 years malaria rates and mortality have dropped (http://www.who.int/malaria/media/world-malaria-report-2016/en/), but there is still a long way to go. Understanding the data is generally one of the most important steps in solving any large problem. I'm excited to see what the Kaggle community can find out about the global trends in malaria over this period, and if we can find out anything about the impact of organisations such as the AMF."
Accidents in India,Traffic accidents in each states/u.t of India from 2001-14.,Rajanand Ilangovan / இராஜ்ஆனந்த் இளங்கோவன்,11,,india,CSV,198 KB,CC4,,,,,https://www.kaggle.com/rajanand/accidents-in-india,
Computer Network Traffic,Traffic from workstation IPs where at least half were compromised,Chris Crawford,11,"Version 1,2017-08-19",,CSV,420 KB,CC0,"4,319 views",365 downloads,3 kernels,,https://www.kaggle.com/crawford/computer-network-traffic,"Context
Computer Network Traffic Data - A ~500K CSV with summary of some real network traffic data from the past. The dataset has ~21K rows and covers 10 local workstation IPs over a three month period. Half of these local IPs were compromised at some point during this period and became members of various botnets.
Content
Each row consists of four columns:
date: yyyy-mm-dd (from 2006-07-01 through 2006-09-30)
l_ipn: local IP (coded as an integer from 0-9)
r_asn: remote ASN (an integer which identifies the remote ISP)
f: flows (count of connnections for that day)
Reports of ""odd"" activity or suspicions about a machine's behavior triggered investigations on the following days (although the machine might have been compromised earlier)
Date : IP 08-24 : 1 09-04 : 5 09-18 : 4 09-26 : 3 6
Acknowledgements
This public dataset was found on http://statweb.stanford.edu/~sabatti/data.html
Inspiration
Can you discover when a compromise has occurred by a change in the pattern of communication?"
Master's Degrees Programs (mastersportal.eu),Discovering thousands of Master's degrees worldwide!,Anas Aboureada,11,"Version 2,2017-10-02|Version 1,2017-09-09","universities and colleges
education",CSV,124 MB,CC0,"2,560 views",250 downloads,,,https://www.kaggle.com/anasfullstack/mastersportal-programs,"Context
I was searching for a master degree program in data-science when I found this awesome website mastersportal, So I just scrapped it to take my time analysing all master programs available around the world.
Content
This dataset contains 60442 different master's degree programs from 99 countries around the world.
Scrapping code
https://github.com/AnasFullStack/Masters-Portal-Scrapper"
Historical Military Battles,Conditions and results from over 600 battles fought in 1600 - 1973 AD,Aleksey Bilogur,11,"Version 1,2017-09-14","history
military
military science",CSV,658 KB,ODbL,"1,987 views",261 downloads,2 kernels,,https://www.kaggle.com/residentmario/database-of-battles,"Context
This dataset is a cleaned-up and modernized version of ""CAA Database of Battles, Version 1990"", shortnamed ""CDB90"". It contains information on over 600 battles that were fought between 1600 AD and 1973 AD. Descriptive data include battle name, date, and location; the strengths and losses on each side; identification of the victor; temporal duration of the battle; and selected environmental and tactical environment descriptors (such as type of fortifications, type of tactical scheme, weather conditions, width of front, etc.).
Content
The data contained therein is split across several files. The most important of these is battles.csv, which is lists and gives information about the battles themselves. Files marked enum describe the keys used by specific fields. Other files provide additional context.
Acknowledgements
The original version of this database was distributed by the U.S. Army Concepts Analysis Agency. The version of this dataset you see here is a cleaned-up version created by Jeffrey Arnold. This dataset, cleanup code, and source data are all available here.
Inspiration
How often were battles fought in various weather conditions?
How often did an attacker or defender achieve the element of surprise? Did it have a significant effect on the outcome?
Did prepared fortifications have a significant effect on outcomes?"
CalCOFI,Over 60 years of oceanographic data,Sohier Dane,11,"Version 1,2017-08-24|Version 2,2017-08-24","ecology
oceanography",CSV,257 MB,Other,"1,248 views",138 downloads,2 kernels,,https://www.kaggle.com/sohier/calcofi,"Context
The CalCOFI data set represents the longest (1949-present) and most complete (more than 50,000 sampling stations) time series of oceanographic and larval fish data in the world. It includes abundance data on the larvae of over 250 species of fish; larval length frequency data and egg abundance data on key commercial species; and oceanographic and plankton data. The physical, chemical, and biological data collected at regular time and space intervals quickly became valuable for documenting climatic cycles in the California Current and a range of biological responses to them. CalCOFI research drew world attention to the biological response to the dramatic Pacific-warming event in 1957-58 and introduced the term “El Niño” into the scientific literature.
The California Cooperative Oceanic Fisheries Investigations (CalCOFI) are a unique partnership of the California Department of Fish & Wildlife, NOAA Fisheries Service and Scripps Institution of Oceanography. The organization was formed in 1949 to study the ecological aspects of the sardine population collapse off California. Today our focus has shifted to the study of the marine environment off the coast of California, the management of its living resources, and monitoring the indicators of El Nino and climate change. CalCOFI conducts quarterly cruises off southern & central California, collecting a suite of hydrographic and biological data on station and underway. Data collected at depths down to 500 m include: temperature, salinity, oxygen, phosphate, silicate, nitrate and nitrite, chlorophyll, transmissometer, PAR, C14 primary productivity, phytoplankton biodiversity, zooplankton biomass, and zooplankton biodiversity.
Content
Each table has several dozen columns. Please see this page for the table details."
Large Purchases by the State of CA,All purchase orders over $5000 from 2012-2015,Sohier Dane,11,"Version 1,2017-08-30","government agencies
finance",Other,156 MB,CC0,"1,747 views",308 downloads,,0 topics,https://www.kaggle.com/sohier/large-purchases-by-the-state-of-ca,"The State Contract and Procurement Registration System (SCPRS) was established in 2003, as a centralized database of information on State contracts and purchases over $5000. eSCPRS represents the data captured in the State's eProcurement (eP) system, Bidsync, as of March 16, 2009. The data provided is an extract from that system for fiscal years 2012-2013, 2013-2014, and 2014-2015
Acknowledgements
This dataset was kindly released by the state of California. You can find the original copy here."
Handwritten Mathematical Expressions,Can you use computer vision to recognize handwritten mathematical expressions?,Rachael Tatman,11,"Version 1,2017-08-11","writing
mathematics
artificial intelligence",Other,114 MB,CC4,"2,458 views",219 downloads,,,https://www.kaggle.com/rtatman/handwritten-mathematical-expressions,"Context
If you've ever had to typeset mathematical expressions, you might have thought: wouldn’t it be great if I could just take a picture of a handwritten expression and have it be recognized automatically? This dataset has all the data you’ll need to build a system to do just that.
Description
The dataset provide more than 11,000 expressions handwritten by hundreds of writers from different countries, merging the data sets from 4 CROHME competitions. Writers were asked to copy printed expressions from a corpus of expressions. The corpus has been designed to cover the diversity proposed by the different tasks and chosen from an existing math corpus and from expressions embedded in Wikipedia pages. Different devices have been used (different digital pen technologies, white-board input device, tablet with sensible screen), thus different scales and resolutions are used. The dataset provides only the on-line signal.
In the last competition CROHME 2013 the test part is completely original and the train part is using 5 existing data sets:
MathBrush (University of Waterloo),
HAMEX (University of Nantes),
MfrDB (Czech Technical University),
ExpressMatch (University of Sao Paulo),
the KAIST data set.
In CROHME 2014 a new test set has been created with 987 new expressions and 2 new tasks has been added: isolated symbol recognition and matrix recognition. Train and test files as the evaluation scripts for these new tasks are provided. For the isolated symbol datasets, elements are extracted from full expression using the existing datasets, which also includes segmentation errors. For the matrix recognition task, 380 new expressions have been labelled and split into training and test sets.
Furthermore, 6 participants of the 2012 competition provide their recognized expressions for the 2012 test part. This data allows research on decision fusion or evaluation metrics.
Technical Details
The ink corresponding to each expression is stored in an InkML file. An InkML file mainly contains three kinds of information:
The ink: a set of traces made of points;
The symbol level ground truth: the segmentation and label information of each symbol in the expression;
The expression level ground truth: the MathML structure of the expression.
The two levels of ground truth information (at the symbol as well as at the expression level) are entered manually. Furthermore, some general information is added in the file:
The channels (here, X and Y);
The writer information (identification, handedness (left/right), age, gender, etc.), if available;
The LaTeX ground truth (without any reference to the ink and hence, easy to render);
The unique identification code of the ink (UI), etc.
The InkML format makes references between the digital ink of the expression, its segmentation into symbols and its MathML representation. Thus, the stroke segmentation of a symbol can be linked to its MathML representation.
The recognized expressions are the outputs of the recognition competitors' systems. It uses the same InkML format, but without the ink information (only segmentation, label and MathML structure).
More details available on CROHME website.
Acknowledgements
This dataset was compiled by Harold Mouchère and is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License. If you use this dataset in your work, please include the following citation:
Harold Mouchère, ICFHR 2014 CROHME: Fourth International Competition on Recognition of Online Handwritten Mathematical Expressions (CROHME-2014) ,2,ID:CROHME-2014_2,URL:http://tc11.cvc.uab.es/datasets/CROHME-2014_2
You might also like:
Handwritten math symbols dataset: Over 100 000 image samples
Arabic Handwritten Digits Dataset"
German Sentiment Analysis Toolkit,3468 German words sorted by sentiment,Rachael Tatman,11,"Version 1,2017-08-16","languages
europe
linguistics",Other,431 KB,Other,"1,549 views",112 downloads,,0 topics,https://www.kaggle.com/rtatman/german-sentiment-analysis-toolkit,"Context:
SentimentWortschatz, or SentiWS for short, is a publicly available German-language resource for sentiment analysis, opinion mining etc. It lists positive and negative polarity bearing words weighted within the interval of [-1; 1] plus their part of speech tag, and if applicable, their inflections. The current version of SentiWS (v1.8b) contains 1,650 positive and 1,818 negative words, which sum up to 15,649 positive and 15,632 negative word forms incl. their inflections, respectively. It not only contains adjectives and adverbs explicitly expressing a sentiment, but also nouns and verbs implicitly containing one.
Format:
SentiWS is organised in two utf8-encoded text files structured the following way:
| \t \t ,..., \n
where \t denotes a tab, and \n denotes a new line.
Acknowledgement:
SentiWS is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/). If you use SentiWS in your work, please cite the following paper:
R. Remus, U. Quasthoff & G. Heyer: SentiWS - a Publicly Available German-language Resource for Sentiment Analysis. In: Proceedings of the 7th International Language Ressources and Evaluation (LREC'10), 2010
This version of the data set was last updated in March 2012."
Universal Product Code Database,One million products & their UPC codes,Rachael Tatman,11,"Version 1,2017-08-19","business
supply chain
product
product management",CSV,60 MB,CC0,"2,743 views",258 downloads,,0 topics,https://www.kaggle.com/rtatman/universal-product-code-database,"Context:
“The Universal Product Code (UPC) is a barcode symbology that is widely used in the United States, Canada, United Kingdom, Australia, New Zealand, in Europe and other countries for tracking trade items in stores.
“UPC (technically refers to UPC-A) consists of 12 numeric digits, that are uniquely assigned to each trade item. Along with the related EAN barcode, the UPC is the barcode mainly used for scanning of trade items at the point of sale, per GS1 specifications.[1] UPC data structures are a component of GTINs and follow the global GS1 specification, which is based on international standards. But some retailers (clothing, furniture) do not use the GS1 system (rather other barcode symbologies or article number systems). On the other hand, some retailers use the EAN/UPC barcode symbology, but without using a GTIN (for products, brands, sold at such retailers only).”
-- Tate. (n.d.). In Wikipedia. Retrieved August 18, 2017, from https://en.wikipedia.org/wiki/Plagiarism. Text reproduced here under a CC-BY-SA 3.0 license.
Content:
This dataset contains just over 1 million UPC codes and the names of the products associated with them.
Acknowledgements:
While UPC’s themselves are not copyrightable, the brand names and trademarks in this dataset remain the property of their respective owners.
Inspiration:
Can you use this dataset to generate new product names?
Can you use this in conjunction with other datasets to disambiguate products?"
Santa Barbara Corpus of Spoken American English,"Transcribed recordings of natural, conversational speech",Rachael Tatman,11,"Version 2,2017-09-15|Version 1,2017-09-12","languages
linguistics",Other,2 GB,Other,"2,751 views",97 downloads,,0 topics,https://www.kaggle.com/rtatman/santa-barbara-corpus-of-spoken-american-english,"Context:
The Santa Barbara Corpus of Spoken American English is based on hundreds of recordings of natural speech from all over the United States, representing a wide variety of people of different regional origins, ages, occupations, and ethnic and social backgrounds. It reflects many ways that people use language in their lives: conversation, gossip, arguments, on-the-job talk, card games, city council meetings, sales pitches, classroom lectures, political speeches, bedtime stories, sermons, weddings, and more. The corpus was collected by the University of California, Santa Barbara Center for the Study of Discourse, Director John W. Du Bois (UCSB), Associate Editors: Wallace L. Chafe (UCSB), Charles Meyer (UMass, Boston), and Sandra A. Thompson (UCSB).
Each speech file is accompanied by a transcript in which phrases are time stamped with respect to the audio recording. Personal names, place names, phone numbers, etc., in the transcripts have been altered to preserve the anonymity of the speakers and their acquaintances and the audio files have been filtered to make these portions of the recordings unrecognizable. Pitch information is still recoverable from these filtered portions of the recordings, but the amplitude levels in these regions have been reduced relative to the original signal. The audio data consists of MP3 format speech files, recorded in two-channel pcm, at 22050Hz.
Contents:
This dataset contains part one of the corpus. The other three parts and additional information can be found here. The following information is included in this dataset:
Recordings: 14 recordings as .mp3 files
Transcripts: Time-aligned transcripts for all 14 recordings, in the CHAT format
Metadata: A .csv with demographic information on speakers, as well as which recordings they appear in. (Some talkers appear in more than one recording.)
Acknowledgements:
The Santa Barbara Corpus was compiled by researchers in the Linguistics Department of the University of California, Santa Barbara. The Director of the Santa Barbara Corpus is John W. Du Bois, working with Associate Editors Wallace L. Chafe and Sandra A. Thompson (all of UC Santa Barbara), and Charles Meyer (UMass, Boston). For the publication of Parts 3 and 4, the authors are John W. Du Bois and Robert Englebretson.
It is distributed here under an CC BY-ND 3.0 US license.
Inspiration:
Currently, the transcriptions are close transcriptions and include disfluencies and overlaps. Can you use NLP to convert them to broad transcriptions without this information?
Can you create a phone-aligned transcription of this dataset? You might find it helpful to use forced alignment."
Austin Bike Share Trips,Information on 649k Bike Rides Across Austin,Jacob Boysen,11,"Version 1,2017-08-10",cycling,CSV,83 MB,CC0,"2,164 views",336 downloads,7 kernels,0 topics,https://www.kaggle.com/jboysen/austin-bike,"Context:
Bike shares are becoming a popular alternative means of transportation. The City of Austin makes data available on >649k bike trips over 2013-2017.
Content:
This data includes information on bike trip start location, stop location, duration, type of bike share user. Bike station location data is also provided.
Dataset Description
Use this dataset with BigQuery
You can use Kernels to analyze, share, and discuss this data on Kaggle, but if you’re looking for real-time updates and bigger data, check out the data on BigQuery, too.
*austin_bikeshare_trips.csv*
bikeid: integer id of bike
checkout_time: HH:MM:SS, see start time for date stamp
duration_minutes: int minutes of trip duration
end_station_id: integer id of end station
end_station_name: string of end station name
month: month, integer
start_station_id: integer id of start station
start_station_name: string of start station name
start_time: YYYY-MM-DD HH:MM:SS
subscriber_type: membership typ e.g. walk up, annual, other bike share, etc
trip_id: unique trip id int
year: year of trip, int
*austin_bikeshare_stations.csv*
latitude: geospatial latitude, precision to 5 places
location: (lat, long)
longitude: geospatial longitude, precision to 5 places
name: station name, str
station_id: unique station id, int
status: station status (active, closed, moved, ACL-only)
Acknowledgements:
This dataset is available from Google Public Data.
Inspiration:
What stations are most popular? At certain times?
What are the average user trip?
Can you predict station usage to improve the ability of bike share employees to supply high-use stations?"
Severely Injured Workers,"~22k Injury Reports for US Workers, 2015-2017",Jacob Boysen,11,"Version 1,2017-09-06","healthcare
employment",CSV,11 MB,CC0,"2,491 views",391 downloads,5 kernels,0 topics,https://www.kaggle.com/jboysen/injured-workers,"Context:
Occupational Safety and Health Administration aka OSHA requires employers to report all severe work-related injuries, defined as an amputation, in-patient hospitalization, or loss of an eye. The requirement began on January 1, 2015.
Content:
This dataset provides information from those reports, including a description of the incident and the name and address of the establishment where it happened. Injuries are coded using the Occupational Injury and Illness Classification System. Data covers ~22k incidents Jan 1 2015-Feb 28 2017. 26 columns describe incident, parties involved, employer, injury sustained, and final outcome.
Acknowledgements:
This dataset was created by [OSHA](https://www.osha.gov/severeinjury/index.html ) and released to the public.
Inspiration:
Which industries have the highest rate of worker injuries? Most severe injuries?
Can you predict injuries for 2016 based on 2015 data?
In which regions are injuries most common?"
Telecom customer,customer churn prediction,abhinav,11,"Version 1,2017-08-27",,CSV,44 MB,Other,"2,530 views",383 downloads,,2 topics,https://www.kaggle.com/abhinav89/telecom-customer,"Telecom customer churn prediction
This data set consists of 100 variables and approx 100 thousand records. This data set contains different variables explaining the attributes of telecom industry and various factors considered important while dealing with customers of telecom industry. The target variable here is churn which explains whether the customer will churn or not. We can use this data set to predict the customers who would churn or who wouldn't churn depending on various variables available."
Chicago Restaurant Inspections,~154k Rows of Inspections Data,City of Chicago,11,"Version 1,2017-08-30",food and drink,CSV,176 MB,CC0,"3,147 views",314 downloads,,2 topics,https://www.kaggle.com/chicago/chi-restaurant-inspections,"Context:
Restaurant inspections ensure that food served to the public at licensed food establishments follows food safety guidelines. The Food Protection Division of the Chicago Department of Public Health (CDPH) is committed to maintaining the safety of food bought, sold, or prepared for public consumption in Chicago by carrying out science-based inspections of all retail food establishments. These inspections promote public health in areas of food safety and sanitation and prevent the occurrence of food-borne illness. CDPH's licensed, accredited sanitarians inspect retail food establishments such as restaurants, grocery stores, bakeries, convenience stores, hospitals, nursing homes, day care facilities, shelters, schools, and temporary food service events. Inspections focus on food handling practices, product temperatures, personal hygiene, facility maintenance, and pest control. All restaurants are subject to certain recurring inspections. Each year a restaurant is subject to annual inspections to ensure continued compliance with City ordinances and regulations. In addition to recurring inspections, restaurants may also be inspected in response to a complaint. Some of these recurring inspections, such as the inspection by the Buildings Department, will be scheduled, while others will not.
Generally inspections are conducted by the Health Department for sanitation and safe food handling practices, the Buildings Department to ensure the safety of the structure, and the Fire Department to ensure safe fire exits.The City's Dumpster Task Force, a collaborative effort between the Health Department and Streets and Sanitation Department, also inspects restaurants to ensure compliance with sanitation regulations.
Content:
Data includes inspection date, results, violations noted, business name and lat/lon, license# and risk. Data covers 01/02/2013-08/28/17.
Acknowledgements:
Data was collected by City of Chicago Department of Health.
Inspiration:
Can you predict restaurant closings?
Do restaurants in certain neighborhoods gather more/less violations?
Any seasonal or time anomalies in the data?"
Chicago - Citywide Payroll Data,Salaries paid to Chicago employees,City of Chicago,11,"Version 1,2017-09-13","cities
money",CSV,2 MB,CC0,"1,331 views",187 downloads,,0 topics,https://www.kaggle.com/chicago/chicago-citywide-payroll-data,"Context
This dataset contains the name, job title, department, and salary of every employee that was on the City of Chicago payroll at the time of capture in mid-2017. It provides a transparent lens into who gets paid how much and for what.
Content
This dataset provides columns for employee name, the city department they work for, their job title, and various fields describing their compensation. Most employee salaries are covered by the Annual Salary field, but some employees paid hourly are covered by a combination of Typical Hours and Hourly Rate fields.
Acknowledgements
This dataset is published as-is by the City of Chicago (here).
Inspiration
How many people do the various city agencies employ, and how much does each department spend on salary in total?
What are the most numerous job titles in civic government employment?
How do Chicago employee salaries compare against salaries of city employees in New York City? Is the difference more or less than the difference in cost of living between the two cities?"
Consumer Reviews of Amazon Products,"A list of 1,500+ reviews of Amazon products like the Kindle, Fire TV Stick, etc.",Datafiniti,11,"Version 1,2017-08-14","databases
product",CSV,18 MB,CC4,"5,837 views",911 downloads,4 kernels,,https://www.kaggle.com/datafiniti/consumer-reviews-of-amazon-products,"About This Data
This is a list of over 1,500 consumer reviews for Amazon products like the Kindle, Fire TV Stick, and more provided by Datafiniti's Product Database. The dataset includes basic product information, rating, review text, and more for each product.
What You Can Do With This Data
You can use this data to analyze Amazon’s most successful consumer electronics product launches; discover insights into consumer reviews and assist with machine learning models. E.g.:
What are the most reviewed Amazon products?
What are the initial and current number of customer reviews for each product?
How do the reviews in the first 90 days after a product launch compare to the price of the product?
How do the reviews in the first 90 days after a product launch compare to the days available for sale?
Map the keywords in the review text against the review ratings to help train sentiment models.
Data Schema
A full schema for the data is available in our support documentation.
About Datafiniti
Datafiniti provides instant access to web data. We compile data from thousands of websites to create standardized databases of business, product, and property information. Learn more.
Want More?
You can get more data like this by joining Datafiniti or requesting a demo."
"Airports, Train Stations, and Ferry Terminals",Openflight.org's database of the worlds transportation hubs,OpenFlights,11,"Version 1,2017-08-29","transport
aviation
public transport
+ 2 more...",CSV,1 MB,ODbL,"2,211 views",469 downloads,3 kernels,0 topics,https://www.kaggle.com/open-flights/airports-train-stations-and-ferry-terminals,"Context
This is a database of airports, train stations, and ferry terminals around the world. Some of the data come from public sources and some of it comes from OpenFlights.org user contributions.
Content
Airport ID Unique OpenFlights identifier for this airport.
Name Name of airport. May or may not contain the City name.
City Main city served by airport. May be spelled differently from Name.
Country Country or territory where airport is located. See countries.dat to cross-reference to ISO 3166-1 codes.
IATA 3-letter IATA code. Null if not assigned/unknown.
ICAO 4-letter ICAO code.
Null if not assigned.
Latitude Decimal degrees, usually to six significant digits. Negative is South, positive is North.
Longitude Decimal degrees, usually to six significant digits. Negative is West, positive is East.
Altitude In feet.
Timezone Hours offset from UTC. Fractional hours are expressed as decimals, eg. India is 5.5.
DST Daylight savings time. One of E (Europe), A (US/Canada), S (South America), O (Australia), Z (New Zealand), N (None) or U (Unknown). See also: Help: Time
Tz database time zone Timezone in ""tz"" (Olson) format, eg. ""America/Los_Angeles"".
Type Type of the airport. Value ""airport"" for air terminals, ""station"" for train stations, ""port"" for ferry terminals and ""unknown"" if not known.
Source Source of this data. ""OurAirports"" for data sourced from OurAirports, ""Legacy"" for old data not matched to OurAirports (mostly DAFIF), ""User"" for unverified user contributions. In airports.csv, only source=OurAirports is included.
Acknowledgements
This dataset was downloaded from Openflights.org under the Open Database license. This is an excellent resource and there is a lot more on their website, so check them out!"
New York City WiFi Hotspots,Every public WiFi hotspot in New York City.,City of New York,11,"Version 2,2017-09-01|Version 1,2017-08-31",internet,Other,1007 KB,CC0,"2,158 views",251 downloads,,0 topics,https://www.kaggle.com/new-york-city/nyc-public-wifi,"Context:
A record of every public WiFi hotspot in New York City.
Content:
The dataset consists of records for every public WiFi hotspot (ones provided by or in partnership with the city) in New York City. It contains over 2500 records overall, and is current as of August 11, 2017.
Acknowledgements:
This dataset was published as-is by the New York City Department of Information Technology & Telecommunications.
Inspiration:
Does free public WiFi tend to cluster around certain (more affluent) areas? Who are the free WiFi providers, and where do they do it? How does NYC WiFi compare to WiFi in other cities, like Buenos Aires? (https://www.kaggle.com/octaviog/buenos-aires-public-wifi-access-points)"
DeepSat (SAT-6) Airborne Dataset,"405,000 image patches each of size 28x28 and covering 6 landcover classes",Chris Crawford,11,"Version 2,2018-01-18|Version 1,2018-01-17","geography
machine learning
image data
multiclass classification",CSV,2 GB,CC0,"7,192 views",83 downloads,3 kernels,0 topics,https://www.kaggle.com/crawford/deepsat-sat6,"DeepSat SAT-6


Originally, images were extracted from the National Agriculture Imagery Program (NAIP) dataset. The NAIP dataset consists of a total of 330,000 scenes spanning the whole of the Continental United States (CONUS). The authors used the uncompressed digital Ortho quarter quad tiles (DOQQs) which are GeoTIFF images and the area corresponds to the United States Geological Survey (USGS) topographic quadrangles. The average image tiles are ~6000 pixels in width and ~7000 pixels in height, measuring around 200 megabytes each. The entire NAIP dataset for CONUS is ~65 terabytes. The imagery is acquired at a 1-m ground sample distance (GSD) with a horizontal accuracy that lies within six meters of photo-identifiable ground control points.
The images consist of 4 bands - red, green, blue and Near Infrared (NIR). In order to maintain the high variance inherent in the entire NAIP dataset, we sample image patches from a multitude of scenes (a total of 1500 image tiles) covering different landscapes like rural areas, urban areas, densely forested, mountainous terrain, small to large water bodies, agricultural areas, etc. covering the whole state of California. An image labeling tool developed as part of this study was used to manually label uniform image patches belonging to a particular landcover class.
Once labeled, 28x28 non-overlapping sliding window blocks were extracted from the uniform image patch and saved to the dataset with the corresponding label. We chose 28x28 as the window size to maintain a significantly bigger context, and at the same time not to make it as big as to drop the relative statistical properties of the target class conditional distributions within the contextual window. Care was taken to avoid interclass overlaps within a selected and labeled image patch.
Content
Each sample image is 28x28 pixels and consists of 4 bands - red, green, blue and near infrared.
The training and test labels are one-hot encoded 1x6 vectors
The six classes represent the six broad land covers which include barren land, trees, grassland, roads, buildings and water bodies.
Training and test datasets belong to disjoint set of image tiles.
Each image patch is size normalized to 28x28 pixels.
Once generated, both the training and testing datasets were randomized using a pseudo-random number generator.
CSV files
X_train_sat6.csv: 324,000 training images, 28x28 images each with 4 channels
y_train_sat6.csv: 324,000 training labels, 1x6 one-hot encoded vectors
X_test_sat6.csv: 81,000 training images, 28x28 images each with 4 channels
y_test_sat6.csv: 81,000 training labels, 1x6 one-hot encoded vectors
The original MAT file
train_x: 28x28x6x324000 uint8 (containing 400000 training samples of 28x28 images each with 4 channels)
train_y: 324,000x6 uint8 (containing 6x1 vectors having labels for the 400000 training samples)
test_x: 28x28x6x18000 uint8 (containing 100000 test samples of 28x28 images each with 4 channels)
test_y: 81,000x6 uint8 (containing 6x1 vectors having labels for the 100000 test samples)
Acknowledgements
The original MATLAB file was converted to multiple CSV files
The original SAT-4 and SAT-6 airborne datasets can be found here:
http://csc.lsu.edu/~saikat/deepsat/
Thanks to:
Saikat Basu, Robert DiBiano, Manohar Karki and Supratik Mukhopadhyay, Louisiana State University Sangram Ganguly, Bay Area Environmental Research Institute/NASA Ames Research Center Ramakrishna R. Nemani, NASA Advanced Supercomputing Division, NASA Ames Research Center"
fatstText Common Crawl,2 million word vectors trained on Common Crawl,Facebook,11,"Version 1,2018-02-13","nlp
pre-trained model",Other,1 GB,CC0,912 views,129 downloads,,3 topics,https://www.kaggle.com/facebook/fatsttext-common-crawl,"English Word Vectors from Common Crawl
About fastText
fastText is a library for efficient learning of word representations and sentence classification. One of the key features of fastText word representation is its ability to produce vectors for any words, even made-up ones. Indeed, fastText word vectors are built from vectors of substrings of characters contained in it. This allows you to build vectors even for misspelled words or concatenation of words.
About the vectors
These pre-trained vectors contain 2 million word vectors trained on Common Crawl (600B tokens).
The first line of the file contains the number of words in the vocabulary and the size of the vectors. Each line contains a word followed by its vectors, like in the default fastText text format. Each value is space separated. Words are ordered by descending frequency.
Acknowledgements
These word vectors are distributed under the Creative Commons Attribution-Share-Alike License 3.0.
P. Bojanowski*, E. Grave*, A. Joulin, T. Mikolov, Enriching Word Vectors with Subword Information
A. Joulin, E. Grave, P. Bojanowski, T. Mikolov, Bag of Tricks for Efficient Text Classification
A. Joulin, E. Grave, P. Bojanowski, M. Douze, H. Jégou, T. Mikolov, FastText.zip: Compressing text classification models

(* These authors contributed equally.)"
VGG-16,VGG-16 Pre-trained Model for Keras,Keras,11,"Version 2,2017-12-12|Version 1,2017-12-07","machine learning
pre-trained model",Other,542 MB,CC0,"2,384 views",77 downloads,5 kernels,,https://www.kaggle.com/keras/vgg16,"VGG16
Very Deep Convolutional Networks for Large-Scale Image Recognition
In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.
Authors: Karen Simonyan, Andrew Zisserman
https://arxiv.org/abs/1409.1556
VGG Architectures
What is a Pre-trained Model?
A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.
Why use a Pre-trained Model?
Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it."
ResNet-50,ResNet-50 Pre-trained Model for Keras,Keras,11,"Version 2,2017-12-12|Version 1,2017-12-06","machine learning
pre-trained model",Other,174 MB,CC0,"2,038 views",319 downloads,42 kernels,0 topics,https://www.kaggle.com/keras/resnet50,"ResNet-50
Deep Residual Learning for Image Recognition
Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity.
An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.
The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.
Authors: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun
https://arxiv.org/abs/1512.03385
Architecture visualization: http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006
What is a Pre-trained Model?
A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.
Why use a Pre-trained Model?
Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it."
CITES Wildlife Trade Database,A year in the international wildlife trade,CITES,11,"Version 1,2018-01-29","animals
environment
plants
international relations",CSV,500 KB,Other,"1,635 views",245 downloads,4 kernels,0 topics,https://www.kaggle.com/cites/cites-wildlife-trade-database,"Context
The Convention on International Trade in Endangered Species of Wild Fauna and Flora, or CITES for short, is an international treaty organization tasked with monitoring, reporting, and providing recommendations on the international species trade. CITES is a division of the IUCN, which is one of the principal international organization focused on wildlife conversation at large. It is not a part of the UN (though its reports are read closely by the UN).
CITES is one of the oldest conservation organizations in existence. Participation in CITES is voluntary, but almost every member nation in the UN (and, therefore, almost every country worldwide) participates. Countries participating in CITES are obligated to report on roughly 5000 animal species and 29000 plant species brought into or exported out of their countries, and to honor limitations placed on the international trade of these species.
Protected species are organized into three appendixes. Appendix I species are those whose trade threatens them with extinction. Two particularly famous examples of Class I species are the black rhinoceros and the African elephant, whose extremely valuable tusks are an alluring target for poachers exporting ivory abroad. There are 1200 such species. Appendix II species are those not threatened with extinction, but whose trade is nevertheless detrimental. Most species in cites, around 21000 of them, are in Appendix II. Finally, Appendix III animals are those submitted to CITES by member states as a control mechanism. There are about 170 such species, and their export or import requires permits from the submitting member state(s).
This dataset records all legal species imports and exports carried out in 2016 (and a few records from 2017) and reported to CITES. Species not on the CITES lists are not included; nor is the significant, and highly illegal, ongoing black market trading activity.
Content
This dataset contains records on every international import or export conducted with species from the CITES lists in 2016. It contains columns identifying the species, the import and export countries, and the amount and characteristics of the goods being traded (which range from live animals to skins and cadavers).
For further details on individual rows and columns refer to the metadata on the /data tab. A much more detailed description of each of the fields is available in the original CITES documentation.
Acknowledgements
This dataset was originally aggregated by CITES and made available online through this downloader tool. The CITES downloader goes back to 1975, however it is only possible to download fully international data two years at a time (or so) due to limitations in the number of rows allowed by the data exporter. If you would like data going further back, check out the downloader. Be warned, though, this data takes a long time to generate!
This data is prepared for CITES by UNEP, a division of the UN, and hence likely covered by the UN Data License.
Inspiration
What is the geospatial distribution of the international plant/animal trade?
How much export/import activity is there for well-known species, like rhinos, elephants, etcetera?
What percent of the trade is live, as opposed to animal products (ivory, skins, cadavers, etcetera)?"
WINGBEATS,MOSQUITO WINGBEAT RECORDINGS,Ilyas Potamitis,10,"Version 1,2018-01-14","animals
optics
sound technology
+ 2 more...",Other,2 GB,CC0,"1,176 views",22 downloads,7 kernels,6 topics,https://www.kaggle.com/potamitis/wingbeats,"Context
The database contains wav recordings from the same optical sensor inserted in-turn into six insectary boxes containing only one mosquito species of both sexes. As the mosquitoes fly randomly through the sensor their wingbeat partially occludes the light from the transmitter to the receiver. The light fluctuation recorded follows the rhythm of the wingbeat. Insect Biometrics, in the context of our work, is a measurable behavioral characteristic of flying insects. Biometric identifiers are related to the shape of the body (main body size, wing shape, wingbeat frequency, pattern movement of the wings). Biometric identification methods use biometric characteristics or traits to verify species/sex identities when insects access endpoint traps following a bait.
Content
• 279,566 wingbeat recordings correctly labeled
• 6 mosquito species (Ae. aegypti, Ae. albopictus, An. arabiensis, An. gambiae, Cu. pipiens, Cu. quinquefasciatus)
• 3 genera of mosquito species (Aedes, Anopheles, Culex)
Acknowledgements
The data have been recorded at the premises of Biogents, Regensburg, Germany (https://www.biogents.com/) and with the help of IRIDEON SA, Spain (http://irideon.eu/ ). The data have been recorded using the device published in:
Potamitis I. and Rigakis I., ""Large Aperture Optoelectronic Devices to Record and Time-Stamp Insects’ Wingbeats,"" in IEEE Sensors Journal, vol. 16, no. 15, pp. 6053-6061, Aug.1, 2016. doi: 10.1109/JSEN.2016.2574762
The REMOSIS project that supported the creation of the database has received funding from the European Union’s Horizon 2020 research and innovation program under grant agreement No 691131.
We gratefully acknowledge the support of NVIDIA Corporation with the donation of a TITAN-X GPU used for training the deep learning networks used to classify mosquitoes’ spectra.
Inspiration
The point of having such recordings is to eventually embed optoelectronic sensors in automatic traps that will report counts, species and sex identity of captured mosquitoes. All species of this dataset can be dangerous as they are potential vectors of pathogens that cause serious illnesses. A widespread network of traps for insects of economic importance such as fruit flies and of hygienic importance such as mosquitoes allows the automatic creation of spatiotemporal maps and cuts down significantly the manual cost of visiting the traps. The creation of historical data can lead to the prediction of outbreaks and risk assessment in general.
We provide code to read the data and extract the power spectral density signature of each wingbeat. We also extract Mel-scaled, filter-bank features. How about wavelets and time-varying autoregressive models? The starter code using top-tier shallow classifiers achieves a mean accuracy of 81-84%. Deep-learning performs better. Can you classify genus, perform clustering, apply transfer learning to spectral data?
Come aboard and help humanity against killer mosquitoes!"
SkillCraft-StarCraft,Classifying Starcraft 2 league-level performance,Dan Ofer,10,"Version 2,2017-12-27|Version 1,2017-12-27","games and toys
video games
sports
internet",CSV,480 KB,CC4,654 views,75 downloads,,0 topics,https://www.kaggle.com/danofer/skillcraft,"Context
Dataset of Starcraft 2 games, played in different leagues/levels.
Content
Screen movements aggregated into screen-fixations. -- Time is recorded in terms of timestamps in the StarCraft 2 replay file. When the game is played on 'faster', 1 real-time second is equivalent to roughly 88.5 timestamps.
Attribute Information:
GameID: Unique ID number for each game (integer)
LeagueIndex: Bronze, Silver, Gold, Platinum, Diamond, Master, GrandMaster, and Professional leagues coded 1-8 (Ordinal)
Age: Age of each player (integer)
HoursPerWeek: Reported hours spent playing per week (integer)
TotalHours: Reported total hours spent playing (integer)
APM: Action per minute (continuous)
SelectByHotkeys: Number of unit or building selections made using hotkeys per timestamp (continuous)
AssignToHotkeys: Number of units or buildings assigned to hotkeys per timestamp (continuous)
UniqueHotkeys: Number of unique hotkeys used per timestamp (continuous)
MinimapAttacks: Number of attack actions on minimap per timestamp (continuous)
MinimapRightClicks: number of right-clicks on minimap per timestamp (continuous)
NumberOfPACs: Number of PACs per timestamp (continuous)
GapBetweenPACs: Mean duration in milliseconds between PACs (continuous)
ActionLatency: Mean latency from the onset of a PACs to their first action in milliseconds (continuous)
ActionsInPAC: Mean number of actions within each PAC (continuous)
TotalMapExplored: The number of 24x24 game coordinate grids viewed by the player per timestamp (continuous)
WorkersMade: Number of SCVs, drones, and probes trained per timestamp (continuous)
UniqueUnitsMade: Unique unites made per timestamp (continuous)
ComplexUnitsMade: Number of ghosts, infestors, and high templars trained per timestamp (continuous)
ComplexAbilitiesUsed: Abilities requiring specific targeting instructions used per timestamp (continuous)
Acknowledgements
Source: 1. Thompson JJ, Blair MR, Chen L, Henrey AJ (2013) Video Game Telemetry as a Critical Tool in the Study of Complex Skill Learning. PLoS ONE 8(9): e75129. [Web Link] -- Results: -- Skip league conditional inference forest classification (Bronze-Gold;Silver-Platinum;Gold-Diamond;Platinum-Masters;Diamond-Professional) showed changing patterns of variable importance with skill.
http://archive.ics.uci.edu/ml/datasets/SkillCraft1+Master+Table+Dataset
Inspiration
Ordinal Classification / regression model to determine League Index (""LeagueIndex"")
Suggest additional features to gather and analyze for predicting leagues/performance.
Are there features which do not increase/decrease linearly as we go up in the leagues?"
Overwatch Game Records,"One player, thousands of games, stats meticulously recorded!",Myles O'Neill,10,"Version 1,2018-01-05","games and toys
video games",CSV,693 KB,CC0,"1,382 views",143 downloads,2 kernels,0 topics,https://www.kaggle.com/mylesoneill/overwatch-game-records,"Overwatch is a team-based multiplayer online first-person shooter video game developed and published by Blizzard Entertainment. It was released in May 2016 for Windows, PlayStation 4, and Xbox One. Overwatch assigns players into two teams of six, with each player selecting from a roster of over 20 characters, known in-game as ""heroes"", each with a unique style of play, whose roles are divided into four general categories: Offense, Defense, Tank, and Support. Players on a team work together to secure and defend control points on a map or escort a payload across the map in a limited amount of time.
I discovered this dataset on the Overwatch Subreddit here: https://www.reddit.com/r/Overwatch/comments/7o8hmg/my_friend_has_recorded_every_game_hes_played/
It represents a ridiculous amount of effort in terms of manually recording game results. This data, whilst in some places incomplete, gives an unprecedented view into the experience of a single overwatch player over the course of years of gameplay. From it you can track the ups and downs, shifts in hero preference and all sorts of other exciting in game trends.
Thanks to JustWingIt for their amazing collecting this data.
I cleaned the data a little and put it into a single CSV."
US Public Assistance for Women and Children,"Where does it come from, who spends it, who gets it.",JohnM,10,"Version 2,2018-01-19|Version 1,2018-01-18","politics
demographics
economics",Other,741 KB,Other,654 views,78 downloads,2 kernels,,https://www.kaggle.com/jpmiller/publicassistance,"Context
This dataset focuses on public assistance in the United States with initial coverage of the WIC Program. The program is formally known as the Special Supplemental Nutrition Program for Women, Infants, and Children (WIC). The program allocates Federal and State funds to help low-income women and children up to age five who are at nutritional risk. Funds are used to provide supplemental foods, baby formula, health care, and nutrition education.
Content
Files include participation data and spending for state WIC programs, and poverty data for each state. Data is for fiscal years 2013-2016, which is actually October 2012 through September 2016.
Motivation
My original purpose here is two-fold:
Explore various aspects of US Public Assistance. Show trends over recent years and better understand differences across state agencies. Although the federal government sponsors the program and provides funding, program are administered at the state level and can widely vary. Indian nations (native Americans) also administer their own programs.
Share with the Kaggle Community the joy - and pain - of working with government data. Data is often spread across numerous agency sites and comes in a variety of formats. Often the data is provided in Excel, with the files consisting of multiple tabs. Also, files are formatted as reports and contain aggregated data (sums, averages, etc.) along with base data.
Additional Content Ideas
The dataset can benefit greatly from additional content. Economics, additional demographics, administrative costs and more. I'd like to eventually explore the money trail from taxes and corporate subsidies, through the government agencies, and on to program participants. All community ideas are welcome!"
"freeCodeCamp Gitter Chat, 2015-2017",Three years and 5 million posts in freeCodeCamp chat,freeCodeCamp,10,"Version 3,2018-01-31|Version 2,2018-01-31|Version 1,2018-01-06","social groups
education
internet",{}JSON,719 MB,Other,"1,049 views",46 downloads,3 kernels,,https://www.kaggle.com/free-code-camp/all-posts-public-main-chatroom,"Context
freeCodeCamp is a web-based non-profit organization and learning platform which teaches programming newcomers how to code. It was founded by Quincy Larson in 2014, who in a 2015 interview stated that ""freeCodeCamp is my effort to correct the extremely inefficient and circuitous way I learned to code...all those things that made learning to code a nightmare to me are things that we are trying to fix with freeCodeCamp."" The original curriculum took approximately 800 hours to complete; today, after several refreshes and additions, there is over 2000 hours of learner's content on the site.
freeCodeCamp also provides several helpful secondary resources for learners. One of them is a freeCodeCamp Gitter chatroom. Gitter is an open-source instant messaging service that lets users share thoughts and ideas with one another. This dataset is a record of activity from this /freeCodeCamp Gitter chatroom, containing posts from students, bots, moderators, and contributors between December 31, 2014 and December 9, 2017.
Content
The data includes the usernames, screen names, timestamps, post content, and metadata of every post made to /freeCodeCamp in the aforementioned three year period. This comes out to nearly 5 million records overall.
The data comes in the form of a set of three JSON files, each named freecodecamp_casual_chatroom_[01/02/03].json. The three files make a continuous dataset containing all posts sent during the aforementioned period, but note that they overlap in a few days. The three files were extracted on 01-06-2016, 09-03-2017, and 12-12-2017, respectively. The included convert.py file was used to concatenate these files into a unified CSV file, freecodecamp_casual_chatroom.csv.
Some preliminary analyses using this dataset can be found at the Github repository for the freeCodeCamp's Open Data Initiative. For more details on specific elements of the dataset refer to the column metadata tab, or to the detailed documentation provided in the Gitter API Documentation.
Acknowledgements
The datasets are a contribution from freeCodeCamp as part of the freeCodeCamp's Open Data Initiative. More information about the rationale of this initiative can be found on this Medium article.
This dataset was extracted using Python code over the Gitter API. All the files were prepared by Evaristo Caraballo (GitHub: @evaristoc).
Records are not anonymised or modified and are presented ""as they are"".
Thanks to freeCodeCamp team, specially to Quincy Larson for supporting the initiative. Thanks to all freeCodeCamp students who kindly allowed to share their personal progress and to Gitter for making these data available.
If you have questions about this dataset, please contact quincy@freecodecamp.com or get in touch with us through https://gitter.im/FreeCodeCamp/DataScience (Gitter registration might be required).
Inspiration
This dataset presents three years of developer chat about web technologies. Can you use it to trace the rise and fall over certain technologies, like Angular and React, over time?
What do programming newcomers tend to get stuck on the most? Are there any canned responses that are particularly common in the freeCodeCamp chatroom?
What other interesting insights into programmer culture can you glean from examining this dataset?"
Protein Data Set,Protein Sequance Data set,SHAHIR,10,"Version 1,2018-02-03","healthcare
biology
multiclass classification",CSV,27 MB,ODbL,"1,603 views",161 downloads,,0 topics,https://www.kaggle.com/shahir/protein-data-set,"Context
This is a protein data set retrieved from RCS PDB.
The PDB archive is a repository of atomic coordinates and other information describing proteins and other important biological macromolecules. Structural biologists use methods such as X-ray crystallography, NMR spectroscopy, and cryo-electron microscopy to determine the location of each atom relative to each other in the molecule. They then deposit this information, which is then annotated and publicly released into the archive by the wwPDB.
The constantly-growing PDB is a reflection of the research that is happening in laboratories across the world. This can make it both exciting and challenging to use the database in research and education. Structures are available for many of the proteins and nucleic acids involved in the central processes of life, so you can go to the PDB archive to find structures for ribosomes, oncogenes, drug targets, and even whole viruses. However, it can be a challenge to find the information that you need, since the PDB archives so many different structures. You will often find multiple structures for a given molecule, or partial structures, or structures that have been modified or inactivated from their native form.
Content
There are two data sets. Both are arranged on ""structureId"" of the protein
pdb_data_no_dups.csv :- Protein data set deatils of classification, extraction methods, etc. Containing 141401 instances and 14 attributes.
data_seq :- Protein sequence information. Containing 467304 instances and 5 attributes.
Acknowledgements
Original data set down loaded from http://www.rcsb.org/pdb/
Inspiration
Protein data base helped the life science community to study about different diseases and come with new drugs and solution that help the human survival."
Quality Prediction in a Mining Process,Explore real industrial data and help manufacturing plants to be more efficient,EduardoMagalhãesOliveira,10,"Version 1,2017-12-07","time series
industry
manufacturing
regression analysis",CSV,51 MB,CC0,960 views,145 downloads,,0 topics,https://www.kaggle.com/edumagalhaes/quality-prediction-in-a-mining-process,"Context
It is not always easy to find databases from real world manufacturing plants, specially mining plants. So, I would like to share this database with the community, which comes from one of the most important parts of a mining process: a flotation plant!
The main goal is to use this data to predict how much impurity is in the ore concentrate. As this impurity is measured every hour, if we can predict how much silica (impurity) is in the ore concentrate, we can help the engineers, giving them early information to take actions (empowering!). Hence, they will be able to take corrective actions in advance (reduce impurity, if it is the case) and also help the environment (reducing the amount of ore that goes to tailings as you reduce silica in the ore concentrate).
Content
The first column shows time and date range (from march of 2017 until september of 2017). Some columns were sampled every 20 second. Others were sampled on a hourly base.
The second and third columns are quality measures of the iron ore pulp right before it is fed into the flotation plant. Column 4 until column 8 are the most important variables that impact in the ore quality in the end of the process. From column 9 until column 22, we can see process data (level and air flow inside the flotation columns, which also impact in ore quality. The last two columns are the final iron ore pulp quality measurement from the lab. Target is to predict the last column, which is the % of silica in the iron ore concentrate.
Inspiration
I have been working in this dataset for at least six months and would like to see if the community can help to answer the following questions:
Is it possible to predict % Silica Concentrate every minute?
How many steps (hours) ahead can we predict % Silica in Concentrate? This would help engineers to act in predictive and optimized way, mitigatin the % of iron that could have gone to tailings.
Is it possible to predict % Silica in Concentrate whitout using % Iron Concentrate column (as they are highly correlated)?"
Style Color Images,Brand and Product Recognition,Olga Belitskaya,10,"Version 3,2017-12-21|Version 2,2017-12-20|Version 1,2017-12-19","photography
clothing
deep learning
+ 2 more...",Other,49 MB,Other,"1,706 views",164 downloads,4 kernels,,https://www.kaggle.com/olgabelitskaya/style-color-images,"History
I have made the database of photos sorted by products and brands. Screenshots were performed only on official brand websites.
Content
The main dataset (style.zip) is 894 color images (150x150x3) with 7 brands and 10 products, and the file with labels style.csv. Photo files are in the .png format and the labels are integers and values.
The file StyleColorImages.h5 consists of preprocessing images of this set: image tensors and targets (labels).
Acknowledgements
I have published the data for absolutely free using by any site visitor. But this database contains the names of famous brands, so it can not be used for commercial purposes.
Usage
Classification, image recognition and colorizing, etc. in a case of a small number of images are useful exercises. The main question we can try to answer with the help of the data is whether the algorithms can recognize the unique design style well enough. To facilitate the task, I chose the most easily recognizable brands with a bright style.
The example of usage
Improvement
There are lots of ways for improving this set and the machine learning algorithms applying to it. At first, it needs to increase the number of photos."
Porto Seguro public kernel results,private vs. public leaderboard results,Andy Harless,10,"Version 2,2017-12-02|Version 1,2017-12-02",,CSV,8 KB,CC0,234 views,7 downloads,2 kernels,0 topics,https://www.kaggle.com/aharless/porto-seguro-public-kernel-results,"Context
Kernels from Porto Seguro. Some are more overfit than others.
Content
Copied by hand from Kaggle. (May contain errors, obviously.) Each kernel is identified by a link to the most recent version that has a verified score matching what is in the Kernels tab. (Notebooks are linked to the Code tab, since that's where the scores are.) In many cases this is the most recent version, so if the kernel is subsequently revised, the link may be wrong (since it will then point to the subsequent most recent version, which may not be scored, or its score may not match the Kernels tab). Most of the fields should be self-explanatory, except ""adjusted"", which adjusts the public-private difference by subtracting the median difference over the whole leaderboard. (Private scores are typically higher, presumably because the cases in private portion of the test data were easier to predict, so an ""adjusted"" value of zero represents a kernel that wouldn't have gained or lost much in the shake-up.)
Acknowledgements
I would like to thank Kaggle, Porto Seguro, and the authors of the kernels.
Inspiration
What factors might help predict how much better or worse a kernel will do on the private leaderboard than on the public? It's one competition: just a start, but you've got to start somewhere."
Yellow Pages of Pakistan,Information of Local Business of Pakistan,Mukarram Pasha,10,"Version 1,2017-12-13","information
business
product",CSV,9 MB,ODbL,805 views,31 downloads,,0 topics,https://www.kaggle.com/mpasha96/yellow-pages-of-pakistan,"Context
I created this dataset to enable everyone to explore local businesses of Pakistan. This dataset might help the local community in gathering information of local businesses. This might also help in local economic development of Pakistan by bridging traders and manufacturers.
Content
Geography: Pakistan
Time period: 1990-2017
Dataset: The dataset contains information of approx 67000 businesses in Pakistan (~5000 in each csv file)
Features: The dataset has total 7 columns - Business Name - Contact Name - Telephone - Website - Services (Description of types of products/services provided by the business) - Address - City
Acknowledgements
This Dataset was created by scraping this website. I wrote the script in Python using BeautifulSoup Library. Link to script: https://tinyurl.com/ybb4bdky
Inspiration
A lot of questions can be answered and analysis can be done using this dataset. Few interesting ideas I can think of are : - Applying NLP techniques on Services column to extract business category - Clustering of categories of business according to cities"
FIFA worldcup 2018 Dataset,All participating teams history,Nuggs,10,"Version 1,2017-12-17","world
russia
sports
technology forecasting",CSV,3 KB,CC4,"1,618 views",179 downloads,,,https://www.kaggle.com/ahmedelnaggar/fifa-worldcup-2018-dataset,"Context: Well, we are a head of the largest football event(Soccer, sorry for the American fellows) worldwide, as excited as we were. Well, at least some of us were, by the draw, here comes the data. History of all previous matches, scores and titles of all participating teams to help us predict who will qualify and may even predict who will win. So, lets get our algorithms going.
Content: The dataset contains 32 entries(of the 32 participating teams of course), each team will have 3 matches in the group stage, so each match is mentioned vs whom, the history between those 2 teams with wins minus losses, let's say Brazil has beaten Argentina 14 times, Argentina won 12 and there were 3 draws, so that will be +2 for brazil and -2 for Argentina, the same goes for goals scored. You will find some entries with N/A and some with zeros, so what is the difference? The zeros mean that the 2 teams evened out, while N/As means that they have no previous history together and they have never faced each other. So, no data available. The matches history is up to 2012-2014. So, there is a couple of years missing here, and the FIFA rank is up to date, which will be updated every month till we get there.
Acknowledgments: Most of the data is pulled from FIFA website except for the matches and scores history, they were pulled manually from various credible sources.
Inspiration: Well, I think this data can help us predict who will head the groups and who comes second, then may be we can progress through round of 16....final. You can ask for more data and I'll be happy to search and update it.
If you like the data or find it useful enough, don't forget the upvote ;)"
SpaceX Launch Data,"Launch, payload, and outcome information for SpaceX missions",Steven Coleman,10,"Version 3,2018-02-12|Version 2,2018-02-02|Version 1,2018-01-22","space
spaceflight",CSV,6 KB,CC3,"1,753 views",259 downloads,,,https://www.kaggle.com/scoleman/spacex-launch-data,"Context
SpaceX designs, manufactures and launches advanced rockets and spacecraft. The company was founded in 2002 to revolutionize space technology, with the ultimate goal of enabling people to live on other planets - SpaceX
Content
The dataset contains mission information for rocket launches conducted by SpaceX (Space Exploration Technologies Corp).
Acknowledgements
Data was obtained via Wikipedia's entry for Falcon 9 and Falcon Heavy launches.
Inspiration
Do you anticipate an increase in launches with the introduction of the Falcon Heavy? How has launch rate increased over time? Do you predict a shift in payload orbits for upcoming launches? How has the customer diversity changed over the years?"
Shanghai Car License Plate Auction Price,Time-series data set (2012-2018),Bo Ju,10,"Version 2,2018-01-25|Version 1,2017-12-19","china
time series",CSV,6 KB,CC0,"1,039 views",127 downloads,,0 topics,https://www.kaggle.com/bogof666/shanghai-car-license-plate-auction-price,"Context
Shanghai uses an auction system to sell a limited number of license plates to fossil-fuel car buyers every month. The average price of this license plate is about $13,000 and it is often referred to as ""the most expensive piece of metal in the world."" So, our goal is to predict the avg price or the lowest price for the next month. This Data set will be updated every month constantly. Have fun!
Content
This data set is gathered by myself with the aid of search engine.
Inspiration
This data set could be used in your first toy example project. Learning algorithms like RNN+LSTM are well fitted into this time-series prediction problem. So, just have fun!"
JCPenney products,"20,000 product listings from JCPenney",PromptCloud,10,"Version 1,2017-09-15",internet,CSV,23 MB,CC4,"3,274 views",454 downloads,3 kernels,,https://www.kaggle.com/PromptCloudHQ/all-jc-penny-products,"Context
This is a pre-crawled dataset, taken as subset of a bigger dataset (more than 3.7 million products) that was created by extracting data from jcpenney.com, a well known retailer.
Content
This dataset has following fields:
sku
name_title
description
list_price
sale_price
category
category_tree
average_product_rating
product_url
product_image_urls
brand
total_number_reviews
reviews
Acknowledgements
This dataset was created by PromptCloud's in-house web-crawling service.
Inspiration
Analyses of list price, sale price, rating and reviews can be performed."
Japanese-English Bilingual Corpus,Japanese-English Bilingual Corpus of Wikipedia's Kyoto Articles,Team AI,10,"Version 3,2017-09-15|Version 2,2017-09-15|Version 1,2017-09-15","languages
linguistics
artificial intelligence",Other,357 MB,Other,"1,716 views",119 downloads,4 kernels,,https://www.kaggle.com/team-ai/japaneseenglish-bilingual-corpus,"Background
NLP is a hot topic currently! Team AI really want's to leverage the NLP research and this an attempt for all the NLP researchers to explore exciting insights from bilingual data
The Japanese-English Bilingual Corpus of Wikipedia's Kyoto Articles” aims mainly at supporting research and development relevant to high-performance multilingual machine translation, information extraction, and other language processing technologies.
Unique Features
A precise and large-scale corpus containing about 500,000 pairs of manually-translated sentences. Can be exploited for research and development of high-performance multilingual machine translation, information extraction, and so on.
The three-step translation process (primary translation -> secondary translation to improve fluency -> final check for technical terms) has been clearly recorded. Enables observation of how translations have been elaborated so it can be applied for uses such as research and development relevant to translation aids and error analysis of human translation.
Translated articles concern Kyoto and other topics such as traditional Japanese culture, religion, and history. Can also be utilized for tourist information translation or to create glossaries for travel guides.
The Japanese-English Bilingual Kyoto Lexicon is also available. This lexicon was created by extracting the Japanese-English word pairs from this corpus.
Sample
One Wikipedia article is stored as one XML file in this corpus, and the corpus contains 14,111 files in total.
The following is a short quotation from a corpus file titled “Ryoan-ji Temple”. Each tag has different implications. For example:
<j>Original Japanese sentence<j> <e type=""trans"" ver=""1"">Primary translation</e> <e type=""trans"" ver=""2"">Secondary translation</e> <e type=""check"" ver=""1"">Final translation</e> <cmt>Comment added by translators</cmt>
Categories
The files have been divided into 15 categories: school, railway, family, building, Shinto, person name, geographical name, culture, road, Buddhism, literature, title, history, shrines and temples, and emperor (Click the link to view a sample file for each category).
Github
https://github.com/venali/BilingualCorpus
Explains how to load the corpus
Acknowledgements
The National Institute of Information and Communications Technology (NICT) has created this corpus by manually translating Japanese Wikipedia articles (related to Kyoto) into English.
Licence
Use and/or redistribution of the Japanese-English Bilingual Corpus of Wikipedia's Kyoto Articles and the Japanese-English Bilingual Kyoto Lexicon is permitted under the conditions of Creative Commons Attribution-Share-Alike License 3.0. Details can be found at http://creativecommons.org/licenses/by-sa/3.0/.
Link to web
https://alaginrc.nict.go.jp/WikiCorpus/index_E.html"
2014 New York City Taxi Trips,Information on Taxi trips in New York City in 2014,Kenton W. Murray,10,"Version 1,2017-07-21",,CSV,489 MB,Other,"3,346 views",389 downloads,,,https://www.kaggle.com/kentonnlp/2014-new-york-city-taxi-trips,"Context
This is data on New York City Taxi Cab trips. It should be useful for adding a lot more training data for the ""New York City Taxi Trip Duration"". The data I am uploading is for 2014. The training data for the competition is for 2016. Hopefully, the underlying data should be very similar over the two years. Compressed, all of the data is over 5 GB, which is more than 10x the allowed data size of Kaggle datasets. This represents a subset of the total data.
Content
The data is in a CSV format with the following fields. It was collected both from driver input and from the GPS coordinates of the cab. It is downloaded from New York City's Open Data website.
vendor_id pickup_datetime dropoff_datetime passenger_count trip_distance pickup_longitude pickup_latitude store_and_fwd_flag dropoff_longitude dropoff_latitude payment_type fare_amount mta_tax tip_amount tolls_amount total_amount imp_surcharge extra rate_code
In the original dataset published by NYC, there is over 165 million trips. This is only 15 million. It was selected as the first 15 million available records of the year.
Acknowledgements
This data comes from the City of New York, who have been leaders in making data available to the public. This comes from their NYC Open Data website (https://opendata.cityofnewyork.us/)
Inspiration
The inspiration is the Taxi competition: https://www.kaggle.com/c/nyc-taxi-trip-duration"
FOREX: EURUSD dataset,"3 years of winning trades in EURUSD 4H, 99 features for operation , make $$$",Rodrigo Salas,10,"Version 1,2017-09-05",,CSV,3 MB,CC0,"2,847 views",246 downloads,,,https://www.kaggle.com/rsalaschile/forex-eurusd-dataset,"Context
Forex is the largest market in the world, predicting the movement of prices is not a simple task, this dataset pretends to be the gateway for people who want to conduct trading using machine learning.
Content
This dataset contains 4479 simulated winning transactions (real data, fictitious money) (3 years 201408-201708) with buy transactions (2771 operations 50.7%) and sell (2208 transactions, 49.3%), to generate this data a script of metatrader was used, operations were performed in time frame 4Hour and fixed stop loss and take profits of 50 pips (4 digits) were used to determine if the operation is winning. Each operation contains a set of classic technical indicators like rsi, mom, bb, emas, etc. (last 24 hours)
Acknowledgements
Thanks to Kaggle for giving me the opportunity to share my passion for machine learning. My profile: https://www.linkedin.com/in/rsx2010/
Inspiration
The problem of predicting price movement is reduced with this dataset to a classification problem:
""use the variables rsi1 to dayOfWeek to predict the type of correct operation to be performed (field=tipo)""
tipo = 0 ==> Operation buy
tipo= 1 ==> Operation = sell:
Good luck
Rodrigo Salas Vallet-Cendre.
rasvc@hotmail.com"
Predict Mortality/Death Rate.,770k records & 121 variables of unit level survey data collected from 9 States.,Rajanand Ilangovan / இராஜ்ஆனந்த் இளங்கோவன்,10,"Version 2,2017-08-09|Version 1,2017-08-08","public health
health",CSV,305 MB,CC4,"1,804 views",283 downloads,,,https://www.kaggle.com/rajanand/mortality,"Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks.
Context:
*Annual Health Survey : Mortality Schedule *
This unit level dataset contains the details relating to death occurred to usual residents of sample household during the reference period and it includes information on sex of deceased, date of death, age at death, registration of death and source of medical attention received before death. For infant deaths, data related to symptoms preceding death is also provided. Mortality Schedule also includes information on various determinants of maternal mortality viz. case of deaths associated with pregnancy, information on factors leading/ contributing to death, symptoms preceding death, time between onset of complications and death, etc.
There are total of 770k observations and 121 variables in this dataset.
Survey:
Base line survey - 2010-11 (4.14 million households in the sample) 1st update - 2011-12 (4.28 million households in the sample) 2nd update - 2012-13 (4.32 million households in the sample)
The survey was conducted in the below 9 states.
A. Empowered Action Group (EAG) States
Uttarakhand (05)
Rajasthan (08)
Uttar Pradesh (09)
Bihar (10)
Jharkhand (20)
Odisha (21)
Chhattisgarh (22)
Madhya Pradesh (23)
B. Assam. (18)
These nine states, which account for about 48 percent of the total population, 59 percent of Births, 70 percent of Infant Deaths, 75 percent of Under 5 Deaths and 62 percent of Maternal Deaths in the country, are the high focus States in view of their relatively higher fertility and mortality.
Content:
The files contains the below columns.
Variable Names:
id
m_id
client_m_id
hl_id
house_no
house_hold_no
state
district
rural
stratum_code
psu_id
m_serial_no
deceased_sex
date_of_death
month_of_death
year_of_death
age_of_death_below_one_month
age_of_death_below_eleven_month
age_of_death_above_one_year
treatment_source
place_of_death
is_death_reg
is_death_certificate_received
serial_num_of_infant_mother
order_of_birth
death_symptoms
is_death_associated_with_pregnan
death_period
months_of_pregnancy
factors_contributing_death
factors_contributing_death_2
symptoms_of_death
time_between_onset_of_complicati
nearest_medical_facility
m_expall_status
field38
hh_id
client_hh_id
currently_dead_or_out_migrated
hh_serial_no
sex
usual_residance
relation_to_head
member_identity
father_serial_no
mother_serial_no
date_of_birth
month_of_birth
year_of_birth
age
religion
social_group_code
marital_status
date_of_marriage
month_of_marriage
year_of_marriage
currently_attending_school
reason_for_not_attending_school
highest_qualification
occupation_status
disability_status
injury_treatment_type
illness_type
symptoms_pertaining_illness
sought_medical_care
diagnosed_for
diagnosis_source
regular_treatment
regular_treatment_source
chew
smoke
alcohol
status
hh_expall_status
client_hl_id
serial_no
building_no
house_status
house_structure
owner_status
drinking_water_source
is_water_filter
water_filteration
toilet_used
is_toilet_shared
household_have_electricity
lighting_source
cooking_fuel
no_of_dwelling_rooms
kitchen_availability
is_radio
is_television
is_computer
is_telephone
is_washing_machine
is_refrigerator
is_sewing_machine
is_bicycle
is_scooter
is_car
is_tractor
is_water_pump
cart
land_possessed
hl_expall_status
fid
isdeadmigrated
residancial_status
iscoveredbyhealthscheme
healthscheme_1
healthscheme_2
housestatus
householdstatus
isheadchanged
fidh
fidx
as
wt
x
schedule_id
year
File content: Mortality_data_dictionary.xlsx : This data dictionary excel work book has the detailed information about each and every column and codes used in the data.
Acknowledgements
Department of Health and Family Welfare, Govt. of India has published this dataset in Open Govt Data Platform India portal under Govt. Open Data License - India."
GloVe: Global Vectors for Word Representation,Pre-trained word vectors from Twitter,joshkyh,10,"Version 2,2017-09-19|Version 1,2017-09-18","languages
linguistics
twitter",Other,246 MB,Other,997 views,58 downloads,3 kernels,0 topics,https://www.kaggle.com/joshkyh/glove-twitter,"The below information is from the project page: https://nlp.stanford.edu/projects/glove/
Context
GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.
Content
Due to size constraints, only the 25 dimension version is uploaded. Please visit the project page for GloVe of other dimensions.
This dataset (https://www.kaggle.com/rtatman/glove-global-vectors-for-word-representation) contains GloVe extracted from Wikipedia 2014 + Gigaword 5.
Nearest neighbors The Euclidean distance (or cosine similarity) between two word vectors provides an effective method for measuring the linguistic or semantic similarity of the corresponding words. Sometimes, the nearest neighbors according to this metric reveal rare but relevant words that lie outside an average human's vocabulary.
Linear substructures The similarity metrics used for nearest neighbor evaluations produce a single scalar that quantifies the relatedness of two words. This simplicity can be problematic since two given words almost always exhibit more intricate relationships than can be captured by a single number. For example, man may be regarded as similar to woman in that both words describe human beings; on the other hand, the two words are often considered opposites since they highlight a primary axis along which humans differ from one another.
In order to capture in a quantitative way the nuance necessary to distinguish man from woman, it is necessary for a model to associate more than a single number to the word pair. A natural and simple candidate for an enlarged set of discriminative numbers is the vector difference between the two word vectors. GloVe is designed in order that such vector differences capture as much as possible the meaning specified by the juxtaposition of two words.
Acknowledgements
Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014.
Inspiration
The dataset specifically includes tokens extracted from Twitter, which unlike tokens from Wikipedia, include many abbreviations that have interesting content."
U.S. Charities and Non-profits,All of the charities and non-profits registered with the IRS,Chris Crawford,10,"Version 1,2017-08-19",,CSV,275 MB,CC0,"1,257 views",153 downloads,,0 topics,https://www.kaggle.com/crawford/us-charities-and-nonprofits,"Context
This dataset comes from ""The Exempt Organization Business Master File Extract"" (EO BMF) which includes cumulative information on tax-exempt organizations.
Content
Data is current up to: 8/14/2017
EIN: Employer Identification Number (EIN)
NAME: Primary Name of Organization
ICO: In Care of Name
STREET: Street Address
CITY: City
STATE: State
ZIP: Zip Code
GROUP: Group Exemption Number
SUBSECTION: Subsection Code
AFFILIATION: Affiliation Code
CLASSIFICATION: Classification Code(s)
RULING: Ruling Date
DEDUCTIBILITY: Deductibility Code
FOUNDATION: Foundation Code
ACTIVITY: Activity Codes
ORGANIZATION: Organization Code
STATUS: Exempt Organization Status Code
TAX_PERIOD: Tax Period
ASSET_CD: Asset Code
INCOME_CD: Income Code
FILING_REQ_CD: Filing Requirement Code
PF_FILING_REQ_CD: PF Filing Requirement Code
ACCT_PD: Accounting Period
ASSET_AMT: Asset Amount
INCOME_AMT: Income Amount (includes negative sign if amount is negative)
REVENUE_AMT: Form 990 Revenue Amount (includes negative sign if amount is negative)
NTEE_CD: National Taxonomy of Exempt Entities (NTEE) Code
SORT_NAME: Sort Name (Secondary Name Line)
There are six data files separated by regions
eo1:
CT
MA
ME
NH
NJ
NY
RI
VT
eo2:
DC
DE
IA
IL
IN
KY
MD
MI
MN
NC
ND
NE
OH
PA
SC
SD
VA
WI
WV
eo3:
AK
AL
AR
AZ
CA
CO
FL
GA
HI
ID
KS
LA
MO
MS
MT
NM
NV
OK
OR
TN
TX
UT
WA
WY
eo4:
AA
AE
AP
AS
FM
GU
MH
MP
PR
PW
VI
eo_pr:
Puerto Rico
eo_xx:
Various international non-profits (too many countries to list). See columns 5 and 6.
Acknowledgements
More information and updated data an be found here"
2016 NYC Real Time Traffic Speed Data Feed,Five minute intervals 'real-time' traffic information within the five boroughs,Chris Cross,10,"Version 1,2017-07-21",,CSV,688 MB,Other,"2,136 views",292 downloads,,,https://www.kaggle.com/crailtap/nyc-real-time-traffic-speed-data-feed,"Context
This data contains 'real-time' traffic information from locations where NYCDOT picks up sensor feeds within the five boroughs of NYC, mostly on major arterials and highways. NYCDOT uses this information for emergency response and management, see Acknowledgements.
Content
NYC Real Time Traffic Speed Data Feed for the year 2016, separated in monthly files of 5 minutes intervals of 'real-time' traffic information within the five boroughs of NYC. Each row represents a given street section (link), for which the average speed, travel time, timestamp and an id of the street section (link) is given. For each link id, information about this link is given in the linkinfo.csv file, e.g., geo coordinates.
Acknowledgements
http://data.beta.nyc/dataset/nyc-real-time-traffic-speed-data-feed-archived https://data.cityofnewyork.us/Transportation/Real-Time-Traffic-Speed-Data/xsat-x5sa"
The Examiner - Spam/Clickbait News Dataset,6 years of crowdsourced journalism,Rohk,10,"Version 3,2017-11-20|Version 2,2017-08-05|Version 1,2017-08-04","news agencies
journalism
historiography
+ 2 more...",CSV,143 MB,CC4,"3,444 views",189 downloads,2 kernels,0 topics,https://www.kaggle.com/therohk/examine-the-examiner,"Context
Presenting a compendium of crowdsourced journalism from the psuedo-news site The Examiner.
This dataset contains the headlines of 3 million articles written by 21000+ authors over 6 years.
While The Examiner was never praised for its quality, it consistently churned out 1000s of articles per day over several years.
At their height in 2011, The Examiner was ranked highly in google search and had enormous shares on social media. At one point it was the 10th largest site on mobile and was attracting 20 M unique visitors a month.
As a platform driven towards advert revenue, most of their content was rushed, unsourced and factually sparse. It still manages to capture in great detail, the trending topics over a long period of time.
Prepared by Rohit Kulkarni
Content
Format: CSV Rows: 3,089,781
Column 1: publish_date Date when the article was published on the site in yyyyMMdd format
Column 2: headline_text Text of the headline in English with rare utf8 chars (<1k)
Start Date: 2010-01-01 End Date: 2015-21-31
Another copy of the file with headlines tokenised to lowercase ascii only is included.
Acknowledgements
Created using Jsoup, Java and Bash.
Similar news datasets exploring other attributes, countries and topics can be seen on my profile.
This dataset is free to use with citation:
Rohit Kulkarni (2017), The Examiner - Spam ClickBait News 2010-2015 [CSV data files], doi:10.7910/DVN/I4HKOO, Retrieved from [this url]
Inspiration
The Examiner had emerged as an early winner in the digital content landscape of the 2000's using catchy headlines.
It changed many roles over the years, from leftist citizen news to a multiuser blogging platform to a content farm.
With falling views its operations were absorbed by AXS in 2014 and the website was finally shut down in June 2016.
The original site and content no longer exists: http://www.examiner.com
This is the last surviving record of its existence."
Diplomacy Betrayal Dataset,Can you predict a betrayal before it happens?,Rachael Tatman,10,"Version 1,2017-07-29","board games
linguistics
sociology
internet",Other,51 MB,Other,"2,116 views",111 downloads,,3 topics,https://www.kaggle.com/rtatman/diplomacy-betrayal-dataset,"This dataset contains a collection of interaction sequences between allies in online Diplomacy [1] games. A sequence consists of consecutive game seasons during which the two players exchange messages and help each other in the game. Half of the sequences end with betrayal, while the other half are part of lasting friendships.
Description
Diplomacy [1] is a popular and engaging strategic board game that is often played online [2, 3]. It is based heavily on communication between the players. Due to its military domination setting, Diplomacy is a well suited environment for studying naturally occurring betrayal and deception.
From a collection of Diplomacy game logs, we identified and extracted ongoing, established, and reciprocal friendships: relationships that contain at least two consecutive and reciprocated acts of support that span at least three seasons in game time, with no more than five seasons passing between two acts of friendship.
We then identified 250 betrayals: the subset of friendships described above that are followed by at least two attacks. To match each betrayal, we selected a friendship that is not followed by any offensive action, but is otherwise nearly identical (in terms of length and relative time within the game). The current dataset consists of these selected betrayals and friendships only.
Each relationship contains a sequence of seasons. Within each season, we provide features extracted from the messages sent by each player.
Acknowledgements:
This dataset is distributed under the Open Data Commons Attribution (ODC-By 1.0) license. It was collected by Vlad Niculae, Srijan Kumar, Jordan Boyd-Graber and Cristian Danescu-Niculescu-Mizil.
If you use this dataset, please cite this paper:
Niculae, V., Kumar, S., Boyd-Graber, J., & Danescu-Niculescu-Mizil, C. (2015). Linguistic harbingers of betrayal: A case study on an online strategy game. In Proceedings of the ACL.
Data format
The dataset is a UTF-8 encoded JSON file, which can be loaded into a Python kernel with the following code:
>>> import json
>>> from io import open
>>> with open(""diplomacy_data.json"", ""r"") as f:
...     diplomacy = json.load(f)
...
It is structured as a list of dictionaries, one for each of the 500 sequences.
>>> len(diplomacy)
500
This is an example of one such entry, with the fields explained:
>>> entry = diplomacy[0]
>>> entry
{
    'idx': 0,           # unique identifier of the dataset entry
    'game': 74,         # unique identifier of the game it comes from
    'betrayal': True,   # whether the friendship ended in betrayal
    'people': u'AT',    # the countries represented by the two players
                        # (in this case, Austria and Turkey)
    'seasons': ...
}
The 'seasons' field is again a list of dictionaries, one for each game season in the friendship sequence. In the example below, there are 8 seasons, each identified by the game year. Decimal notation is used to denote the season in each year. For example, 1906.0 is the spring of 1906 and 1906.5 is the fall of 1906. Each season is also marked with what interaction the two players have at the end of the discussion: whether the players supported one another ('support'), attacked one another ('attack'), or did not have explicit military interactions (null).
>>> seasons = entry['seasons']
>>> len(seasons)
8
>>> seasons[0]
{
    'season': 1906.5,           # fall of the year 1906 (game time)
    'interaction': {
        'victim': u'support',   # the victim supported the betrayer
        'betrayer': u'support'  # the betrayer supported the victim
    },
    'messages': {
        'victim': ...,
        'betrayer': ...
    }
}
The ['messages']['victim'] and ['messages']['betrayer'] fields are lists of features of each message sent by the victim to the betrayer, and by the betrayer to the victim, respectively:
>>> msgs = seasons[0]['messages']['betrayer']
>>> len(msgs)
6
>>> msgs[0]
{
    ""n_words"": 146,             # number of words in the message
    ""n_sentences"": 9,           # number of sentences in the message

    ""n_requests"": 7,            # number of request sentences
    ""politeness"": 0.8320,       # politeness of the requests (from 0 to 1)
                                # (using the Stanford Politeness
                                # Classifier available at [4])
    ""sentiment"": {
        ""positive"": 1,          # no. sentences with positive sentiment
        ""neutral"": 3,           #      ""      ""      neutral sentiment
        ""negative"": 5           #      ""      ""      negative sentiment
    },                          # (using Stanford Sentiment Analysis [5])

    ""lexicon_words"": {          # words and phrases matching several
        ""disc_expansion"": [     # linguistic and psycholinguistic lexicons
            ""until"",            # (see below for details)
            ""yet"",
            ""instead""
        ],
        ""premise"": [
            ""for"",
            ""for""
        ],
        ...
    },
    ""frequent_words"": [         # frequent words in the message
        ""more"",                 # (occurring in at least 50 messages
        ""let"",                  # and 5 friendships overall)
        ""keep"",
        ""...
    ]
}
The words in each list are in random order. The order of messages within a season is also randomized. This measure is in place to protect the privacy of the players and of their conversations.
The lexicons used to construct the ""lexicon_words"" field are:
'claim', 'premise': Argumentation structure markers [6]
'allsubj': Subjective markers [7]
'disc_*': Discourse markers from the Penn Discourse Treebank. [8] Includes 'disc_comparison', 'disc_expansion', 'disc_contingency', 'disc_temporal_future' and 'disc_temporal_rest' (we manually split 'temporal' from PDT into 'temporal_future' and 'temporal_rest' to capture planning).
References
[1] https://en.wikipedia.org/wiki/Diplomacy_%28game%29 [2] http://www.floc.net/dpjudge/ [3] http://usak.asciiking.com/ [4] http://politeness.mpi-sws.org/ [5] http://nlp.stanford.edu/sentiment/ [6] C. Stab and I. Gurevych. Identifying Argumentative Discourse Structures in Persuasive Essays. In: Proceedings of EMNLP, 2014. https://www.ukp.tu-darmstadt.de/data/argumentation-mining/ [7] E. Riloff and J. Wiebe. Learning extraction patterns for subjective expressions. In: Proceedings of EMNLP, 2003. http://www.anthology.aclweb.org/W/W03/W03-1014.pdf [8] https://www.seas.upenn.edu/~pdtb/"
Blog Authorship Corpus,"Over 600,000 posts from more than 19 thousand bloggers",Rachael Tatman,10,"Version 2,2017-08-16|Version 1,2017-08-16","languages
linguistics
internet",CSV,763 MB,Other,"1,677 views",140 downloads,3 kernels,0 topics,https://www.kaggle.com/rtatman/blog-authorship-corpus,"Context:
“A blog (a truncation of the expression ""weblog"") is a discussion or informational website published on the World Wide Web consisting of discrete, often informal diary-style text entries (""posts""). Posts are typically displayed in reverse chronological order, so that the most recent post appears first, at the top of the web page. Until 2009, blogs were usually the work of a single individual, occasionally of a small group, and often covered a single subject or topic.” -- Wikipedia article “Blog”
This dataset contains text from blogs written on or before 2004, with each blog being the work of a single user.
Content:
The Blog Authorship Corpus consists of the collected posts of 19,320 bloggers gathered from blogger.com in August 2004. The corpus incorporates a total of 681,288 posts and over 140 million words - or approximately 35 posts and 7250 words per person.
Each blog is presented as a separate file, the name of which indicates a blogger id# and the blogger’s self-provided gender, age, industry and astrological sign. (All are labeled for gender and age but for many, industry and/or sign is marked as unknown.)
All bloggers included in the corpus fall into one of three age groups: * 8240 ""10s"" blogs (ages 13-17), * 8086 ""20s"" blogs(ages 23-27) * 2994 ""30s"" blogs (ages 33-47).
For each age group there are an equal number of male and female bloggers.
Each blog in the corpus includes at least 200 occurrences of common English words. All formatting has been stripped with two exceptions. Individual posts within a single blogger are separated by the date of the following post and links within a post are denoted by the label urllink.
Acknowledgements
The corpus may be freely used for non-commercial research purposes. Any resulting publications should cite the following:
J. Schler, M. Koppel, S. Argamon and J. Pennebaker (2006). Effects of Age and Gender on Blogging in Proceedings of 2006 AAAI Spring Symposium on Computational Approaches for Analyzing Weblogs. URL: http://www.cs.biu.ac.il/~schlerj/schler_springsymp06.pdf
Inspiration:
This dataset contains information on writers demographics, including their age, gender and zodiac sign. Can you build a classifier to guess someone’s zodiac sign from blog posts they’ve written?
Which are bigger: differences between demographic groups or differences between blogs on different topics?
You may also like:
News and Blog Data Crawl: Content section from over 160,000 news and blog articles
20 Newsgroups: A collection of ~18,000 newsgroup documents from 20 different newsgroups"
Thai Sentiment Analysis Toolkit,"Positive, negative and swear words in Thai",Rachael Tatman,10,"Version 1,2017-09-07","languages
asia
linguistics",Other,35 KB,CC4,"1,431 views",87 downloads,,0 topics,https://www.kaggle.com/rtatman/thai-sentiment-analysis-toolkit,"Context:
Sentiment analysis is the task of computationally labeling whether the content of text is positive or negative. One common approach to this is to compile lists of words which have a positive connotation (like “wonderful”, “lovely” and “best”) and a negative connotation (like “bad”, “horrible” or “awful”). Then you count how many positive and how many negative
Content:
This dataset contains three lists of Thai words:
swear words (94 words)
positive words (512 words)
negative words (1218 words)
Each list a .txt file with one word per line. The character encoding is UTF-8.
Acknowledgements:
This dataset was compiled by Wannaphong Phatthiyaphaibun and is reproduced here under a CC-BY-SA 4.0 license. (You may also be interested in his translation of Python 3 documentation into Thai on this blog.)
Inspiration:
Can you analyze the sentiment in this corpus of Thai? Is there a difference in sentiment between the WIkipedia and Government parts of the corpus?"
"Incidents Around Austin, TX",5 years of Austin Incidents Data,Jacob Boysen,10,"Version 1,2017-08-19",,Other,108 MB,CC0,"1,053 views",89 downloads,,,https://www.kaggle.com/jboysen/austin-incidents,"Context:
Anticipating public nuisances and allocating proper resources is a critical part of public duties.
Content:
This dataset contains 5 years (2008-2011, 2016) worth of public incidents, both criminal and non-criminal. Data includes time, location, description, and unique key.
Acknowledgements:
This dataset was compiled by the City of Austin and published on Google Cloud Public Data.
Dataset Description
Use this dataset with BigQuery You can use Kernels to analyze, share, and discuss this data on Kaggle, but if you’re looking for real-time updates and bigger data, check out the data on BigQuery, too.
Inspiration:
Are there notable time variations in incidences?
Can you predict incidence patterns for 2016 based on the 2008-2011 training data?
Do weather patterns or other changes related to incidence changes?"
Grasping Dataset,A grasping dataset from simulation using Shadow Robot's Smart Grasping Sandbox,ugocupcic,10,"Version 4,2017-09-11|Version 3,2017-08-30|Version 2,2017-08-30|Version 1,2017-08-29",robotics,CSV,485 MB,GPL,"2,846 views",115 downloads,4 kernels,3 topics,https://www.kaggle.com/ugocupcic/grasping-dataset,"Context
At Shadow Robot, we are leaders in robotic grasping and manipulation. As part of our Smart Grasping System development, we're developing different algorithms using machine learning.
This first public dataset was created to investigate the use of machine learning to predict the stability of a grasp. Due to the limitations of the current simulation, it is a restricted dataset - only grasping a ball. The dataset is annotated with an objective grasp quality and contains the different data gathered from the joints (position, velocity, effort).
You can find all the explanations for this dataset over on Medium.
Inspiration
I'll be more than happy to discuss this dataset as well as which dataset you'd like to have to try your hands at solving real world robotic problems focused on grasping using machine learning. Let's connect on twitter (@ugocupcic)!"
Sensor readings from a wall-following robot,Data collected from a robot while navigating around a room,UCI Machine Learning,10,"Version 1,2017-09-06",robotics,CSV,1 MB,Other,"1,859 views",157 downloads,2 kernels,0 topics,https://www.kaggle.com/uciml/wall-following-robot,"Context
The data were collected as the SCITOS G5 navigated through the room following the wall in a clockwise direction, for 4 rounds. To navigate, the robot uses 24 ultrasound sensors arranged circularly around its ""waist"". The numbering of the ultrasound sensors starts at the front of the robot and increases in clockwise direction.
The provided files comprise three diferent data sets.
The first one contains the raw values of the measurements of all 24 ultrasound sensors and the corresponding class label (Moving forward, turning left, etc). Sensor readings are sampled at a rate of 9 samples per second.
The second one contains four sensor readings named 'simplified distances' and the corresponding class label l (Moving forward, turning left, etc). These simplified distances are referred to as the 'front distance', 'left distance', 'right distance' and 'back distance'. They consist, respectively, of the minimum sensor readings among those within 60 degree arcs located at the front, left, right and back parts of the robot.
The third one contains only the front and left simplified distances and the corresponding class labell (Moving forward, turning left, etc).
It is worth mentioning that the 24 ultrasound readings and the simplified distances were collected at the same time step, so each file has the same number of rows (one for each sampling time step).
The wall-following task and data gathering were designed to test the hypothesis that this apparently simple navigation task is indeed a non-linearly separable classification task. Thus, linear classifiers, such as the Perceptron network, are not able to learn the task and command the robot around the room without collisions. Nonlinear neural classifiers, such as the MLP network, are able to learn the task and command the robot successfully without collisions.
If some kind of short-term memory mechanism is provided to the neural classifiers, their performances are improved in general. For example, if past inputs are provided together with current sensor readings, even the Perceptron becomes able to learn the task and command the robot successfully. If a recurrent neural network, such as the Elman network, is used to learn the task, the resulting dynamical classifier is able to learn the task using less hidden neurons than the MLP network.
Files with different number of sensor readings were built in order to evaluate the performance of the classifiers with respect to the number of inputs.
Content
File sensor_readings_24.csv:
US1: ultrasound sensor at the front of the robot (reference angle: 180°) - (numeric: real)
US2: ultrasound reading (reference angle: -165°) - (numeric: real)
US3: ultrasound reading (reference angle: -150°) - (numeric: real)
US4: ultrasound reading (reference angle: -135°) - (numeric: real)
US5: ultrasound reading (reference angle: -120°) - (numeric: real)
US6: ultrasound reading (reference angle: -105°) - (numeric: real)
US7: ultrasound reading (reference angle: -90°) - (numeric: real)
US8: ultrasound reading (reference angle: -75°) - (numeric: real)
US9: ultrasound reading (reference angle: -60°) - (numeric: real)
US10: ultrasound reading (reference angle: -45°) - (numeric: real)
US11: ultrasound reading (reference angle: -30°) - (numeric: real)
US12: ultrasound reading (reference angle: -15°) - (numeric: real)
US13: reading of ultrasound sensor situated at the back of the robot (reference angle: 0°) - (numeric: real)
US14: ultrasound reading (reference angle: 15°) - (numeric: real)
US15: ultrasound reading (reference angle: 30°) - (numeric: real)
US16: ultrasound reading (reference angle: 45°) - (numeric: real)
US17: ultrasound reading (reference angle: 60°) - (numeric: real)
US18: ultrasound reading (reference angle: 75°) - (numeric: real)
US19: ultrasound reading (reference angle: 90°) - (numeric: real)
US20: ultrasound reading (reference angle: 105°) - (numeric: real)
US21: ultrasound reading (reference angle: 120°) - (numeric: real)
US22: ultrasound reading (reference angle: 135°) - (numeric: real)
US23: ultrasound reading (reference angle: 150°) - (numeric: real)
US24: ultrasound reading (reference angle: 165°) - (numeric: real)
Classes: Move-Forward, Slight-Right-Turn, Sharp-Right-Turn, Slight-Left-Turn
File: sensor_readings_4.csv:
SD_front: minimum sensor reading within a 60 degree arc located at the front of the robot - (numeric: real)
SD_left: minimum sensor reading within a 60 degree arc located at the left of the robot - (numeric: real)
SD_right: minimum sensor reading within a 60 degree arc located at the right of the robot - (numeric: real)
SD_back: minimum sensor reading within a 60 degree arc located at the back of the robot - (numeric: real)
Classes: Move-Forward, Slight-Right-Turn, Sharp-Right-Turn, Slight-Left-Turn
File: sensor_readings_2.csv:
SD_front: minimum sensor reading within a 60 degree arc located at the front of the robot - (numeric: real)
SD_left: minimum sensor reading within a 60 degree arc located at the left of the robot - (numeric: real)
Classes: Move-Forward, Slight-Right-Turn, Sharp-Right-Turn, Slight-Left-Turn
Acknowledgements
These datasets were downlaoded from the UCI Machine Learning Repository
Lichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.
Inspiration
Use these ultrasound readings to predict the class, i.e. given these readings, is the robot moving straight? turning left?"
Bioassay Datasets,21 assays from PubChem that measure compound activity,UCI Machine Learning,10,"Version 1,2017-09-08","biology
health sciences
scientific method
+ 2 more...",CSV,215 MB,CC0,"2,045 views",122 downloads,,,https://www.kaggle.com/uciml/bioassay-datasets,"Context
The drug-development process is time-consuming and expensive. In High-Throughput Screening (HTS), batches of compounds are tested against a biological target to test the compound's ability to bind to the target. Targets might be antibodies for example. If the compound binds to the target then it is active for that target and known as a hit.
Virtual screening is the computational or in silico screening of biological compounds and complements the HTS process. It is used to aid the selection of compounds for screening in HTS bioassays or for inclusion in a compound-screening library.
Drug discovery is the first stage of the drug-development process and involves finding compounds to test and screen against biological targets. This first stage is known as primary-screening and usually involves the screening of thousands of compounds.
This dataset is a collection of 21 bioassays (screens) that measure the activity of various compounds against different biological targets.
Content
Each bioassay is split into test and train files.
Here are some descriptions of some of the assays compounds. The source, unfortunately, does not have descriptions for every assay. That's the nature of the beast for finding this kind data and was also pointed out in the original study.
Primary screens
AID362 details the results of a primary screening bioassay for Formylpeptide Receptor Ligand Binding University from the New Mexico Center for Molecular Discovery. It is a relatively small dataset with 4279 compounds and with a ratio of 1 active to 70 inactive compounds (1.4% minority class). The compounds were selected on the basis of preliminary virtual screening of approximately 480,000 drug-like small molecules from Chemical Diversity Laboratories.
AID604 is a primary screening bioassay for Rho kinase 2 inhibitors from the Scripps Research Institute Molecular Screening Center. The bioassay contains activity information of 59,788 compounds with a ratio of 1 active compound to 281 inactive compounds (1.4%). 57,546 of the compounds have known drug-like properties.
AID456 is a primary screen assay from the Burnham Center for Chemical Genomics for inhibition of TNFa induced VCAM-1 cell surface expression and consists of 9,982 compounds with a ratio of 1 active compound to 368 inactive compounds (0.27% minority). The compounds have been selected for their known drug-like properties and 9,431 meet the Rule of 5 [19].
AID688 is the result of a primary screen for Yeast eIF2B from the Penn Center for Molecular Discovery and contains activity information of 27,198 compounds with a ratio of 1 active compound to 108 inactive compounds (0.91% minority). The screen is a reporter-gene assay and 25,656 of the compounds have known drug-like properties.
AID373 is a primary screen from the Scripps Research Institute Molecular Screening Center for endothelial differentiation, sphingolipid G-protein-coupled receptor, 3. 59,788 compounds were screened with a ratio of 1 active compound to 963 inactive compounds (0.1%). 57,546 of the compounds screened had known drug-like properties.
AID746 is a primary screen from the Scripps Research Institute Molecular Screening Center for Mitogen-activated protein kinase. 59,788 compounds were screened with a ratio of 1 active compound to 162 inactive compounds (0.61%). 57,546 of the compounds screened had known drug-like properties.
AID687 is the result of a primary screen for coagulation factor XI from the Penn Center for Molecular Discovery and contains activity information of 33,067 compounds with a ratio of 1 active compound to 350 inactive compounds (0.28% minority). 30,353 of the compounds screened had known drug-like properties.
Primary and Confirmatory
AID604 (primary) with AID644 (confirmatory)
AID746 (primary) with AID1284 (confirmatory)
AID373 (primary) with AID439 (confirmatory)
AID746 (primary) with AID721 (confirmatory)
Confirmatory
AID1608 is a different type of screening assay that was used to identify compounds that prevent HttQ103-induced cell death. National Institute of Neurological Disorders and Stroke Approved Drug Program. The compounds that prevent a release of a certain chemical into the growth medium are labelled as active and the remaining compounds are labelled as having inconclusive activity. AID1608 is a small dataset with 1,033 compounds and a ratio of 1 active to 14 inconclusive compounds (6.58% minority class).
AID644
AID1284
AID439
AID721
AID1608
AID644
AID1284
AID439
AID721
Acknowledgements
Original study: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2820499/
Data downloaded form UCI ML repository:
Lichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.
Inspiration
Drug development is expensive. Use this virtual bio assay data to classify compounds as hits (active) against their biological targets."
Fireballs,Bolide impacts by the kiloton since 1988.,NASA,10,"Version 1,2017-08-23","government agencies
space",CSV,51 KB,Other,"1,349 views",113 downloads,2 kernels,0 topics,https://www.kaggle.com/nasa/fireballs,"Fireballs and bolides are astronomical terms for exceptionally bright meteors that are spectacular enough to to be seen over a very wide area. A world map shows a visual representation of the data table that provides a chronological data summary of fireball and bolide events provided by U.S. Government sensors. Ground-based observers sometimes also witness these events at night, or much more rarely in daylight, as impressive atmospheric light displays. This website is not meant to be a complete list of all fireball events. Only the brightest fireballs are noted.
Content
The accompanying table provides information on the date and time of each reported fireball event with its approximate total optical radiated energy and its calculated total impact energy. When reported, the event’s geographic location, altitude and velocity at peak brightness are also provided. Note that data are not provided in real-time and not all fireballs are reported. A blank (empty) field in the table indicates the associated value was not reported.
For more information about fireballs and bolides, please see https://cneos.jpl.nasa.gov/fireballs/intro.html.
Field legend
Peak Brightness Date/Time (UT) The date and time in UT (Universal Time) of this event's peak brightness.
Latitude (deg.) Geodetic latitude in degrees north (N) or south (S) of the equator for this event.
Longitude (deg.) Geodetic longitude in degrees east (E) or west (W) of the prime meridian for this event.
Altitude (km) Altitude in kilometers (km) above the reference geoid for this event.
Velocity (km/s) The magnitude of the meteor's pre-impact velocity in kilometers per second (km/s).
Velocity Components (km/s) The magnitude of the meteor's pre-impact velocity in a geocentric Earth-fixed reference frame defined as follows: the z-axis is directed along the Earth's rotation axis towards the celestial north pole, the x-axis lies in the Earth's equatorial plane, directed towards the prime meridian, and the y-axis completes the right-handed coordinate system.
Total Radiated Energy (J) The approximate total radiated energy in the atmosphere in Joules [a unit of energy given in kilograms times velocity squared, or kg × (m/s)2]
Calculated Total Impact Energy (kt) The impact energy of the event in kilotons of TNT (kt) computed from an empirical expression relating radiated and impact energy
Acknowledgements
This dataset was kindly made available by NASA. You can find the original dataset at https://cneos.jpl.nasa.gov/fireballs/
You might also be interested in their Planetary Defense FAQ."
Food Ingredient Lists,"A list of 10,000 food products and their ingredients.",Datafiniti,10,"Version 1,2017-09-20","databases
food and drink
nutrition
product",CSV,5 MB,CC4,"3,646 views",410 downloads,,0 topics,https://www.kaggle.com/datafiniti/food-ingredient-lists,"About this Data
This is a list of 10,000 different food listings and their ingredients provided by Datafiniti's Product Database. The dataset includes the brand, name, manufacturer, category, features, and more for each product.
What You Can Do With This Data
A similar dataset was used to determine if it's cheaper to eat in or eat out. Discover insights into ingredients used in various foods. E.g.:
What's the distribution of the number of ingredients per listing?
What are the most common ingredients used?
What is the total cost of ingredients needed for a homemade meal versus restaurant meal?
Data Schema
A full schema for the data is available in our support documentation.
About Datafiniti
Datafiniti provides instant access to web data. We compile data from thousands of websites to create standardized databases of business, product, and property information. Learn more.
Want More?
You can get more data like this by joining Datafiniti or requesting a demo."
JCPenney products,"20,000 product listings from JCPenney",PromptCloud,10,"Version 1,2017-09-15",internet,CSV,23 MB,CC4,"3,276 views",454 downloads,3 kernels,,https://www.kaggle.com/PromptCloudHQ/all-jc-penny-products,"Context
This is a pre-crawled dataset, taken as subset of a bigger dataset (more than 3.7 million products) that was created by extracting data from jcpenney.com, a well known retailer.
Content
This dataset has following fields:
sku
name_title
description
list_price
sale_price
category
category_tree
average_product_rating
product_url
product_image_urls
brand
total_number_reviews
reviews
Acknowledgements
This dataset was created by PromptCloud's in-house web-crawling service.
Inspiration
Analyses of list price, sale price, rating and reviews can be performed."
OpenAddresses - Europe,Addresses and geolocations for European countries,OpenAddresses,10,"Version 1,2017-08-04",,CSV,7 GB,Other,"1,131 views",210 downloads,,0 topics,https://www.kaggle.com/openaddresses/openaddresses-europe,"Context
OpenAddresses's goal is to connect the digital and physical worlds by sharing geographic coordinates, street names, house numbers and postal codes.
Content
This dataset contains one data file for each of these countries:
States included in this dataset:
Field descriptions:
LON - Longitude
LAT - Latitude
NUMBER - Street number
STREET - Street name
UNIT - Unit or apartment number
CITY - City name
DISTRICT - ?
REGION - ?
POSTCODE - Postcode or zipcode
ID - ?
HASH - ?
Acknowledgements
Data collected around 2017-07-25 by OpenAddresses (http://openaddresses.io).
Address data is essential infrastructure. Street names, house numbers and postal codes, when combined with geographic coordinates, are the hub that connects digital to physical places.
Data licenses can be found in LICENSE.txt.
Data source information can be found at https://github.com/openaddresses/openaddresses/tree/9ea72b079aaff7d322349e4b812eb43eb94d6d93/sources
Inspiration
Use this dataset to create maps in conjunction with other datasets to map weather, crime, or how your next canoing trip."
Japanese-English Bilingual Corpus,Japanese-English Bilingual Corpus of Wikipedia's Kyoto Articles,Team AI,10,"Version 3,2017-09-15|Version 2,2017-09-15|Version 1,2017-09-15","languages
linguistics
artificial intelligence",Other,357 MB,Other,"1,718 views",119 downloads,4 kernels,,https://www.kaggle.com/team-ai/japaneseenglish-bilingual-corpus,"Background
NLP is a hot topic currently! Team AI really want's to leverage the NLP research and this an attempt for all the NLP researchers to explore exciting insights from bilingual data
The Japanese-English Bilingual Corpus of Wikipedia's Kyoto Articles” aims mainly at supporting research and development relevant to high-performance multilingual machine translation, information extraction, and other language processing technologies.
Unique Features
A precise and large-scale corpus containing about 500,000 pairs of manually-translated sentences. Can be exploited for research and development of high-performance multilingual machine translation, information extraction, and so on.
The three-step translation process (primary translation -> secondary translation to improve fluency -> final check for technical terms) has been clearly recorded. Enables observation of how translations have been elaborated so it can be applied for uses such as research and development relevant to translation aids and error analysis of human translation.
Translated articles concern Kyoto and other topics such as traditional Japanese culture, religion, and history. Can also be utilized for tourist information translation or to create glossaries for travel guides.
The Japanese-English Bilingual Kyoto Lexicon is also available. This lexicon was created by extracting the Japanese-English word pairs from this corpus.
Sample
One Wikipedia article is stored as one XML file in this corpus, and the corpus contains 14,111 files in total.
The following is a short quotation from a corpus file titled “Ryoan-ji Temple”. Each tag has different implications. For example:
<j>Original Japanese sentence<j> <e type=""trans"" ver=""1"">Primary translation</e> <e type=""trans"" ver=""2"">Secondary translation</e> <e type=""check"" ver=""1"">Final translation</e> <cmt>Comment added by translators</cmt>
Categories
The files have been divided into 15 categories: school, railway, family, building, Shinto, person name, geographical name, culture, road, Buddhism, literature, title, history, shrines and temples, and emperor (Click the link to view a sample file for each category).
Github
https://github.com/venali/BilingualCorpus
Explains how to load the corpus
Acknowledgements
The National Institute of Information and Communications Technology (NICT) has created this corpus by manually translating Japanese Wikipedia articles (related to Kyoto) into English.
Licence
Use and/or redistribution of the Japanese-English Bilingual Corpus of Wikipedia's Kyoto Articles and the Japanese-English Bilingual Kyoto Lexicon is permitted under the conditions of Creative Commons Attribution-Share-Alike License 3.0. Details can be found at http://creativecommons.org/licenses/by-sa/3.0/.
Link to web
https://alaginrc.nict.go.jp/WikiCorpus/index_E.html"
Congressional Voting Records,All roll call votes made by the United States Congress 1789-2017,VoteView,10,"Version 1,2017-08-11","history
government
politics",CSV,510 MB,Other,"1,328 views",124 downloads,2 kernels,0 topics,https://www.kaggle.com/voteview/congressional-voting-records,"DW-Nominate scores of congressional voting behavior regularly appears in media such as the New York Times, Washington Post, and 538. This dataset contains the voting records used to generate those scores and additional features related to the DW-NOMINATE calculations.
Content
This dataset contains descriptive data as well as ideological data for congressional rollcalls, individual member votes, members of congress, and parties. You can find information such the descriptions of rollcalls, what proportion of voting members were correctly classified by the ideological cutting line for that rollcall, the ideological position of members of congress, and more.
Both the rollcall data and the data on members are split into chambers and congresses. The data on parties is a dataset with some metadata about all of the different parties as well as their average ideological position and membership size broken down by congress and chamber.
The full details behind the DW-NOMINATE calculations may be helpful in interpreting some of this data. The technical details of the DW-NOMINATE model can be found in Poole's Spatial Models of Parliamentary Voting. Poole and Rosenthal's Ideology and Congress explores the nature of voting in Congress and the political history of the United States through the lens of the ideological dimensions recovered by DW-NOMINATE.
Acknowledgements
This dataset was prepared by the team at VoteView. Please visit their site if you require up-to-date records. You may also be interested in their blog.
Inspiration
-Using national scores as a training set, can you develop polarization scores for you own state legislature? -Can you find correlates that help explain changes in DW-NOMINATE scores?"
2017 March ML Mania Processed Predictions,Forecasting the 2017 March Madness Kaggle Competition,Willie Liao,10,"Version 17,2017-04-04|Version 16,2017-04-02|Version 15,2017-04-02|Version 14,2017-03-27|Version 13,2017-03-27|Version 12,2017-03-26|Version 11,2017-03-26|Version 10,2017-03-25|Version 9,2017-03-24|Version 8,2017-03-24|Version 7,2017-03-20|Version 6,2017-03-20|Version 5,2017-03-20|Version 4,2017-03-19|Version 3,2017-03-19|Version 2,2017-03-19|Version 1,2017-03-18",,CSV,89 MB,CC4,"1,586 views",142 downloads,14 kernels,3 topics,https://www.kaggle.com/willieliao/2017-march-ml-mania-processed-predictions,"Unlike other Kaggle competitions, the leaderboard for March Madness changes as the NCAA Basketball tournament progresses. Part of the fun is seeing how the rankings will change due to upcoming games. To enable easier analysis, I've cleansed and processed the predictions dataset that William Cukierski created."
Devanagari Character Dataset Large,Large Devanagari Handwritten Character Dataset,Ashok Kumar Pant,10,"Version 4,2017-06-23|Version 3,2017-06-23|Version 2,2017-06-23|Version 1,2017-06-23",,Other,63 MB,CC4,452 views,47 downloads,,0 topics,https://www.kaggle.com/ashokpant/devanagari-character-dataset-large,"Context
This dataset is created for the deep learning based Devanagari character recognition research evaluation, 2015.
Content
The dataset contains mixed categories for Devanagari numerals (10 classes) and consonants (36 classes). Dataset is explicitly separated into train and test set. Train set contains total 78,200 samples with 1700 samples per class for total 46 classes and test set contains total 13,800 samples with 300 samples per class for total 46 classes. Dataset is collected from the school level students.
This dataset is collected and maintained by the following research members,
Shailesh Acharya
Ashok Kumar Pant
Prashnna Kumar Gyawali
Citation
Please cite in your publications if it helps your research:
@inproceedings{ashok2015deep, 
    author={S. Acharya and A. K. Pant and P. K. Gyawali}, 
    booktitle={2015 9th International Conference on Software, Knowledge, Information Management and Applications (SKIMA)}, 
    title={Deep learning based large scale handwritten Devanagari character recognition}, 
    year={2015}, 
    pages={1-6},
    month={Dec}
 }"
Mercedes-Benz Competition Leaderboard Shakeup,"""What has just happened?!""",dmi3kno,10,"Version 2,2017-07-15|Version 1,2017-07-13",,CSV,39 MB,Other,648 views,39 downloads,4 kernels,,https://www.kaggle.com/dmi3kno/mercedesbenz-competition-leaderboard-shakeup,"Mercedes-Benz Greener Manufacturing Competition
Mercedes-Benz hosted a Kaggle competition in June-July 2017. Participants were invited to predict the time it takes to test the car using an anonymized set of 377 variables.
In this competition, Daimler is challenging Kagglers to tackle the curse of dimensionality and reduce the time that cars spend on the test bench. Competitors will work with a dataset representing different permutations of Mercedes-Benz car features to predict the time it takes to pass testing.
What is in the dataset
Current dataset represents Public and Private Leaderboard standings for every day of competition (from 31-May-2017 to 11-07-2017). Data has been collected from Kaggle leaderboard tables after the competition has ended.
Acknowledgements
We would like to thank @BreakfastPirat for inspiration, Daimler team for hosting the competition, Kaggle team for support and all fellow participants in the Mercedes data challenge for their courage and perseverance.
Questions for Inspiration
1) Validate the leaderboard shakeup statistics
2) Try to reproduce the leaderboard progression visualizations
3) When did the teams start overfitting? What makes one prone to overfitting?
4) Investigate public submissions statistics. Can the pattern of submissions predict the final score?
5) Can you detect which teams have has a robust cross-validation strategy from their submissions stats?"
Spelling Corrector,Datasets from Peter Norvig's classic spelling corrector in half a page of Python,Adam Mathias Bittlingmayer,10,"Version 2,2017-05-25|Version 1,2017-05-25","languages
linguistics",Other,7 MB,Other,"3,023 views",246 downloads,16 kernels,,https://www.kaggle.com/bittlingmayer/spelling,"From Peter Norvig's classic How to Write a Spelling Corrector
One week in 2007, two friends (Dean and Bill) independently told me they were amazed at Google's spelling correction. Type in a search like [speling] and Google instantly comes back with Showing results for: spelling. I thought Dean and Bill, being highly accomplished engineers and mathematicians, would have good intuitions about how this process works. But they didn't, and come to think of it, why should they know about something so far outside their specialty?
I figured they, and others, could benefit from an explanation. The full details of an industrial-strength spell corrector are quite complex (you can read a little about it here or here). But I figured that in the course of a transcontinental plane ride I could write and explain a toy spelling corrector that achieves 80 or 90% accuracy at a processing speed of at least 10 words per second in about half a page of code.
A Kernel has been added with Peter's basic spell.py and evaluation code to set a baseline. Minimal modifications were made so that it runs on this environment.
Data files
big.txt is required by the code. That's how it learns the probabilities of English words. You can prepend more text data to it, but be sure to leave in the little Python snippet at the end.
Testing files
The other files are for testing the accuracy. The baseline code should get 75% of 270 correct on spell-testset1.txt, and 68% of 400 correct on spell-testset2.txt.
I've also added some other files for more extensive testing. The example Kernel runs all of them but birkbeck.txt by default. Here's the output:
Testing spell-testset1.txt
75% of 270 correct (6% unknown) at 32 words per second 
Testing spell-testset2.txt
68% of 400 correct (11% unknown) at 28 words per second 
Testing wikipedia.txt
61% of 2455 correct (24% unknown) at 21 words per second 
Testing aspell.txt
43% of 531 correct (23% unknown) at 15 words per second 
The larger datasets take a few minutes to run. birkbeck.txt takes more than a few minutes.
You can try adding other datasets, or splitting these ones in meaningful ways - for example a dataset of only words of 5 characters or less, or 10 characters or more, or without uppercase - to understand the effect of changes you make on different types of words.
Languages
The data and testing files include English only for now. In principle it is easily generalisable to other languages."
Most Popular Quotes on Goodreads,A collection of the most popular and recent quotes on Goodreads,Pramud,10,"Version 2,2017-04-18|Version 1,2017-04-18","literature
linguistics",{}JSON,1 MB,Other,"2,756 views",538 downloads,6 kernels,0 topics,https://www.kaggle.com/pramud/most-popular-quotes-on-goodreads,"Context
Goodreads is the world’s largest site for readers and book recommendations. Their mission is to help people find and share books they love. In addition to tracking the books you're reading, have read, and want to read, users on Goodreads can see what their friends are reading, get personalized recommendations, and browse community reviews. The website is also a great place to look for inspirational quotes from authors.
Content & Inspiration
This dataset contains the most popular and recent quotes shared on Goodreads. Along with each quote, you'll get its tags, the number of likes it received, and the author of the quote. You can use this dataset to study which tags receive the most likes, what characteristics the most popular quotes have in common, use NLP techniques like sentiment analysis, or even generate your own novel quotes (pun intended).
Before you begin, an inspirational quote from Arthur Conan Doyle, Sherlock Holmes:
""It is a capital mistake to theorize before one has data. Insensibly one begins to twist facts to suit theories, instead of theories to suit facts."""
Breast Histology Images,Classify IDC vs non IDC images,Jegs,10,"Version 3,2017-05-13|Version 2,2017-05-12|Version 1,2017-05-12",,Other,40 MB,CC0,"2,425 views",484 downloads,8 kernels,3 topics,https://www.kaggle.com/simjeg/lymphoma-subtype-classification-fl-vs-cll,"This dataset consists of 5547 breast histology images of size 50 x 50 x 3, curated from Andrew Janowczyk website and used for a data science tutorial at Epidemium. The goal is to classify cancerous images (IDC : invasive ductal carcinoma) vs non-IDC images."
Uttar Pradesh Assembly Elections 2017,Dataset for constituency wise results for all candidates,Ankit,10,"Version 1,2017-04-22",politics,CSV,292 KB,CC0,"3,070 views",388 downloads,21 kernels,,https://www.kaggle.com/ankit2106/uttar-pradesh-assembly-elections-2017,"Context
The assembly election results for Uttar Pradesh(UP) were surprising to say the least. Never in the past has any single party secured a similar mandate. UP with a population of around 220 million is as big as the whole of united states. It has 403 constituencies each having its own demographic breakup. The election was conducted in 7 phases.
Content
The dataset has 8 variables:
seat_allotment: As some of you might be aware that there was a coalition between INC and SP which materialized pretty late into the campaign. Hence, in a few constituencies the high command of the 2 parties could not convince contestants to forfeit their nomination. In such constituencies, there is a situation that is called a friendly fight(FF) where candidates from both parties INC and SP are contesting instead of just one. These constituencies are marked by the flag FF (Friendly Fight). Others are INC (contested by INC), SP(Contested by SP) and DNC(Contested by none)
phase: The phase in which the election was conducted.
ac_no: Assembly constituency number
ac: Assembly constituency name
district: District to which the ac belongs
candidate: Candidate name
party: Party name
votes: votes for each candidate
Source: Scraped from eciresults.nic.in"
COMBO-17 Galaxy Dataset,Galaxies with brightness measurements in 17 visible bands,Megan Risdal,10,"Version 1,2017-04-17","astronomy
space",CSV,2 MB,Other,"1,836 views",131 downloads,2 kernels,0 topics,https://www.kaggle.com/mrisdal/combo17-galaxy-dataset,"This dataset was obtained here and their description is reproduced below.
Astronomical background
Galaxies are fundamental structures in the Universe. Our Sun lives in the Milky Way Galaxy we can see as a patchy band of light across the sky. The components of a typical galaxy are: a vast number of stars (total mass ~106-1011 Mo where Mo is the unit of a solar mass), a complex interstellar medium of gas and dust from which stars form (typically 1-100% of the stellar component mass), a single supermassive black hole at the center (typically <1% of the stellar component mass), and a poorly understood component called Dark Matter with mass ~5-10-times all the other components combined.
Over the ~14 billion years since the Big Bang, the rate at which galaxies convert interstellar matter into stars has not been constant, and thus the brightness and color of galaxies change with cosmic time. This phenomenon has several names in the astronomical community: the history of star formation in the Universe, chemical evolution of galaxies, or simply galaxy evolution. A major effort over several decades has been made to quantify and understand galaxy evolution using telescopes at all wavelengths.
The traditional tool for such studies has been optical spectroscopy which easily reveals signatures of star formation in nearby galaxies. However, to study star formation in the galaxies recently emerged after the Big Bang, we must examine extremely faint galaxies which are too faint for spectroscopy, even using the biggest available telescopes. A feasible alternative is to obtain images of faint galaxies at random locations in the sky in narrow spectral bands, and thereby construct crude spectra. First, statistical analysis of such multiband photometric datasets are used to classify galaxies, stars and quasars. Second, for the galaxies, multivariate regression is made to develop photometric estimates of redshift, which is a measure both of distance from us and age since the Big Bang. Third, one can examine galaxy colors as a function of redshift (after various corrections are made) to study the evolution of star formation. The present dataset is taken after these first two steps are complete.
Contents
Wolf et al. (2004) provide the first public catalog of a large dataset (63,501 objects) with brightness measurements in 17 bands in the visible band. (Note that the Sloan Digital Sky Survey provides a much larger dataset of 108 objects with measurements in 5 bands.) We provide here a subset of their catalog with 65 columns of information on 3,462 galaxies. These are objects in the Chandra Deep Field South field which Wolf and colleagues have classified as `Galaxies'. The column headings are formally described in their Table 3, and the columns we provide are summarized here with brief commentary:
Col 1: Nr, object number
Col 2-3: Total R (red band) magnitude and its error. This was the band at which the basic catalog was constructed. Magnitudes are inverted logarithmic measures of brightness. A galaxy with R=21 is 100-times brighter than one with R=26. The error is the standard deviation derived from detailed knowledge of the measurement process. This dataset is an excellent example of astronomical datasets where each variable is accompanied by heteroscedastic measurement errors of known variances.
Col 4-5: ApDRmag is the difference between the total and aperture magnitude in the R band. This is a rough measure of the size of the galaxy in the image where ApDRmag=0 corresponds to a point source. Negative values are not physically meaningful. mu_max is the central surface brightness of the object in the R band. The difference between Rmag and mu_max should also be an indicator of galaxy size.
Col 6-9: Mcz and MCzml are two redshift estimates. Mcz is the preferred value. e.Mcz is its estimated error, and chi2red is the reduced chi-squared value of the least-squares fit of the 17-band magnitudes to the best-fit template galaxy spectrum. Galaxies with large e.Mcz or chi2red might be omitted as unreliable.
Col 10-29: These give the absolute magnitudes (i.e. intrinsic luminosities) of the galaxy in 10 bands, with their measurement errors. They are based on the measured magnitudes and the redshifts, and represent the intrinsic luminosities of the galaxies; a galaxy with M=-15 is 100-times less luminous than one with M=-20. These magnitudes are not all independent of each others, but the are important for representing intrinsic properties of the galaxies. Below is one of several redshift-stratified plots of the B-band absolute magnitude (abscissa) against the difference of magnitude (i.e. ratio of luminosities) between the 2800A ultraviolet and blue band, which is a sensitive indicator of star formation. A redshift-dependent bimodal distribution is seen.
Col 30-55: Observed brightnesses in 13 bands in sequence from 420 nm in the ultraviolet to 915 nm in the far red. These are given in linear variables with units of photon flux densities, photons/m2/s/nm. Again, each measurement is accompanied by a measurement error which can be used to distinguish measurement from intrinsic dispersions in the distributions.
Col 56-65: Observed brightnesses in 5 traditional broad spectral bands, UBVRI. These are largely redundant with the 13 bands in the previous columns.
Statistical exercises
Examine basic characteristics of the survey which are not of scientific interest. These include the absence of high-redshift (i.e. distant) high-absolute-magnitude (i.e. faint) galaxies; the dropoff in flux with redshift; the dropoff in image size with redshift.
Study these two populations as a function of redshift to investigate the evolution of star formation."
Popular websites across the globe,General information on some of the most viewed sites country wise,bpali26,10,"Version 2,2017-05-27|Version 1,2017-05-26","world
internet",CSV,3 MB,Other,"4,243 views",667 downloads,7 kernels,4 topics,https://www.kaggle.com/bpali26/popular-websites-across-the-globe,"Context
This dataset includes some of the basic information of the websites we daily use. While scrapping this info, I learned quite a lot in R programming, system speed, memory usage etc. and developed my niche in Web Scrapping. It took about 4-5 hrs for scrapping this data through my system (4GB RAM) and nearly about 4-5 days working out my idea through this project.
Content
The dataset contains Top 50 ranked sites from each 191 countries along with their traffic (global) rank. Here, country_rank represent the traffic rank of that site within the country, and traffic_rank represent the global traffic rank of that site.
Since most of the columns meaning can be derived from their name itself, its pretty much straight forward to understand this dataset. However, there are some instances of confusion which I would like to explain in here:
1) most of the numeric values are in character format, hence, contain spaces which you might need to clean on.
2) There are multiple instances of same website. for.e.g. Yahoo. com is present in 179 rows within this dataset. This is due to their different country rank in each country.
3)The information provided in this dataset is for the top 50 websites in 191 countries as on 25th May 2017 and is subjected to change in future time due to the dynamic structure of ranking.
4) The dataset inactual contains 9540 rows instead of 9550(50*191 rows). This was due to the unavailability of information for 10 websites.
PS: in case if there are anymore queries, comment on this, I'll add an answer to that in above list.
Acknowledgements
I wouldn't have done this without the help of others. I've scrapped this information from publicly available (open to all) websites namely: 1) http://data.danetsoft.com/ 2) http://www.alexa.com/topsites , of which i'm highly grateful. I truly appreciate and thanks the owner of these sites for providing us with the information that I included today in this dataset.
Inspiration
I feel that there this a lot of scope for exploring & visualization this dataset to find out the trends in the attributes of these websites across countries. Also, one could try predicting the traffic(global) rank being a dependent factor on the other attributes of the website. In any case, this dataset will help you find out the popular sites in your area."
Heart Disease Ensemble Classifier,Lm vs Ensemble for classification,Dano,10,"Version 2,2017-12-02|Version 1,2017-01-25",,Other,32 KB,CC0,"4,763 views",633 downloads,6 kernels,2 topics,https://www.kaggle.com/danimal/heartdiseaseensembleclassifier,"Context
Just throwing it out their for anyone and everyone that is interested in heart disease.
Content
Dataset consisting of 14 attributes and 303 observations that were used for the purpose of heart disease classification of a given patient.
Acknowledgements
Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D. University Hospital, Zurich, Switzerland: William Steinbrunn, M.D. University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D. V.A. Medical Center, Long Beach and Cleveland Clinic Foundation: Robert Detrano, M.D., Ph.D
Inspiration
I'd like to have outside input on this model and create a hybrid classifier that can be used by MD's in underserved communities."
Chronic KIdney Disease dataset,Data has 25 feattures which may predict a patient with chronic kidney disease,Mansoor Iqbal,10,"Version 1,2017-04-13",,CSV,47 KB,Other,"4,068 views",508 downloads,3 kernels,0 topics,https://www.kaggle.com/mansoordaku/ckdisease,"# Context
First, I am new to ML, and just in case I slip up, apologies in advance!! So, I am doing an online ML course and this is an assignment where we are supposed to practice scikit-learn's PCA routine. Since the course has been ARCHIVED - which means the discussion posts are not answered!! - hence my posting of the problem here.
What better way to learn than to get so many experts giving me feedback ... right?
# Content
The data was taken over a 2-month period in India with 25 features ( eg, red blood cell count, white blood cell count, etc). The target is the 'classification', which is either 'ckd' or 'notckd' - ckd=chronic kidney disease. There are 400 rows
The data needs cleaning: in that it has NaNs and the numeric features need to be forced to floats. Basically, we were instructed to get rid of ALL ROWS with Nans, with no threshold - meaning, any row that has even one NaN, gets deleted.
Part 1: We are asked to choose 3 features (bgr, rc, wc), visualize them, then run the PCA with n_components=2. the PCA is to be run twice: one with no scaling and the second run WITH scaling. And this is where my issue starts ... in that after scaling I can hardly see any difference!
I will stop here for now till I get feedback and then move to Part 2.
Acknowledgements
The dataset is available at: https://archive.ics.uci.edu/ml/datasets/Chronic_Kidney_Disease
Inspiration
I would like to get an intuitive and a practical understanding of PCA."
NSE India stocks (Indices),1 minute intraday datasets,ramamet,10,"Version 1,2017-05-12",finance,CSV,40 MB,ODbL,"3,272 views",452 downloads,9 kernels,0 topics,https://www.kaggle.com/ramamet4/nse-stocks-database,"Context
nifty50.csv The NIFTY 50 index is National Stock Exchange of India's benchmark stock market index for Indian equity market. It is a well diversified 50 stock index accounting for 22 sectors of the economy. It is used for a variety of purposes such as bench-marking fund portfolios, index based derivatives and index funds.
banknifty.csv Bank Nifty represents the 12 most liquid and large capitalized stocks from the banking sector which trade on the National Stock Exchange (NSE). It provides investors and market intermediaries a benchmark that captures the capital market performance of Indian banking sector.
Content
A data frame with 8 variables: index, date, time, open, high, low, close and id. For each year from 2013 to 2016, the number of trading data of each minute of given each date. The currency of the price is Indian Rupee (INR).
index : market id
date: numerical value (Ex. 20121203- to be converted to 2012/12/03)
time: factor (Ex. 09:16)
open: numeric (opening price)
high: numeric (high price)
low: numeric (low price)
close: numeric (closing price)
Inspiration
Initial raw data sets are very complex and mixed datatypes. These are processed properly using R libraries like dplyr, stringr and other data munging packages. The desired outputs are then converted into a CSV format to use for further analysis."
James Comey Testimony,Full transcript of Comey's Testimony to Senate Intelligence Committee,Matt Snell,10,"Version 5,2017-06-21|Version 4,2017-06-21|Version 3,2017-06-21|Version 2,2017-06-12|Version 1,2017-06-12","law
politics",CSV,363 KB,Other,"1,694 views",113 downloads,3 kernels,2 topics,https://www.kaggle.com/mattsnellaai/comeytestimony,"Context
Data gathered from the James Comey testimony to the Senate Intelligence Committee on June 8th, 2017 regarding possible Russian influence in the 2016 U.S. presidential election.
Content
Content includes the full transcript in transcript.csv as well as a breakdown of questions asked by each senator, their political affiliation, and Comey's response.
All CSVs are UTF-8.
Acknowledgements
Rod Castor - Initial Python script. AppliedAI.
Inspiration
Initially I did analysis to determine length of question by party, length of Comey response by party, number of times each word is used and words with a large difference by party. (Clinton used 16x more by Republicans).
Further analysis to follow as time permits."
Chemical Health Effects and Toxicities,Blended datasets of the health effects and toxicities of various chemicals,Khepry Quixote,10,"Version 1,2017-06-01","healthcare
environment
chemistry",CSV,1 MB,CC4,"2,604 views",330 downloads,5 kernels,0 topics,https://www.kaggle.com/khepryquixote/chemical-health-effects-and-toxicities,"Context
Chemicals have health effects: some recognized and some suspected. A comprehensive list of those chemicals and their health effects would be beneficial to those wishing to associate those chemicals' health effects with other sets of data. The datasets uploaded here represent the ""blending"" of various individual chemical health effect datasets as compiled by Scorecard and hosted by the GoodGuide web site.
Content
As a general rule, each dataset has a chemical's Chemical Abstract Society Registry Number (CASRN), for example ""100-00-5"", its name (e.g. ""P-NITROCHLOROBENZENE""), one or more health effect categories (e.g. ""recognized"" or ""suspected""), and one or more health effects (e.g. ""kidney"" and/or ""neurotoxicity"") , as well as the organization of provenance (e.g. ""HAZMAP"" and/or ""RTECS"") for the recognized and/or suspected health effects.
Acknowledgements
A team from the Environmental Defense Fund created the individual datasets that presently reside at GoodGuide's Scorecard's Health Effects web page. The ""blended"" datasets uploaded herein are various compilations of those individual datasets into ones more suitable for inclusion in data analyses and visualizations.
Inspiration
Initially, the ""blended"" datasets were used to associate health effects with fracking well chemical disclosures, as such joining of data was not readily available to most data analysts. Examples of such datasets can be found at FrackingData.org's FracFocus Data web page."
The ExtraSensory Dataset,Behavioral Context Recognition In-the-Wild,Yonatan Vaizman,10,"Version 1,2017-06-07",psychology,CSV,24 MB,CC4,"2,840 views",211 downloads,3 kernels,0 topics,https://www.kaggle.com/yvaizman/the-extrasensory-dataset,"Context
Behavioral Context refers to a wide range of attributes describing what is going on with you: where you are (home, school, work, at the beach, at a restaurant), what you are doing (sleeping, eating, in a meeting, computer work, exercising, shower), who you are with (family, friends, co-workers), your body posture state (sitting, standing, walking, running), and so on. The ability to automatically (effortlessly, frequently, objectively) recognize behavioral context can serve many domains. Medical applications can monitor physical activity or eating habits; aging-at-home programs can log older adults' physical, social, and mental behavior; personal assistant systems can better server the user if they are aware of the context. In-the-wild (in real life), natural behavior is complex, composed of different aspects, and has high variability. You can run outside at the beach, with friends with your phone in the pocket; you can also run indoors, at the gym, on a treadmill, with your phone motionless next to you. This high variability makes context-recognition a hard task to perform in-the-wild.
Content
The ExtraSensory Dataset was collected from 60 participants where each person participated approximately 7 days. We installed our data-collection mobile app on their personal phone and it was used to collect both sensor-measurements and context-labels. The sensor-measurements were recorded automatically for a window of 20-seconds every minute. This included accelerometer, gyroscope, magnetometer, audio, location, and phone-state from the person's phone, as well as accelerometer and compass from an additional smartwatch that we provided. In addition, the app's interface had many mechanisms for self-reporting the relevant context-labels, including reporting past context, near future, responding to notifications, and more. The flexible interface allowed to collect many labels with minimal effort and interaction-time, to avoid interfering with the natural behavior. The data was collected in-the-wild: participants used their phone in any way that was convenient to them, they engaged in their regular behavior and reported an combinations of labels that fit their context.
For every participant (or ""user""), the dataset has a CSV file with pre-computed features that we extracted from the sensors and with labels. Each row has a separate example (representing 1 minute) and is indexed by the timestamp (seconds since the epoch). There are columns for the sensor-features, with the prefix of the column name indicating the sensor it came from (e.g. prefix ""raw_acc:"" indicating a feature came from the raw phone accelerometer measurements). There are columns for 51 diverse context-labels and the value for an example-label pair is either 1 (the label is relevant for the example), 0 (the label is not relevant), or 'NaN' (missing information).
Here, we provide data for 2 of the 60 participants. You can use this partial data to get familiar with the data and practice algorithms. The full dataset is publicly available at http://extrasensory.ucsd.edu. The website has additional parts of the data (such as a wider range of the original reported labels, location coordinates, mood labels from part of the participants). If you use the data for your publications, you are required to cite our original paper Vaizman, Y., Ellis, K., and Lanckriet, G. ""Recognizing Detailed Human Context In-the-Wild from Smartphones and Smartwatches"". IEEE Pervasive Computing, vol. 16, no. 4, October-December 2017, pp. 62-74. Read the information at http://extrasensory.ucsd.edu and the original paper for more details.
Acknowledgements
The dataset was collected by Yonatan Vaizman and Katherine Ellis, under the supervision of prof. Gert Lanckriet, all from the department of Electrical and Computer Engineering, University of California, San Diego.
Inspiration
The ExtraSensory Dataset can serve as a benchmark to compare methods for context-recognition (or context-awareness, activity recognition, daily activity detection). You can focus on specific sensors or on specific context-labels. You can suggest new models and classifiers, train them on the data and evaluate their performance on the data."
Wage Estimates,Modeled wage estimates of average hourly wages,US Bureau of Labor Statistics,10,"Version 1,2017-06-30",income,CSV,60 MB,CC0,"2,715 views",303 downloads,2 kernels,2 topics,https://www.kaggle.com/bls/wage-estimates,"Context:
The Occupational Employment Statistics (OES) and National Compensation Survey (NCS) programs have produced estimates by borrowing from the strength and breadth of each survey to provide more details on occupational wages than either program provides individually. Modeled wage estimates provide annual estimates of average hourly wages for occupations by selected job characteristics and within geographical location. The job characteristics include bargaining status (union and nonunion), part- and full-time work status, incentive- and time-based pay, and work levels by occupation.
Direct estimates are based on survey responses only from the particular geographic area to which the estimate refers. In contrast, modeled wage estimates use survey responses from larger areas to fill in information for smaller areas where the sample size is not sufficient to produce direct estimates. Modeled wage estimates require the assumption that the patterns to responses in the larger area hold in the smaller area.
The sample size for the NCS is not large enough to produce direct estimates by area, occupation, and job characteristic for all of the areas for which the OES publishes estimates by area and occupation. The NCS sample consists of 6 private industry panels with approximately 3,300 establishments sampled per panel, and 1,600 sampled state and local government units. The OES full six-panel sample consists of nearly 1.2 million establishments.
The sample establishments are classified in industry categories based on the North American Industry Classification System (NAICS). Within an establishment, specific job categories are selected to represent broader occupational definitions. Jobs are classified according to the Standard Occupational Classification (SOC) system.
Content:
Summary: Average hourly wage estimates for civilian workers in occupations by job characteristic and work levels. These data are available at the national, state, metropolitan, and nonmetropolitan area levels.
Frequency of Observations: Data are available on an annual basis, typically in May.
Data Characteristics: All hourly wages are published to the nearest cent.
Acknowledgements:
This dataset was taken directly from the Bureau of Labor Statistics and converted to CSV format.
Inspiration:
This dataset contains the estimated wages of civilian workers in the United States. Wage changes in certain industries may be indicators for growth or decline. Which industries have had the greatest increases in wages? Combine this dataset with the Bureau of Labor Statistics Consumer Price Index dataset and find out what kinds of jobs you would need to afford your snacks and instant coffee!"
"US Supreme Court Cases, 1946-2016",How have court decisions over legal issues changed over time?,Washington University,10,"Version 1,2017-01-26",law,CSV,3 MB,CC4,"2,022 views",235 downloads,4 kernels,0 topics,https://www.kaggle.com/wustl/supreme-court,"Content
The Supreme Court database is the definitive source for researchers, students, journalists, and citizens interested in the United States Supreme Court. The database contains more than two hundred variables regarding each case decided by the Court between the 1946 and 2015 terms. Examples include the identity of the court whose decision the Supreme Court reviewed, the parties to the suit, the legal provisions considered in the case, and the votes of the Justices. The database codebook is available here.
Acknowledgements
The database was compiled by Professor Spaeth of Washington University Law and funded with a grant from the National Science Foundation."
Active Satellites in Orbit Around Earth,Which country has the most satellites in orbit? What are they used for?,Union of Concerned Scientists,10,"Version 1,2017-01-30",space,CSV,336 KB,CC4,"2,642 views",285 downloads,6 kernels,0 topics,https://www.kaggle.com/ucsusa/active-satellites,"Content
The Satellite Database is a listing of active satellites currently in orbit around the Earth. The database includes basic information about the satellites and their orbits, but does not contain the detailed information necessary to locate individual satellites. The information included in the database is publicly accessible and free and was collected from corporate, scientific, government, military, non-governmental, and academic websites available to the public. No copyrighted material was used, nor did we subscribe to any commercial databases for information.
We have attempted to include all currently active satellites. However, satellites are constantly being launched, decommissioned, or simply abandoned, and the list may inadvertently contain some satellites that are no longer active but for which we have not yet received information.
Acknowledgements
The Satellite Database is produced and updated quarterly by Teri Grimwood."
Groundhog Day Forecasts and Temperatures,How accurate is Punxsutawney Phil's winter weather forecast?,Punxsutawney Groundhog Club,10,"Version 1,2017-02-01","climate
cultural mythology",CSV,7 KB,CC0,"3,136 views",298 downloads,35 kernels,0 topics,https://www.kaggle.com/groundhogclub/groundhog-day,"Context
Thousands gather at Gobbler’s Knob in Punxsutawney, Pennsylvania, on the second day of February to await the spring forecast from a groundhog known as Punxsutawney Phil. According to legend, if Phil sees his shadow the United States is in store for six more weeks of winter weather. But, if Phil doesn’t see his shadow, the country should expect warmer temperatures and the arrival of an early spring.
Acknowledgements
The historical weather predictions were provided by the Punxsutawney Groundhog Club, and the average monthly temperatures were published by NOAA's National Climatic Data Center."
NY Philharmonic Performance History,"All Performances, 1842-Present",New York Philharmonic,10,"Version 3,2017-08-16|Version 2,2017-08-16|Version 1,2017-08-03","music
composers
musicians",CSV,246 MB,CC0,"1,676 views",230 downloads,3 kernels,4 topics,https://www.kaggle.com/nyphil/perf-history,"Context:
The New York Philharmonic played its first concert on December 7, 1842. Since then, it has merged with the New York Symphony, the New/National Symphony, and had a long-running summer season at New York's Lewisohn Stadium. The Performance History database documents all known concerts of all of these organizations, amounting to more than 20,000 performances.
Content:
Dataset is a single csv with over 800k rows. Data contains information on season, orchestra, venue, date, time, conductor, work title, composer, movement, and soloists.
Acknowledgements:
This dataset was compiled by the New York Philharmonic. Original json files hosted here. Original json files were flattened and joined on guid to form a single csv file. Image courtesy of Larisa Birta.
Inspiration:
Nearly 175 years of performance history, covering over 11k unique works--which composers are most popular? Have there been any trends in popularity by conductor or by season?"
Beat The Bookie: Odds Series Football Dataset,"+500,000 matches, 11-year odds dataset from 32 bookies for 1,005 leagues",Austro,10,"Version 2,2017-10-24|Version 1,2017-10-09","association football
sports",CSV,84 MB,CC4,"1,141 views",125 downloads,,0 topics,https://www.kaggle.com/austro/beat-the-bookie-worldwide-football-dataset,"The Challenge
The online sports gambling industry employs teams of data analysts to build forecast models that turn the odds at sports games in their favour. While several betting strategies have been proposed to beat bookmakers, from expert prediction models and arbitrage strategies to odds bias exploitation, their returns have been inconsistent and it remains to be shown that a betting strategy can outperform the online sports betting market. We designed a strategy to beat football bookmakers with their own numbers:
""Beating the bookies with their own numbers - and how the online sports betting market is rigged"", by Lisandro Kaunitz, Shenjun Zhong and Javier Kreiner.
Here, we make the full dataset publicly available to the Kaggle community. We also provide the codes, raw SQL database and the online real-time dashboard that were used for our study on github.
Our strategy proved profitable in a 10-year historical simulation using closing odds, a 6-month historical simulation using minute to minute odds, and a 5-month period during which we staked real money with the bookmakers. We would like to challenge the Kaggle community to improve our results:
Can your strategy consistently beat the sports betting market over thousands of bets across leagues around the world?
Do time series odds movements offer insightful information that a betting strategy can exploit?
Can you outperform the bookmakers’ predictions included in the odds data by creating a better model?
What's inside the Beat The Bookie dataset
10 year historical closing odds:
479,440 football games from 818 leagues around the world
Games from 2005-01-01 to 2015-07-30.
Maximum, average and count of active odds at closing time (start of the match)
Betting odds from up to 32 providers
Details about the match: date and time, league, teams, 90-minute score
14-months time series odds:
92,647 football games from 1005 leagues around the world
Games from 2015-09-01 to 2016-11-22
Hourly sampled odds time series, from up to 32 bookmakers from 72 hours before the start of each game
Details about the match: date and time, league, teams, 90-minute score
The dataset was assembled over months of scraping online sport portals.
We hope you enjoy your sports betting simulations (but remember... the house always wins in the end).
Acknowledgements
Ben Fulcher was of great help when we were drafting the paper. Ben has also developed a very nice toolbox for time-series analysis, which might be relevant for the analysis of this dataset."
Swedish central bank interest rate and inflation,Historic Swedish interest rate 1908-2001 and Swedish inflation consumer price,Christian Nygaard,10,"Version 1,2016-09-07",finance,CSV,2 KB,CC4,"4,372 views",463 downloads,21 kernels,0 topics,https://www.kaggle.com/cnygaard/sweden-interest-rate-inflation,"This a blend dataset that contains historic Swedish interest rates from 1908-2001 Source/Källa: Sveriges riksbank and Swedish inflation rate 1908-2001 fetched from Sweden's statistic central bureau SCB.
Content: Blend of Swedish historic central bank interest rate diskkonto and Swedish SCB Consument price index
Acknowledgements / Original data sets:
Swedish central bank interest rate diskkonto http://www.riksbank.se/sv/Rantor-och-valutakurser/Sok-rantor-och-valutakurser/
Consumer price index http://www.scb.se/sv_/Hitta-statistik/Statistik-efter-amne/Priser-och-konsumtion/Konsumentprisindex/Konsumentprisindex-KPI/33772/33779/Konsumentprisindex-KPI/33831/
Data set cover images: Wikipedia https://sv.wikipedia.org/wiki/Enkronan#/media/File:1_Krona_1927,_1.jpg https://en.wikipedia.org/wiki/Flag_of_Sweden#/media/File:Flag_of_Sweden.svg
Inspiration: Question: How does central bank interest rate effect inflation? What are the interest rate inflation rate delays? Verify ROC R^2 inflation/interest rate causation.
Content:
Interestrate and inflation Sweden 1908-2001.csv
Columns
Period Year
Central bank interest rate diskonto average Percent
Inflation Percent
Price level Integer"
Bank Marketing,- a balanced dataset (sample taken from UCI),Bargava,10,"Version 1,2016-12-11",,CSV,897 KB,CC0,"10,244 views",871 downloads,8 kernels,2 topics,https://www.kaggle.com/rouseguy/bankbalanced,"This dataset is taken from UCI : https://archive.ics.uci.edu/ml/datasets/Bank+Marketing
The dataset in UCI is imbalanced. A balanced sample was taken from that dataset to create this. Features remain the same as the original one."
Brazilian Motor Insurance Market,An introduction to the Brazilian motor insurance market trends,Rodrigo Domingos,10,"Version 4,2017-01-11|Version 3,2017-01-02|Version 2,2017-01-02|Version 1,2016-12-31",automobiles,CSV,2 MB,Other,"5,121 views",381 downloads,8 kernels,2 topics,https://www.kaggle.com/rodrigodomingos/brazilian-insurance-motor-market,"Context
The Brazilian government compiles motor insurance data from ago/2000 to ago/2016 and makes it available for public consumption.
Content
There's information about the performance of Brazilian insurance motor like Premium, claim and commission.
Acknowledgements
SUSEP is a governmental office responsible for collect, house and share this information: http://www2.susep.gov.br/menuestatistica/SES/principal.aspx
Inspiration
This information can answer questions about performance and trends regarding to Brazilian motor line of business and make user able to gain insight about this market.
I know this data well but my inspiration to share this data on Kaggle is to discuss and see different points of view."
Environmental Sound Classification 50,raw audio classification of environmental sounds,marc moreaux,10,"Version 3,2017-10-18|Version 2,2017-10-17|Version 1,2017-10-17","categorical data
acoustics",Other,153 MB,CC4,"2,180 views",140 downloads,,0 topics,https://www.kaggle.com/mmoreaux/environmental-sound-classification-50,"Context
Audio classification is often proposed as MFCC classification problem. With this dataset, we intend to give attention to raw audio classification, as performed in the Wavenet network.
Content
The dataset consists in 50 WAV files sampled at 16KHz for 50 different classes.
To each one of the classes, corresponds 40 audio sample of 5 seconds each. All of these audio files have been concatenated by class in order to have 50 wave files of 3 min. 20sec.
In our example notebook, we show how to access the data and visualize a piece of it.
Acknowledgements
We have not much credit in proposing the dataset here. Much of the work have been done by the authors of the ESC-50 Dataset for Environmental Sound Classification. In order to fit on Kaggle, we processed the files with the to_wav.py file present in the original repository. You might also notice that we transformed the data from OGG to WAV as the former didn't seem to be supported in Anaconda.
Inspiration
You might use this dataset to challenge your algorithms in classifying from raw audio ;)"
Racing Kings (chess variant),Over 1.5 million racing king chess variant positions,SalvadorDali,10,"Version 1,2016-11-29",board games,CSV,81 MB,ODbL,"5,986 views",134 downloads,6 kernels,4 topics,https://www.kaggle.com/salvadordali/racingkings,"1.8 million positions of racing king chess variant
Racing kings is a popular chess variant.
Each player has a standard set of pieces without pawns. The opening setup is as below.
In this game, check is entirely forbidden: not only is it forbidden to move ones king into check, but it is also forbidden to check the opponents king. The purpose of the game is to be the first player that moves his king to the eight row. When white moves their king to the eight row, and black moves directly after that also their king to the last row, the game is a draw (this rule is to compensate for the advantage of white that they may move first.) Apart from the above, pieces move and capture precisely as in normal chess.
To learn a little bit more about a game and to experience the evaluation of the position, you can play a couple of games here. Do not forget to select Racing kings chess variant and to analyse the game at the end with the machine. Keep in mind that the evaluation score on lichess website is from -10 to 10 and slightly different than in my dataset.
What you get:
2 csv files train.csv and validate.csv with 1.5 mln and ~0.35 mln positions. Both have an identical structure: FEN of the position and the score.
The score is real value in [-1, 1] range. The closer it is to 1/-1, the more probable is the win of a white/black player. Due to the symmetry I will explain the score only for a white player (for black is the same just with a negative sign.
1 means that white already won (the game is already finished)
0.98 white has a guaranteed(*) win in maximum 1 move
0.96 ... 2 moves
0.94 ... 3 moves
....
0.82 ... in 9 moves
0.80 ... in 10 or more moves
from 0.4 to 0.8 - white has big advantage. For a good player it is not hard to win in such situation
from 0.2 to 0.4 - white has some advantage. Might be hard to convert it to a win
from 0 to 0.2 - white has tiny advantage
0 means that the position is either a draw or very close to a draw
(*) Guaranteed means that the machine has found a forced sequence of moves that allows white player to win no matter what moves the opponent will make. If the opponent makes the best moves - the game will finish in x moves, but it can finish faster if the black player makes a mistake.
Your goal is to use predict a score of the position knowing its FEN.
Use train.csv to build your model and evaluate the performance on the validate.csv dataset (without looking/using it). I used MAE score in my analysis.
Construction of the dataset
Dataset was constructed by me. I created a bot that plays many games against itself. The bot takes 1 second to analyse the position and selects the move based on the score of position. It took almost a month to generate these positions.
What is the purpose?
Currently the plan is to use ML + reinforcement learning to build my own chess bot that will not use alpha-beta prunning for position evaluation and self-play. In a couple of days I will release my own findings as kernels."
Marginal Revolution Blog Post Data,"Author Name, Post Title, Word count, Comment Count, Date, Category Tag",williamnowak,10,"Version 2,2016-09-18|Version 1,2016-09-02","economics
linguistics",CSV,16 MB,CC0,"2,435 views",136 downloads,5 kernels,0 topics,https://www.kaggle.com/wpncrh/marginal-revolution-blog-post-data,"The following dataset contains data on blog posts from MarginalRevolution.com. For posts from Jan. 1, 2010 to 9/17/2016, the following attributes are gathered.
Author Name
Post Title
Post Date
Post content (words)
Number of Words in post
Number of Comments in post
Dummy variable for several commonly used categories
The data was scraped using Python's Beautiful Soup package, and cleaned in R. See my github page (https://github.com/wnowak10/) for the Python and R code."
Parkinson's Disease Observations,Variables Regarding Parkinson's Disease,KrishnaThiyagarajan,10,"Version 1,2016-10-02",,CSV,868 KB,Other,"4,020 views",414 downloads,4 kernels,,https://www.kaggle.com/krisht/parkinsonsdisease,Gives a bunch of data regarding parkinson's disease. Some of these variables have a very high correlation with each other.
SciRate quant-ph,Papers published in arXiv/quant-ph between 2012-2016 with number of Scites,Peter Wittek,10,"Version 7,2017-02-13|Version 6,2017-02-12|Version 5,2017-02-11|Version 4,2017-01-05|Version 3,2017-01-04|Version 2,2017-01-04|Version 1,2017-01-01","research
physics
linguistics",CSV,32 MB,CC4,"3,014 views",117 downloads,11 kernels,,https://www.kaggle.com/peterwittek/scirate-quant-ph,"Context
I was curious about the hot topics in quantum physics as reflected by the quant-ph category on arXiv. Citation counts have a long lag, and so do journal publications, and I wanted a more immediate measure of interest. SciRate is fairly well known in this community, and I noticed that after the initial two-three weeks, the number of Scites a paper gets hardly increases further. So the number of Scites is both immediate and near constant after a short while.
Content
The main dataset (scirate_quant-ph.csv) is the metadata of all papers published in quant-ph between 2012-01-01 and 2016-12-31 that had at least ten Scites, as crawled on 2016-12-31. It has six columns:
The id column as exported by pandas.
The arXiv id.
The year of publication.
The month of publication.
The day of publication.
The number of Scites (this column defines the order).
The title.
All authors separates by a semicolon.
The abstract.
The author names were subjected to normalization and the chances are high that the same author only appears with a unique name.
The name normalization was the difficult part in compiling this collection, and this is why the number of Scites was lower bounded. A second file (scirate_quant-ph_unnormalized.csv) includes all papers that appeared between 2012-2016 irrespective of the number of Scites, but the author names are not normalized. The actual number of Scites for each paper may show a slight variation between the two datasets because the unnormalized version was compiled more than a month later.
Acknowledgements
Many thanks to SciRate for tolerating my crawling trials and not blacklisting my IP address.
Inspiration
Unleash topic models and author analysis to find out what or who is hot in quantum physics today. Build a generative model to write trendy fake titles like SnarXiv does it for hep-th."
Web crawler for real estate market,Identify mispriced places in the residential rental market,Frédéric Girod,10,"Version 3,2017-03-24|Version 2,2017-03-24|Version 1,2017-03-21","home
internet",Other,341 KB,ODbL,"6,185 views",451 downloads,2 kernels,,https://www.kaggle.com/fredgirod/web-crawler-for-real-estate-market,"Creating a rental market database for data analysis and machine learning.
How does it work ?
You scrape the property ads (sale or rent) on internet and you get a dataset.
Then 3 fancy solutions are possible:
Run your webcrawler everyday for a specific place, upload the data in your data warehouse, and monitor the trends in real estate market prices.
Apply machine learning to your database and get a sense of the relative expensiveness of the properties.
Localize every property ads on a Google map using color-coded points in order to visualize the most cheap and expensive neighborhoods.
Original Data Source
For the sake of example, and for proximity reasons, we fetched information from a mid-sized Swiss city, called Lausanne, based in the south of Switzerland. The country has the particularity that people get often puzzled by the level of prices swarming almost everywhere in the rental markets. This is mostly related to the very high living standards prevailing over here. So we used one of the public property ads available in this french-speaking part of the country : https://www.homegate.ch/
Because the booming Swiss housing market is mainly a rental market (foreign investments have been riding high for the sales of property, and mortgage loans are closed to record low), I focused on real estate for rent ads in the Homegate website.
Building a webcrawler
In the Kernels section, you will find out how the Python looks like. I used BeautifulSoup and Urllib Python libraries to grab data from the website. As you can figure out, the code is simple, but really efficient.
What you get
In this example, I extracted data as of 03/17/2017, and I named the DataFrame ""Output"", available in CSV format to make the data compatible with most commonly preferred tools for analysis. It allows you to get a DataFrame with 12 columns:
the date
is it a rent or a buy
the location
the address of the property
the zip code
the available description of the property
the number of rooms
the surface
the floor
the price
the source
Machine learning
In the Kernels section, you will see a very simple ML algorithm applied to the dataset in order to the ""theoretical"" price of each asset, at the end of the code. For the sake of simplicity, I ran a very straightforward linear regression using only 3 features (the 3 only quantitative factors I have at hand) :
the number of rooms
the floor
the surface
I know what you're thinking right at the moment : those 3 features can barely explain the price of a property. Other determinants, such as the location, the neighborhood, the fact that it is outdated, badly maintained by a students roommate partying every night, ... , are of interest when it comes to assessing an appartment. But straightaway, I reduced the model to this.
Google Map display of the property ads and their relative expensiveness
cf Capture.PNG file
Upcoming improvements
Add new features to machine learning process, especially a dummy variable accounting for the neighborhood to which the property pertains.
See to what extent a logistic regression could overcome a linear regressor.
Test more complex machine learning algorithms.
Display trends in rental property prices, for each neighborhood, after establishing a larger database (with a few weeks of scraped data)."
Kospi Stock Price,Korean Stock Kospi Prices,Oh InQueue,10,"Version 1,2017-09-30","finance
economics",Other,155 MB,CC0,"1,517 views",128 downloads,,0 topics,https://www.kaggle.com/gomjellie/kospi-price-data,"Context
I got all these .csv files using pandas data reader but getting every single kospi data through pandas data reader is annoying. so I decided to share this files.
Content
Files
kospi.csv contains average kospi price. you can use this for checking whether if korean stock is day-off or not. xxxxxx.csv contains each single price records. xxxxxx is it's unique ticker.
Columns
Date
format - \d{4}-\d{2}-\d{2}
Open
format - \d{1,}\.\d{1}
High
format - \d{1,}\.\d{1}
Low
format - \d{1,}\.\d{1}
Close
format - \d{1,}\.\d{1}
Adj Close
format - \d{1,}\.\d{1}
Volume
format - \d+
Acknowledgements
blog post which describes how i got these data's. you might need this to update csv files.
git repository git repository
Inspiration
Good luck."
Recipe Ingredients Dataset,Use Recipe Ingredients to Categorize the Cuisine,Kaggle,10,"Version 1,2017-01-19",,{}JSON,15 MB,Other,"6,187 views",656 downloads,2 kernels,0 topics,https://www.kaggle.com/kaggle/recipe-ingredients-dataset,"Context
If you're in Northern California, you'll be walking past the inevitable bushels of leafy greens, spiked with dark purple kale and the bright pinks and yellows of chard. Across the world in South Korea, mounds of bright red kimchi greet you, while the smell of the sea draws your attention to squids squirming nearby. India’s market is perhaps the most colorful, awash in the rich hues and aromas of dozens of spices: turmeric, star anise, poppy seeds, and garam masala as far as the eye can see. Some of our strongest geographic and cultural associations are tied to a region's local foods.
This dataset was featured in our completed playground competition entitled What's Cooking? The objective of the competition was to predict the category of a dish's cuisine given a list of its ingredients.
Content
The data are stored in JSON format.
train.json - the training set containing recipes id, type of cuisine, and list of ingredients
test.json - the test set containing recipes id, and list of ingredients
An example of a recipe node in train.json can be found here or in the file preview section below.
Acknowledgements
This unique dataset was provided by Yummly and featured in a Kaggle playground competition for fun and practice. Visit the competition page if you are interested in checking out past discussions, competition leaderboard, or more details regarding the competition. If you are curious to see how your results rank compared to others', you can still make a submission at the competition submission page!"
Australian Marriage Law Postal Survey,Should the law be changed to allow same-sex couples to marry?,Australian Bureau of Statistics,10,"Version 2,2017-11-15|Version 1,2017-11-15","lgbt
love
government",CSV,294 KB,CC0,"1,288 views",93 downloads,3 kernels,,https://www.kaggle.com/australian-bureau-of-statistics/australian-marriage-law-postal-survey,"Introduction
On 9 August 2017, the Treasurer, under the Census and Statistics Act 1905, directed the Australian Statistician to collect and publish statistical information from all eligible Australians on the Commonwealth Electoral Roll, about their views on whether or not the law should be changed to allow same-sex couples to marry.
The voluntary survey asked one question: should the law be changed to allow same-sex couples to marry? Respondents were asked to mark one box – Yes or No – on the survey form.
Survey materials were mailed to eligible Australians on the Commonwealth Electoral Roll as at 24 August 2017.
A range of strategies were implemented to assist all eligible Australians who wished to complete the survey to do so. A survey response was received from 12,727,920 (79.5%) eligible Australians.
The ABS implemented robust systems and controls for the processing, coding and publication of statistical data. Detailed information on the systems used as well as the accuracy and integrity of the data is available in the Survey Process and the Quality and Integrity Statement.
The official statistics include a count of responses (Yes, No and Response Not Clear) by Federal Electoral Division (FED), State/Territory and National. This also includes a count of eligible Australians who have not participated in the survey.
Information from the Commonwealth Electoral Roll has been used to independently produce a participation rate by age and gender for each FED, State/Territory and National. This rate has been published by gender, for each of the following age groups: 18-19 years, 20-24 years, 25-29 years, 30-34 years, 35-39 years, 40-44 years, 45-49 years, 50-54 years, 55-59 years, 60-64 years, 65-69 years, 70-74 years, 75-79 years, 80-84 years, and 85+ years.
National results
Should the law be changed to allow same-sex couples to marry?
Of the eligible Australians who expressed a view on this question, the majority indicated that the law should be changed to allow same-sex couples to marry, with 7,817,247 (61.6%) responding Yes and 4,873,987 (38.4%) responding No. Nearly 8 out of 10 eligible Australians (79.5%) expressed their view.
All states and territories recorded a majority Yes response. 133 of the 150 Federal Electoral Divisions recorded a majority Yes response, and 17 of the 150 Federal Electoral Divisions recorded a majority No response.
Data Source
All data presented here comes from the official ABS website: https://marriagesurvey.abs.gov.au/
This data was cleaned by Myles O'Neill and Richard Dear to make it easier to work with."
Tatoeba Sentences,A graph of sentences with multi-language translations,dalgacik,9,"Version 2,2017-10-09|Version 1,2017-10-01","languages
linguistics",CSV,236 MB,CC3,"1,019 views",30 downloads,2 kernels,0 topics,https://www.kaggle.com/dalgacik/tatoeba-sentences,"Tatoeba Sentences Corpus
This data is directly from the Tatoeba project: https://tatoeba.org/ It is a large collection of sentences in multiple languages. Many of the sentences are contained with translations in multiple languages. It is a valuable resource for Machine Translation and many Natural Language Processing projects."
Air pollutants measured in Seoul,"Yellow dust, fine dust, where and when to avoid?","Jihye Sofia Seo, Ph.D.",9,"Version 1,2017-11-24","cities
atmospheric sciences
pollution",CSV,275 KB,CC4,"1,307 views",207 downloads,3 kernels,0 topics,https://www.kaggle.com/jihyeseo/seoulairreport,"Context
Sadly, Seoul, South Korea has some of the most polluted air in the world. Since Seoul also represents 25-50% of the South Korean population, the air quality is a concern to many.
It used to be that in Korea, we have bad air quality in spring (yellow wind blowing from the Chinese Yellow River), and clear air in autumn. Now with more industries in China, the air is getting worse in Korea in a different seasonality pattern. This is known as Asian Dust.
Content
Hourly measurement on several air pollutants in dozens of districts in Seoul.
Acknowledgements
Data downloaded from here. http://data.seoul.go.kr/openinf/sheetview.jsp?infId=OA-2275&tMenu=11 We thank Seoul Open Data Plaza for making the datasets available. http://english.seoul.go.kr/policy-information/key-policies/informatization/seoul-open-data-plaza/
The banner photos are via JEONGUK HA on Unsplash
Inspiration
Recently, fine dusts are posing a big problem in Korea. https://www.ft.com/content/b49a9878-141b-11e7-80f4-13e067d5072c"
5-Day Data Challenge Sign-Up Survey Responses,What are folks’ backgrounds? And do they prefer cats or dogs?,Rachael Tatman,9,"Version 3,2017-12-13|Version 2,2017-12-13|Version 1,2017-10-24",categorical data,CSV,706 KB,CC0,"1,078 views",120 downloads,46 kernels,,https://www.kaggle.com/rtatman/5day-data-challenge-signup-survey-responses,"Context:
This dataset contains survey responses to a survey that people could complete when they signed up for the 5-Day Data Challenge.
On December 12, 2017 survey responses for the second 5-Day Data Challenge were added. For this version of the challenge, participants could sign up for either an intro version or a more in-depth regression challenge.
Content:
The optional survey included four multiple-choice questions:
Have you ever taken a course in statistics?
Yep
Yes, but I've forgotten everything
Nope
Do you have any previous experience with programming?
Nope
I have a little bit of experience
I have quite a bit of experience
I have a whole lot of experience
What's your interest in data science?
Just curious
It will help me in my current job
I want to get a job where I use data science
Other
Just for fun, do you prefer dogs or cat?
Dogs 🐶
Cats 🐱
Both 🐱🐶
Neither 🙅
In order to protect privacy, the data has been shuffled (so there’s no temporal order to the responses) and a random 2% of the data has been removed (so even if you know that someone completed the survey, you cannot be sure that their responses are included in this dataset). In addition, all incomplete responses have been removed, and any text entered in the “other” free response field has been replaced with the text “other”.
Acknowledgements:
Thanks to everyone who completed the survey! :)
Inspiration:
Is there a relationship between how much programming experience someone has and why they’re interested in data science?
Are more experienced programmers more likely to have taken statistics?
Do people tend to prefer dogs, cats, both or neither? Is there a relationship between what people prefer and why they’re interested in data science?"
Lord Of The Rings Data,Character and Movie Data,Moko Sharma,9,"Version 7,2017-12-12|Version 6,2017-12-12|Version 5,2017-12-11|Version 4,2017-12-11|Version 3,2017-12-11|Version 2,2017-12-10|Version 1,2017-11-27","literature
classification",CSV,1008 KB,Other,"1,575 views",121 downloads,3 kernels,0 topics,https://www.kaggle.com/mokosan/lord-of-the-rings-character-data,"Context
As a huge LOTR fan, I was excited to have acquired this character data from the Lord of the Rings Wiki. I scraped this data using F#; the repository can be found here: https://github.com/MokoSan/FSharpAdvent.
Content
Data consists of character names, the url in the wiki and the respective race.
Acknowledgements
Wouldn't have been able to publish this data set unless it was for the work done by the great people of the wiki page."
StarCraft II matches history,Predict the results of matches in StarCraft 2 using historical data,alimbekovkz,9,"Version 1,2017-10-18","games and toys
video games
history
strategy",CSV,23 MB,Other,"1,421 views",83 downloads,2 kernels,0 topics,https://www.kaggle.com/alimbekovkz/starcraft-ii-matches-history,"Context
This data set is a collection of all StarCraft pro-player 2 matches. The data is taken from the site - http://aligulac.com/
Content
Dataset data - 18 October 2017. You can parse actual data. Just use my script (Github)
This dataset contains 10 variables:
match_date -Date of match in format mm/dd/yyyy
player_1 - Player 1 Nickname
player_1_match_status - Match status for Player 1: winner or loser
score - match score (example: 1-0, 1-2 etc)
player_2 - Player 2 Nickname
player_2_match_status - Match status for Player 2: winner or loser
player_1_race - Player 1 Race: Z - Zerg, P - Protoss, T - Terran
player_2_race - Player 2 Race: Z - Zerg, P - Protoss, T - Terran
addon - Game addon: WoL- Wings of Liberty, HotS - Heart of the Swarm, LotV - Legacy of the Void
tournament_type - online or offline
Acknowledgements
The source is http://aligulac.com/
Inspiration
Questions worth exploring:
Predict the outcome of a match between two players
or whatever you want ...."
All UK Active Companies By SIC And Geolocated,"3,801,733 company details",Brian J,9,"Version 2,2017-11-16|Version 1,2017-11-15",business,CSV,50 MB,CC0,921 views,83 downloads,,,https://www.kaggle.com/dalreada/all-uk-active-companies-by-sic-and-geolocated,"I work with UK company information on a daily basis, and I thought it would be useful to publish a list of all active companies, in a way that could be used for machine learning.
There are 3,801,733 rows in the dataset, one for each active company. The postcode which is included in the dataset has been geolocated, and the resultant latitude and longitudes have been included, along with the Standard Industrial Classification Code, and date of incorporation.
The company list is from the publicly available 1st November 2017 Companies House snapshot.
The postcode geolocations and SIC Codes are from the gov.uk website.
In the file AllCompanies.csv each row is formatted as follows:
CompanyNumber - in the format of 99999999 for England/Wales, SC999999 for Scotland and NI999999 for Northern Ireland.
IncorporationDate - in British date format, dd/mm/yyyy
RegisteredAddressPostCode - standard British format Postcode
Latitude - to 6 decimal places
Longitude - to 6 decimal places
SIC - 5 digits or if not known, None - see separate file for description of each code.
Inspiration Possible uses for this data is to see where certain types of companies are located in the UK, and how over time they multiply and spread throughout the UK.
Training ML algorithms to predict where there are a high (or low) density of certain types of companies, and where would be a good area for a company to be located, if it wanted minimal competition, or the inverse, where there are clusters of high densities, where it might be easier to recruit specialised staff.
A useful addition would be to overlay population density, which I am currently working on as an option for this dataset.
I am sure there are many more possible uses for this data in ways, that I cannot imagine.
This is my first go at publishing a dataset on any medium, so any useful tips and hints would be extremely welcome.
Links to the raw data sources are here:
Companies House http://download.companieshouse.gov.uk/en_output.html
Postcode to Geolocation https://data.gov.uk/dataset/national-statistics-postcode-lookup-uk
SIC Codes https://www.gov.uk/government/publications/standard-industrial-classification-of-economic-activities-sic"
"Publication and usage reports, 1998-2017-10 (BR)",Publication and access reports of SciELO Brazil from 1998 to October 2017.,SciELO,9,"Version 1,2017-11-13","research
brazil",Other,57 MB,CC0,565 views,33 downloads,,0 topics,https://www.kaggle.com/scieloorg/publishing-and-usage-reports-1998-201710-br,"Context
SciELO (Scientific Electronic Library Online) is an international research communication program launched in 1998 and implemented through a decentralized network of national collections of peer reviewed journals from 15 countries – 12 from Latin America, Portugal, Spain and South Africa – that jointly publish over 1 thousand journals and about 50 thousand articles per year. A thematic collection on Public Health is also operated by the SciELO Program. All collections are accessible via the network portal – http://www.scielo.org.
SciELO aims at the progress of research through the improvement of peer reviewed journals from all disciplines published by scientific and professional associations, academic institutions and public or private research and development institutions. The specific objectives are to increase in a sustainable way the quality, visibility, usage, impact and credibility of the indexed journals and the research they communicate. A key characteristic of SciELO is multilingual publishing, so journals can publish articles in one or multiple languages including the simultaneous publishing of the same article in more than one language.
SciELO Program develops itself according to three principles. First, the conception that scientific knowledge is a public good and therefore should be available openly in the Web. Second, the network operation envisaging to strengthen collaboration and interchange of information and experience, creating scale and lessen the costs. Third, quality control as an essential policy and practice at the level of articles, journals and collections, adoption and compliance with bibliographic and interoperability standards.
SciELO operation and development are carried out following three main action lines. The first is professionalization, which means to produce journals according to the state of art. The second is internationalization, which means to strengthen the active participation of SciELO Journals and Program in the international flow of scientific information. The third is operational and financial sustainability, which means to develop conditions to assure journals to be published on time with a well-established financial model.
All collections follow the SciELO Publishing Model, which comprises three main functions. First, the indexing of journals with metadata of articles, including the bibliographic references of the indexed articles and of the articles they cite. Second, the full text of articles which are available in HTML, PDF and progressively in XML JATS compatible according the SciELO Publishing Schema. Third, the dissemination and interoperability of journals and articles with bibliographic indexes and systems.
SciELO Brazil, led by SciELO / FAPESP Program, acts as the SciELO Network secretariat and coordinates the maintenance of the methodological and technological platform, while the operation of the network collections and journals are decentralized and led by national research agencies.
Content
This dataset contains publication and access reports for the documents of the SciELO Brazil collection between 1998 and October 2017.
The reports present metadata of journals, totals of issues and published documents, thematic areas, authors' affiliation, bibliographic references, use licenses and more. Further details can be found at http://docs.scielo.org/projects/scielo-processing/pt/latest/public_reports.html (in Portuguese, with notes in English)."
Electron Microscopy 3D Segmentation,A copy of the EPFL CVLab dataset,Kevin Mader,9,"Version 1,2017-03-30","neuroscience
microtechnology",Other,496 MB,Other,"1,831 views",176 downloads,7 kernels,0 topics,https://www.kaggle.com/kmader/electron-microscopy-3d-segmentation,"Context
The dataset available for download on this webpage represents a 5x5x5µm section taken from the CA1 hippocampus region of the brain, corresponding to a 1065x2048x1536 volume. The resolution of each voxel is approximately 5x5x5nm.
Content
Two image datasets in 3D of Electron Microscopy data with accompanying labels. The data is provided as multipage TIF files that can be loaded in Fiji, R, KNIME, or Python
Acknowledgements
The dataset was copied from http://cvlab.epfl.ch/data/em directly and only placed here to utilize the Kaggle's kernel and forum capabilities. Please acknowledge the CV group dataset for publication or any other uses
Data Citations
A. Lucchi Y. Li and P. Fua, Learning for Structured Prediction Using Approximate Subgradient Descent with Working Sets, Conference on Computer Vision and Pattern Recognition, 2013.
A. Lucchi, K.Smith, R. Achanta, G. Knott, P. Fua, Supervoxel-Based Segmentation of Mitochondria in EM Image Stacks with Learned Shape Features, IEEE Transactions on Medical Imaging, Vol. 30, Nr. 11, October 2011.
Challenges
How accurately can the segmentation be performed with neural networks?
Is 3D more accurate than 2D for segmentation?
How can mistakes critical to structure or connectivity be penalized more heavily, how would a standard ROC penalize them?"
French Reddit Discussion,"A French written dialog corpus from Reddit, with 1,583,083 utterances in total.",breandan,9,"Version 1,2017-09-28","languages
social groups
demographics
+ 2 more...",Other,211 MB,CC0,"1,066 views",68 downloads,,,https://www.kaggle.com/breandan/french-reddit-discussion,"LELÚ is a French dialog corpus that contains a rich collection of human-human, spontaneous written conversations, extracted from Reddit’s public dataset available through Google BigQuery. Our corpus is composed of 556,621 conversations with 1,583,083 utterances in total. The code to generate this dataset can be found in our GitHub Repository.
The archive spf.tar.gz contains Reddit discussions in an XML file with the following format:
<dialog>
    <s link_id=""4rqtz"" subreddit_id=""2qhjz"">
        <utt uid=""1"" comment_id=""123"" parent_id=""4rqtz"" score=""1"" create_utc=""12458129356"">Hey, how are you?</utt>
        <utt uid=""2"" comment_id=""124"" parent_id=""123"" score=""1"" create_utc=""12458129486"">I’m fine thank you!</utt>
        <utt uid=""1"" comment_id=""125"" parent_id=""124"" score=""1"" create_utc=""12458139804"">Nice!</utt>
    </s>
    <s link_id=""8y1br"" subreddit_id=""2qhjz"">
        <utt uid=""1"" comment_id=""126"" parent_id=""124""  score=""1"" create_utc=""12458129310"">Who’s around for lunch?</utt>
        <utt uid=""2"" comment_id=""127"" parent_id=""126"" score=""1"" create_utc=""12458139345"">Me!</utt>
        <utt uid=""3"" comment_id=""128"" parent_id=""127"" score=""1"" create_utc=""12458149382"">Me too!</utt>
    </s>
</dialog>
The tag attributes can be described as follows:
link_id: ID of the parent Reddit post.
subreddit_id: ID of the subreddit.
uid: ID of the comment author.
comment_id: ID of the Reddit comment.
parent_id: ID of the parent Reddit comment.
We have split up the conversation trees into short sequential conversations using a heuristic described in our paper, LELÚ: A French Dialog Corpus from Reddit, however the full conversation trees can be reconstructed using the comment_id and parent_id attributes of the <utt> tag.
Data was collected from the following subreddits: /r/france, /r/FrancaisCanadien, /r/truefrance, /r/paslegorafi, and /r/rance."
Romania Earthquake Historical Data,Data from 1975 until published date,Daia Alexandru,9,,geology,CSV,94 KB,ODbL,,,,,https://www.kaggle.com/alexandrudaia/romania-earthquake-historical-data,
Vincent van Gogh's paintings,Automatically Identifying Van Gogh's Paintings,Guilherme Folego,9,"Version 1,2016-08-26",,CSV,119 KB,Other,"3,640 views",189 downloads,,,https://www.kaggle.com/gfolego/vangogh,"This is the dataset VGDB-2016 built for the paper ""From Impressionism to Expressionism: Automatically Identifying Van Gogh's Paintings"", which has been published on the 23rd IEEE International Conference on Image Processing (ICIP 2016).
To the best of our knowledge, this is the very first public and open dataset with high quality images of paintings, which also takes density (in Pixels Per Inch) into consideration. The main research question we wanted to address was: Is it possible to distinguish Vincent van Gogh's paintings from his contemporaries? Our method achieved a F1-score of 92.3%.
There are many possibilities for future work, such as:
Increase the dataset. This includes Wikimedia Commons and WikiArt. Unfortunately, Google Art Project does not allow downloads.
Deal with density normalization. There is a lot of data available without such normalization (e.g., Painting-91 and Painter by Numbers). It is possible analyze how this affects accuracy.
Experiment with multi-class and open-set recognition.
Try to identify the painting style, movement, or school.
Maybe study painting authorship verification: given two paintings, are they from the same author?
Is it possible to detect artificially generated paintings? Are they useful for dataset augmentation?
The paper is available at IEEE Xplore (free access until October 6, 2016): https://dx.doi.org/10.1109/icip.2016.7532335
The dataset has been originally published at figshare (CC BY 4.0): https://dx.doi.org/10.6084/m9.figshare.3370627
The source code is available at GitHub (Apache 2.0): https://github.com/gfolego/vangogh
If you find this work useful in your research, please cite the paper! :-)
@InProceedings{folego2016vangogh,
    author = {Guilherme Folego and Otavio Gomes and Anderson Rocha},
    booktitle = {2016 IEEE International Conference on Image Processing (ICIP)},
    title = {From Impressionism to Expressionism: Automatically Identifying Van Gogh's Paintings},
    year = {2016},
    month = {Sept},
    pages = {141--145},
    doi = {10.1109/icip.2016.7532335}
}
Keywords: Art; Feature extraction; Painting; Support vector machines; Testing; Training; Visualization; CNN-based authorship attribution; Painter attribution; Data-driven painting characterization"
MovieLens 100K Dataset,"Stable benchmark dataset. 100,000 ratings from 1000 users on 1700 movies",Prajit Datta,9,"Version 1,2017-01-05",,Other,15 MB,CC0,"8,122 views",801 downloads,15 kernels,0 topics,https://www.kaggle.com/prajitdatta/movielens-100k-dataset,"SUMMARY & USAGE LICENSE
MovieLens data sets were collected by the GroupLens Research Project at the University of Minnesota.
This data set consists of: * 100,000 ratings (1-5) from 943 users on 1682 movies. * Each user has rated at least 20 movies. * Simple demographic info for the users (age, gender, occupation, zip)
The data was collected through the MovieLens web site (movielens.umn.edu) during the seven-month period from September 19th, 1997 through April 22nd, 1998. This data has been cleaned up - users who had less than 20 ratings or did not have complete demographic information were removed from this data set. Detailed descriptions of the data file can be found at the end of this file.
Neither the University of Minnesota nor any of the researchers involved can guarantee the correctness of the data, its suitability for any particular purpose, or the validity of results based on the use of the data set. The data set may be used for any research purposes under the following conditions:
 * The user may not state or imply any endorsement from the
   University of Minnesota or the GroupLens Research Group.

 * The user must acknowledge the use of the data set in
   publications resulting from the use of the data set
   (see below for citation information).

 * The user may not redistribute the data without separate
   permission.

 * The user may not use this information for any commercial or
   revenue-bearing purposes without first obtaining permission
   from a faculty member of the GroupLens Research Project at the
   University of Minnesota.
If you have any further questions or comments, please contact GroupLens .
CITATION
To acknowledge use of the dataset in publications, please cite the following paper:
F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages. DOI=http://dx.doi.org/10.1145/2827872
ACKNOWLEDGEMENTS
Thanks to Al Borchers for cleaning up this data and writing the accompanying scripts.
PUBLISHED WORK THAT HAS USED THIS DATASET
Herlocker, J., Konstan, J., Borchers, A., Riedl, J.. An Algorithmic Framework for Performing Collaborative Filtering. Proceedings of the 1999 Conference on Research and Development in Information Retrieval. Aug. 1999.
FURTHER INFORMATION ABOUT THE GROUPLENS RESEARCH PROJECT
The GroupLens Research Project is a research group in the Department of Computer Science and Engineering at the University of Minnesota. Members of the GroupLens Research Project are involved in many research projects related to the fields of information filtering, collaborative filtering, and recommender systems. The project is lead by professors John Riedl and Joseph Konstan. The project began to explore automated collaborative filtering in 1992, but is most well known for its world wide trial of an automated collaborative filtering system for Usenet news in 1996. The technology developed in the Usenet trial formed the base for the formation of Net Perceptions, Inc., which was founded by members of GroupLens Research. Since then the project has expanded its scope to research overall information filtering solutions, integrating in content-based methods as well as improving current collaborative filtering technology.
Further information on the GroupLens Research project, including research publications, can be found at the following web site:
    http://www.grouplens.org/
GroupLens Research currently operates a movie recommender based on collaborative filtering:
    http://www.movielens.org/
DETAILED DESCRIPTIONS OF DATA FILES
Here are brief descriptions of the data.
ml-data.tar.gz -- Compressed tar file. To rebuild the u data files do this: gunzip ml-data.tar.gz tar xvf ml-data.tar mku.sh
u.data -- The full u data set, 100000 ratings by 943 users on 1682 items. Each user has rated at least 20 movies. Users and items are numbered consecutively from 1. The data is randomly ordered. This is a tab separated list of user id | item id | rating | timestamp. The time stamps are unix seconds since 1/1/1970 UTC
u.info -- The number of users, items, and ratings in the u data set.
u.item -- Information about the items (movies); this is a tab separated list of movie id | movie title | release date | video release date | IMDb URL | unknown | Action | Adventure | Animation | Children's | Comedy | Crime | Documentary | Drama | Fantasy | Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi | Thriller | War | Western | The last 19 fields are the genres, a 1 indicates the movie is of that genre, a 0 indicates it is not; movies can be in several genres at once. The movie ids are the ones used in the u.data data set.
u.genre -- A list of the genres.
u.user -- Demographic information about the users; this is a tab separated list of user id | age | gender | occupation | zip code The user ids are the ones used in the u.data data set.
u.occupation -- A list of the occupations.
u1.base -- The data sets u1.base and u1.test through u5.base and u5.test u1.test are 80%/20% splits of the u data into training and test data. u2.base Each of u1, ..., u5 have disjoint test sets; this if for u2.test 5 fold cross validation (where you repeat your experiment u3.base with each training and test set and average the results). u3.test These data sets can be generated from u.data by mku.sh. u4.base u4.test u5.base u5.test
ua.base -- The data sets ua.base, ua.test, ub.base, and ub.test ua.test split the u data into a training set and a test set with ub.base exactly 10 ratings per user in the test set. The sets ub.test ua.test and ub.test are disjoint. These data sets can be generated from u.data by mku.sh.
allbut.pl -- The script that generates training and test sets where all but n of a users ratings are in the training data.
mku.sh -- A shell script to generate all the u data sets from u.data."
Hand Tremor Dataset for Biometric Recognition,New biometric recognition trait based on hand tremor via leap motion device,hakmesyo,9,"Version 3,2017-11-06|Version 2,2017-11-04|Version 1,2017-11-01","neuroscience
artificial intelligence
computer security",CSV,244 KB,Other,"1,532 views",81 downloads,2 kernels,2 topics,https://www.kaggle.com/hakmesyo/hand-tremor-dataset-for-biometric-recognition,"Context
This dataset is partly associated to the ""Hand Tremor Based Biometric Recognition Using Leap Motion Device"" paper (doi: 10.1109/ACCESS.2017.2764471 ). Objective is to investigate whether hand jitter can be treated as a new behavioral biometric recognition trait in the filed od security so that imitating and/or reproducing artificially can be avoided.
Content
Dataset contains five subjects. 1024 samples each subject's spatiotemporal hand tremor signals as a time series data were acquired via leap motion device. Features are X, Y, Z and Mixed (Average) channels. Channel represents displacement value of adjacent frames (difference between current and previous positions) and finally the last item is class label having value from 1 to 5.
Acknowledgements
I would like to thanks to our volunteer donor who provides us valuable hand tremor data.
Inspiration
Please read the ""Hand Tremor Based Biometric Recognition Using Leap Motion Device"" paper for more details and feature extraction methods. If you have any questions related to the preprocessing and/or processing the dataset please do not hesitate to contact with me via e-mail: hakmesyo@gmail.com . It should be noted that, data acquisition software was implemented in Java (Netbeans) and I utilized Processing, Open Cezeri Library and Weka tools alongside."
Periodic Table of Elements Mapped to Stocks,Elements & Minerals with known and hidden relationships to Stocks,starmine.ai,9,"Version 1,2017-05-25","healthcare
chemistry
finance",CSV,8 MB,CC0,"4,984 views",337 downloads,20 kernels,4 topics,https://www.kaggle.com/biomimic/periodic-table-of-elements-mapped-to-stocks,"Periodic Table of Elements & Minerals with known and hidden relationships to Stocks, ETFs & Options for additional signal boosting built with starmine.ai http://starmine.ai
Rows contain stock symbols. Columns contain scores that represent known and hidden relationships between elements and stocks.
How the scoring is calculated
What kind of things can be done with custom concept columns/features?
Create unique sectors or clusters based on concepts and hidden relationships and compare their gains to the S&P
Determine if price correlations have similar concept or keyword correlations
Examine symbiotic, parasitic and sympathetic relationships between equities
Automatically create baskets of stocks based on concepts and/or keywords
Detach the custom columns and append them to other proprietary inhouse datasets
Select a Data Context (e.g. Biological, Chemical, GeoPhysical and others) to derive different signals
Use stock symbols as custom concept column labels and model cross-correlations between equities
Create features using trending terms anywhere on the internet"
Ofcom UK Broadband Speed 2016 Open dataset,Annual reports on UK's fixed broadband speed 2016 - An Open Dataset,mariakatosvich,9,"Version 2,2016-08-29|Version 1,2016-08-26",networks,CSV,31 MB,ODbL,"4,532 views",283 downloads,8 kernels,2 topics,https://www.kaggle.com/qwikfix/uk-broadband-speeds-2016,"Ofcom annual reports on the UK’s fixed broadband, mobile and WiFi networks, digital television, digital radio and internet infrastructure.
Ofcom gathers data from the main fixed broadband Internet Service Providers (BT, KCOM, Sky, TalkTalk and Virgin Media) on both their retail services and the services they provide to other ISPs as a wholesale service. More information can be found here.
GLA connectivity map shows a summary version of the download speed data. Next generation broadband map - https://maps.london.gov.uk/webmaps/nextgenbroadband/"
Online Chinese Chess (Xiangqi),"10,000 games of Blitz xiangqi",boyofans,9,"Version 2,2017-02-15|Version 1,2017-02-12",board games,CSV,15 MB,CC4,"3,414 views",151 downloads,5 kernels,2 topics,https://www.kaggle.com/boyofans/onlinexiangqi,"Content
Xiangqi, also known as Chinese Chess, is one of the most popular board game in China and Southeastern Asia that is played by millions of people every single day. More information on the rules and the history of Xiangqi can be found from the Xiangqi wikipedia page
The dataset contains 10,000 game logs of Blitz xiangqi played on playOK.com, scraped off playOK API with Python. In particular, the games in the dataset have ID numbers between 57380690 and 57390689. The game records are stored in two separate files:
gameinfo.csv which contains players information and game result in
gameID
game_datetime
blackID
blackELO
redID
redELO
winner
moves.csv which contains game moves in
gameID
turn: A number denoting at which turn of the game the move was made.
side
move: Moves are recorded with the WXF notation. Explainations can be found at XQinEnglish.com
Acknowledgements
Data is scraped from the playOK.com game logs API. Cover photo is from Rosino under CC BY-SA 2.0.
Misc.
There are millions of game logs on playOK.com but I decided to cut the data off at 10,000 games due to file size. If you need more games, check the GitHub repository of my online xiangqi scraper."
Brazilian Aeronautics Accidents,Occurrences involving aircrafts from the last 10 years in Brazil,Paulo Henrique Vasconcellos,9,"Version 2,2017-02-08|Version 1,2017-02-08","brazil
aviation",CSV,615 KB,Other,"2,442 views",202 downloads,8 kernels,2 topics,https://www.kaggle.com/paulovasconcellos/aeronautics-accidents-in-brazil,"Context
For many years airplanes have been considered the second safest transport mean in the world - losing just to elevators. Traveling great distances in short time, those aircrafts have brought several advantaged for the world, both in commercial and regular application. Unfortunately, as any transport mean, aircrafts have their own count of tragedies. The last event envolving airplanes - to the publication date - was the accident envolving the brazilian soccer team Chapecoense and a LAMIA's aircraft, which was transporting them to Colombia for a Championship. This tragedy brought back discussions and controversies about aircraft's security and human capacity during aeronautics occurrences.
Content
This dataset was available by CENIPA - Centro de Investigação e Prevenção de Acidentes aeronáuticos - or Aeronautical Accidents Investigation and Prevention Center. Such files contains informations about occurrences which envolved aircrafts in the last 10 years. You may access more updated data by visiting Brazilian Open Data's official website, or clicking in the download links below.
Acknowledgements
This dataset is available for studies and analysis thanks to CENIPA."
Web visitor interests,1 week web access logs,Yurii Biurher,9,"Version 1,2016-09-21",,CSV,3 MB,Other,"5,599 views",248 downloads,,0 topics,https://www.kaggle.com/yburger/web-visitor-interests,"Aggregated visitor interests compiled from https://www.jc-bingo.com/ web access logs. Includes visitor IP address, user-agent string, visitor country, accessed page languages and topics. Dataset is disseminated free of charge and used in basic customer preferences predictive modeling."
Things on Reddit,The top 100 products in each subreddit from 2015 to 2017,Aleksey Bilogur,9,"Version 1,2017-10-26","business
product
reddit
internet",Other,8 MB,Other,"1,345 views",74 downloads,4 kernels,0 topics,https://www.kaggle.com/residentmario/things-on-reddit,"Methodology
This is a data dump of the top 100 products (ordered by number of mentions) from every subreddit that has posted an amazon product. The data was extracted from Google Bigquery's Reddit Comment database. It only extracts Amazon links, so it is certainly a subset of all products posted to Reddit.
The data is organized in a file structure that follows:
reddits/<first lowercase letter of subreddit>/<subreddit>.csv
An example of where to find the top products for /r/Watches would be:
reddits/w/Watches.csv
Definitions
Below are the column definitions found in each <subreddit>.csv file.
name The name of the product as found on Amazon.
category The category of the product as found on Amazon.
amazon_link The link to the product on Amazon.
total_mentions The total number of times that product was found on Reddit.
subreddit_mentions The total number of times that product was found on that subreddit.
Want more?
You can search and discover products more easily on ThingsOnReddit
Acknowledgements
This dataset was published by Ben Rudolph on GitHub, and was republished as-is on Kaggle."
Video Game Sales and Ratings,Video game sales data from VGChartz with corresponding ratings from Metacritic,KendallGillies,9,"Version 1,2017-01-26",,CSV,1 MB,Other,"3,991 views",634 downloads,15 kernels,0 topics,https://www.kaggle.com/kendallgillies/video-game-sales-and-ratings,"Context
This data set contains a list of video games with sales greater than 100,000 copies along with critic and user ratings. It is a combined web scrape from VGChartz and Metacritic along with manually entered year of release values for most games with a missing year of release. The original coding was created by Rush Kirubi and can be found here, but it limited the data to only include a subset of video game platforms. Not all of the listed video games have information on Metacritic, so there data set does have missing values.
Content
The fields include:
Name - The game's name
Platform - Platform of the games release
Year_of_Release - Year of the game's release
Genre - Genre of the game
Publisher - Publisher of the game
NA_Sales - Sales in North America (in millions)
EU_Sales - Sales in Europe (in millions)
JP_Sales - Sales in Japan (in millions)
Other_Sales - Sales in the rest of the world (in millions)
Global_Sales - Total worldwide sales (in millions)
Critic_score - Aggregate score compiled by Metacritic staff
Critic_count - The number of critics used in coming up with the critic score
User_score - Score by Metacritic's subscribers
User_count - Number of users who gave the user score
Rating - The ESRB ratings
Acknowledgements
Again the main credit behind this data set goes to Rush Kirubi. I just commented out two lines of his code.
Also the original inspiration for this data set came from Gregory Smith who originally scraped the data from VGChartz, it can be found here."
Swedish Crime Rates,Reported crimes in Sweden from 1950 to 2015,MGN,9,"Version 2,2017-02-19|Version 1,2017-02-19",crime,CSV,6 KB,CC0,"5,256 views",632 downloads,14 kernels,,https://www.kaggle.com/mguzmann/swedishcrime,"Context
Swedish crime statistics from 1950 to 2015
Content
This data set contains statistics on reported crimes in Sweden (by 100.000) from 1950 to 2015. It contains the following columns:
crimes.total: total number of reported crimes
crimes.penal.code: total number of reported crimes against the criminal code
crimes.person: total number of reported crimes against a person
murder: total number of reported murder
sexual.offences: total number of reported sexual offences
rape: total number of reported rapes
assault: total number of reported aggravated assaults
stealing.general: total number of reported crimes involving stealing or robbery
robbery: total number of reported armed robberies
burglary: total number of reported armed burglaries
vehicle.theft: total number of reported vehicle thefts
house.theft: total number of reported theft inside a house
shop.theft: total number of reported theft inside a shop
out.of.vehicle.theft: total number of reported theft from a vehicle
criminal.damage: total number of reported criminal damages
other.penal.crimes: number of other penal crime offenses
fraud: total number of reported frauds
narcotics: total number of reported narcotics abuses
drunk.driving: total number of reported drunk driving incidents
Year: the year
population: the total estimated population of Sweden at the time
Acknowledgements
Raw data taken from: https://www.bra.se/bra/bra-in-english/home/crime-and-statistics/crime-statistics.html"
NFL Arrests 2000-2017,"A record of reported NFL Arrests with details about Crime, Team and Player",Patrick Murphy,9,"Version 2,2017-04-06|Version 1,2017-03-23",american football,CSV,174 KB,Other,"10,216 views",377 downloads,2 kernels,0 topics,https://www.kaggle.com/patrickmurphy/nfl-arrests,"Context
""These are arrests, charges and citations of NFL players for crimes more serious than common traffic violations. Almost all of the players belonged to an NFL roster at the time of the incident. In rare cases, a free agent is included only if that player later signs with an NFL team. The data comes from media reports and public records. It cannot be considered fully complete because records of some player arrests might not have been found for various reasons, including lack of media coverage or accessible public records. Many resolutions to these cases also are pending or could not be immediately found."" (Source)
Content
This data covers January 2000 to March 2017. Like mentioned above, it is not fully complete. In the future I hope to add files to add dimensions like USA crime rates, team info, player info, team season records
Column Name | Description | Example data
DATE | Date of the Incident | 3/7/2017
TEAM | Team Identifier at time of incident | SEA (35 total)
NAME | Player Name | Aldon Smith (627 total)
POSITION | Player's Position at time of incident | TE (18 total)
CASE | Incident Type | Cited (10 total)
CATEGORY | Incident Crime Categories, a comma separated list of crime types | DUI (103 unique sets)
DESCRIPTION | A short text description of the incident | Suspected of stealing golf cart, driving drunk, resisting arrest in Scottsdale, Ariz.
OUTCOME | Incident outcome description | Resolution undetermined.
Acknowledgements
The original database was conceived and created by sports writer Brent Schrotenboer of USA Today. http://www.usatoday.com/sports/nfl/arrests/
Past Research:
The Rate of Domestic Violence Arrests Among NFL Players - Benjamin Morris (FiveThirtyEight)
I found this data set August of 2015 and created http://nflarrest.com/ that attempts to provide a visual tool to explore the data set and a RESTful API.
Inspiration
Can the next arrest team or crime or date be predicted?
Does the number of arrests in the previous season, pre-season, in season effect overall Team season record(Wins,losses,playoff progression).
How does the NFL arrest rate compare to the nation on average?
How does the NFL arrest rate compare to populations with similar affluency?
How do crime rates (e.g DUI rates) compare to the geographic area the team represents?"
Urdu-Nepali Parallel Corpus,A part of speech tagged corpus for Urdu & Nepali,Rachael Tatman,9,"Version 1,2017-10-06","languages
india
asia
linguistics",Other,6 MB,CC4,548 views,36 downloads,,0 topics,https://www.kaggle.com/rtatman/urdunepali-parallel-corpus,"Context:
Pakistan has a rich multilingual and multicultural heritage, with about 70 spoken languages, deriving from a diverse set of Indo-Aryan, Indo-Iranian, Sino-Tibetan and Dravidian language families. More than half of these languages also have a written form, employing (predominantly) Perso-Arabic Nastalique and Arabic Naskh writing styles. Gujarati, Gurmuki and Tibetan scripts are also used by some communities, while some others are in the process of defining their writing systems. These languages exhibit a diverse set of sounds and underlying linguistic structures which are both linguistically and computationally exciting and challenging. Most of these languages are not well-studied or well-modeled, and present a vast training ground for researchers in linguistics and computer science.
This dataset provides resources for two languages spoken in Pakistan: Nepali and Urdu. Urdu is the national language of Pakistan, while Nepali is mainly spoken in a small immigrant community.
Content:
This corpus is made of two documents, one in Nepali and one in Urdu. Each document is available with and without part of speech tags. They are parallel to the 100,000 words of common English source from PENN Treebank corpus, available through Linguistic Data Consortium (LDC).
The part of speech tags are those in the Penn Treebank, and additional information can be found in the included .csv file.
Acknowledgements:
This dataset was collected and made available by the Center for Language Engineering at the University of Engineering and Technology UET in Lahore. The work has been supported by the Language Resource Association (GSK) of Japan and International Development Research Center (IDRC) of Canada, through PAN Localization project (www.PANL10n.net). It is distributed here under a CC-BY-NC-SA 3.0 license.
Inspiration:
Nepali and Urdu are written in two different scripts (usually Devanagari and Nastaʿlīq, respectively), but are in the same language family. Can you identify congruent characters in each writing system?
Can you automatically identify which words in Urdu and Nepali are cognates (descended from a common root)?
Can you use these files to build a part of speech tagger for Nepali? Urdu?"
Case Data from San Francisco 311,SF311 cases created since 7/1/2008 with location data,DataSF,9,"Version 1,2017-01-14",crime,CSV,636 MB,Other,"1,894 views",131 downloads,7 kernels,0 topics,https://www.kaggle.com/datasf/case-data-from-san-francisco-311,"Context
This San Francisco 311 dataset contains all 311 cases created since 7/1/2008 (~2M). SF311 is a way for citizens to obtain information, report problems, or submit service requests to the City and County of San Francisco.
Potential question(s) to get started with!
What are some effective visualizations for conveying 311 incidences and trends?
How do 311 requests vary by neighborhood? or source? Over time or seasonally?
What attributes have the greatest effect on how long it takes a case to close?
Is there a way to identify duplicative reports (when multiple people create a 311 report for the same incidence)?
Fields
Please see DataSF's 311 Case Data FAQ here
CaseID - (Numeric) - The unique ID of the service request created.
Opened - (Timestamp) - The date and time when the service request was made
Closed - (Timestamp) - The date and time when the service request was closed
Updated - (Timestamp) - The date and time when the service request was last modified. For requests with status=closed, this will be the date the request was closed
Status - (Text) - The current status of the service request.
Status Notes - (Text) - Explanation of why status was changed to current state or more details on current status than conveyed with status alone
Responsible Agency - (Text) - The agency responsible for fulfilling or otherwise addressing the service request.
Category - (Text) - The Human readable name of the specific service request type
Request Type - (Text) - More specific description of the problem related to the Category
Request Details - (Text) - More specific description of the problem related to the Request Type
Address - (Text) - Human readable address or description of location
Supervisor District - (Numeric) - Supervisor District
Neighborhood - (Text) - Neighborhood
Point - (Geometry: Point) - latitude and longitude using the (WGS84) projection.
Source - (Text) - How the service request was made
Media URL - (Text) - Url to media
We have included the following commonly used geographic shapefile(s):
Supervisor Districts as of April 2012
Neighborhoods
Acknowledgements
Data provided by SF311 via the San Francisco Open Data Portal at https://data.sfgov.org/d/vw6y-z8j6
PDDL 1.0 ODC Public Domain Dedication and Licence (PDDL)
Photo via Flickr Jeremy Brooks (CC BY-NC 2.0)"
Baboon Mating and Genetic Admixture,Data on genetic admixture and likelihood of successful mating between baboons,Dryad Digital Repository,9,"Version 1,2016-11-07",animals,CSV,2 MB,CC0,"2,848 views",185 downloads,4 kernels,0 topics,https://www.kaggle.com/dryad/baboon-mating,"This dataset contains over 12k observations of male-female baboon pairs from a population of baboons that has recently seen genetic admixture from a different (but closely-related) taxon. The data contains genetic and social information for the male and female baboons, whether they mated, and whether the mating resulted in conception of offspring.
Acknowledgements
The original journal article that this data was collected for:
Tung J, Charpentier MJE, Mukherjee S, Altmann J, Alberts SC (2012) Genetic effects on mating success and partner choice in a social mammal. The American Naturalist 180(1): 113-129. http://dx.doi.org/10.1086/665993
The Data Dryad page that this data was downloaded from:
Tung J, Charpentier MJE, Mukherjee S, Altmann J, Alberts SC (2012) Data from: Genetic effects on mating success and partner choice in a social mammal. Dryad Digital Repository. http://dx.doi.org/10.5061/dryad.4r9h61v8
Abstract
(from the original paper)
Mating behavior has profound consequences for two phenomena—individual reproductive success and the maintenance of species boundaries—that contribute to evolutionary processes. Studies of mating behavior in relation to individual reproductive success are common in many species, but studies of mating behavior in relation to genetic variation and species boundaries are less commonly conducted in socially complex species. Here we leveraged extensive observations of a wild yellow baboon (Papio cynocephalus) population that has experienced recent gene flow from a close sister taxon, the anubis baboon (Papio anubis), to examine how admixture-related genetic background affects mating behavior. We identified novel effects of genetic background on mating patterns, including an advantage accruing to anubis-like males and assortative mating among both yellow-like and anubis-like pairs. These genetic effects acted alongside social dominance rank, inbreeding avoidance, and age to produce highly nonrandom mating patterns. Our results suggest that this population may be undergoing admixture-related evolutionary change, driven in part by nonrandom mating. However, the strength of the genetic effect is mediated by behavioral plasticity and social interactions, emphasizing the strong influence of social context on mating behavior in socially complex species.
The Data
This dataset contains over 12,000 observations of the following variables:
female_id: three letter ""short name"" ID for the female in a potentially consorting pair; each female has a unique ID
male_id: three letter ""short name"" ID for the male in a potentially consorting pair; each male has a unique ID
cycle_id: a unique number assigned to each female-estrus cycle combination
consort: whether the female-male pair consorted (1) or not (0), given the opportunity to do so
conceptive: whether the estrus cycle resulted in a conception (1) or not (0)
female_hybridscore: an estimate of the proportion of the female's genome that represents anubis baboon ancestry; for details of the estimation procedure, see Materials and Methods and Tung et al (2008)
male_hybridscore: an estimate of the proportion of the male's genome that represents anubis baboon ancestry; for details of the estimation procedure, see Materials and Methods and Tung et al (2008)
female_gendiv: an estimate of the female's genetic diversity; for details of the estimation procedure, see Materials and Methods
male_gendiv: an estimate of the male's genetic diversity; for details of the estimation procedure, see Materials and Methods
gen_distance: an estimate of the genetic distance (Queller-Goodnight r) between the male and female of a potentially consorting pair
female_age: the age of the female in a potentially consorting pair
male_rank: the ordinal rank of the male in a potentially consorting pair
female_rank: the ordinal rank of the female in a potentially consorting pair
males_present: the number of adult males present in the group of the potentially consorting pair
females_present: the number of adult females present in the group of the potentially consorting pair
male_rank_transform: ordinal male rank transformed to reflect fit (given number of males in a group) to the priority-of-access model; see Materials and Methods and Appendix for more details
gen_distance_transform: genetic distance estimate transform to test whether consortship probabilities decrease with genetic distance as well as genetic similarity
rank_interact: the multiplicative interaction of male rank and female rank in the potentially consorting pair
assort_index: assortative mating index, calculated from the hybrid scores of the male and female of a potentially consorting pair; see Materials and Methods for additional detail
female_age_transform: female age transformed to test for a higher probability of consortship behavior for maximally fertile (middle-aged) females
Inspiration
Here are a few ideas for things to look at in this dataset:
does genetic distance affect mating probability?
does age of the female baboon affect conception probability?
does social rank of the male affect mating probability?"
Pesticide Data Program (2013),Study of pesticide residues in food,United States Department of Agriculture,9,"Version 1,2016-11-17","food and drink
agriculture",CSV,106 MB,Other,"2,382 views",314 downloads,4 kernels,0 topics,https://www.kaggle.com/usdeptofag/pesticide-data-program-2013,"Context
This dataset contains information on pesticide residues in food. The U.S. Department of Agriculture (USDA) Agricultural Marketing Service (AMS) conducts the Pesticide Data Program (PDP) every year to help assure consumers that the food they feed themselves and their families is safe. Ultimately, if EPA determines a pesticide is not safe for human consumption, it is removed from the market.
The PDP tests a wide variety of domestic and imported foods, with a strong focus on foods that are consumed by infants and children. EPA relies on PDP data to conduct dietary risk assessments and to ensure that any pesticide residues in foods remain at safe levels. USDA uses the data to better understand the relationship of pesticide residues to agricultural practices and to enhance USDA’s Integrated Pest Management objectives. USDA also works with U.S. growers to improve agricultural practices.
Content
While the original 2013 MS Access database can be found here, the data has been transferred to a SQLite database for easier, more open use. The database contains two tables, Sample Data and Results Data. Each sampling includes attributes such as extraction method, the laboratory responsible for the test, and EPA tolerances among others. These attributes are labeled with codes, which can be referenced in PDF format here, or integrated into the database using the included csv files.
Inspiration
What are the most common types of pesticides tested in this study?
Do certain states tend to use one particular pesticide type over another?
Does pesticide type correspond more with crop type or location (state)?
Are any produce types found to have higher pesticide levels than assumed safe by EPA standards?
By combining databases from several years of PDP tests, can you see any trends in pesticide use?
Acknowledgement
This dataset is part of the USDA PDP yearly database, and the original source can be found here."
"Executions in the United States, 1976-2016","Use of capital punishment or ""death penalty"" in criminal justice system",Death Penalty Information Center,9,"Version 1,2017-01-25","crime
law",CSV,154 KB,Other,"3,206 views",427 downloads,10 kernels,2 topics,https://www.kaggle.com/usdpic/execution-database,"Content
The execution database includes records of every execution performed in the United States since the Supreme Court reinstated the death penalty in 1976. Federal executions are indicated by FE in the state field and included in the region in which the crime occurred. The information in this database was obtained from news reports, the Department of Corrections in each state, and the NAACP Legal Defense Fund.
Acknowledgements
The execution database was compiled and published by the Death Penalty Information Center. Victim details, including quantity, sex, and race, were acquired from the Criminal Justice Project's Death Row USA report."
California Kindergarten Immunization Rates,How many new students contributed to “herd immunity” between 2000 and 2015?,Brian Roach,9,"Version 3,2017-08-30|Version 2,2017-08-30|Version 1,2017-08-12","diseases
public health",CSV,7 MB,CC0,"2,609 views",326 downloads,2 kernels,2 topics,https://www.kaggle.com/broach/california-kindergarten-immunization-rates,"Context
Vaccinations provide people the ability to develop immunity to particular diseases. When the majority of a population is vaccinated, “herd immunity” protects those who have not been vaccinated by blocking the spread of these diseases. A medical research paper published by The Lancet in 1998 suggested an association between the Measles/Mumps/Rubella (MMR) vaccine and Autism spectrum disorders. The paper was later fully-retracted due controversy surrounding the lead author, who had financial conflicts of interest and allegedly manipulated the study data. However, it generated worldwide concern over the safety of MMR and other types of vaccines, including Diphtheria/Tetanus/Pertussis (DTP).
In California by 2010, the growing trend for parents to opt out of having their children receive vaccines over the following decade coincided with the largest Pertussis outbreak in more than 60 years. Reduced vaccination frequency was also linked to a high-profile measles outbreak in 2014 that began at Disneyland. The resulting California state legislation (Senate Bill 277), signed June 2015, made it much more difficult for parents to opt out of vaccinations for their children. The data set will allow you to explore individual public and private school vaccination rates of incoming Kindergarten students for the 2000 to 2014 school years.
Content
The data are records for every school with ten or more students reporting the number of incoming Kindergarteners who provided either proof of immunization, personal beliefs exemption (PBE), or permanent medical exemption (PME). Annual records for the 2000-2001 through 2014-2015 school years have been formatted and combined. Common variables in these annual data sets included in the merged file are the number of students, school name, school county, the number of PBEs, PMEs, and number of students vaccinated for:
Diphtheria/Tetanus/Pertussis (DTP)
Polio
Measles/Mumps/Rubella (MMR)
One additional file contains 5 years of county-level Pertussis case numbers and rates. Another additional data file contains the number of infant Pertussis cases for infants under three months old for each county in California between 2014-2015.
Geographic data are available in a file based on scripted geocode calls using the ggmap R package to find latitude and longitude data using the school names and county names. Not all calls returned a valid coordinate, so additional indicator variables in this file indicate the quality of the match. The isSchool indicator variable is 1 if the geocode search meta data included ""school"" and the countyMatch indicator is 1 if the latitude and longitude coordinates are contained within the appropriate county in CA.
References:
Retracted Lancet Research Article
Report on 2010 Pertussis Outbreak
Acknowledgements
Individual data files and detailed annual reports for every school year in this data set are provided by the California Department of Public Health (CDPH). Individual schools and licensed child care facilities are required to report immunization information to CDPH every year to maintain compliance with the California Health and Safety Code. Additional details as well as child care and 7th grade data files can be found on the CDPH website: https://www.cdph.ca.gov/programs/immunize/Pages/ImmunizationLevels.aspx
County level case data were pulled from the following report: https://archive.cdph.ca.gov/programs/immunize/Documents/Pertussis_Report_1-7-2015.pdf
Infant Pertussis data were reported to CDPH as of 2/10/2016. Additional Pertussis reports can be found here: https://www.cdph.ca.gov/programs/immunize/Pages/PertussisSummaryReports.aspx
Inspiration
While the Disneyland measles outbreak received much media attention, Pertussis outbreaks in California present great health risks to infants and the elderly. Can you predict which counties and schools are at greatest risk for outbreaks and/or quantify the association between vaccination rates and the number infant Pertussis cases?"
Mr Donald Trump Speeches,Psychological profile of Donald Trump based on his spoken language,Binks,9,"Version 1,2017-08-13","languages
presidents
politics
+ 2 more...",CSV,13 MB,Other,"2,679 views",242 downloads,3 kernels,,https://www.kaggle.com/binksbiz/mrtrump,"Context:
Youtube has introduced automatic generation of subtitles based on speech recognition of uploaded video. This dataset provides collection of subtitles Donald Trump's uploaded speeches. It serves as database for an introduction to algorithmic analysis of spoken language.
Content:
Mr Donald Trump speeches dataset consists of 836 subtitles (sets of words) retrieved from Youtube playlists: ""Donald Trump Speeches & Events"", ""DONALD TRUMP SPEECHES & PRESS CONFERENCE"", ""President Donald Trump Weekly Address 2017"", ""President Donald Trump's First 100 Days | NBC News"", ""Donald Trump Rally Speech Events Press Conference Rallies Playlist"".
This dataset consists of a single CSV file MrTrumpSpeeches.csv. The columns are: 'id', 'playlist', 'upload_date', 'title', 'view_count', 'average_rating', 'like_count', 'dislike_count', 'subtitles', which are delimited with tilde character '~'.
Text data in columns 'subtitles' is not sentence based, there are not commas or dots. It is only stream of words being translated from speech into text by GoogleVoice (more here https://googleblog.blogspot.com.au/2009/11/automatic-captions-in-youtube.html).
Acknowledgements:
The data was downloaded using youtube-dl package.
Inspiration:
I'm interested in psychological profiles of people speaking based on language used. (For example see https://medium.com/@TSchnoebelen/trump-does-not-talk-like-a-woman-breaking-news-gender-continues-to-be-complicated-and-confusing-4c0d28b41d7)"
NYC Transit Data,"Station locations, performance metrics, and turnstile data for early 2016",Nick Wagner,9,"Version 2,2017-07-22|Version 1,2017-07-21",,CSV,1 GB,Other,"2,572 views",262 downloads,,,https://www.kaggle.com/monsieurwagner/nyctransit,"Context
This dataset includes locations of NYC subway stops, performance data for all transit modes, and turnstile data.
Content
The stop and performance data were last update June 2017. The turnstile data is for every week in January-July 2016.
Acknowledgements
All credit to the Metropolitan Transportation Authority of the State of New York and NYC Open Data. Data downloaded from the MTA's http://datamine.mta.info/
Inspiration
This dataset is intended as an accompaniment to the new New York City Taxi Trip Duration challenge. I'm wondering if transit intensity or quality near where a taxi ride starts (or ends) affects how long a taxi ride will last."
New York City Taxi Trip - Distance Matrix,"Google's Distance Matrix API for ""New York City Taxi Trip Duration"" challenge",Debanjan,9,"Version 2,2017-08-03|Version 1,2017-07-29",,CSV,5 MB,CC0,"1,769 views",151 downloads,6 kernels,3 topics,https://www.kaggle.com/debanjanpaul/new-york-city-taxi-trip-distance-matrix,"Context
The idea is to measure recommended route distance and duration(average based on historical data) between two co-ordinates using Google's Distance Matrix API.
Content
Google's distance and duration data appended (as fetched from the API based on the co-ordinate given) to the Kaggle's dataset for ""New York City Taxi Trip Duration"" challenge.
Additionally, great-circle distance between two co-ordinates are also given.
Acknowledgements
The data was retrieved on 29th of July, 2017 from the Google's Distance Matrix API based on Kaggle's dataset given for ""New York City Taxi Trip Duration"" challenge.
Great Circle distance calculated between two co-ordinates using ""geopy""."
U.S. Commercial Aviation Industry Metrics,"Monthly passengers, flights, seat-miles, and revenue-miles from 2002 to 2017",Franklin Bradfield,9,"Version 2,2017-07-13|Version 1,2017-07-12","transport
aviation
vehicles",CSV,2 MB,CC0,"2,208 views",351 downloads,2 kernels,,https://www.kaggle.com/shellshock1911/us-commercial-aviation-industry-metrics,"Context
Have you taken a flight in the U.S. in the past 15 years? If so, then you are a part of monthly data that the U.S. Department of Transportation's TranStats service makes available on various metrics for 15 U.S. airlines and 30 major U.S airports. Their website unfortunately does not include a method for easily downloading and sharing files. Furthermore, the source is built in ASP.NET, so extracting the data is rather cumbersome. To allow easier community access to this rich source of information, I scraped the metrics for every airline / airport combination and stored them in separate CSV files.
Occasionally, an airline doesn't serve a certain airport, or it didn't serve it for the entire duration that the data collection period covers*. In those cases, the data either doesn't exist or is typically too sparse to be of much use. As such, I've only uploaded complete files for airports that an airline served for the entire uninterrupted duration of the collection period. For these files, there should be 174 time series points for one or more of the nine columns below. I recommend any of the files for American, Delta, or United Airlines for outstanding examples of complete and robust airline data.
* No data for Atlas Air exists, and Virgin America commenced service in 2007, so no folders for either airline are included.
Content
There are 13 airlines that have at least one complete dataset. Each airline's folder includes CSV file(s) for each airport that are complete as defined by the above criteria. I've double-checked the files, but if you find one that violates the criteria, please point it out. The file names have the format ""AIRLINE-AIRPORT.csv"", where both AIRLINE and AIRPORT are IATA codes. For a full listing of the airlines and airports that the codes correspond to, check out the airline_codes.csv or airport_codes.csv files that are included, or perform a lookup here. Note that the data in each airport file represents metrics for flights that originated at the airport.
Among the 13 airlines in data.zip, there are a total of 161 individual datasets. There are also two special folders included - airlines_all_airports.csv and airports_all_airlines.csv. The first contains datasets for each airline aggregated over all airports, while the second contains datasets for each airport aggregated over all airlines. To preview a sample dataset, check out all_airlines_all_airports.csv, which contains industry-wide data.
Each file includes the following metrics for each month from October 2002 to March 2017:
Date (YYYY-MM-DD): All dates are set to the first of the month. The day value is just a placeholder and has no significance.
ASM_Domestic: Available Seat-Miles in thousands (000s). Number of domestic flights * Number of seats on each flight
ASM_International*: Available Seat-Miles in thousands (000s). Number of international flights * Number of seats on each flight
Flights_Domestic
Flights_International*
Passengers_Domestic
Passengers_International*
RPM_Domestic: Revenue Passenger-Miles in thousands (000s). Number of domestic flights * Number of paying passengers
RPM_International*: Revenue Passenger-Miles in thousands (000s). Number of international flights * Number of paying passengers
* Frequently contains missing values
Acknowledgements
Thanks to the U.S. Department of Transportation for collecting this data every month and making it publicly available to us all.
Source: https://www.transtats.bts.gov/Data_Elements.aspx
Inspiration
The airline / airport datasets are perfect for practicing and/or testing time series forecasting with classic statistical models such as autoregressive integrated moving average (ARIMA), or modern deep learning techniques such as long short-term memory (LSTM) networks. The datasets typically show evidence of trends, seasonality, and noise, so modeling and accurate forecasting can be challenging, but still more tractable than time series problems possessing more stochastic elements, e.g. stocks, currencies, commodities, etc. The source releases new data each month, so feel free to check your models' performances against new data as it comes out. I will update the files here every 3 to 6 months depending on how things go.
A future plan is to build a SQLite database so a vast array of queries can be run against the data. The data in it its current time series format is not conducive for this, so coming up with a workable structure for the tables is the first step towards this goal. If you have any suggestions for how I can improve the data presentation, or anything that you would like me to add, please let me know. Looking forward to seeing the questions that we can answer together!"
Uniqlo (FastRetailing) Stock Price Prediction,Tokyo Stock Exchange Data (LightWeight CSV) in 2016 for Beginners,Daisuke Ishii,9,"Version 2,2017-08-07|Version 1,2017-08-05","time series
finance",CSV,66 KB,CC0,"3,673 views",330 downloads,16 kernels,8 topics,https://www.kaggle.com/daiearth22/uniqlo-fastretailing-stock-price-prediction,"Context
We are doing Fintech data hakathon in Tokyo everyweek. Let's predict stock price in Tokyo Stock Exchange.
毎週水曜日東京・渋谷で開催している、Team AI ""FinTech Data Hackathon""の題材として、 身近なユニクロ(ファーストリテイリング)の株価予測モデルをオープンイノベーションで構築します。 https://www.meetup.com/Machine-Learning-Meetup-by-team-ai/events/242154425/
Content
Training; 5 year daily stock price info of FastRetailing(Uniqlo). You should predict ""close"" price. Test: 1 week daily stock price
Acknowledgements
Thanks to open market data http://k-db.com/
Inspiration
Let's build basic stock prediction model together! 公開されたモデルを実際の取引に使う場合は十分注意ください。弊社側やコミュニティメンバー側では損失の責任は持てません。"
Clash Royale Matches,Dataset which contain stats of Clash Royale matches,S1M0N38,9,"Version 1,2017-07-31",,Other,396 MB,CC0,"1,418 views",80 downloads,3 kernels,2 topics,https://www.kaggle.com/s1m0n38/clash-royale-matches-dataset,"Content
Every line in the dataset represents the stats of a match (the deck of two player, some players stats, the result of the match, game_type, ...) Every line is in .json format The data collection starts on 2017 - 07 - 12 (yyyy/mm/dd) The data set contain 400k matches"
Bus Breakdown and Delays NYC,When and why a bus was delayed ? Bus delays 2015 to 2017,saagie_anthony,9,"Version 1,2017-07-21",,CSV,33 MB,CC0,"1,712 views",192 downloads,6 kernels,0 topics,https://www.kaggle.com/anthobau/busbreakdownanddelays,"Context
Bus Breakdown and Delays You can find the road where the traffic was heavy for the New York City Taxi Trip Duration playground.
Content
The Bus Breakdown and Delay system collects information from school bus vendors operating out in the field in real time. Bus staff that encounter delays during the route are instructed to radio the dispatcher at the bus vendor’s central office. The bus vendor staff are then instructed to log into the Bus Breakdown and Delay system to record the event and notify OPT. OPT customer service agents use this system to inform parents who call with questions regarding bus service. The Bus Breakdown and Delay system is publicly accessible and contains real time updates. All information in the system is entered by school bus vendor staff.
You can find data for years 2015 to 2017."
Parkinson's Vision-Based Pose Estimation Dataset,Pose estimates of Parkinson's patients using deep learning,limi44,9,"Version 1,2017-09-01",,Other,132 MB,Other,"1,872 views",114 downloads,,,https://www.kaggle.com/limi44/parkinsons-visionbased-pose-estimation-dataset,"Context
The data includes 2D human pose estimates of Parkinson's patients performing a variety of tasks (e.g. communication, drinking from a cup, leg agility). Pose estimates were produced using Convolutional Pose Machines (CPM, https://arxiv.org/abs/1602.00134).
The goal of this project was to use features derived from videos of Parkinson's assessment to predict the severity of parkinsonism and dyskinesia based on clinical rating scales.
Content
Data was acquired as part of a study to measure the minimally clinically important difference in Parkinson's rating scales. Participants received a two hour infusion of levodopa followed by up to two hours of observation. During this time, they were assessed at regular intervals and assessments were video recorded for post-hoc ratings by neurologists. There were between 120-130 videos per task.
The data includes all movement trajectories (extracted frame-by-frame) from the videos of Parkinson's assessments using CPM, as well as confidence values produced by CPM. Ground truth ratings of parkinsonism and dyskinesia severity are included using the UDysRS, UPDRS, and CAPSIT rating scales.
Camera shake has been removed from trajectories (see paper for more details). No other preprocessing has been performed. Files are saved in JSON format. For information on how to deal with files, see data_import_demo.ipynb or view online at https://github.com/limi44/Parkinson-s-Pose-Estimation-Dataset.
Acknowledgements
We would like to acknowledge the staff and patients at Toronto Western Hospital for their time and assistance in this study.
For the papers accompanying these results and more details on data collection and processing, please see:
[1] M.H. Li, T.A. Mestre, S.H. Fox, B. Taati, Vision-Based Assessment of Parkinsonism and Levodopa-Induced Dyskinesia with Deep Learning Pose Estimation, arXiv:1707.09416 [Cs]. (2017). http://arxiv.org/abs/1707.09416.
[2] M.H. Li, T.A. Mestre, S.H. Fox, B. Taati, Automated Vision-Based Analysis of Levodopa-Induced Dyskinesia with Deep Learning, in: 2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), Jeju Island, South Korea, 2017.
[3] T.A. Mestre, I. Beaulieu-Boire, C.C. Aquino, N. Phielipp, Y.Y. Poon, J.P. Lui, J. So, S.H. Fox, What is a clinically important change in the Unified Dyskinesia Rating Scale in Parkinson’s disease?, Parkinsonism & Related Disorders. 21 (2015) 1349–1354. doi:10.1016/j.parkreldis.2015.09.044.
Inspiration
In our study, we aimed to evaluate the readiness of off-the-shelf human pose estimation and deep learning for clinical applications in Parkinson's disease. We hope that others may find this dataset useful for furthering progress in technology-based monitoring of neurological disorders.
Banner
Photo by jesse orrico on Unsplash."
London Borough Demographics,Analyze profiles of London's boroughs,Jones,9,"Version 1,2017-06-04","cities
demographics",CSV,23 KB,ODbL,"1,930 views",178 downloads,2 kernels,2 topics,https://www.kaggle.com/marshald/london-boroughs,"Content
The London boroughs profiles Data about demography, diversity, labour market, economy, community safety, housing, environment, transport, children, health and governance
Acknowledgements
Thanks for taking your time to look at this data and thanks for any suggestions.
Inspiration
I am new to data analysis and I would like some suggestions on how to analyse this dataset and possibly create a visualasation, or predictive analysis."
Boston House Prices,Regression predictive modeling machine learning problem from end-to-end Python,Manimala,9,"Version 1,2017-08-04",,CSV,48 KB,Other,"3,757 views",459 downloads,2 kernels,,https://www.kaggle.com/vikrishnan/boston-house-prices,"Context
To Explore more on Regression Algorithm
Content
Each record in the database describes a Boston suburb or town. The data was drawn from the Boston Standard Metropolitan Statistical Area (SMSA) in 1970. The attributes are deﬁned as follows (taken from the UCI Machine Learning Repository1): CRIM: per capita crime rate by town 2. ZN: proportion of residential land zoned for lots over 25,000 sq.ft. 3. INDUS: proportion of non-retail business acres per town 4. CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) 5. NOX: nitric oxides concentration (parts per 10 million) 1https://archive.ics.uci.edu/ml/datasets/Housing 123 20.2. Load the Dataset 124 6. RM: average number of rooms per dwelling 7. AGE: proportion of owner-occupied units built prior to 1940 8. DIS: weighted distances to ﬁve Boston employment centers 9. RAD: index of accessibility to radial highways 10. TAX: full-value property-tax rate per $10,000 11. PTRATIO: pupil-teacher ratio by town 12. B: 1000(Bk−0.63)2 where Bk is the proportion of blacks by town 13. LSTAT: % lower status of the population 14. MEDV: Median value of owner-occupied homes in $1000s We can see that the input attributes have a mixture of units.
Acknowledgements
Thanks to Dr.Jason"
World of Warcraft Demographics,A short survey meant to look for any links between player and how they play.,Alyssa,9,"Version 1,2017-08-05",,CSV,13 KB,CC0,"1,442 views",104 downloads,,2 topics,https://www.kaggle.com/avenn98/world-of-warcraft-demographics,"Context
This dataset looks at the demographics of World of Warcraft players–gender, age, sexuality, etc.–and how they play the game–role, race, class–to see if there is an association between any of them. Specifically, I was interested in how gender and sexuality affects the gender of the character they play, but there are many other things to look at. This data was gathered through a Google Forms survey, which was then posted on Reddit, Tumblr, Twitter, and my WoW guild's Discord server.
Content
There are 14 columns, 100 rows (not including the titles). 12 of those columns were gathered from a Google Forms survey, and the last two were added by hand. They are:
Timestamp: Useless. Just when the survey was completed.
Gender: The gender of the player.
Sexuality: The sexuality of the player.
Age: Age of the player.
Country: Country the player lives in.
Main: The gender of character the player mains
Faction: The faction the player mains.
Server: The server(s) the player mains.
Role: The role(s) the player mains (DPS, Healer, Tank, or any combination therein).
Class: The class(es) the player mains. This was a question where the respondent check any number of boxes, so there are many different ways it could be answered. Hard to analyze.
Race: The race(s) the player mains. Same as class.
Max: The number of 110s (max level) the player has. Only numerical variable in the dataset.
Attracted: The gender the player is attracted to.
Type: The ""type"" of person the player is. Combines gender and sexuality (""gay woman"", ""bi male"", etc.)
Acknowledgements
This dataset belongs to me. I created the survey and compiled the data. However, I would like to thank stormwind-keep on Tumblr and earth2gem on Twitter for helping me get the survey out to a broader audience.
Inspiration
I already ran a bunch of my own analyses using R, but I could not find a good way to analyze the Class and Race variables. If anyone can figure that one out, please do."
Digimon Database,A database of Digimon and their moves from Digimon Story CyberSleuth,Rachael Tatman,9,"Version 1,2017-07-14",games and toys,CSV,58 KB,CC4,"2,210 views",233 downloads,50 kernels,0 topics,https://www.kaggle.com/rtatman/digidb,"Context:
Digimon, short for “digital monsters”, is a franchise which revolves around a core mechanic of capturing, caring for and training monsters and then engaging in combat with them. It’s similar to Pokémon.
This dataset contains information on digimon from “Digimon Digimon Story: Cyber Sleuth”, released for Playstation Vita in 2015 and Playstation 4 in 2016.
Content:
This database contains three files: a list of all the digimon that can be captured or fought in Cyber Sleuth, all the moves which Digimon can perform, and all the Support Skills. (Support Skills are a passive, stackable, team-wide buff. Each species of Digimon is associated with a single Support Skill.)
Acknowledgements:
This dataset was created by Mark Korsak and is used here with permission. You can find an interactive version of this database here. http://digidb.io/
Inspiration:
This dataset will help you theorycraft the ultimate team as well as ask interesting questions.
Which set of moves will get the best ratio of attack power to SP spent?
Which team of 3 digimon have the highest attack? Defense?
What’s the tradeoff between HP and SP?
Are some types over- or under-represented?
Both the moves and support skills have short text descriptions. Can an NLP analysis reveal underlying clusters of moves?
Are different types and attributes evenly represented across stages?"
CMU Pronouncing Dictionary,"The pronunciation of over 134,000 North American English words",Rachael Tatman,9,"Version 1,2017-08-09","languages
united states
linguistics",Other,3 MB,Other,"1,239 views",75 downloads,3 kernels,0 topics,https://www.kaggle.com/rtatman/cmu-pronouncing-dictionary,"Context:
Pronouncing dictionaries contain a set of words as they appear in written texts as well as their pronunciations. They are often used by researchers who are working on speech technology applications.
Content:
CMUdict (the Carnegie Mellon Pronouncing Dictionary) is a free pronouncing dictionary of English, suitable for uses in speech technology. It was created and is maintained by the Speech Group in the School of Computer Science at Carnegie Mellon University. The version available here was current as of August 8, 2017.
The pronunciations in this dictionary are annotated in ARPABET. More information on APRABET can be found here and here. In this transcription system, each speech sound is represented with a unique one or two letter code, with a space between each speech sound. Vowels are followed by a 1 if they receive the primary stress in a word, and a 0 if they do not.
Acknowledgements
The Carnegie Mellon Pronouncing Dictionary, in its current and previous versions is Copyright (C) 1993-2014 by Carnegie Mellon University. Use of this dictionary for any research or commercial purpose is completely unrestricted. If you make use of or redistribute this material, please acknowledge its origin in your descriptions.
For more information on the terms under which this dataset is distributed, see the LICENSE file.
Inspiration
Can you create an automatic mapping from English orthography (the way a word is spelled) to a word’s pronunciation? How well does this work on out-of-domain words?
Some words in the dictionary have multiple pronunciations. Can you predict which words are more likely to have more than one pronunciation? Does the length of the word have an effect? Its frequency? The sounds in it?"
Spy Plane Finder,Identify Candidate US Government Spy Planes,Jacob Boysen,9,"Version 1,2017-08-12","government agencies
government",CSV,66 MB,CC0,"1,510 views",94 downloads,,0 topics,https://www.kaggle.com/jboysen/spy-plane-finder,"Context:
BuzzFeed had previously reported on flights of spy planes operated by the FBI and the Department of Homeland Security (DHS), and reasoned that it should be possible to train a machine learning algorthim to identify other aircraft performing similar surveillance, based on characteristics of the aircraft and their flight patterns. You can read the story here, and additional analysis and code by Peter Aldhous can be found here.
Content:
BuzzFeed News obtained more than four months of aircraft transponder detections from the plane tracking website Flightradar24, covering August 17 to December 31, 2015 UTC, containing all data displayed on the site within a bounding box encompassing the continental United States, Alaska, Hawaii, and Puerto Rico.
Flightradar24 receives data from its network of ground-based receivers, supplemented by a feed from ground radars provided by the Federal Aviation Administration (FAA) with a five-minute delay.
After parsing from the raw files supplied by Flightradar24, the data included the following fields, for each transponder detection:
adshex Unique identifier for each aircraft, corresponding to its ""Mode-S"" code, in hexademical format.
flight_id Unique identifier for each ""flight segment,"" in hexadecimal format. A flight segment is a continuous series of transponder detections for one aircraft. There may be more than one segment per flight, if a plane disappears from Flightradar24's coverage for a period --- for example when flying over rural areas with sparse receiver coverage. While being tracked by Fightradar24, planes were typically detected several times per minute.
latitude, longitude Geographic location in digital degrees.
altitude Altitude in feet.
speed Ground speed in knots.
squawk Four-digit code transmitted by the transponder.
type Aircraft manufacter and model, if identified.
timestamp Full UTC timestamp.
track Compass bearing in degrees, with 0 corresponding to north.
We also calculated:
steer Change in compass bearing from the previous transponder detection for that aircraft; negative values indicate a turn to the left, positive values a turn to the right.
Feature engineering
First we filtered the data to remove planes registered abroad, based on their adshex code, common commercial airliners, based on their type, and aircraft with fewer than 500 transponder detections.
Then we took a random sample of 500 aircraft and calculated the following for each one:
duration of each flight segment recorded by Flightradar24, in minutes.
boxes Area of a rectangular bounding box drawn around each flight segment, in square kilometers.
Finally, we calculated the following variables for each of the aircraft in the larger filtered dataset:
duration1,duration2,duration3,duration4,duration5 Proportion of flight segment durations for each plane falling into each of five quantiles calculated from duration for the sample of 500 planes. The proportions for each aircraft must add up to 1; if the durations of flight segments for a plane closely matched those for a typical plane from the sample, these numbers would all approximate to 0.2; a plane that mostly flew very long flights would have large decimal fraction for duration5.
boxes1,boxes2,boxes3,boxes4,boxes5 Proportion of bounding box areas for each plane falling into each of five quantiles calculated from boxes for the sample of 500 planes.
speed1,speed2,speed3,speed4,speed5 Proportion of speed values recorded for the aircraft falling into each of five quantiles recorded for speed for the sample of 500 planes.
altitude1,altitude2,altitude3,altitude4,altitude5 Proportion of altitude values recorded for the aircraft falling into each of five quantiles recorded for altitude for the sample of 500 planes.
steer1,steer2,steer3,steer4,steer5,steer6,steer7,steer8 Proportion of steer values for each aircraft falling into bins set manually, after observing the distribution for the sample of 500 planes, using the breaks: -180, -25, -10, -1, 0, 1, 22, 45, 180.
flights Total number of flight segments for each plane.
squawk_1 Squawk code used most commonly by the aircraft.
observations Total number of transponder detections for each plane.
type Aircraft manufacter and model, if identified, else unknown.
The resulting data for 19,799 aircraft are in the file planes_features.csv.
Acknowledgements:
This dataset was created by Peter Aldhous from raw Flightradar24 data, as well as FAA data.
Inspiration:
Peter used a Random Forest classifier--would another approach be better? Worse?
Compare your list of candidates to his here.
This data is from 2015--can you grab up to date data from ADS-B Exchange and find any new candidate planes?"
Open Data 500 Companies,The first comprehensive study of U.S. companies using open government data,GovLab,9,"Version 1,2017-06-23",business,CSV,478 KB,CC4,"4,252 views",426 downloads,,0 topics,https://www.kaggle.com/govlab/open-data-500-companies,"Context
The Open Data 500, funded by the John S. and James L. Knight Foundation (http://www.knightfoundation.org/) and conducted by the GovLab, is the first comprehensive study of U.S. companies that use open government data to generate new business and develop new products and services.
Study Goals
Provide a basis for assessing the economic value of government open data
Encourage the development of new open data companies
Foster a dialogue between government and business on how government data can be made more useful
The Govlab's Approach
The Open Data 500 study is conducted by the GovLab at New York University with funding from the John S. and James L. Knight Foundation. The GovLab works to improve people’s lives by changing how we govern, using technology-enabled solutions and a collaborative, networked approach. As part of its mission, the GovLab studies how institutions can publish the data they collect as open data so that businesses, organizations, and citizens can analyze and use this information.
Company Identification
The Open Data 500 team has compiled our list of companies through (1) outreach campaigns, (2) advice from experts and professional organizations, and (3) additional research.
Outreach Campaign
Mass email to over 3,000 contacts in the GovLab network
Mass email to over 2,000 contacts OpenDataNow.com
Blog posts on TheGovLab.org and OpenDataNow.com
Social media recommendations
Media coverage of the Open Data 500
Attending presentations and conferences
Expert Advice
Recommendations from government and non-governmental organizations
Guidance and feedback from Open Data 500 advisors
Research
Companies identified for the book, Open Data Now
Companies using datasets from Data.gov
Directory of open data companies developed by Deloitte
Online Open Data Userbase created by Socrata
General research from publicly available sources
What The Study Is Not
The Open Data 500 is not a rating or ranking of companies. It covers companies of different sizes and categories, using various kinds of data.
The Open Data 500 is not a competition, but an attempt to give a broad, inclusive view of the field.
The Open Data 500 study also does not provide a random sample for definitive statistical analysis. Since this is the first thorough scan of companies in the field, it is not yet possible to determine the exact landscape of open data companies."
Stanford Open Policing Project - Texas,Data on Traffic and Pedestrian Stops by Police in Texas,Stanford Open Policing Project,9,"Version 1,2017-07-11","government agencies
crime
law",Other,3 GB,Other,"1,244 views",98 downloads,,0 topics,https://www.kaggle.com/stanford-open-policing/stanford-open-policing-project-texas,"Context:
On a typical day in the United States, police officers make more than 50,000 traffic stops. The Stanford Open Policing Project team is gathering, analyzing, and releasing records from millions of traffic stops by law enforcement agencies across the country. Their goal is to help researchers, journalists, and policymakers investigate and improve interactions between police and the public.
If you'd like to see data regarding other states, please go to https://www.kaggle.com/stanford-open-policing.
Content:
This dataset includes over 2 gb of stop data from Texas, covering all of 2010 onwards. Please see the data readme for the full details of the available fields.
Acknowledgements:
This dataset was kindly made available by the Stanford Open Policing Project. If you use it for a research publication, please cite their working paper: E. Pierson, C. Simoiu, J. Overgoor, S. Corbett-Davies, V. Ramachandran, C. Phillips, S. Goel. (2017) “A large-scale analysis of racial disparities in police stops across the United States”.
Inspiration:
How predictable are the stop rates? Are there times and places that reliably generate stops?
Concerns have been raised about jurisdictions using civil forfeiture as a funding mechanism rather than to properly fight drug trafficking. Can you identify any jurisdictions that may be exhibiting this behavior?"
Stanford Open Policing Project - Florida,Data on Traffic and Pedestrian Stops by Police in Florida,Stanford Open Policing Project,9,"Version 1,2017-07-22","government agencies
crime
law",Other,1008 MB,Other,942 views,89 downloads,,0 topics,https://www.kaggle.com/stanford-open-policing/stanford-open-policing-project-florida,"Context:
On a typical day in the United States, police officers make more than 50,000 traffic stops. The Stanford Open Policing Project team is gathering, analyzing, and releasing records from millions of traffic stops by law enforcement agencies across the country. Their goal is to help researchers, journalists, and policymakers investigate and improve interactions between police and the public.
If you'd like to see data regarding other states, please go to https://www.kaggle.com/stanford-open-policing.
Content:
This dataset includes over 1 gb of stop data from Florida. Please see the data readme for the full details of the available fields.
Acknowledgements:
This dataset was kindly made available by the Stanford Open Policing Project. If you use it for a research publication, please cite their working paper: E. Pierson, C. Simoiu, J. Overgoor, S. Corbett-Davies, V. Ramachandran, C. Phillips, S. Goel. (2017) “A large-scale analysis of racial disparities in police stops across the United States”.
Inspiration:
How predictable are the stop rates? Are there times and places that reliably generate stops?
Concerns have been raised about jurisdictions using civil forfeiture as a funding mechanism rather than to properly fight drug trafficking. Can you identify any jurisdictions that may be exhibiting this behavior?"
Stanford Open Policing Project - Bundle 1,Data on Traffic and Pedestrian Stops by Police in many states,Stanford Open Policing Project,9,"Version 1,2017-07-28","government agencies
crime
law",CSV,2 GB,Other,"1,092 views",157 downloads,,0 topics,https://www.kaggle.com/stanford-open-policing/stanford-open-policing-project-bundle-1,"Context:
On a typical day in the United States, police officers make more than 50,000 traffic stops. The Stanford Open Policing Project team is gathering, analyzing, and releasing records from millions of traffic stops by law enforcement agencies across the country. Their goal is to help researchers, journalists, and policymakers investigate and improve interactions between police and the public.
If you'd like to see data regarding other states, please go to https://www.kaggle.com/stanford-open-policing.
Content:
This dataset includes stop data from AZ, CO, CT, IA, MA, MD, MI and MO. Please see the data readme for the full details of the available fields.
Acknowledgements:
This dataset was kindly made available by the Stanford Open Policing Project. If you use it for a research publication, please cite their working paper: E. Pierson, C. Simoiu, J. Overgoor, S. Corbett-Davies, V. Ramachandran, C. Phillips, S. Goel. (2017) “A large-scale analysis of racial disparities in police stops across the United States”.
Inspiration:
How predictable are the stop rates? Are there times and places that reliably generate stops?
Concerns have been raised about jurisdictions using civil forfeiture as a funding mechanism rather than to properly fight drug trafficking. Can you identify any jurisdictions that may be exhibiting this behavior?"
NASDAQ financial fundamentals,Essential fundamental indicators for 2389 companies included into NASDAQ index.,finintelligence.com,9,"Version 1,2017-08-07","business
finance
economics",CSV,15 MB,CC4,"2,142 views",233 downloads,,0 topics,https://www.kaggle.com/finintelligence/nasdaq-financial-fundamentals,"Context
Dataset contains essential financial fundamental indicators for 2389 companies included into NASDAQ index extracted from https://finintelligence.com service. Dataset contains 10 indicators from Income, Cash Flow and Assets statements.
Content
Dataset contains essential financial fundamental indicators for 2389 companies included into NASDAQ index extracted from https://finintelligence.com service. Dataset contains 10 indicators from Income, Cash Flow and Assets statements. Some indicators are automatically calculated, for example:
Companies often don’t report Q4 indicators. From this reasons Q4 amounts may be calculated as annual amount - (Q1 + Q2 + Q3) amounts.
Companies report some indicators ambiguously. For example companies may report Revenue as Sales of Services, Sales of Goods and in many other ways. From that reason Revenue and some other indicators were automatically mapped from variety of reported source indicators.
All indicators in datasets are provided for calendar quarters for years of 2014, 2015 and 2016. Periods are rounded to calendar quarters, for example Feb 1 - May 1 period is included as Q1 into dataset.
Each record contains period, company name, stock tickers (some companies have several tickers), indicator name and amount. Data is originally extracted from companies SEC filings.
Acknowledgements
Data is prepared and provided by https://finintelligence.com service. Service provides intelligent access to variety of corporate financial data extracted from companies documents, press releases and news. Service is absolutely free, no payment required. Visit us today!
Happy data hacking!"
EMPRES Global Animal Disease Surveillance,Global animal disease outbreaks from the last 2 years,Rob Harrand,9,"Version 1,2017-08-24","veterinary medicine
animals
medicine",CSV,3 MB,ODbL,931 views,152 downloads,3 kernels,0 topics,https://www.kaggle.com/tentotheminus9/empres-global-animal-disease-surveillance,"Context
Data downloaded from the EMPRES Global Animal Disease Information System.
Content
Data shows the when, where and what of animal disease outbreaks from the last 2 years, including African swine fever, Foot and mouth disease and bird-flu. Numbers of cases, deaths, etc are also included.
Acknowledgements
This data is from the Food and Agriculture Organization of the United Nations. The EMPRES-i system can be access here Read more about the details of the system here"
Cat Dataset,"Over 9,000 images of cats with annotated facial features",Chris Crawford,9,"Version 2,2018-02-17|Version 1,2018-02-17","animals
image processing
image data
object labeling",Other,2 GB,CC0,555 views,34 downloads,,0 topics,https://www.kaggle.com/crawford/cat-dataset,"Context
The CAT dataset includes over 9,000 cat images. For each image, there are annotations of the head of cat with nine points, two for eyes, one for mouth, and six for ears.
Content
The annotation data are stored in a file with the name of the corresponding image plus .""cat"" at the end. There is one annotation file for each cat image. For each annotation file, the annotation data are stored in the following sequence:
Number of points (default is 9)
Left Eye
Right Eye
Mouth
Left Ear-1
Left Ear-2
Left Ear-3
Right Ear-1
Right Ear-2 -Right Ear-3
Acknowledgements
Weiwei Zhang, Jian Sun, and Xiaoou Tang, Cat Head Detection - How to Effectively Exploit Shape and Texture Features, Proc. of European Conf. Computer Vision, vol. 4, pp.802-816, 2008.
Dataset originally found on the Internet Archive at https://archive.org/details/CAT_DATASET

* This dataset is dedicated to Dan Becker, a huge cat lover. *"
Traditional Decor Patterns,Pattern Classification,Olga Belitskaya,9,"Version 1,2018-01-15","photography
classification
deep learning",Other,48 MB,Other,981 views,66 downloads,,0 topics,https://www.kaggle.com/olgabelitskaya/traditional-decor-patterns,"History
I have made the database of photos sorted by countries and pattern types. Screenshots were performed only on official websites.
Content
The main dataset (decor.zip) is 485 color images (150x150x3) of traditional decor patterns and the file with labels decor.csv. Photo files are in the .png format and the labels are integers and values.
The file DecorColorImages.h5 consists of preprocessing images of this set: image tensors and targets (labels).
Acknowledgements
I have published the data for absolutely free usage by any site visitor. But this database contains the names of famous traditional decor styles, so it can not be used for commercial purposes.
Usage
Classification, image recognition or generation, colorizing, etc. in a case of a small number of images are useful exercises. There are three kinds of classification here: by country, by pattern, by types of image (pattern itself or product).
Improvement
It's possible to find lots of ways for improving this set and the machine learning algorithms applying to it because of many traditional patterns in the world."
Indoor location determination with RSSI,"120K RSSI samples to determine X,Y,Z coordinates in a two-level building",Amir Malekpour,9,"Version 2,2018-01-19|Version 1,2018-01-19",internet,CSV,380 KB,CC4,613 views,57 downloads,,0 topics,https://www.kaggle.com/amirma/indoor-location-determination-with-rssi,"Context
In 2007, I wrote my masters thesis on indoor location determination using Wi-Fi received signal strength indicator (RSSI). As a part of my research, I gathered 120K RSSI samples from 4 access points on floor 1 and 2 of the building of my faculty. I wrote a custom software to do this (no reliable open source software for this at the time) and sampling was a long and painful process. With my limited ML learning skills at the time, I came up with a simple algorithm that was able to find the location of a Wi-Fi device in that building with 85% accuracy. I imagine this is a solved problem now and state of the art indoor positioning systems are way more accurate today. Yet, I decided to find and publish this data set because it's small and simple enough for practice and at the same time, it has some peculiarity for advanced data science and ML fun.
Content
The samples were taken in a two-level building. Each row in the dataset is a single RSSI sample from one of the 4 access points. Access points are identified by one of the letters A, B, C, or D. Physical coordinates of the location where each sample was taken is identified by X,Y, Z coordinates with Z being the floor (1 or 2). At each coordinate, multiple samples are taken, where sample number is identified by a field name sequence. The reason for taking multiple samples is that signal strength fluctuates due to things like scattering and reflection, specially in buildings with moving objects and people. So, to have reliable measurements, one needs 10s of samples from each access points to make for the variability.
Having- said this, each row of the sample set has the format: ap ,signal, sequence ,x,y,z where:
ap: Access point identifier. one of A, B, C or D
signal: Signal strength from access point ap. Note: RSSI values in the table are negated. That is, smaller values in this cell mean stronger signal reception from the access point.
sequence: The sequence of sample from this particular access point at this particular coordinate
x,y,z:: Coordinates where this sample was taken
Note: do not assume all locations have the same number of samples from all access points. For example, at location (x=1,y=1,z=1) we might have a samples from A, b samples from B, c samples from C and d samples from D and a != b != c!= d. Why? That's because in some areas we might have poor reception (or complete lack thereof) from an access point and so we end up with fewer or no samples.
Here's the floor plan of the building where sampling took place
These are the locations of the access points:
{""A"": (23, 17, 2), ""B"": (23, 41, 2), ""C"" : (1, 15, 2), ""D"": (1, 41, 2)}
You can also find the location of access points on the map.
By poking at this data and comparing it with the floor plan, you'll learn interesting things about Wi-Fi radio signals (in 2.4 GHz frequency) and how they behave in indoors. One challenge is to come up with a ML model that given RSSI from the 4 access points finds the (x,y,z) coordinates of the location."
Bad Bad Words,1617 Unique Bad Words,Nick Brooks,9,"Version 1,2018-02-22","languages
crime
linguistics
internet",CSV,13 KB,CC0,209 views,34 downloads,,0 topics,https://www.kaggle.com/nicapotato/bad-bad-words,"Context
The purpose of this dataset is to support the Toxic Comment Classification Competition.
The goal is to help Jigsaw create a model detecting language toxicity levels.
Building of the this dataset is available on my Github.
Content
This dataset contains a lot of bad words.
Acknowledgements
Carnegie Mellon University
user2592414 on StackExchange
Inspiration
Use this for good."
2015 de-identified NY inpatient discharge (SPARCS),Patient characteristics and charges,Jonas Almeida,9,"Version 1,2018-01-25",,CSV,109 MB,CC0,726 views,102 downloads,2 kernels,0 topics,https://www.kaggle.com/jonasalmeida/2015-deidentified-ny-inpatient-discharge-sparcs,"Public Health Data
This is the public dataset made available at https://health.data.ny.gov/Health/Hospital-Inpatient-Discharges-SPARCS-De-Identified/82xm-y6g8 by the Dept of Health of New York state. The following description can be found at that page:
The Statewide Planning and Research Cooperative System (SPARCS) Inpatient De-identified File contains discharge level detail on patient characteristics, diagnoses, treatments, services, and charges. This data file contains basic record level detail for the discharge. The de-identified data file does not contain data that is protected health information (PHI) under HIPAA. The health information is not individually identifiable; all data elements considered identifiable have been redacted. For example, the direct identifiers regarding a date have the day and month portion of the date removed.
It would be nice to ...
... for example, be able to predict length of stay in the hospital using the parameters likely to be available when teh patient is admitted."
Derivatives Trading,Algorithmic trading using machine learning,Ron Leplae,9,"Version 1,2017-08-30",finance,CSV,1 MB,CC0,"2,450 views",159 downloads,,,https://www.kaggle.com/rleplae/derivatives-trading,"This series of datasets represent trade signals on continuous derivates contracts.
Each dataset is composed out of lines. Every line represents a single event and contains the features at the point of the event firing.
The goal is to build a model that can filter in real-time the generated signals and improve the overall performance of the trade robot."
Gone With The Wind,Text Mining with Novels,My Khe Nguyen,9,"Version 2,2017-12-28|Version 1,2017-12-17",linguistics,Other,2 MB,Other,774 views,51 downloads,3 kernels,0 topics,https://www.kaggle.com/mykhe1097/text-mining-gone-with-the-wind,"Context
I am fond of reading romances, so I find practice text mining with novels extremely amusing. I also think this is a good way to approach literary works in quantitative light and gain some new insights.
Content
The dataset includes a Plain Text file and a Microsoft Word file of the book Gone With The Wind by Margaret Mitchell. There are some problems with UTF-8 so more cleaning is needed.
Acknowledgements
I converted this dataset from full PDF file of the book created by Don Lainson dlainson@sympatico.ca, sponsored by Project Gutenberg of Australia eBooks. http://campbellmgold.com/archive_ebooks/gone_with_the_wind_mitchell.pdf
Inspiration
I have started on some sentiment analysis based on chapter number, and based on characters. What is the general air of this classic? Which chapter is the most depressing? Which words are most associated with Scarlett? Rhett Butler? What are the sentiments regarding the War? And the Abolition?"
Last Words of Death Row Inmates,Text Mining with Farewell Words,My Khe Nguyen,9,"Version 1,2017-12-31","crime
demographics",CSV,464 KB,ODbL,"1,758 views",201 downloads,,,https://www.kaggle.com/mykhe1097/last-words-of-death-row-inmates,"1. Context
Capital punishment is one of the controversial human rights issues in the United States. While surfing the Internet for an interesting dataset, I came across this database by Texas Department of Criminal Justice, which comprises of the offenders' last words before execution. Some of the statements are:
""...Young people, listen to your parents; always do what they tell you to do, go to school, learn from your mistakes. Be careful before you sign anything with your name. Never, despite what other people say..."" (Ramiro Hernandez, executed on April 9th, 2014)
""First and foremost I'd like to say, ""Justice has never advanced by taking a life"" by Coretta Scott King. Lastly, to my wife and to my kids, I love y'all forever and always. That's it."" (Taichin Preyor, executed on July 27th, 2017)
As I skimmed these lines, I decided to create this dataset.
2. Content
This dataset includes information on criminals executed by Texas Department of Criminal Justice from 1982 to November 8th, 2017. In Furman v Georgia in 1972, the Supreme Court considered a group of consolidated cases, whereby it severely restricted the death penalty. However, like other states, Texas adjusted its legislation to address the Court's concern and once again allow for capital punishment in 1973. Texas adopted execution by lethal injection in 1977 and in 1982, the starting year of this dataset, the first offender was executed by this method.
The dataset consists of 545 observations with 21 variables. They are:
- Execution: The order of execution, numeric.
- LastName: Last name of the offender, character.
- FirstName: First name of the offender, character.
- TDCJNumber: TDCJ Number of the offender, numeric.
- Age: Age of the offender, numeric.
- Race: Race of the offender, categorical : Black, Hispanic, White, Other.
- CountyOfConviction: County of conviction, character.
- AgeWhenReceived: Age of offender when received, numeric.
- EducationLevel: Education level of offender, numeric.
- Native County: Native county of offender, categorical : 0 = Within Texas, 1= Outside Texas.
- PreviousCrime : Whether the offender committed any crime before, categorical: 0= No, 1= Yes.
- Codefendants: Number of co-defendants, numeric.
- NumberVictim: Number of victims, numeric.
- WhiteVictim, HispanicVictim, BlackVictim, VictimOtherRace. FemaleVictim, MaleVictim: Number of victims with specified demographic features, numeric.
- LastStatement: Last statement of offender, character.
3. Acknowledgement
This dataset is derived from the database by Texas Department of Criminal Justice which can be found in this link: http://www.tdcj.state.tx.us/death_row/dr_executed_offenders.html . It can be seen that the original one has fewer than 10 variables and is embedded with some links to sub-datasets, so I manually inputted more variables based on those links.
There are some complications with this dataset. Firstly, the dataset was manually created so mistakes are inevitable, though I have tried my best to minimize them. Secondly, the recording of offender information is not complete and consistent. For example, sometimes the education level of GED is interpreted as 11 years, at other times as 9 or 10 years. ""None"" and ""NA"" are used interchangeably, making it hard to distinguish between 0 and NA in the coded variable. The victim's information is often omitted, so I rely on the description of the crime for the names and pronouns to make a judgement of the number of victims and their gender. Finally, the last statements are sometimes recorded in the first person and sometimes in the third, so the word choice might not be original. That being said, I find this dataset meaningful and worth sharing.
4. Inspiration
What are the demographics of the death row inmates? What are the patterns of their last statements? What is the relationship between the two?"
Cryptocurrencies,Historical price data for 1200 cryptocurrencies (excluding BTC),Michael Pang,9,"Version 3,2018-01-10|Version 2,2018-01-09|Version 1,2018-01-09","finance
internet",CSV,9 MB,CC0,"1,024 views",172 downloads,,0 topics,https://www.kaggle.com/akababa/cryptocurrencies,"Context
Thousands of cryptocurrencies have sprung up in the past few years. Can you predict which one will be the next BTC?
Content
The dataset contains daily opening, high, low, close, and trading volumes for over 1200 cryptocurrencies (excluding bitcoin).
Acknowledgements
https://timescaledata.blob.core.windows.net/datasets/crypto_data.tar.gz
Inspiration
Speculative forces are always at work on cryptocurrency exchanges - but do they contain any statistically significant features?"
BLE RSSI Dataset for Indoor localization,Bluetooth Low Energy iBeacon RSSI Dataset for Indoor localization and Navigation,Mehdi Mohammadi,9,"Version 1,2018-01-26","universities and colleges
navigation
networks
multiclass classification",CSV,662 KB,CC4,778 views,467 downloads,,,https://www.kaggle.com/mehdimka/ble-rssi-dataset,"Content
The dataset was created using the RSSI readings of an array of 13 ibeacons in the first floor of Waldo Library, Western Michigan University. Data was collected using iPhone 6S. The dataset contains two sub-datasets: a labeled dataset (1420 instances) and an unlabeled dataset (5191 instances). The recording was performed during the operational hours of the library. For the labeled dataset, the input data contains the location (label column), a timestamp, followed by RSSI readings of 13 iBeacons. RSSI measurements are negative values. Bigger RSSI values indicate closer proximity to a given iBeacon (e.g., RSSI of -65 represent a closer distance to a given iBeacon compared to RSSI of -85). For out-of-range iBeacons, the RSSI is indicated by -200. The locations related to RSSI readings are combined in one column consisting a letter for the column and a number for the row of the position. The following figure depicts the layout of the iBeacons as well as the arrange of locations.
Attribute Information
location: The location of receiving RSSIs from ibeacons b3001 to b3013; symbolic values showing the column and row of the location on the map (e.g., A01 stands for column A, row 1).
date: Datetime in the format of ‘d-m-yyyy hh:mm:ss’
b3001 - b3013: RSSI readings corresponding to the iBeacons; numeric, integers only.
Acknowledgements
Provider: Mehdi Mohammadi and Ala Al-Fuqaha, {mehdi.mohammadi, ala-alfuqaha}@wmich.edu, Department of Computer Science, Western Michigan University
Citation Request:
M. Mohammadi, A. Al-Fuqaha, M. Guizani, J. Oh, “Semi-supervised Deep Reinforcement Learning in Support of IoT and Smart City Services,” IEEE Internet of Things Journal, Vol. PP, No. 99, 2017.
Inspiration
How unlabeled data can help for an improved learning system. How a GAN model can synthesizes viable paths based on the little labeled data and larger set of unlabeled data."
Indian hotels on Booking.com,"6,000 Indian hotels on Booking.com",PromptCloud,9,"Version 1,2017-09-16","hotels
internet",CSV,12 MB,CC4,"1,723 views",248 downloads,,0 topics,https://www.kaggle.com/PromptCloudHQ/indian-hotels-on-bookingcom,"Context
This is a pre-crawled dataset, taken as subset of a bigger dataset (more than 94,000 hotels) that was created by extracting data from Booking.com, a leading travel portal.
Content
This dataset has following fields:
address
city
country
crawl_date
hotel_brand
hotel_description
hotel_facilities
hotel_star_rating
image_count
latitude
locality
longitude
pageurl
property_id
property_name
property_type
province
qts
room_count
room_type
similar_hotel
site_review_count
site_review_rating
site_stay_review_rating
sitename
special_tag
state
uniq_id
zone
Acknowledgements
This dataset was created by PromptCloud's in-house web-crawling service.
Inspiration
Analyses of the property ratings and property type can be performed."
wordbatch,,anttip,8,"Version 2,2018-01-11|Version 1,2018-01-08",,Other,2 MB,GPL,"1,299 views",327 downloads,18 kernels,,https://www.kaggle.com/anttip/wordbatch,This dataset does not have a description yet.
Help Blind Community to walk,Predict the action of walk after recognizing view in the image,Hamza Zafar,8,"Version 2,2018-01-18|Version 1,2018-01-17","video games
walking
healthcare",Other,99 MB,CC0,515 views,18 downloads,,0 topics,https://www.kaggle.com/hamzafar/look4me,"Problem Description:
While GPS enabled devices can guide you path on street or on roads. These devices are limited in their usage because these can’t help in walking thorough the enclosed spaces like rooms etc. and thus limitation for blind people to walk in enclosed space.¬ We come up with an idea to model first person walk in rooms to come out from the rooms or enclosed spaces like whenever any person sees a wall in front of him he moves to right or left and fine tune his direction and get out of the room and avoids wall collisions. So we have collected data to guide a person especially blind person to go out of room doors. The typical workflow consists of approach of Computer vision in the following way: The data for first person walking simulator is gathered in a game environment where room’s maps are created and images are captured and labelled walk by Forward, Left and Right classes. Typically human walk is more complicated, but we in this stage only collected three type of action. In the aim to predict the direction to come out of the room.
Content:
A human first recognize the view in front of him and then decides an action by taking Step in forward, sideways, left , right or more complex movement. The same is done during data gathering stage i.e. we captured image first and labelled as Forward if agent can move forward or right if he need to turn right to go out of the door/room.
The whole community of deep learning is invited to help us in walking of disabled person."
Pakistan Intellectual Capital,Teaching Computer Science,Zeeshan-ul-hassan Usmani,8,"Version 1,2017-12-25","visual arts
computer science
machine learning",CSV,325 KB,CC0,"3,006 views",47 downloads,4 kernels,,https://www.kaggle.com/zusmani/pakistanintellectualcapitalcs,"Context
Pakistan has a large number of public and private universities offering degrees in multiple disciplines. There are 162 universities out of which 64 are in private sector and 98 are public sector/government universities recognized by the Higher Education Commission of Pakistan (HEC).
According to HEC, Pakistani universities are producing over half a million graduates per year, which include over more than 10,000 Computer Science/IT graduates.
From year 2001 to 2015 there is a mass increase in number of enrollment in universities. The recent statistics shows that in 2015, 1,298,600 students enrolled in different levels of degree, 869,378 in Bachelors (16 years), 63,412 in Bachelors (17 years), 219,280 in Masters (16 years), 124,107 in M.Phil/MS, 14,373 in Ph.D, and 8,319 in P.G.D. However, in 2014 the number of doctoral degree awarded were 1,351 only.
Moreover, according to HEC report, in 2014-2015 there are over 10,125 fulltime Ph.D. faculty teaching in Pakistan in all disciplines. Computer Science and related disciplines are widely taught in Pakistan with over 90 universities offering this discipline with qualified faculty. According to our dataset, there are 504 PhD faculty members in Computer Science in Pakistan for 10,000 students. So we have a PhD faculty member for every 20 students on average in computer science program.
Current Student to PhD Professor Ratio in Pakistan is 130:1 (while India is going towards 10:1 in Post-Graduate and 25:1 in Undergrad education).
Here is world's Top 100 universities with Student to Staff Ratio.
Content
Dataset: The dataset contains list of computer science/IT professors from 89 different universities of Pakistan.
Variables: The dataset contains Serial No, Teacher’s Name, University Currently Teaching, Department, Province University Located, Designation, Terminal Degree, Graduated from (university for professor), Country of graduation, Year, Area of Specialization/Research Interests, and some Other Information
Acknowledgements
Data has been collected from respective university websites. Some of the universities did not mention about their faculty profiles or were unavailable (hence the limitation of this dataset). The statistics mentioned above are gathered by Higher Education Commission of Pakistan (HEC) website and other web resources.
Inspiration
Here is what I like you to do:
Which area of interest/expertise is in abundance in Pakistan and where we need more people?
How many professors we have in Data Sciences, Artificial Intelligence, or Machine Learning?
Which country and university hosted majority of our teachers?
Which research areas were most common in Pakistan?
How does Pakistan Student to PhD Professor Ratio compare against rest of the world, especially with USA, India and China?
Any visualization and patterns you can generate from this data
Let me know how I can improve this dataset and best of luck with your work"
Simplified Human Activity Recognition w/Smartphone,Recordings of subjects performing activities while carrying inertial sensors.,Marcos Boaglio,8,"Version 1,2018-01-05",,CSV,5 MB,Other,"1,360 views",121 downloads,5 kernels,0 topics,https://www.kaggle.com/mboaglio/simplifiedhuarus,"This is a simplified version of the ""Human Activity Recognition Using Smartphones Data Set "" that can be found in: (https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones)
The sole purpose of this simplification is to use it as a teaching tool at the introductory Machine Learning Course of Universidad de Palermo (Buenos Aires, Argentina). URL of the competition: https://www.kaggle.com/t/5c27656d61ec4808bcbddd67ac1fdc5a
There are no claimed rights of any kind, please refer to original dataset (link above) in order to get the full dataset.
Abstract: Human Activity Recognition database built from the recordings of 30 subjects performing activities of daily living (ADL) while carrying a waist-mounted smartphone with embedded inertial sensors.
Data Set Characteristics: Multivariate, Time-Series
Source:
Jorge L. Reyes-Ortiz(1,2), Davide Anguita(1), Alessandro Ghio(1), Luca Oneto(1) and Xavier Parra(2) 1 - Smartlab - Non-Linear Complex Systems Laboratory DITEN - Università degli Studi di Genova, Genoa (I-16145), Italy. 2 - CETpD - Technical Research Centre for Dependency Care and Autonomous Living Universitat Politècnica de Catalunya (BarcelonaTech). Vilanova i la Geltrú (08800), Spain activityrecognition '@' smartlab.ws
Data Set Information:
The experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually. The obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers was selected for generating the training data and 30% the test data.
The sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. From each window, a vector of features was obtained by calculating variables from the time and frequency domain.
Check the README.txt file for further details about this dataset.
An updated version of this dataset can be found at (https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones) It includes labels of postural transitions between activities and also the full raw inertial signals instead of the ones pre-processed into windows."
Bay Area Rapid Transit Ridership,2016-2017 daily station-to-station BART ridership,SFuh,8,"Version 1,2018-02-10",,CSV,50 MB,CC0,677 views,96 downloads,,0 topics,https://www.kaggle.com/saulfuh/bart-ridership,"Context
BART, short for ""Bay Area Rapid Transit"", is the transit system severing the San Francisco Bay Area in California. BART operates six routes, 46 stations, and and 112 miles of track. It serves an average weekday ridership of 423,000 people, making it the fifth-busiest rapid transit system in the United States.
This dataset contains daily information on BART ridership for a period covering all of 2016 and part of 2017. Unlike some other rapid transit system datasets, this data includes movements between specific stations (there are just over 2000 station-to-station combinations).
Content
This dataset is split in three files. station_info.csv includes generic information on individual stations. date-hour-soo-dest-2017.csv contains daily inter-station ridership for (part of) 2017. date-hour-soo-dest-2016.csv contains daily inter-station ridership for all of 2016.
Acknowledgements
Would like to thank the BART organization for recording the data and providing it to the public.
https://www.bart.gov/about/reports/ridership
Inspiration
Which BART station is the busiest?
What is the least popular BART route?
When is the best time to go to SF if you want to find a seat?
Which day of the week is the busiest?
How many people take the BART late at night?
Does the BART ever stop in a station without anyone going off or on?"
Synthetic Speech Commands Dataset,"Text-to-speech counterparts for the ""Speech Commands Data Set v0.01""",JohannesBuchner,8,"Version 1,2017-12-23","languages
acoustics
communication
human-computer interaction",Other,1 GB,CC4,460 views,103 downloads,,0 topics,https://www.kaggle.com/jbuchner/synthetic-speech-commands-dataset,"Context
We would like to have good open source speech recognition
Commercial companies try to solve a hard problem: map arbitrary, open-ended speech to text and identify meaning
The easier problem should be: detect a predefined sequence of sounds and map it to a predefined action.
Lets tackle the simplest problem first: Classifying single, short words (commands)
Audio training data is difficult to obtain.
Approaches
The parent project (spoken verbs) created synthetic speech datasets using text-to-speech programs. The focus there is on single-syllable verbs (commands).
The Speech Commands dataset (by Pete Warden, see the TensorFlow Speech Recognition Challenge) asked volunteers to pronounce a small set of words: (yes, no, up, down, left, right, on, off, stop, go, and 0-9).
This data set provides synthetic counterparts to this real world dataset.
Open questions
One can use these two datasets in various ways. Here are some things I am interested in seeing answered:
What is it in an audio sample that makes it ""sound similar""? Our ears can easily classify both synthetic and real speech, but for algorithms this is still hard. Extending the real dataset with the synthetic data yields a larger training sample and more diversity.
How well does an algorithm trained on one data set perform on the other? (transfer learning) If it works poorly, the algorithm probably has not found the key to audio similarity.
Are synthetic data sufficient for classifying real datasets? If this is the case, the implications are huge. You would not need to ask thousands of volunteers for hours of time. Instead, you could easily create arbitrary synthetic datasets for your target words.
A interesting challenge (idea for competition) would be to train on this data set and evaluate on the real dataset.
Synthetic data creation
Here I describe how the synthetic audio samples were created. Code is available at https://github.com/JohannesBuchner/spoken-command-recognition, in the ""tensorflow-speech-words"" folder.
The list of words is in ""inputwords"". ""marvin"" was changed to ""marvel"", because ""marvin"" does not have a pronounciation coding yet.
Pronounciations were taken from the British English Example Pronciation dictionary (BEEP, http://svr-www.eng.cam.ac.uk/comp.speech/Section1/Lexical/beep.html ). The phonemes were translated for the next step with a translation table (see compile.py for details). This creates the file ""words"". There are multiple pronounciations and stresses for each word.
A text-to-speech program (espeak) was used to pronounce these words (see generatetfspeech.sh for details). The pronounciation, stress, pitch, speed and speaker were varied. This gives >1000 clean examples for each word.
Noise samples were obtained. Noise samples (airport babble car exhibition restaurant street subway train) come from AURORA (https://www.ee.columbia.edu/~dpwe/sounds/noise/), and additional noise samples were synthetically created (ocean white brown pink). (see ../generatenoise.sh for details)
Noise and speech were mixed. The speech volume and offset were varied. The noise source, volume was also varied. See addnoise.py for details. addnoise2.py is the same, but with lower speech volume and higher noise volume. All audio files are one second (1s) long and are in wav format (16 bit, mono, 16000 Hz).
Finally, the data was compressed into an archive and uploaded to kaggle.
Acknowledgements
This work built upon
Pronounciation dictionary: BEEP: http://svr-www.eng.cam.ac.uk/comp.speech/Section1/Lexical/beep.html
Noise samples: AURORA: https://www.ee.columbia.edu/~dpwe/sounds/noise/
eSPEAK: http://espeak.sourceforge.net/ and mbrola voices http://www.tcts.fpms.ac.be/synthesis/mbrola/mbrcopybin.html
Please provide appropriate citations to the above when using this work.
To cite the resulting dataset, you can use:
APA-style citation: ""Buchner J. Synthetic Speech Commands: A public dataset for single-word speech recognition, 2017. Available from https://www.kaggle.com/jbuchner/synthetic-speech-commands-dataset/"".
BibTeX @article{speechcommands, title={Synthetic Speech Commands: A public dataset for single-word speech recognition.}, author={Buchner, Johannes}, journal={Dataset available from https://www.kaggle.com/jbuchner/synthetic-speech-commands-dataset/}, year={2017} }
Thanks to everyone trying to improve open source voice detection and speech recognition.
Links
https://www.kaggle.com/jbuchner/spokenverbs
https://www.kaggle.com/c/tensorflow-speech-recognition-challenge
https://github.com/JohannesBuchner/spoken-command-recognition/"
EmoSim508,An Emoji Similarity Baseline Dataset with 508 Emoji Pairs and Similarity Ratings,Sanjaya Wijeratne,8,"Version 1,2017-12-26","popular culture
linguistics
internet",{}JSON,255 KB,CC4,649 views,31 downloads,,0 topics,https://www.kaggle.com/sanjayaw/emosim508,"Content
EmoSim508 is the largest emoji similarity dataset that provides emoji similarity scores for 508 carefully selected emoji pairs. The most frequently co-occurring emoji pairs in a tweet corpus (that contains 147 million tweets) was used for creating the dataset and each emoji pair was annotated for its similarity using 10 human annotators. EmoSim508 dataset also consists of the emoji similarity scores generated from 8 different emoji embedding models proposed in ""A Semantics-Based Measure of Emoji Similarity"" paper by Wijeratne et al.
Acknowledgements
EmoSim508 was developed by Sanjaya Wijeratne, Lakshika Balasuriya, Amit Sheth and Derek Doran. EmoSim508 is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported (CC BY-NC-SA 3.0) license. Please cite the following paper when using EmoSim508 dataset:
Sanjaya Wijeratne, Lakshika Balasuriya, Amit Sheth, and Derek Doran. 2017. A Semantics-Based Measure of Emoji Similarity. In Proceedings of WI ’17, Leipzig, Germany, August 23-26, 2017, 8 pages. DOI: 10.1145/3106426.3106490
@inproceedings{DBLP:conf/webi/WijeratneBSD17, author = {Sanjaya Wijeratne and Lakshika Balasuriya and Amit Sheth and Derek Doran}, title = {A semantics-based measure of emoji similarity}, booktitle = {Proceedings of the International Conference on Web Intelligence, Leipzig, Germany, August 23-26, 2017}, pages = {646--653}, year = {2017}, doi = {10.1145/3106426.3106490} }
You can also find more information about the dataset on the project website.
Inspiration
Can you use these emoji similarity ratings to better group emoji in emoji keyboards?
Can you use EmoSim508 as a baseline dataset to test your emoji similarity approach's performance?
Can you use these emoji similarity ratings to suggest emoji domain names if they are already taken?"
Binance Crypto Klines,"Minutely crypto currencies open/close prices, high/low, trades and others",Binance,8,"Version 4,2018-02-20|Version 3,2018-01-30|Version 2,2018-01-21|Version 1,2018-01-16","time series
finance
money
+ 2 more...",Other,531 MB,CC0,664 views,65 downloads,,,https://www.kaggle.com/binance/binance-crypto-klines,"Context
Each file contains klines for 1 month period with 1 minute intervals. File name formating looks like mm-yyyy-SMB1SMB2 (e.g. 11-2017-XRPBTC).
This data set contains now only XRP/BTC and ETH/USDT symbol pair now, but it will be expand soon.
Features
Open time -> timestamp (milliseconds)
Open price -> float
High price -> float
Low price -> float
Close price -> float
Volume -> float
Quote asset volume -> float
Close time -> timestamp (milliseconds)
Number of trades -> int
Taker buy base asset volume -> float
Taker buy quote asset volume -> float
Acknowledgements
This dataset was collected from Binance Exchange | Worlds Largest Crypto Exchange
Inspiration
This data set could inspire you on most efficient trading algorithms."
"U.S. Public Pensions Data, fiscal years 2001-2016","Trends in Investments, Benefits, and Funding for U.S. Public Pensions",meep,8,"Version 3,2018-01-02|Version 2,2018-01-02|Version 1,2017-12-11","finance
government
demographics",CSV,2 MB,CC4,"1,287 views",154 downloads,,,https://www.kaggle.com/meepbobeep/us-public-pensions-data-fiscal-years-20012016,"Context
The Public Plans Dataset compiles publicly-available information on major U.S. public pension plans from their annual reporting. This includes state-level plans as well as some large plans from localities such as New York City and Chicago.
Content
The Public Plans Dataset has data compiled from the actuarial reports and CAFRs (comprehensive annual financial reports) for major plans. It includes balance sheet (assets/liabilities), income (e.g., investment income), and cash flow (e.g., benefit payments) information. In addition, there are items such as number of beneficiaries receiving payment, the number of active participants, the number vested, etc.
The current set covers fiscal years 2001 - 2016, which usually run mid-year to mid-year (this can differ by plan).
Acknowledgements
Data from Public Plans Data: http://publicplansdata.org/public-plans-database/
The Public Plans Data project comes out of a partnership between the Center for Retirement Research at Boston College (CRR) and the Center for State and Local Government Excellence (SLGE). The National Association of State Retirement Administrators (NASRA), which has been collecting and sharing public plan data since 2001, supports the partnership by providing review and assistance on the development of data models, validation of data, and development and administration of surveys. More here: http://publicplansdata.org/about/our-research/
Cover Photo by ashutosh nandeshwar on Unsplash
Inspiration
The Center for Retirement Research often produces research briefs based on this data, which can be found here: http://publicplansdata.org/research/issue-briefs/?category=crr
Questions I have been pursuing using this data:
What has been driving decreasing funded ratios in U.S. pensions?
Which pension plans are sustainable? Which are in trouble?
Is there a link between pension fundedness and investment strategy?
It may be useful to link these data sets to information on various state/local revenue amounts, in order to determine sustainability of pension costs."
Old Newspapers,A cleaned subset of HC Corpora newspapers,Liling Tan,8,"Version 6,2017-11-16|Version 5,2017-08-22|Version 4,2017-08-22|Version 3,2017-08-22|Version 2,2017-08-22|Version 1,2017-08-22","news agencies
history
linguistics
internet",Other,2 GB,CC0,"2,672 views",317 downloads,,0 topics,https://www.kaggle.com/alvations/old-newspapers,"Context
The HC Corpora was a great resource that contains natural language text from various newspapers, social media posts and blog pages in multiple languages. This is a cleaned version of the raw data from newspaper subset of the HC corpus.
Originally, this subset was created for a language identification task for similar languages
Content
The columns of each row in the .tsv file are:
Langauge: Language of the text.
Source: Newspaper from which the text is from.
Date: Date of the article that contains the text.
Text: Sentence/paragraph from the newspaper
The corpus contains 16,806,041 sentences/paragraphs in 67 languages:
Afrikaans
Albanian
Amharic
Arabic
Armenian
Azerbaijan
Bengali
Bosnian
Catalan
Chinese (Simplified)
Chinese (Traditional)
Croatian
Welsh
Czech
German
Danish
Danish
English
Spanish
Spanish (South America)
Finnish
French
Georgian
Galician
Greek
Hebrew
Hindi
Hungarian
Icelandic
Indonesian
Italian
Japanese
Khmer
Kannada
Korean
Kazakh
Lithuanian
Latvian
Macedonian
Malayalam
Mongolian
Malay
Nepali
Dutch
Norwegian (Bokmal)
Punjabi
Farsi
Polish
Portuguese (Brazil)
Portuguese (EU)
Romanian
Russian
Serbian
Sinhalese
Slovak
Slovenian
Swahili
Swedish
Tamil
Telugu
Tagalog
Thai
Turkish
Ukranian
Urdu
Uzbek
Vietnamese
Languages in HC Corpora but not in this (yet):
Estonian
Greenlandic
Gujarati
Acknowledge
All credits goes to Hans Christensen, the creator of HC Corpora.
Dataset image is from Philip Strong.
Inspire
Use this dataset to:
create a language identifier / detector
exploratory corpus linguistics (It’s one capstone project from Coursera’s data science specialization )"
Las Vegas TripAdvisor Reviews,500 reviews from hotels on the Las Vegas Strip,Chris Crawford,8,"Version 2,2017-08-29|Version 1,2017-08-23","cities
hotels",CSV,59 KB,Other,"3,073 views",260 downloads,,,https://www.kaggle.com/crawford/las-vegas-tripadvisor-reviews,"Context
This dataset includes quantitative and categorical features from online reviews from 21 hotels located in Las Vegas Strip, extracted from TripAdvisor. All the 504 reviews were collected between January and August of 2015.
Content
The dataset contains 504 records and 20 tuned features, 24 per hotel (two per each month, randomly selected), regarding the year of 2015. The CSV contains a header, with the names of the columns corresponding to the features.
Acknowledgements
Lichman, M. (2013). UCI Machine Learning Repository http://archive.ics.uci.edu/ml. Irvine, CA: University of California, School of Information and Computer Science
Downloaded form UCI Machine Learning Repository
Inspiration
Do machine learning algorithms take into account what happens in Vegas stays in Vegas?"
All Shark Tank (US) pitches & deals,Can you predict if a company/product pitch would succeed in Shark tank?,Nilesh Sakpal,8,"Version 2,2017-08-29|Version 1,2017-08-29","decision theory
business
finance
linguistics",CSV,142 KB,CC0,"2,444 views",170 downloads,2 kernels,,https://www.kaggle.com/neiljs/all-shark-tank-us-pitches-deals,"Context
Shark Tank is a great show based on an interesting concept wherein entrepreneurs and founders pitch their businesses in front of seasoned investors (aka sharks) who decide whether or not to invest in the businesses based on multiple parameters.
The show has many versions in different regions, and this database is for the US version, featuring, among other guest sharks, Mark Cuban, Robert Herjavec, Daymond John, Kevin O'Leary, Barbara Corcoran, and Lori Greiner.
The investment decisions on the show are merely handshake deals which are followed up by a detailed due-diligence and subsequent final investment decisions. Many of the deals taking place on the show do not go through.
Among many other points, some of the major decision vectors for the sharks to make a deal are:
The relevance of the business to their fields of interest and exposure (Daymond for fashion, Lori for QVC, Kevin for Wines, etc.)
The pitch quality (preparation, energy, etc. of the presenter)
Health of the business (Financials, debts, etc.)
Valuation (The most important)
Since elements such as pitch quality, exact financials disclosed, and specifics of what communication happened between the sharks and the presenters can be considered to be copyrighted to the show, I picked up the publically available details of the pitches and the results (deal = YES OR NO) and the associated shark(s) from websites, consolidated and cleaned the data, and presented in this dataset.
The idea is that a text vector based learning algorithm might be able to predict, given a description of a new pitch, how likely is the pitch to succeed in the shark tank, and even which shark might be more interested in the pitch.
Content
The dataset contains following headers:
Season_Epi_code - The data spans all 8 seasons of Shark Tank (US) and this code gives the season and the episode for indexing purposes. Format = SEE (101 = 1st season 1st Episode, 826 = 8th season 26th Episode)
Pitched_Business_Identifier - A short name of the pitched business
Pitched_Business_Desc - Brief description of the pitched business. Combination of text from more than one source has been added here, and there might be repetition or a very small description.
Deal_Status - Status of whether the pitched business got a deal in the episode where at least one shark and the presenters agreed on a particular deal. Format = (YES = 1, NO = 0)
Deal_Shark - Which of the most common sharks agreed on the episode along with the presenters for a deal? Format = either single shark's initials or '+' separated values of more than one shark's initials
Initials used: BC - Barbara Corcoran DJ - Daymond John KOL - Kevin O'Leary LG - Lori Greiner MC - Mark Cuban RH - Robert Herjavec
Note: While I have tried my best to collect, consolidate and clean the data, I do not make any claims of completeness or accuracy of data in the dataset. The user assumes the entire risk with respect to the use of this dataset.
Acknowledgements
ABC for producing such an entertaining, educational and well-managed show. Photo by Jakob Owens on Unsplash
Inspiration
The idea is that a text vector based learning algorithm might be able to predict, given a description of a new pitch, how likely is the pitch to succeed in the shark tank, and even which shark might be more interested in the pitch.
I have planned to cover the 5 most interesting solutions (EDA as well as actual prediction models) in a series of blog posts on thinkpatcri.com"
Interest Rate Records,Historic H15 Release Data from the Federal Reserve,Sohier Dane,8,"Version 1,2017-08-19","finance
government
banking",CSV,2 MB,Other,"1,985 views",312 downloads,,0 topics,https://www.kaggle.com/sohier/interest-rate-records,"The H15 Release from the federal reserve provides daily interest rate information for a variety of core interest rates, such as T bills and the federal funds rate. For some rates, this dataset extends all the way back to 1954.
The rates included are:
time_period
federal_funds
1_month_nonfinancial_commercial_paper
2_month_nonfinancial_commercial_paper
3_month_nonfinancial_commercial_paper
1_month_financial_commercial_paper
2_month_financial_commercial_paper
3_month_financial_commercial_paper
prime_rate
discount_rate
4_week_treasury_bill
3_month_treasury_bill
6_month_treasury_bill
1_year_treasury_bill
1_month_treasury_constant_maturity
3_month_treasury_constant_maturity
6_month_treasury_constant_maturity
1_year_treasury_constant_maturity
2_year_treasury_constant_maturity
3_year_treasury_constant_maturity
5_year_treasury_constant_maturity
7_year_treasury_constant_maturity
10_year_treasury_constant_maturity
20_year_treasury_constant_maturity
30_year_treasury_constant_maturity
5_year_inflation_indexed_treasury_constant_maturity
7_year_inflation_indexed_treasury_constant_maturity
10_year_inflation_indexed_treasury_constant_maturity
20_year_inflation_indexed_treasury_constant_maturity
30_year_inflation_indexed_treasury_constant_maturity
inflation_indexed_long_term_average
Please see https://www.federalreserve.gov/releases/h15/ for notices, caveats, and newly released data."
Baltimore 911 Calls,Records of 2.8 million calls from 2015 onwards,Sohier Dane,8,"Version 2,2017-08-30|Version 1,2017-08-30",crime,CSV,282 MB,CC3,"2,124 views",211 downloads,2 kernels,0 topics,https://www.kaggle.com/sohier/baltimore-911-calls,"This dataset records the time, location, priority, and reason for calls to 911 in the city of Baltimore.
Acknowledgements
This dataset was kindly made available by the City of Baltimore. They update the data daily; you can find the original version here.
Inspiration
The study discussed in this Atlantic article reviewing 911 calls in Milwaukuee found that that incidents of police violence lead to large drops in the number of 911 calls. Does this hold true for Baltimore as well?"
HSE Thai Corpus,A 35 Million Word Corpus of Thai,Rachael Tatman,8,"Version 1,2017-09-06","languages
asia
linguistics
internet",CSV,430 MB,Other,747 views,45 downloads,,0 topics,https://www.kaggle.com/rtatman/hse-thai-corpus,"Context:
The Thai language is the primary language of Thailand and a recognized minority language in Cambodia. It has approximately twenty million native speakers, in addition to 44 million second language speakers. It is written in Thai script (also called the Thai alphabet) which is notable for being the first writing system to incorporate tonal markers. Thai is written without spaces between words.
Content:
The HSE Thai Corpus is a corpus of modern texts written in Thai language. The texts, containing in whole 50 million tokens, were collected from various Thai websites (mostly news websites). To make it easier for non-Thai-speakers to comprehend and use texts in the corpus the researchers decided to separate words in each sentence with spaces.
The data for the corpus was collected by means of Scrapy. To tokenize texts the Pythai module was used. The text in this dataset is encoded in UTF-8.
The corpus can be searched using a web interface at this site.
This dataset contains text from two sources: Wikipedia and thaigov.go.th. The former is licensed under a standard Wikipedia license, and the latter under an Open Government License for Thailand, which can be viewed here (In Thai).
Acknowledgements:
The Thai Corpus was developed by the team of students of HSE School of Linguistics in Moscow under the guidance of professor Boris Orekhov. The team consisted of Grigory Ignatyev, Alexandra Ershova, Anna Kuznetsova, Tatyana Shalganova, Daniil Kolomeytsev and Nikolai Mikulin. The consulting help on Thai language was provided by Nadezhda Motina. Natalia Filippova, Elizaveta Kuzmenko, Tatyana Gavrilova, Elena Krotova, Elmira Mustakimova, Olga Sozinova, Aleksandra Martynova, Maria Sheyanova, Marina Kustova and Julia Badryzlova also contributed to the project.
Inspiration:
In this corpus, unlike in most written Thai, words have been separated for you with spaces. Can you remove spaces and write an algorithm to identify word boundaries?"
"Home Mortgage Disclosure Act Data, NY, 2015",Data on ~440k Home Mortgage Decisions in NY,Jacob Boysen,8,"Version 1,2017-08-17",,CSV,338 MB,CC0,"1,520 views",173 downloads,,,https://www.kaggle.com/jboysen/ny-home-mortgage,"Context:
The Home Mortgage Disclosure Act (HMDA) requires many financial institutions to maintain, report, and publicly disclose information about mortgages.
Content:
This dataset covers all mortgage decisions made in 2015 for the state of New York. Data for additional states and years can be accessed here.
Acknowledgements:
This dataset was compiled by the Consumer Finance Protection Board.
Inspiration:
Where are mortgages most likely to be approved?
Can you predict mortgage decisions based on the criteria provided here?"
Who's the Boss? People with Significant Control,Snapshot of UK Business Ownership Details,Jacob Boysen,8,"Version 1,2017-08-23",business,Other,3 GB,CC0,"2,053 views",243 downloads,,0 topics,https://www.kaggle.com/jboysen/uk-psc,"Context:
The People with significant control (PSC) snapshot is a data snapshot containing the full list of PSC's provided to Companies House. The Prime Minister first put corporate transparency on the international agenda when he chaired the G8 summit in Lough Erne and secured commitment to action, the commitment to enhance corporate transparency in the UK was reaffirmed at London’s International Anti-Corruption Summit in May 2016. Since then the EU and G20 countries have also agreed to act. The UK is the first country in the G20 to create a public register of this kind.
The UK has high standards of business behaviour and corporate governance. The overwhelming majority of UK companies contribute productively to the UK economy, abide by the law and make a valuable contribution to society. But there are exceptions. Some of the features of the company structure which make it good for business also make it attractive to criminals. Companies can be misused to facilitate a range of criminal activities - from money laundering to tax evasion, corruption to terrorist financing. Sometimes those individuals running companies will not conduct themselves in accordance with the high standards we expect in the UK, posing a risk to other companies and consumers alike.
Information about the ownership and control of UK corporate entities will bring benefits for law enforcement, business, civil society and citizens. By making this information publicly available, free of charge, the government is setting a standard that we are persuading other countries to follow.
Content:
A person of significant control is someone that holds more than 25% of shares or voting rights in a company, has the right to appoint or remove the majority of the board of directors or otherwise exercises significant influence or control. This is a snapshot of data in zipped JSON form, as of Aug 23 2017. Daily updated snapshots and streaming API details can be found here. The People with Significant Control (PSC) register includes information about the individuals who own or control companies including their name, month and year of birth, nationality, and details of their interest in the company. From 30 June 2016, UK companies (except listed companies) and limited liability partnerships (LLPs) need to declare this information when issuing their annual confirmation statement to Companies House.
Acknowledgements:
Guidance here. The data is collected by UK government.
Inspiration:
Who owns the most businesses? In certain areas?
Any weird looking situations where ownership might be obscured?"
US Facility-Level Air Pollution (2010-2014),EPA Toxic & Greenhouse Gas Emissions Data,Jeremy Seibert,8,"Version 6,2017-11-23|Version 5,2017-11-23|Version 4,2017-11-23|Version 3,2017-10-07|Version 2,2017-09-19|Version 1,2017-09-18","united states
pollution
demographics",CSV,6 MB,CC0,"1,751 views",221 downloads,2 kernels,0 topics,https://www.kaggle.com/jaseibert/us-facilitylevel-air-pollution-20102014,"Context
This dataset includes the 2010-2014 ""Facility-Level"" emissions data, combined with geographical & industry-related data. It is based on the EPA's Toxic Release Inventory (TRI) & Greenhouse Gas Reporting Inventory (GHG), the national system of nomenclature that is used to describe the industry-related emissions.
Although the EPA publishes and maintains the TRI & GHG report in various forms, the combination of the two is not readily available. Hence this dataset.
Content
The CSV has 28 columnar variables defined as:
UniqueID
Facility name
Rank TRI '14
Rank GHG '14
Latitude
Longitude
Location address
City
State
ZIP
County
FIPS code
Primary NAICS
Second primary NAICS
Third primary NAICS
Industry type
Parent companies 2014 (GHG)
Parent companies 2014 (TRI)
TRI air emissions 14 (in pounds)
TRI air emissions 13 [and previous years]
GHG direct emissions 14 (in metric tons)
GHG direct emissions 13 [and previous years]
GHG Facility Id
Second GHG Facility Id [and Third, Fourth, etc.]
TRI Id
Second TRI Id [and Third, Fourth, etc.]
FRS Id
Second FRS Id [and Third, Fourth, etc.]
Acknowledgements
This dataset was made available by the Center for Public Integrity."
UFC Fights Data 1993 - 2/23/2016,UFC 1 to UFC Fight Night 83 Fight Data,Chris Formey,8,"Version 2,2017-09-13|Version 1,2017-09-13",sports,CSV,2 MB,Other,"1,574 views",160 downloads,2 kernels,3 topics,https://www.kaggle.com/cformey24/ufc-fights-data-1993-2232016,"Context
I found this at https://www.reddit.com/r/datasets/comments/47a7wh/ufc_fights_and_fighter_data/
All credit goes to reddit user geyges and Sherdog.
I do not own the data.
Content
This data has multiple categorical variables from every UFC fight from UFC 1 in 1993 - 2/23/2016.
Acknowledgements
Reddit u/geyges Sherdog UFC
Inspiration
So much information can be gained from this relevant to understanding how the sport has evolved over the years."
Net Neutrality Accountability,Contribution Amount and Net Neutrality position for members of Congress,Christopher Lambert,8,"Version 1,2017-12-12","politics
internet",{}JSON,71 KB,CC4,878 views,67 downloads,2 kernels,,https://www.kaggle.com/theriley106/net-neutrality-accountability,"Content
Dataset containing monetary contributions from the Telecom industry, as well as presumed Net Neutrality position.
The dataset contains: Name, Contribution Amount, Vote, Political Party, State, Political Position
The voting section refers to the vote on S.J.Res. 34 - taken place in March, 2017."
Localization Data for Posture Reconstruction,Recordings of five people while wearing localization tags,UCI Machine Learning,8,"Version 2,2017-09-06|Version 1,2017-09-06","healthcare
biotechnology
programming",CSV,21 MB,Other,"1,603 views",82 downloads,,0 topics,https://www.kaggle.com/uciml/posture-reconstruction,"Context
These datums represent a multi-agent system for the care of elderly people living at home on their own, with the aim to prolong their independence. The system was designed to provide a reliable, robust and flexible monitoring by sensing the user in the environment, reconstructing the position and posture to create the physical awareness of the user in the environment, reacting to critical situations, calling for help in the case of an emergency, and issuing warnings if unusual behavior was detected.
Content
Columns descriptions
Sequence Name: A01, A02, A03, A04, A05, B01, B02, B03, B04, B05, ...,E05
A-E represent a person (5 total)
01, 02, 03, 04, 05 = Tag Numbers
Tag identifiers
ANKLE_LEFT = 010-000-024-033
ANKLE_RIGHT = 010-000-030-096
CHEST = 020-000-033-111
BELT = 020-000-032-221
Time stamp
Date
Format = dd.MM.yyyy HH:mm:ss:SSS
x coordinate of the tag
y coordinate of the tag
z coordinate of the tag
activity
walking, falling, 'lying down', lying, 'sitting down', sitting, 'standing up from lying', 'on all fours', 'sitting on the ground', 'standing up from sitting', 'standing up from sitting on the ground
Acknowledgements
B. Kaluza, V. Mirchevska, E. Dovgan, M. Lustrek, M. Gams, An Agent-based Approach to Care in Independent Living, International Joint Conference on Ambient Intelligence (AmI-10), Malaga, Spain, In press
Inspiration
Given these data, can you classify the persons activity from the tags they wore?"
Identifying Interesting Web Pages,Automatically Extracting Features for Concept Learning from the Web,UCI Machine Learning,8,"Version 1,2017-09-14","web sites
internet",Other,2 MB,Other,"2,857 views",272 downloads,5 kernels,2 topics,https://www.kaggle.com/uciml/identifying-interesting-web-pages,"Context
The problem is to predict user ratings for web pages (within a subject category). The HTML source of a web page is given. Users looked at each web page and indicated on a 3 point scale (hot medium cold) 50-100 pages per domain.
Content
This database contains HTML source of web pages plus the ratings of a single user on these web pages. Web pages are on four separate subjects (Bands- recording artists; Goats; Sheep; and BioMedical).
Acknowledgement
Data originally from the UCI ML Repository. Donated by:
Michael Pazzani Department of Information and Computer Science, University of California, Irvine Irvine, CA 92697-3425 pazzani@ics.uci.edu
Concept based Information Access with Google for Personalized Information Retrieval"
Vehicle Fuel Economy,Mileage and more for 1948-2018,US Environmental Protection Agency,8,"Version 2,2017-09-14|Version 1,2017-09-14","government agencies
vehicles",CSV,17 MB,CC0,"2,557 views",321 downloads,,0 topics,https://www.kaggle.com/epa/vehicle-fuel-economy,"Fuel economy data are the result of vehicle testing done at the Environmental Protection Agency's National Vehicle and Fuel Emissions Laboratory in Ann Arbor, Michigan, and by vehicle manufacturers with oversight by EPA.
Content
Please see the csvs labeled with 'fields' for descriptions of the data fields; there are too many to list here.
Acknowledgements
This dataset was kindly provided by the US EPA. You can find the original dataset, which is updated regularly, here.
Inspiration
How has the rate of change of fuel economy changed over time?
Do simple clustering techniques on vehicles lead to the same groupings that are typically associated with manufacturers, such as putting Porsche and BMW together in a luxury car group?"
Producer Price Index,Statistical measures of change in prices of producer goods,US Bureau of Labor Statistics,8,"Version 1,2017-09-13",economics,Other,136 MB,CC0,"1,637 views",201 downloads,2 kernels,0 topics,https://www.kaggle.com/bls/producer-price-index,"Context
The US Bureau of Labor Statistics monitors and collects day-to-day information about the market price of raw inputs and finished goods, and publishes regularized statistical assays of this data. The Consumer Price Index and the Producer Price Index are its two most famous products. The former tracks the aggregate dollar price of consumer goods in the United States (things like onions, shovels, and smartphones); the latter (this dataset) tracks the cost of raw inputs to the industries producing those goods (things like raw steel, bulk leather, and processed chemicals).
The US federal government uses this dataset to track inflation. While in the short term the raw dollar value of producer inputs may be volatile, in the long term it will always go up due to inflation --- the slowly decreasing buying power of the US dollar.
Content
This dataset consists of a packet of files, each one tracking regularized cost of inputs for certain industries. The data is tracked-month to month with an index out of 100.
Acknowledgements
This data is published online by the US Bureau of Labor Statistics.
Inspiration
How does the Producer Price Index compare against the Consumer Price Index?
What have the largest spikes in input costs been, historically? Can you determine why they occurred?
What is the overall price index trend amongst US producers?"
Vietnam War Bombing Operations,Details on 4.8 Million Runs,United States Air Force,8,"Version 1,2017-09-14",military,CSV,2 GB,CC0,"1,492 views",152 downloads,,0 topics,https://www.kaggle.com/usaf/vietnam-war-bombing-operations,"Context:
THOR is a painstakingly cultivated database of historic aerial bombings from World War I through Vietnam. THOR has already proven useful in finding unexploded ordinance in Southeast Asia and improving Air Force combat tactics. Our goal is to see where public discourse and innovation takes this data. Each theater of warfare has a separate data file, in addition to a THOR Overview.
Content:
4.8 million rows with 47 columns describing each run. See the data dictionary here.
Acknowledgements:
THOR is a dataset project initiated by Lt Col Jenns Robertson and continued in partnership with Data.mil, an experimental project, created by the Defense Digital Service in collaboration with the Deputy Chief Management Officer and data owners throughout the U.S. military.
Inspiration:
Which campaigns saw the heaviest bombings?
Which months saw the most runs?
What were the most used aircraft?"
Exchange Rates,Exchange rates as far back as 1971 between the USA and 23 countries,Federal Reserve,8,"Version 1,2017-09-06","finance
economics",CSV,2 MB,CC0,"2,275 views",363 downloads,,0 topics,https://www.kaggle.com/federalreserve/exchange-rates,"The Federal Reserve's H.10 statistical release provides data on exchange rates between the US dollar, 23 other currencies, and three benchmark indexes. The data extend back to 1971 for several of these.
Please note that the csv has six header rows. The first contains the
Acknowledgements
This dataset was provided by the US Federal Reserve. If you need the current version, you can find their weekly updates here.
Inspiration
Venezuela is both unusually dependent on oil revenues and experiencing unusual degrees of political upheaval. Can you determine which movements in their currency were driven by changes in the oil price and which were driven by political events?"
Word2Vec Sample,Sample Word2Vec Model,NLTK Data,8,"Version 1,2017-08-20",,Other,132 MB,Other,"1,588 views",115 downloads,2 kernels,0 topics,https://www.kaggle.com/nltkdata/word2vec-sample,"Context
The Word2Vec sample model redistributed by NLTK is used to demonstrate how word embeddings can be used together with Gensim.
The detailed demonstration can be found on https://github.com/nltk/nltk/blob/develop/nltk/test/gensim.doctest
Acknowledgements
Credit goes to Word2Vec and Gensim developers."
NYC Dog Names,Popularity of 16k dog names,City of New York,8,"Version 1,2017-08-31","cities
animals",CSV,143 KB,CC0,"1,778 views",207 downloads,3 kernels,0 topics,https://www.kaggle.com/new-york-city/nyc-dog-names,"Context:
The NYC Department of Health requires all dog owners to license their dogs. The resulting names data was released on GitHub with a nice interactive D3 word cloud. Additional data (including type and color) is available from WNYC here.
Content:
This data covers dog names, and the counts of each name."
Cheltenham Crime Data,Cheltenham Township Police Department incident dataset,Mike Chirico,8,"Version 23,2017-12-16|Version 22,2017-11-27|Version 21,2017-11-26|Version 20,2017-11-14|Version 19,2017-10-24|Version 18,2017-10-13|Version 17,2017-09-19|Version 16,2017-09-02|Version 15,2017-08-11|Version 14,2017-07-27|Version 13,2017-07-15|Version 12,2017-07-08|Version 11,2017-06-26|Version 10,2017-06-04|Version 9,2017-05-24|Version 8,2017-05-04|Version 7,2017-04-18|Version 6,2017-04-18|Version 5,2017-04-06|Version 4,2017-03-25|Version 3,2017-03-21|Version 2,2017-03-02|Version 1,2017-02-26",crime,CSV,1 MB,ODbL,"3,009 views",247 downloads,40 kernels,0 topics,https://www.kaggle.com/mchirico/chtpd,"Cheltenham PA, Crime Data
Cheltenham is a home rule township bordering North Philadelphia in Montgomery County. It has a population of about 37,000 people. You can find out more about Cheltenham on wikipedia.
Cheltenham's Facebook Groups. contains postings on crime and other events in the community.
Getting Started
Reading Data is a simple python script for getting started.
If you prefer to use R, there is an example Kernel here.
Proximity to Philadelphia
This township borders on Philadelphia, which may or may not influence crime in the community. For Philadelphia crime patterns, see the Philadelphia Crime Dataset.
Reference
Data was obtained from socrata.com"
"Clinical, Anthropometric & Bio-Chemical Survey",1.89m records & 53 variables of unit level annual health survey data from India.,Rajanand Ilangovan / இராஜ்ஆனந்த் இளங்கோவன்,8,"Version 1,2017-08-09","india
public health
health",CSV,320 MB,CC4,666 views,142 downloads,,0 topics,https://www.kaggle.com/rajanand/cab-survey,"Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks.
Context:
*Annual Health Survey : Mortality Schedule *
This unit level dataset contains the details of Clinical, Anthropometric & Bio-chemical (CAB) Survey. To supplement the information provided by Annual Health Survey (AHS),a biomarker component has been introduced in AHS to collect data on nutritional status, life style diseases like diabetes & hypertension and anemia in Empowered Action Group (EAG) States & Assam. This component, namely Clinical, Anthropometric and Bio-chemical (CAB) survey, is conducted on a sub-sample of AHS in all EAG States namely Bihar, Chhattisgarh, Jharkhand, Madhya Pradesh, Odisha, Rajasthan, Uttarakhand & Uttar Pradesh and Assam.
There are total of 1.89million observations and 53 variables in this dataset.
Survey:
Base line survey - 2010-11 (4.14 million households in the sample) 1st update - 2011-12 (4.28 million households in the sample) 2nd update - 2012-13 (4.32 million households in the sample)
These nine states, which account for about 48 percent of the total population, 59 percent of Births, 70 percent of Infant Deaths, 75 percent of Under 5 Deaths and 62 percent of Maternal Deaths in the country, are the high focus States in view of their relatively higher fertility and mortality.
Content:
The files contains the below columns.
Variable Names:
state_code
district_code
rural_urban
stratum
PSU_ID
ahs_house_unit
house_hold_no
date_survey
test_salt_iodine
record_code_iodine
record_code_iodine_reason
sl_no
Sex
usual_residance
usual_residance_Reason
identification_code
Age_Code
Age
date_of_birth
month_of_birth
year_of_birth
Weight_measured
Weight_in_kg
Length_height_measured
length_height_code
Length_height_cm
Haemoglobin_test
Haemoglobin
Haemoglobin_level
BP_systolic
BP_systolic_2_reading
BP_Diastolic
BP_Diastolic_2reading
Pulse_rate
Pulse_rate_2_reading
Diabetes_test
fasting_blood_glucose
fasting_blood_glucose_mg_dl
Marital_status
gauna_perfor_not_perfor
duration_pregnanacy
first_breast_feeding
is_cur_breast_feeding
day_or_month_for_breast_feeding_code
day_or_month_for_breast_feeding
water_month
ani_milk_month
semisolid_month_or_day
solid_month
vegetables_month_or_day
illness_type
illness_duration
treatment_type
File content: Mortality_data_dictionary.xlsx : This data dictionary excel work book has the detailed information about each and every column and codes used in the data.
Acknowledgements
Department of Health and Family Welfare, Govt. of India has published this data in Open Govt Data Platform India portal under Govt. Open Data License - India."
Street Network of New York in GraphML,Analyse the New York City Street Network!,Chris Cross,8,"Version 1,2017-08-03",networks,Other,59 MB,ODbL,"1,761 views",205 downloads,3 kernels,0 topics,https://www.kaggle.com/crailtap/street-network-of-new-york-in-graphml,"Context
Having such a task as predicting the travel time of taxis, it can be insightful to have a deeper look at the underlying street network of the city. Network Analysis can enable us to get insights for why certain taxi trips take longer than others given some basic network properties. Examples for the analysis can be: calculate the shortest path, measure the influence of specific streets on the robustness of the network or find out which streets are key points in the network when it comes to traffic flow.
Content
This dataset contains one large Graph for the Street Network of New York City in GraphML format and a subgraph for the area of Manhattan for fast testing of your Analysis. Each Graph was created with the awesome python package https://github.com/gboeing/osmnx which is not available on Kaggle. The Graphs nodes attributes are taken from OSM and contain information to which other nodes they are connected, how long the connection is, which speed limit it has etc.
Acknowledgements
https://github.com/gboeing/osmnx
Inspiration
Explore the New York Street Network, gain a deeper understanding for network analysis and craft some useful Features for the Taxi Trip Prediction Competition!"
Adult Census Income with AI,Artificial Intelligence to boost data science,Maxime Fuccellaro,8,"Version 1,2017-08-08","finance
politics
demographics",CSV,19 MB,CC0,"1,617 views",132 downloads,2 kernels,0 topics,https://www.kaggle.com/blackbee2016/adult-census-income-with-ai,"Context
This data was extracted from the 1994 Census bureau database by Ronny Kohavi and Barry Becker (Data Mining and Visualization, Silicon Graphics). A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1) && (HRSWK>0)). The prediction task is to determine whether a person makes over $50K a year.
In order to make the job better we used artificial Intelligence to automatically modify the columns.
Content
This Dataset contains the initial Dataset columns as well as the new ones obtained by feeding the original US Census Dataset to PredicSis.ai in order to automatically : 1. Discretise continuous variables into relevant intervals. 2. Group values of categorical variables together in order to reduce the modality of the variables.
Acknowledgements
https://archive.ics.uci.edu/ml/datasets/Census+Income
https://www.kaggle.com/uciml/adult-census-income
https://predicsis.ai
Inspiration
We want to see by how much auto ML/AI improves the data scientist work quality."
"Country Socioeconomic Status Scores, Part II",Country-weighted measures of SES,sdorius,8,"Version 1,2017-07-15","demographics
economics
sociology",CSV,90 KB,ODbL,"1,854 views",205 downloads,,,https://www.kaggle.com/sdorius/countryses,"This dataset contains estimates of the socioeconomic status (SES) position of each of 149 countries covering the period 1880-2010. Measures of SES, which are in decades, allow for a 130 year time-series analysis of the changing position of countries in the global status hierarchy. SES scores are the average of each country’s income and education ranking and are reported as percentile rankings ranging from 1-99. As such, they can be interpreted similarly to other percentile rankings, such has high school standardized test scores. If country A has an SES score of 55, for example, it indicates that 55 percent of the countries in this dataset have a lower average income and education ranking than country A. ISO alpha and numeric country codes are included to allow users to merge these data with other variables, such as those found in the World Bank’s World Development Indicators Database and the United Nations Common Database.
See here for a working example of how the data might be used to better understand how the world came to look the way it does, at least in terms of status position of countries.
VARIABLE DESCRIPTIONS:
unid: ISO numeric country code (used by the United Nations)
wbid: ISO alpha country code (used by the World Bank)
SES: Country socioeconomic status score (percentile) based on GDP per capita and educational attainment (n=174)
country: Short country name
year: Survey year
gdppc: GDP per capita: Single time-series (imputed)
yrseduc: Completed years of education in the adult (15+) population
region5: Five category regional coding schema
regionUN: United Nations regional coding schema
DATA SOURCES:
The dataset was compiled by Shawn Dorius (sdorius@iastate.edu) from a large number of data sources, listed below. GDP per Capita:
Maddison, Angus. 2004. 'The World Economy: Historical Statistics'. Organization for Economic Co-operation and Development: Paris. GDP & GDP per capita data in (1990 Geary-Khamis dollars, PPPs of currencies and average prices of commodities). Maddison data collected from: http://www.ggdc.net/MADDISON/Historical_Statistics/horizontal-file_02-2010.xls.
World Development Indicators Database Years of Education 1. Morrisson and Murtin.2009. 'The Century of Education'. Journal of Human Capital(3)1:1-42. Data downloaded from http://www.fabricemurtin.com/ 2. Cohen, Daniel & Marcelo Cohen. 2007. 'Growth and human capital: Good data, good results' Journal of economic growth 12(1):51-76. Data downloaded from http://soto.iae-csic.org/Data.htm
Barro, Robert and Jong-Wha Lee, 2013, ""A New Data Set of Educational Attainment in the World, 1950-2010."" Journal of Development Economics, vol 104, pp.184-198. Data downloaded from http://www.barrolee.com/
Maddison, Angus. 2004. 'The World Economy: Historical Statistics'. Organization for Economic Co-operation and Development: Paris. 13.
United Nations Population Division. 2009."
ENEM 2015,"Data from ENEM 2015, the Brazilian High School National Exam.",Gustavo Bonesso,8,"Version 4,2017-07-06|Version 3,2017-07-05|Version 2,2017-07-05|Version 1,2017-07-05",education,CSV,2 GB,CC0,"1,198 views",107 downloads,,,https://www.kaggle.com/gbonesso/enem2015,"Context
This dataset was downloaded from INEP, a department from the Brazilian Education Ministry. It contains data from the applicants for the 2015 National High School Exam.
Content
Inside this dataset there are not only the exam results, but the social and economic context of the applicants.
Acknowledgements
The original dataset is provided by INEP (http://portal.inep.gov.br/microdados). I removed some information from original files to fit the file size into the Kaggle constraints.
Inspiration
The objective is to explore the dataset to achieve a better understanding of the social and economic context of the applicants in the exams results."
SEC (EDGAR) Company Names & CIK Keys,Includes all company names and cik keys from the SEC database.,DataP,8,"Version 1,2017-07-07",,CSV,53 MB,ODbL,"1,245 views",131 downloads,,,https://www.kaggle.com/dattapiy/sec-edgar-companies-list,"Context
All companies in the SEC EDGAR database. Companies are listed with company name and their unique CIK key. Data-set had 663000 companies listed, which includes all companies in the EDGAR database.
Content
Will update later.
Acknowledgements
Will update later.
Inspiration
Will update later."
Recommender Click Logs- Sowiport,User behavior for multiple recommender systems,Sohier Dane,8,"Version 1,2017-08-15",,Other,2 GB,Other,"1,969 views",169 downloads,,0 topics,https://www.kaggle.com/sohier/recommender-click-logs-sowiport,"This dataset contains 28 million recommendation and click/no click pairs from users of the Sowiport library. From the abstract of the pre-print discussing the dataset:
Stereotype and most-popular recommendations are widely neglected in the research-paper recommender-system and digital-library community. In other domains such as movie recommendations and hotel search, however, these recommendation approaches have proven their effectiveness. We were interested to find out how stereotype and most-popular recommendations would perform in the scenario of a digital library. Therefore, we implemented the two approaches in the recommender system of GESIS’ digital library Sowiport, in cooperation with the recommendations-as-aservice provider Mr. DLib. We measured the effectiveness of most-popular and stereotype recommendations with click-through rate (CTR) based on 28 million delivered recommendations. Most-popular recommendations achieved a CTR of 0.11%, and stereotype recommendations achieved a CTR of 0.124%. Compared to a “random recommendations” baseline (CTR 0.12%), and a content-based filtering baseline (CTR 0.145%), the results are discouraging. However, for reasons explained in the paper, we concluded that more research is necessary about the effectiveness of stereotype and most-popular recommendations in digital libraries.
This dataset was kindly made available by the authors of ""Stereotype and Most-Popular Recommendations in the Digital Library Sowiport"" under the CC-BY 3.0 license. You can find additional information at http://mr-dlib.org/."
Who Dies? Physics Puzzle Dataset,Who can predict the outcome of a physics puzzle better - human or machine?,Christian Vorhemus,8,"Version 1,2017-06-22","video games
artificial intelligence",CSV,365 MB,CC0,"1,930 views",64 downloads,,,https://www.kaggle.com/christianvorhemus/physicspuzzlewhodies,"The Game
""Who Dies?"" is a simple physics puzzle available for Android. Randomly, a world full of stones, monsters, coil springs, slingshots and other objects is created. The user has to guess, which monster will get hit by a stone or falls of the platform when gravity is turned on. He gets point for every right guess, the more points he collects, the more complex the worlds will get.
The Development
For development, Phaser was used, the game map is a 35x20 grid. Each tile in this grid can contain different blockers or objects.
The Data
Every time a user is playing the game, the position of all objects is recorded as well as the selection the user has made and the final set of monsters who died. The dataset consists of 5 columns: DATETIME is a timestamp of when a user played the game. COMPLEXITY is a parameter that measures the difficulty of the game (1 = easy, 100 = hard). MAP is a JSON array containing 3 arrays: The first array contains immobile foreground objects described by a ""type"" property including x and y coordinates. The following list gives an overview about the the most commonly used types:
The second array contains immobile background objects that don't interact with the game objects and are therefore not relevant. The third array contains movable foreground objects. These could be: monsters (""guys""), balls (""smallball"", ""ball"" and ""bigball"" with or without an initial rotation to the left or right), spring (catapults boxes and balls up in the air but not monsters), box, chain, seesaw, spin, switch and switchwall (if a ball touches a switch, all switchwalls with the same color as the switch change their visibility and become transparent to foreground objects or vice versa). Column ""MONSTERS_SELECTED"" contains all the monsters a player thought will get hit by a ball (ordered by the selection time) ""MONSTERS_HIT"" contains all monsters that were actually killed by balls or fell of the platform (ordered by time)
The Goal
There are several interesting outcomes, for example:
Creating a ML algorithm that is able to correctly predict the outcome of the game (which monster will die)
Creating a ML algorithm that is able to correctly predict which monsters a user will most likely pick
Creating a ML algorithm that is able to create new (better?) game worlds"
The Sign Language Analyses (SLAY) Database,A database of information on the grammers of signed languages,Rachael Tatman,8,"Version 1,2017-07-14","grammar
languages
linguistics",CSV,30 KB,CC0,"1,485 views",103 downloads,,0 topics,https://www.kaggle.com/rtatman/sign-language-analyses,"Context:
Signed languages have many unique ways to encode meaning. Some of these ways include using different handshapes, motions, which direction the palm and wrist are facing, whether one hand or two is used, and facial expressions. This dataset compares which different sign languages use which of these grammatical building blocks.
Content:
This database contains information on the parameters used by 87 signed languages, taken from various academic sources and compiled by hand.
Acknowledgements:
This dataset was collected by Rachael Tatman during the process of linguistic research and is released to the public domain. The database can be cited by reference to this paper:
Tatman, R. (2015). The Sign Language Analyses (SLAY) Database⋆ (Vol. 33). University of Washington Working Papers in Linguistics. https://depts.washington.edu/uwwpl/vol33/2-Tatman-SLAY.pdf
Inspiration:
One analysis of this data is presented can be found in this paper, , but there are plenty of additional questions that could be asked. Some examples: - Does a language’s geographic location factor into what parameters it uses? - Does the year that a grammatical analysis was published have an effect on how many parameters it proposes for a language?
You may also be interested in:
Atlas of Pidgin and Creole Language Structures
World Language Family Map
World Atlas of Language Structures: Information on the linguistic structures in 2,679 languages"
Brazilian Portuguese Literature Corpus,3.7 million word corpus of Brazilian literature published between 1840-1908,Rachael Tatman,8,"Version 1,2017-07-28","languages
literature
brazil
linguistics",Other,23 MB,CC0,"1,126 views",75 downloads,,0 topics,https://www.kaggle.com/rtatman/brazilian-portuguese-literature-corpus,"Context:
Brazilian literature is the literature written in the Portuguese language by Brazilians or in Brazil, including works written prior to the country’s independence in 1822. Throughout its early years, literature from Brazil followed the literary trends of Portugal, whereas gradually shifting to a different and authentic writing style in the course of the 19th and 20th centuries, in the search for truly Brazilian themes and use of the Portuguese language.
Content:
This dataset contains over 3.7 million words of Brazilian literature written between 1840 and 1908. There are 81 distinct works in this corpus, written by Adolfo Caminha, Aluisio Azevedo, Bernardo Guimaraes, Joaquim Manuel de Macedo, Jose de Alencar, Machado de Assis and Manuel Antonio de Almeida.
Inspiration:
Can you automatically identify topics/themes in each of these works?
Can you automatically identify the author of a specific text? (You might want to split each author’s works into a test set and training set.)"
Austin Crime Statistics,"159k Crime Reports, 2014-2016",Jacob Boysen,8,"Version 1,2017-08-04",crime,CSV,19 MB,CC0,"1,333 views",153 downloads,2 kernels,0 topics,https://www.kaggle.com/jboysen/austin-crime,"Context:
Crime in growing cities such as Austin changes with the population. This data covers individual crimes reported in Austin, primarily 2014-2015.
Content:
159k rows of data on type of crime reported, location by various attributes (lat/lon, council district, census tract) and time are included. Clearance status by Austin PD is also recorded where available.
Acknowledgements:
Data was prepared from a txt file accessed via Google Cloud BigQuery Public Datasets. Image by Tobias Zils.
Inspiration:
Are there any clear seasonal or hourly trends in certain crimes? Which crimes are most often cleared by Austin PD, and which remain open? How long do clearances take?"
Trappist-1 Solar System,Data from the recently announced 7 exoplanet system,NASA,8,"Version 2,2017-02-24|Version 1,2017-02-24",space,CSV,4 KB,CC0,"3,389 views",240 downloads,8 kernels,,https://www.kaggle.com/nasa/trappist1,"Context
On February 22, 2017 NASA announces the discovery of the Trappist-1 Solar System, which contains 7 earth sized exoplanets orbiting close to a dim star. The planets' orbits are situated in the 'Goldilocks zone', making them prime candidates for extraterrestrial life.
Content
The data published here was pulled from NASA and CalTech's Exoplanet Archive: http://exoplanetarchive.ipac.caltech.edu/index.html
Column contents are explained further: http://exoplanetarchive.ipac.caltech.edu/docs/API_exoplanet_columns.html
Acknowledgements
Use of the NASA Exoplanet Archive, which is operated by the California Institute of Technology, under contract with the National Aeronautics and Space Administration under the Exoplanet Exploration Program
Inspiration
A playground to work with data from an exciting discovery. A lot of questions remained to be answered, pending future studies."
Producer Price Index,Statistical measures of change in prices of producer goods,US Bureau of Labor Statistics,8,"Version 1,2017-09-13",economics,Other,136 MB,CC0,"1,639 views",201 downloads,2 kernels,0 topics,https://www.kaggle.com/bls/producer-price-index,"Context
The US Bureau of Labor Statistics monitors and collects day-to-day information about the market price of raw inputs and finished goods, and publishes regularized statistical assays of this data. The Consumer Price Index and the Producer Price Index are its two most famous products. The former tracks the aggregate dollar price of consumer goods in the United States (things like onions, shovels, and smartphones); the latter (this dataset) tracks the cost of raw inputs to the industries producing those goods (things like raw steel, bulk leather, and processed chemicals).
The US federal government uses this dataset to track inflation. While in the short term the raw dollar value of producer inputs may be volatile, in the long term it will always go up due to inflation --- the slowly decreasing buying power of the US dollar.
Content
This dataset consists of a packet of files, each one tracking regularized cost of inputs for certain industries. The data is tracked-month to month with an index out of 100.
Acknowledgements
This data is published online by the US Bureau of Labor Statistics.
Inspiration
How does the Producer Price Index compare against the Consumer Price Index?
What have the largest spikes in input costs been, historically? Can you determine why they occurred?
What is the overall price index trend amongst US producers?"
Open Beauty Facts,The free cosmetic products database,Open Food Facts,8,"Version 2,2017-09-18|Version 1,2017-02-22","biology
chemistry",Other,12 MB,ODbL,"4,126 views",578 downloads,4 kernels,,https://www.kaggle.com/openfoodfacts/openbeautyfacts,"Open Beauty Facts - The free cosmetic products database.
Open your cosmetics and know what you use on your body
Be part of our collaborative, free and open database of cosmetics products from around the world!
A cosmetics products database
Open Beauty Facts is a database of cosmetics products with ingredients, allergens, indicia and all the tidbits of information we can find on product labels.
Made by everyone - Crowdsourcing
Open Beauty Facts is a non-profit association of volunteers.
Many contributors like you have added 2 000+ products using our Android, iPhone or Windows Phone app or their camera to scan barcodes and upload pictures of products and their labels.
For everyone - Open data
Data about cosmetics is of public interest and has to be open. The complete database is published as open data and can be reused by anyone and for any use. Check-out the cool reuses or make your own!
You can grow the database in your country easily using the mobile apps:
Android version: https://play.google.com/store/apps/details?id=org.openbeautyfacts.scanner
iPhone/iPad version: https://itunes.apple.com/us/app/open-beauty-facts/id1122926380
You can also browse the web version at: https://world.openbeautyfacts.org"
Correlates of War: Interstate Wars,"Countries, dates, and fatalities of all wars between 1816 and 2007",University of Michigan,8,"Version 1,2017-02-04","war
international relations",CSV,105 KB,Other,"2,651 views",290 downloads,2 kernels,,https://www.kaggle.com/umichigan/interstate-wars,"Content
The Correlates of War (COW) Project has utilized a classification of wars that is based upon the status of territorial entities, in particular focusing on those that are classified as members of the interstate system. Wars have been categorized by whether they primarily take place between/among states, between/among a state and a non-state actor, and within states.
Within the COW war typology, an interstate, intrastate, or extrastate war must meet same definitional requirements of all wars in that the war must involve sustained combat, involving organized armed forces, resulting in a minimum of 1,000 battle-related combatant fatalities within a twelve month period. For a state to be considered a war participant, the minimum requirement is that it has to either commit 1,000 troops to the war or suffer 100 battle-related deaths.
When Correlates of War scholars J. David Singer and Melvin Small first extended their study of war to include intrastate wars in Resort to Arms, they established the requisite condition that for a conflict to be a war, it must involve armed forces capable of “effective resistance” on both sides. They then developed two alternative criteria for defining effective resistance: “both sides had to be initially organized for violent conflict and prepared to resist the attacks of their antagonists, or the weaker side, although initially unprepared, is able to inflict upon the stronger opponents at least five percent of the number of fatalities it sustains.” The effective resistance criteria were specifically utilized to differentiate wars from massacres, one-sided state killings, or general riots by unorganized individuals.
Acknowledgements
The dataset was created by Meredith Reid Sarkees, American University, and Professor Frank Wayman, University of Michigan-Dearborn, and published by the Correlates of War Project."
Ideology Scores of Supreme Court Justices,Measure of individual justices' ideology on political spectrum per term,University of Michigan,8,"Version 1,2017-02-10","law
politics",CSV,40 KB,Other,"2,188 views",84 downloads,4 kernels,0 topics,https://www.kaggle.com/umichigan/court-justices,"Context
Measuring the relative location of U.S. Supreme Court justices on an ideological continuum allows us to better understand the politics of the high court. Such measures are an important building blocking of statistical models of the Supreme Court, the separation of powers system, and the judicial hierarchy.
Content
The ""Martin-Quinn"" judicial ideology scores are estimated for every justice serving from the October 1937 term to the present. The measures are estimated using a dynamic item response theory model, allowing judicial ideology to trend smoothly through time. Since the scores are estimated from a probability model, they can be used to form other quantities of interest, such as locating the pivotal ""median"" justice, as all well the location of each case in the policy space.
Acknowledgements
The Martin-Quinn scores were developed by Andrew D. Martin of the University of Michigan and Kevin M. Quinn of the UC Berkeley School of Law and supported by a National Science Foundation grant. The scores are based on the 2016 release of the Supreme Court Database."
Israeli Settlements in the West Bank,Is the population of Israeli settlements rising or falling?,Central Bureau of Statistics,8,"Version 1,2017-02-03",politics,CSV,12 KB,CC0,"2,543 views",159 downloads,6 kernels,,https://www.kaggle.com/ilcbs/israeli-settlements,"Content
This dataset includes a record for each Israeli neighborhood in East Jerusalem and settlement in the West Bank recognized by Israeli authorities; therefore, Israeli outposts, constructed without government authorization, and Nahal settlements, established by Israel Defense Forces and thus regarded as military bases, are excluded. Each row includes the settlement name in English and Hebrew, year established, regional council, location in latitude/longitude coordinates and relative to the West Bank Barrier, and population estimates for the past fifteen years.
Acknowledgements
The settlement population statistics were collected by the Israel Central Bureau of Statistics."
OpenAddresses - U.S. West,Addresses and geo-locations for the U.S. West,OpenAddresses,8,"Version 1,2017-08-03",,CSV,2 GB,Other,851 views,138 downloads,,0 topics,https://www.kaggle.com/openaddresses/openaddresses-us-west,"Context
OpenAddresses's goal is to connect the digital and physical worlds by sharing geographic coordinates, street names, house numbers and postal codes.
Content
This dataset contains one datafile for each state in the U.S. West region.
States included in this dataset:
Alaska - ak.csv
Arizona - az.csv
California - ca.csv
Colorado - co.csv
Hawaii - hi.csv
Idaho - id.csv
Montana - mt.csv
New Mexico - nm.csv
Nevada - nv.csv
Oregon - or.csv
Utah - ut.csv
Washington - wa.csv
Wyoming - wy.csv
Field descriptions:
LON - Longitude
LAT - Latitude
NUMBER - Street number
STREET - Street name
UNIT - Unit or apartment number
CITY - City name
DISTRICT - ?
REGION - ?
POSTCODE - Postcode or zipcode
ID - ?
HASH - ?
Acknowledgements
Data collected around 2017-07-25 by OpenAddresses (http://openaddresses.io).
Address data is essential infrastructure. Street names, house numbers and postal codes, when combined with geographic coordinates, are the hub that connects digital to physical places.
Data licenses can be found in LICENSE.txt.
Data source information can be found at https://github.com/openaddresses/openaddresses/tree/9ea72b079aaff7d322349e4b812eb43eb94d6d93/sources
Inspiration
Use this dataset to create maps in conjunction with other datasets for crime or weather"
EveryPolitician,Open data on politicians from 10 countries,EveryPolitician,8,"Version 1,2017-08-15",,{}JSON,42 MB,ODbL,"1,196 views",175 downloads,,0 topics,https://www.kaggle.com/everypolitician/everypoliticiansample,"Context
EveryPolitician is a project with the goal of providing data about every politician in the world. They collect open data on as many politicians as they can find and these datasets are just a small sample of the data available at http://www.everypolitician.org.
Content
Each country has their own governmental structure and EveryPolitician provides data for as many countries as possible. At the time of publishing, there was information on politicians from 233 countries. I chose to publish JSON files for these 10 countries:
Australia
Brazil
China
France
India
Nigeria
Russia
South_Africa
UK
US
These JSON files follow the POPOLO format
Acknowledgements
These data were collected from http://everypolitician.org/. Their website has more data than I have published here - this is a small sample."
Top Running Times,Who are the fastest people in the world?,Jguerreiro,8,"Version 2,2017-05-20|Version 1,2017-05-19",running,CSV,1 MB,Other,"1,573 views",234 downloads,8 kernels,0 topics,https://www.kaggle.com/jguerreiro/running,"Content
Track all-time top performances (top 1000) for Olympics distances and the half marathon.
Acknowledgements
The data was scraped from http://www.alltime-athletics.com/index.html"
Pokemon Trainers Dataset,Set of Pokemon Trainers scraped from Bulbapedia,Liam Cusack,8,"Version 3,2017-04-08|Version 2,2017-04-08|Version 1,2017-04-03",video games,SQLite,2 MB,ODbL,"2,834 views",180 downloads,3 kernels,0 topics,https://www.kaggle.com/lrcusack/pokemontrainers,"Context
I saw the Pokemon dataset from Alberto Barradas a few weeks ago and I started tinkering around with it to build my own dataset. My first step was building up some python classes for Pokemon, then for Trainers. For the Pokemon class I implemented the statistic calculation as defined here and battle mechanics as defined here. My goal was to battle all possible combinations of all pokemon trainers, but that quickly proved infeasible. As it turns out, there are over 200,000 TRILLION such combinations, so a simulation like that would take some major computing resources that I do not have. So instead, I thought I'd try battles that actually might exist by scraping Bulbapedia for trainers and what pokemon they have. It was a little hacky and it's not a fully exhaustive list (e.g. some trainer classes like special characters are recorded differently, so I skipped them), but it's still a pretty long list of trainers. I used the name from Bulbapedia to look up the Pokemon in Alberto Barrada's Pokemon dataset for base stats, then used the stat determination calculation from Bulbapedia to fill in the Pokemon's stats. I used TinyDB to store the result. See the code here
Content
The data is structured in a SQLite file with two tables, Trainers and Pokemon. Trainers has a trainerID and trainername; Pokemon has trainerID (foreign key) place (position in Trainer's party), pokename (name of the pokemon) and the following stats/types from the original Pokemon with Stats dataset: ""name"", ""level"", ""type1"", ""type2"", ""hp"", ""maxhp"", ""attack"", ""defense"", ""spatk"", ""spdef"", and ""speed"", most of which should be self explanatory. ""hp"" is the current hp of the pokemon, ""maxhp"" is the hp stat of the pokemon.
Acknowledgements
Alberto Barradas - Pokemon Data Set; Bulbapedia - Trainer data and Stat mechanics; Azure Heights Pokemon Lab - Battle mechanics (not used in this data, but still helpful in general); TinyDB - relatively quick, zero-config, json file based NoSQL database;
Inspiration
My goal is to simulate all (or at least some of) these trainers battling each other and record the result. That would mean a dataset of Pokemon battles that could be used to answer all sorts of trivial Pokemon questions. Stay tuned, now that I have the dataset and my model working, it shouldn't be too long!"
Presidential Inaugural Addresses,"US president name, date, and speech text",AdhokshajaPradeep,8,"Version 1,2017-03-31","politics
linguistics",CSV,787 KB,CC0,"2,117 views",240 downloads,12 kernels,,https://www.kaggle.com/adhok93/presidentialaddress,"Context
Every Presidency starts off with the Inaugural Address. This defines the course for the next 4 years. How do the length, word usage, lexical diversity change from President to President?
Content
The data was scraped from http://www.bartleby.com/124/ using R's rvest web scraping library. The procedure can be found here The data set is in the .csv format. The columns are : Name ,Inaugural Address , Date and text.
Acknowledgements
I would like to thank http://www.bartleby.com for making their data available for free.
Inspiration
I saw a documentary by Vox on Presidential Inaugural Speeches. They conducted a study on the common characteristics amongst speeches by influential presidents. Through a data driven approach, we can find several interesting insights which help correlate a President's influence and his inaugural speech. What word patterns do influential Presidents use often? How does speech length vary?"
Australian Football League Database,"All of the key stats, game by game, player by player",MichaelStone,8,"Version 2,2017-05-18|Version 1,2017-04-27",rugby league,Other,8 MB,ODbL,"2,821 views",307 downloads,4 kernels,3 topics,https://www.kaggle.com/stoney71/aflstats,"Context
I wanted to study player stats for Australian Rules Football (AFL), using Machine Learning to identify who the key players are, predict performance and results. My aim is to create the Ultimate Tipping predictor or maybe a Fantasy League tool. I couldn't find a dataset anywhere so I created my own GCD project and am sharing the database for anybody to explore.
Content
Every key stat from Kicks to Clangers to Bounces. Every player, game by game.
That is:
Over 45,000 rows of data
1,048 Individual Players
Seasons 2012 to 2016
22 Key Player stats per game
Match result, winning margin & location
All in the one CSV file
If you have any further suggestions please Comment below. I plan to add weather data from a separate source in a future version.
Acknowledgements
Data is taken with thanks from afltables.com and www.footywire.com
Inspiration
I want to see key insights into the players' performance that nobody has realised before. With tipping contests in the AFL as popular as any other sport, surely this is just waiting for Data Science to take over and pick winners like never before!!"
Public Transport in Zurich,Public transport routes and schedules for the city of Zurich,LAdams,8,"Version 2,2017-03-20|Version 1,2017-03-19",transport,CSV,475 MB,CC0,"2,384 views",165 downloads,6 kernels,0 topics,https://www.kaggle.com/laa283/zurich-public-transport,"Source
The data was collected and organized by https://data.stadt-zuerich.ch/ specifically under the link https://data.stadt-zuerich.ch/dataset/vbz-fahrzeiten-ogd
Data
The data table is a variance analysis of the times certain trams and busses should have departed and when they actually departed.
Interesting Questions / Challenges
What is the fastest way between Klusplatz and Oerlikon at different times of day?
What is the most punctual tram stop in Zurich?"
Elevation Data meets SF Fire Department Calls,Do the fires climb? Do fire fighters only fight fire?,BillurEngin,8,"Version 2,2017-01-09|Version 1,2016-12-23",firefighting,CSV,685 MB,Other,"2,180 views",110 downloads,4 kernels,0 topics,https://www.kaggle.com/bengin/SanFranciscoFireDepartmentCalls,"Context
This dataset is generated via merging ""San Francisco Fire Department Calls"" and ""San Francisco Elevation Data"". Fire Calls-For-Service includes all fire units responses to calls. Each record includes the call number, neighborhood, location, unit type, call type, and all relevant time intervals are also included.
Content
Call Type: Type of call the incident falls into.
Call Final Disposition: Disposition of the call (Code). For example TH2: Transport to Hospital - Code 2, FIR: Resolved by Fire Department
Unit Type: Unit type
Received DtTm: Date and time of call is received at the 911 Dispatch Center.
Response DtTm: Date and time this unit acknowledges the dispatch and records that the unit is en route to the location of the call.
On Scene DtTm: Date and time the unit records arriving to the location of the incident
Call Type Group: Call types are divided into four main groups: Fire, Alarm, Potential Life Threatening and Non Life Threatening.
Neighborhood District: Neighborhood District associated with this address, boundaries available here
Location: Latitude and longitude of address obfuscated either to the midblock, intersection or call box
Elevation:Elevation in meters
Acknowledgements
San Francisco Fire Department calls are downloaded from SFOpen webpage.
San Francisco DEM (Digital elevation models) file is obtained from National Centers for Environmental Information web page*
*Carignan, K.S., L.A. Taylor, B.W. Eakins, R.J. Caldwell, D.Z. Friday, P.R. Grothe, and E. Lim, 2011. Digital Elevation Models of Central California and San Francisco Bay: Procedures, Data Sources and Analysis, NOAA Technical Memorandum NESDIS NGDC-52, U.S. Dept. of Commerce, Boulder, CO, 49 pp.
Inspiration
Do fire fighters only fight fire? There is a wide range of calls directed to FD, what is the leading cause?
How often do firefighters actually fight a fire on a given day/week?
How fast do they respond to calls? Does the elevation lag the response?
Are there special times/months where they receive more or less calls?
Is there a relationship between the elevation and the rate or type of calls?"
News and Blog Data Crawl,"Content section from over 160,000 news and blog articles",Patrick J,8,"Version 1,2017-05-13","news agencies
linguistics
internet",CSV,459 MB,CC0,"2,304 views",182 downloads,,0 topics,https://www.kaggle.com/patjob/articlescrape,"Context
This content was scraped for a previous project in 2014. I thought this community might find it useful.
It was originally used as part of an English learning application, which automatically tailored exercises that were optimized to accelerate language acquisition. Unfortunately, the app was not commercially viable.
Content
Each record contains the following variables
body: The article body text.
title: The article header.
last_crawl_date: The date that this article was crawled.
url: The original URL of the article.
Due to upload size limits, I've had to remove many of the articles. But the original is well over the 500MB upload limit.
The file may contain duplicate or low-value records. It also contains broken tags and characters. The corpus should be cleaned before use.
Inspiration
Use NLP and classification to figure out which web site an article came from."
Awesome Public Datasets as Neo4j Graph,"Scraped, wrangled CSV and Neo4j Graph Database",Manav Sehgal,8,"Version 1,2016-12-20",,CSV,3 MB,CC0,"3,989 views",190 downloads,,0 topics,https://www.kaggle.com/startupsci/awesome-datasets-graph,"Context
The awesome datasets graph is a Neo4j graph database which catalogs and classifies datasets and data sources as scraped from the Awesome Public Datasets GitHub list.
Content
We started with a simple list of links on the Awesome Public Datasets page. We now have a semantic graph database with 10 labels, five relationship types, nine property keys, and more than 400 nodes. All within 1MB of database footprint. All database operations are query driven using the powerful and flexible Cypher Graph Query Language.
The download includes CSV files which were created as an interim step after scraping and wrangling the source. The download also includes a working Neo4j Graph Database. Login: neo4j | Password: demo.
Acknowledgements
Data scraped from Awesome Public Datasets page. Prepared for the book Data Science Solutions.
Inspiration
While we have done basic data wrangling and preparation, how can this graph prove useful for your data science workflow? Can we record our data science project decisions taken across workflow stages and how the data catalog (datasources, datasets, tools) use cases help in these decisions by achieving data science solutions strategies?"
2017 State Assembly Election Results,2017 Indian states assembly election results by constituency,Sid Shetty,8,"Version 1,2017-04-24",politics,CSV,823 KB,CC0,"1,783 views",144 downloads,3 kernels,0 topics,https://www.kaggle.com/iamsidshetty/2017-state-assembly-election-results,"The dataset contains the 2017 assembly elections results for 5 Indian States; Manipur (MR), Goa (GA), Uttar Pradesh (UP), Uttarakhand (UT) and Punjab (PB).
The data was scraped from Election Commission of India website: ECI
The Scrapper used for this data collection is here
Data Fields
The datasets contains 6 files; one for each state and the last one is the aggregated data for all 5 states. Each data file has the following 5 fields: State, Constituency, Candidate, Party, Votes"
Urban Dictionary Terms,"A collection of 4,272 words from UrbanDictionary",athontz,8,"Version 1,2017-03-28","dictionaries
linguistics",CSV,1 MB,CC0,"2,845 views",268 downloads,5 kernels,0 topics,https://www.kaggle.com/athontz/urban-dictionary-terms,"Context
I scraped all of the currently available Urban Dictionary pages (611) on 3/26/17
Content
word - the slang term added to urban dictionary
definition - the definition of said term
author - the user account who contributed the term
tags - a list of the hashtags used
up - upvotes
down - downvotes
date - the date the term was added to Urban Dictionary
Acknowledgements
I would like to thank my good friend Neil for giving the idea to scrape these terms."
IndieGoGo Project Statistics,"Over 91,000 IndieGoGo project statistics",LiamLarsen,8,"Version 1,2017-04-27","business
finance",CSV,1008 MB,ODbL,"2,266 views",311 downloads,5 kernels,,https://www.kaggle.com/kingburrito666/indiegogo-project-statistics,"Context
id
title
nearest_five_percent
tagline
cached_collected_pledges_count
igg_image_url
compressed_image_url
balance
currency_code
amt_time_left
url
category_url
category_name
category_slug
card_type
collected_percentage
partner_name
in_forever_funding
friend_contributors
friend_team_members
source_url
from the datasets forum part of kaggle
Nobody posted this and I thought it might be cool"
Help with Real Estate Closed Price Model,Local Real Estate Firm Looking for Your Machine Learning Expertise,samdeeplearning,8,"Version 6,2017-06-27|Version 5,2017-05-25|Version 4,2017-05-25|Version 3,2017-05-25|Version 2,2017-05-23|Version 1,2017-05-23",home,CSV,29 KB,Other,"2,241 views",264 downloads,17 kernels,4 topics,https://www.kaggle.com/samdeeplearning/vt-nh-real-estate,"Context
A local Vermont/New Hampshire real estate firm is looking into modeling closed prices for houses. This dataset contains features of houses in three towns in Vermont, which make up a sizable chunk of the real estate firm's business.
Content
MLS is the real estate information platform that is publicly available. Features were exported from an MLS web platform. Features include # of baths, # of bedrooms, and # of acres. There are also categorical features, such as town and address.
Hint: Natural language processing techniques that identify and leverage the road that a house is on may improve prediction accuracy.
Acknowledgements
Thank you to AH.
Goal
There is a Train, Validate, and, Test. Can you show a cross validated result that beats 10.0% error in closed price? You can use any measure to train your model - RMSE, RMSLE, etc.; however, the accuracy metric is simply mean percent error!
Please Note: These houses can be uniquely identified on the MLS website, which does also have photos of the houses. Computer Vision techniques that retrieve information from photos on the data are of interest to the company, but are not encouraged for this simple dataset, which serves as a jumping off point for future endeavors as it contains data that is already compiled and understood by the firm."
Human Instructions,200K formalised step-by-step instructions in English,paolo,8,"Version 2,2017-03-04|Version 1,2017-03-04","linguistics
internet",Other,5 GB,CC4,"2,730 views",151 downloads,,,https://www.kaggle.com/paolop/human-instructions,"The Web of Know-How: Human Instructions Dataset
Overview
This dataset has been produced as part of the The Web of Know-How project
To cite this dataset use: Paolo Pareti, Benoit Testu, Ryutaro Ichise, Ewan Klein and Adam Barker. Integrating Know-How into the Linked Data Cloud. Knowledge Engineering and Knowledge Management, volume 8876 of Lecture Notes in Computer Science, pages 385-396. Springer International Publishing (2014) (PDF) (bibtex)
Quickstart: if you want to experiment with the most high-quality data before downloading all the datasets, download the file *9of11_knowhow_wikihow*, and optionally files instruction set entities, Process - Inputs, Process - Outputs, Process - Step Links and wikiHow categories hierarchy.
Data representation based on the PROHOW vocabulary
Data extracted from existing web resources is linked to the original resources using the Open Annotation specification
Data concerning the manual evaluation of this dataset is available here
Data also available from datahub
Available Datasets
Instruction datasets:
Datasets *1of11_knowhow_wikihow* to *9of11_knowhow_wikihow* contain instructions from the wikiHow website. Instructions are allocated in the datasets in order of popularity. This means that the most popular and high-quality instructions are found in 9of11_knowhow_wikihow, while the least popular ones are in dataset 1of11_knowhow_wikihow. These instructions are also classified according to the hierarchy found in wikiHow categories hierarchy. wikiHow instructions are community generated. As a result of this, each task is described by at most one set of instructions, which is usually of high quality thanks to the collective community contributions.
Datasets *10of11_knowhow_snapguide* to *11of11_knowhow_snapguide* contain instructions from the Snapguide website. These instructions are not sorted by their popularity, but they are classified in one of the Snapguide categories. Snapguide instructions are created by single users. As a result of this there might be multiple sets of instructions to achieve the same task; however these instructions on average contain more noise as they are not peer-reviewed.
Links datasets:
The Process - Inputs datasets contain detailed information about the inputs of the sets of instructions, including links to DBpedia resources
The Process - Outputs datasets contains detailed information about the outputs of the sets of instructions, including links to DBpedia resources
The Process - Step Links datasets contains links between different sets of instructions
Other datasets:
The wikiHow categories hierarchy dataset contains information on how the various wikiHow categories are hierarchically structured, and how they relate to the Snapguide categories.
The instruction set entities dataset lists all the top-level entities in a sets of instructions. In other words, all the entities which correspond to the title of a set of instructions.
The wikiHow community links dataset lists the links manually created by the wikiHow community of users that interlink entities belonging to different sets of instructions.
Data Model
The following figure is a simple example of how the PROHOW vocabulary is used in the datasets. Instructions in the dataset can have more complex structures, for example instructions could have multiple methods, steps could have further sub-steps, and complex requirements could be decomposed into sub-requirements.
SPARQL queries
Sample SPARQL queries are available here.
Sample SPARQL Endpoint
A sample SPARQL endpoint is available at Dydra. You can use this URI for federated queries. To get you started, here are some sample queries. NOTE: this endpoint exposes only a subset of the dataset, more specifically files:
*8of11_knowhow_wikihow*
*9of11_knowhow_wikihow*
instruction set entities
Process - Inputs
Process - Outputs
Process - Step Links
wikiHow categories hierarchy
Multilingual Version of the Data
A multilingual version of this data can be found on Kaggle. This multilingual dataset contains over 800K instructions in 16 languages.
A Graphical Visualisation Tool and Live Demo
The HowLinks tool can be used to visualise this dataset and its links to DBpedia in a web browser.
A live demo of this tool (although only serving a subset of this dataset) is available here.
Statistics
23,033,490: number of triples.
2,610,223: number of labelled RDF nodes.
215,959: number of instructions. 77% from wikiHow (datasets *1of11_knowhow_wikihow* to *9of11_knowhow_wikihow*) and 23% from Snapguide (datasets *10of11_knowhow_snapguide* to *11of11_knowhow_snapguide*).
255,101: number of process inputs linked to 8,453 distinct DBpedia concepts (dataset Process - Inputs)
4,467: number of process outputs linked to 3,439 distinct DBpedia concepts (dataset Process - Outputs)
193,701: number of step links between 114,166 different sets of instructions (dataset Process - Step Links)
This dataset is partially based on original instructions from wikiHow and Snapguide accessed on the 16th of July 2014. DOI: http://dx.doi.org/10.7488/ds/1394
For any queries and requests contact: Paolo Pareti"
"ARXIV data from 24,000+ papers",Papers published between 1992 and 2017,Neel Shah,8,"Version 1,2017-03-25",,CSV,6 MB,ODbL,"1,673 views",106 downloads,,,https://www.kaggle.com/neelshah18/arxivdataset,"Context
Collection of 24000+ paper meta data.
Content
This data contains all paper related to ML, CL, NER, AI and CV field publish between 1992 to 2017.
Acknowledgements
arXiv is open source library for research papers. Thanks to arXiv for spreading knowledge.
Inspiration
To know what research is going on in the computer science all around the world."
Bible Verses from King James Version,"Text of Bible by book, chapter, and verse",Brian Liao,8,"Version 1,2017-03-16",faith and traditions,CSV,5 MB,CC4,"3,480 views",214 downloads,,,https://www.kaggle.com/phyred23/bibleverses,"Context
On qxczv.pw, a 4chan styled board, an anon posted the whole Bible in King James Version. I chose to scrape it and format it into a Bible data set.
Content
Data is in CSV in the format: citation, book, chapter, verse, text. For example: citation: Genesis 1:1 book: Genesis chapter: 1 verse: 1 text: ""In the beginning God created the heaven and the earth. ""
Acknowledgements
I'd like to thank qxczv.pw, Andrew Palmer, Jessica Butterfield, Gary Handwerk, Brian Wurtz, and the whole Lake Washington High School. Papa Bless.
Inspiration
I am unsure what data can be analysis from this data set but am thinking graphing distributions of words or running natural language processing on this could be interesting. Send me a pm if you have any ideas."
How News Appears on Social Media,Comparing What's on Twitter and Reddit to What's Happening in the World,MorganMazer,8,"Version 1,2017-05-01","news agencies
internet",CSV,920 KB,Other,"2,507 views",384 downloads,5 kernels,2 topics,https://www.kaggle.com/socialmedianews/how-news-appears-on-social-media,"Context
As part of a capstone project, we wanted to compare what social media users are talking about to what's going on in the world to see if and how social media users care about news events. We scraped data from Twitter, Reddit, reliable news sources, and Google Trending Topics.
Content
This data set includes nine tables: Twitter, news, Google Trending Topics, and six popular subreddits (news, worldnews, upliftingnews, sports, politics, television).
Twitter: trending topic, date trending, sentiment analysis scores, most common word associated with the trend, most common pairs of words associated with the trend.
News: headlines (collected from BBC News, USA Today, and the Washington Post), date the article was posted.
Google Trending Topics: trending topic, date trending.
Subreddits: post title, time, date, score (upvotes - downvotes), number of comments.
Acknowledgements
This data was collected as part of a semester project in the Capstone in Social Network Analytics at Virginia Tech, Spring 2017, taught by Siddharth Krishnan. The data was collected over a period of eight days in April 2017.
Inspiration
What do social media users care about, and in what ways do they care? What may they not know about? What types of trends appear most on each social media platform? Are people who get the majority of their news from social media able to get an accurate and comprehensive idea of what is going on? How can algorithms such as Twitter’s trending topics algorithm influence and shape what users talk about, read, and react to?"
Equitable Sharing Spending Dataset,Raw Data from the Controversial Equitable Sharing Program,The Washington Post,8,"Version 1,2016-12-03",finance,CSV,10 MB,Other,"1,879 views",98 downloads,,0 topics,https://www.kaggle.com/washingtonpost/equitable-sharing-spending-dataset,"Context
In early 2016, The Washington Post wrote that the Justice Department is ""resuming a controversial practice that allows local police departments to funnel a large portion of assets seized from citizens into their own coffers under federal law.
The ""Equitable Sharing Program"" gives police the option of prosecuting some asset forfeiture cases under federal instead of state law, particularly in instances where local law enforcement officers have a relationship with federal authorities as part of a joint task force. Federal forfeiture policies are more permissive than many state policies, allowing police to keep up to 80 percent of assets they seize."" (link to the full article can be found here).
This is the raw data from the Department of Justice’s Equitable Sharing Agreement and Certification forms that was released by the U.S. Department of Justice Asset Forfeiture and Money Laundering Section.
Content
spending_master.csv is the main spending dataset that contains 58 variables.
notes.csv lists the descriptions for all variables.
Acknowledgements
The original dataset can be found here. The data was originally obtained from a Freedom of Information Act request fulfilled in December 2014.
Inspiration
Which agency/sector/item received the most amount of funds from the Justice Department?
How many agencies received non-cash assets from the federal government through Equitable Sharing?
Are there any trends in the total equitable sharing fund across agencies?"
Parole hearings in New York State,Scraped list of parole hearings between 2014-2016,Parole Hearing Data Project,8,"Version 1,2016-12-08","crime
law",CSV,9 MB,Other,"1,846 views",156 downloads,6 kernels,,https://www.kaggle.com/parole-hearing-data/parole-hearings-in-new-york-state,"Context
In New York over 10,000 parole eligible prisoners are denied release every year, and while the consequences of these decisions are costly (at $60,000 annually to incarcerate one individual, and more to incarcerate older individuals with illnesses), the process of how these determinations are made is unclear. Advocates for parole reform argue that parole commissioners too often base their decisions on ""the nature of the crime"" for which the individual was convicted, rather than on that individual's accomplishments and growth while serving a sentence in prison.
The Parole Hearing Data Project is part of a broader body of work that can be found on the Museum of the American Prison's website.
Content
Dataset includes sex, race / ethnicity, housing or interview facility, parole board interview type, and interview decision among other factors. Scraping is up-to-date as of July 2016.
Descriptions provided by the Department of Corrections and Community Service (DOCCS) can be found here.
Acknowledgements
Data was collected and managed by Nikki Zeichner, Rebecca Ackerman, and John Krauss. Original dataset, including a scraper to gather the latest updates, can be found here.
Inspiration
Does the housing or interview facility play a role in the parole decision? Are any of these facilities particularly likely to influence a positive decision?
Do interview decisions vary based on race / ethnicity? Sex?
What is the typical time between entry and release?"
"Speed Camera Violations in Chicago, 2014-2016",Daily volume of speed limit violations recorded by cameras on Chicago streets,Chicago Police Department,8,"Version 2,2017-09-13|Version 1,2017-03-10","crime
vehicles",CSV,17 MB,CC0,"3,498 views",372 downloads,19 kernels,2 topics,https://www.kaggle.com/chicagopolice/speed-violations,"Content
This dataset reflects the daily volume of speed violations that have been recorded by each camera installed in the City of Chicago as part of the Automated Speed Enforcement Program. The data reflects violations that occurred from July 1, 2014 until December 31, 2016. The reported violations were collected by the camera and radar system and reviewed by two separate city contractors. This dataset contains all violations regardless of whether a citation was issued.
Acknowledgements
The speed camera data was collected and published by the Chicago Police Department on the City of Chicago data portal website.
Inspiration
What neighborhood has the highest density of speed cameras? Do speed cameras capture more violations on weekdays or weekends? Which camera has captured the most violations? Has the number of speed violations recorded decreased over time?"
Member States of the European Union,What divides and unites the 28 countries in the European Union?,Eurostat,8,"Version 1,2017-03-15","politics
international relations",CSV,4 KB,CC0,"2,904 views",320 downloads,4 kernels,,https://www.kaggle.com/eurostat/european-union,"Context
The European Union is a unique economic and political union between twenty-eight countries that together cover much of the continent. It was created in the aftermath of the Second World War to foster economic cooperation and thus avoid conflict. The result was the European Economic Community (EEC), established in 1958, with Belgium, Germany, France, Italy, Luxembourg and the Netherlands as its members. What began as an economic union has evolved into an organization spanning policy areas, from climate, environment and health to external relations and security, justice and migration. The 1993 name change from the European Economic Community (EEC) to the European Union (EU) reflected this.
The European Union has delivered more than half a century of peace, stability and prosperity, helped raise living standards and launched a single European currency: the Euro. In 2012, the EU was awarded the Nobel Peace Prize for advancing the causes of peace, reconciliation, democracy and human rights in Europe. The single market is the EU's main economic engine, enabling most goods, services, money and people to move freely.
Content
The European Union covers over 4 million square kilometers and has 508 million inhabitants — the world’s third largest population after China and India. This dataset includes information on each EU member state, candidate state, or European Free Trade Agreement (EFTA) signatory state.
Acknowledgements
The membership, population, and economic data was published by the European Commission's Eurostat. Gross domestic product and GDP per capita in US dollars was provided by the World Bank.
Inspiration
How has the European Union grown in the past fifty years? What is the largest country by population or surface area? Which country has the largest economy by gross domestic product? How many different languages are spoken across all the member states?"
GPS Watch Data,Kitefoil training GPS Watch data,Anthony Goldbloom,8,"Version 11,2017-09-05|Version 10,2017-08-22|Version 9,2017-08-20|Version 8,2017-08-20|Version 7,2017-08-20|Version 6,2017-01-15|Version 5,2017-01-01|Version 4,2016-11-13|Version 3,2016-11-07|Version 2,2016-11-01|Version 1,2016-10-31",,Other,86 MB,CC0,"2,540 views",117 downloads,8 kernels,0 topics,https://www.kaggle.com/antgoldbloom/gps-watch-data,"I use a GPS Watch to measure and angle while kitefoiling. I uploaded my GPS Watch data to analyze whether or not I'm getting faster and pointing higher (when kitefoil racing, you want to increase boat speed while pointing as close to the wind as possible).
This analysis will also be useful to other kitefoilers. And hopefully it's a nice example that allows others to analyze GPS Watch data for their chosen sport."
Coal Production Referenced from data.gov.in,Data about coal production from diffrent sectors in India,VineetKothari,8,"Version 1,2016-11-06",energy,CSV,34 KB,ODbL,"2,077 views",224 downloads,7 kernels,,https://www.kaggle.com/vineetkothari/coalproduction,Data from data.gov.in about the coal production in diffrent sectors india
Encrypted Stock Market Data from Numerai,"~100,000 rows of cleaned, regularized and encrypted equities data.",Numerai,8,"Version 1,2016-12-03",,CSV,35 MB,CC0,"3,958 views",127 downloads,9 kernels,,https://www.kaggle.com/numerai/encrypted-stock-market-data-from-numerai,"Context
This is a sample of the training data used in the Numerai machine learning competition. https://numer.ai/about
Content
The data is cleaned, regularized and encrypted global equity data. The first 21 columns (feature1 - feature21) are features, and target is the binary class you’re trying to predict.
Goal
We want to see what the Kaggle community will produce with this dataset using Kernels."
World Population,World Bank Statistics,walla2ae,8,"Version 1,2016-10-20",,CSV,281 KB,ODbL,"2,961 views",431 downloads,14 kernels,,https://www.kaggle.com/walla2ae/world-population,"Center for International Earth Science Information Network ( CIESIN )/Columbia University. 2013. Urban-Rural Population and Land Area Estimates Version 2. Palisades, NY: NASA Socioeconomic Data and Applications Center ( SEDAC )."
What.CD Hip Hop,"75k releases tagged ""hip.hop"" on What.CD",Nolan Conaway,8,"Version 1,2016-10-31",music,SQLite,10 MB,Other,"2,768 views",176 downloads,7 kernels,0 topics,https://www.kaggle.com/nolanbconaway/whatcd-hiphop,"I used the What.CD API to obtain information on all torrents tagged ""hip.hop"" (75,719 releases as of October 22 2016). The result is a sqlite database with two tables:
torrents
This table contains information about all 75,719 releases in the database. It has the following fields:
groupName (text): Release title
totalSnatched (integer): Number of times the release has been downloaded.
artist (text): Artist / group name.
groupYear (integer): Release year.
releaseType (text): Release type (e.g., album, single, mixtape)
groupId (integer): Unique release identifier from What.CD. Used to ensure no releases are duplicates.
id (integer): unique identifier (essentially an index).
tags
This table contains the tags associated with each of the releases in the torrents table. Because being tagged 'hip.hop' is requisite for inclusion in the database, this tag is not listed.
index (integer): Index.
id (integer): Release identifier (can be matched with id field in the torrents table).
tag (text): Tag.
I love hip hop, so this database has become my go-to data for satisfying curiosities I might have."
SVHN Preprocessed Fragments,Image Classification (Digit Recognition),Olga Belitskaya,8,"Version 1,2017-10-24","numbers
classification
deep learning
+ 2 more...",Other,1 GB,Other,"1,129 views",99 downloads,,,https://www.kaggle.com/olgabelitskaya/svhn-preproccessed-fragments,"Context
SVHN is a real-world image dataset.
Fragments of this dataset were preprocessed:
fields of photos that do not contain digits were cut off;
the photos were formatted to the standard 32X32 size;
three color channels were converted into one channel (grayscaled);
each of the resulting images was represented as an array of numbers;
the data were converted into .csv files.
Content
64000 32x32 greyscaled images of number photos with 1-5 digits (represented as arrays).
11 categories of labels (ten ""digit"" categories and one ""empty character"" category).
Information about file names in the original dataset.
Acknowledgements
The original data contains a notice ""for non-commercial use only"".
Inspiration
Image recognition and classification is a huge part of machine learning practice. In addition, this data is based on real photos."
The Works of Charles Dickens,Collected from Project Gutenberg [text],FuzzyFrogHunter,8,"Version 3,2017-10-14|Version 2,2017-10-14|Version 1,2017-10-14","books
writing
literature",Other,24 MB,Other,873 views,77 downloads,,0 topics,https://www.kaggle.com/fuzzyfroghunter/dickens,"Context
This is a collection of all the works of Charles Dickens that are available through Project Gutenberg.
This dataset is subject to the Project Gutenberg license.
It is very possible that I have missed some of his works. Please add them and update the ""Last updated"" date below if you get the chance. Otherwise, if you leave a comment on this dataset, I will try and do so myself.
I did some very rough deduplication of his works. If I missed anything, please call it out with a comment and I will rectify the situation.
Content
Last updated: October 13, 2017
To be Read at Dusk - 924-0.txt
The Seven Poor Travellers - pg1392.txt
The Pickwick Papers - 580-0.txt
A Message from the Sea - pg1407.txt
The Old Curiosity Shop - 700-0.txt
Pictures from Italy - 650-0.txt
The Magic Fishbone A Holiday Romance from the Pen of Miss Alice Rainbird, Aged 7 - pg23344.txt
A Tale of Two Cities A Story of the French Revolution - 98-0.txt
The Life And Adventures Of Nicholas Nickleby - 967-0.txt
Little Dorrit - 963-0.txt
The Uncommercial Traveller - 914-0.txt
Oliver Twist - pg730.txt
Three Ghost Stories - 1289-0.txt
The Chimes - 653-0.txt
Mugby Junction - 27924-0.txt
Great Expectations - 1400-0.txt
The Battle of Life - pg676.txt
David Copperfield - 766-0.txt
Bleak House - pg1023.txt
Sketches by Boz illustrative of everyday life and every-day people - 882-0.txt
The Haunted Man and the Ghost's Bargain - 644-0.txt
A Child's History of England - pg699.txt
American Notes for General Circulation - 675-0.txt
Hunted Down [1860] - 807-0.txt
Hard Times - 786-0.txt
The Mystery of Edwin Drood - 564-0.txt
Dickens' Stories About Children Every Child Can Read - pg32241.txt
The Cricket on the Hearth A Fairy Tale of Home - 678-0.txt
Our Mutual Friend - 883-0.txt
A Christmas Carol - pg19337.txt
Barnaby Rudge - 917-0.txt
Some Christmas Stories - 1467-0.txt
Acknowledgements
Content taken from Project Gutenberg
Image taken from Wikimedia Commons
Inspiration
Making this data available for any kind of textual analysis. I intend for this to be part of a series."
Taiwan PTT stock topics and intraday trading chats,Find out the relationship of BBS user activity/texts and stock prices,"Huang, Peng-Hsuan",8,"Version 1,2017-10-06","languages
finance
linguistics",CSV,7 MB,ODbL,"1,106 views",83 downloads,,0 topics,https://www.kaggle.com/randyrose2017/pttstock,"Context
PPT is the biggest BBS in Taiwan, it contained lots of news and discussion in different boards. PTT stock board is really popular because many of the users would give their opinions on trends of the market. Most of them tried to predict the trend of the index(^TWII). It would be interesting to know if the users activities or texts was correlated with index price or trend.
Content
There are 3 csv files.
""PTT_stock_p3000_p3718.csv""
It contains about 2 years of the stock discussion topic name, topic_URL, author ID, number of people liked it or not(push type).
""daychat_push_60d_1006.csv""
It contains 60 days of instant intraday trading chats texts (2017/7/17~2017/10/06)
""TWII_20151001_20171006.csv""
Daily OHLC , volume, up or down, %changes and volatility level of ^TWII historical prices between 2015/10/01 and 2017/10/06
Acknowledgements
All of the text data could be found on: https://www.ptt.cc/bbs/Stock/index.html.
The historical price data of Taiwan stock market index (^TWII) was from Yahoo Finance.
Inspiration
There are few NLP data presented in Mandarin. It would be a new challenge to interpret texts not written in English.
Since the text data were all about stock or index going up or down, so it would be interesting to know which ID predicts accurate or misleading the others about the market trend.
Is volatility level correlated with intraday trading chat amounts or topic amounts of the day?
Can you tell which is the most popular stock they are observing, and the trend is going up or down?"
Car_sales,"carsales,cars_sales,carsale,car_sale",Harshit Sinha,8,"Version 1,2017-10-23",,CSV,16 KB,Other,"2,258 views",471 downloads,,,https://www.kaggle.com/hsinha53/car-sales,"car_sales data set contains all the information from manufacturer, type, brand, category, price etc."
Run Activities,Can you predict sport performance from the weather?,Mirko Mälicke,8,"Version 2,2017-11-27|Version 1,2017-11-25","running
sports
weather",{}JSON,3 MB,CC4,"1,572 views",162 downloads,,0 topics,https://www.kaggle.com/mmaelicke/run-activites,"Context
I have desk job. A very interesting desk job but nevertheless a desk job. Therefore I started doing sports a while ago and now I can't stop anymore. Like every other Geek I need a gadet for every hobby I have and in this case it was a GPS-Sport-Smartwatch: The vivoactive and the vivoactive HR by Garmin. They also offer a data analysis center and as a data scientist I of course had to export the data and employ some analysis that go beyond bar charts. This dataset is basically the bulk-downloaded-not-cleaned-dataset from the mentioned data center. For those interested: there is a nice github project for bulk downloading from garmin connect: https://github.com/kjkjava/garmin-connect-export. The weather data has to be included by hand.
Content
You can find 155 samples, each one representing one sport activity, mainly from the black forest. There are a lot of useless colmuns, which either contain no data or the same value for every sample. You will have to identify these columns in any case and remove them. The ZIP includes the GPX tracks of all activities and can be used as well. The two devices use different GPS sensors and are from my feeling of different precision and reliability (untested). Additionally, the HR device had a lot connectivity problems since summer 2017. The devices lost the signal during a numerous amount of runs and therefore the distance value is not always correct. Usually the start and end timestamps are correct (except for one case) and the GPX files might help to figure out which track I was using. With a single exception all start and ending points are the same in all tracks. This means I started and ended recording at the same cross-roads, not exactly the same position. If you decide to open the GPX files in a GIS, you should be able to repair the affected datasets. I did this using QGIS. You will find one instance with two activities on a single day. This is actually the same activity, where I went to the peak of Rosskopf in the Black Forest. Because my brain was undersupplied after runnning up there I ended the activity instead of pausing it, that's why I ended up with two activities, that have to be merged together.
Soft data
For the dataset, there is also some soft data which might be helpful:
I commute 130 km to work since June 2016, usually on Mon, Tue, Wed. Possibly, I spent less time on sports since then on these days.
I wrote my Master thesis from Sep / 2015 until Mar 2016. Maybe I was doing more sports during the thesis (except for the last two weeks?)
Since I commute, I would say I do sports less frequently, but on longer distances and over higher elevation gains
I bought new shoes in August 2016, which are WAY more comfortable
Acknowledgements
The bulk downloading script for garmin connect was really helpful: https://github.com/kjkjava/garmin-connect-export. Without this tool I most likely would not have created this dataset.
Inspiration
I am personally very interested if my running performance is dependent on specific weather conditions and eventually predictable. Another interesting thing would be to see how other people rate the performance based on the given data. I use this dataset also during my teaching in a Python and a statistics class at University. I decided to upload the data and some of my teaching notebooks to Kaggle (in the near future) in oder to give my students access to external comments on my kernels, givem them the opportunity to upload their solutions to a bigger community and eventually scan your kernels on the data."
"Harvard Course Enrollments, Fall 2015",Enrollment numbers for every Harvard course offered in Fall Term 2015,Harvard University,8,"Version 1,2016-11-13",education,CSV,138 KB,CC0,"3,135 views",287 downloads,4 kernels,0 topics,https://www.kaggle.com/harvard-university/course-enrollment-stats,"This dataset contains enrollment numbers for every course offered at Harvard during Fall Term 2015.
The Data
The course enrollment data contains the following fields:
COURSE: the course name (consists of the department/program abbreviation and a course number/letter; the abbreviation and the number/letter are separated by a space)
DEPARTMENT: the abbreviation for this course's department
COURSEID: a unique identifier for the course
CLASSNBR: another unique identifier for the course?
TOTALENROLLMENT: total number of students enrolled, from every school
GSAS: number of students enrolled from the Graduate School of Arts and Sciences
HCOL: number of students enrolled from Harvard College (undergraduate)
NONDGR: number of non-degree-seeking students enrolled
VUS: number of students enrolled from the Visiting Undergraduate Students Program
XREG: number of students from other universities who are cross-registered in the course
Note that there is also a row whose COURSE value is TOTALS: and whose DEPARTMENT, COURSEID, and CLASSNBR values are empty. This row lists the total number of students from each school (GSAS, HCOL, etc) in all of the courses.
For more info on what each of the courses is, check out the Harvard Course Catalog.
Acknowledgments
All of the data in this dataset comes from The Harvard Open Data Dataverse. The specific citation is:
Mehta, Neel, 2016, ""Course enrollment stats"", doi:10.7910/DVN/9MWTYO, Harvard Dataverse, V1 [UNF:6:PA8A+2yr3nGT9I9XGhWeIg==]"
AP Computer Science A Exam Dataset,AP CS A Exam Pass Rates Across States,Institute for Computing Education at Georgia Tech,8,"Version 1,2016-11-13","education
computer science",CSV,30 KB,Other,"3,833 views",259 downloads,7 kernels,,https://www.kaggle.com/iceatgt/ap-computer-science-a-exam-dataset,"Context
The datasets contain all the data for the number of CS AP A exam taken in each state from 1998 to 2013, and detailed data on pass rates, race, and gender from 2006-2013. The data was complied from the data available at http://research.collegeboard.org/programs/ap/data. This data was originally gathered by the CSTA board, but Barb Ericson of Georgia Tech keeps adding to it each year.
Content
historical.csv contains data for the number of CS AP A exam taken in each state from 1998 to 2013:
state: US states
1998-2013
Pop: population
pass_06_13.csv contains exam pass rates, race and gender data from 2006 to 2013 for selected states.
pass_12_13.csv contains exam pass rates, race and gender information for every state for 2012 and 2013.
Acknowledgements
The original datasets can be found here and here.
Inspiration
Using the datasets, can you examine the temporal trends in the exam pass rates by race, gender, and geographical location?"
Missing people in Russia,Records for 2014-2017,Alexander Minushkin,7,"Version 3,2018-02-01|Version 2,2017-11-29|Version 1,2017-11-21","russia
crime",CSV,2 MB,CC4,"1,094 views",85 downloads,,,https://www.kaggle.com/miniushkin/missing-people-in-russia,"Context
This is official open data from The Ministry of Internal Affairs of the Russian Federation on missing and wanted people, identified and unindentified corpses. Original data available here source.
Content
File meta.csv - contain information about data source and contact information of original owners in Russian.
File structure-20140727.csv - describe datastructure in Russian. Main things that you need to know about data columns are here:
""Subject"" - The official name of the subject of statistical reporting. That's Russian regions, note that Crimean Federal District and city of Sevastopol are included.
""Point FPSR"" - Item of the Federal Statistical Work Plan. You don't need to know this.
""Name of the statistical factor"" - this one speaks for itself. Available factors:
-- Identified persons from among those who were wanted, including those who disappeared from the bodies of inquiry, investigation, court.
-- Total cases on the identification of citizens on unidentified corpses that were on the register.
-- Total wanted persons, including those who disappeared from the bodies of inquiry, investigation, court.
-- Identified persons from among the wanted persons, including those missing.
-- Total wanted persons.
-- Number (balance) of unreturned missing persons in relation to 2011 (%)
-- Number (balance) of unresolved criminals against 2011 (%)
-- Total discontinued cases in connection with the identification of the person
-- Total wanted persons, including those missing
-- Identified persons from the number of wanted persons
""Importance of the statistical factor"" - value of correspondent statistical factor.
Files data-%Y%m%d-structure-20140727.csv contain actual data. Names of the files contain release date. Data aggregated by quarters of each year, for example data-20150127-structure-20140727.csv - data for whole 2014 year data-20150627-structure-20140727.csv - data for Q1 and Q2 of 2015
File translate.csv is used to simplify translation from Russian to English. See usage in the kernel.
Acknowledgements
Thanks to newspaper Komsomolskaya Pravda for bringing up the issue of missing kids in Russia.
Thanks to Liza Alert - Volunteer Search and Rescue Squad for efforts in rescue of missing people in Russia.
Photo by Alessio Lin on Unsplash
Inspiration
Missing people, especially kids, is a serious problem. However there is not much detailed information about it. Russian officials provide overall information without detalisation of victim's age. As a result many speculations appear in media on this topic:
Last year about 200,000 reports of missing people were filed with police in Russia.
45,000 kids lost every year
More than 15,000 kids lost every year
Radio interview - starting from minute 7:55 main point: ""More than 15K kids lost completely, i.e. was not ever found""
Some insights to official data can be found here interview, year 2012: ""Annually in Russia about 20 thousand minors disappear, in 90% of cases the police find children"".
Still there is no information about kids in recent years. If you have any reliable sources, please share."
Pokemon Images,A collection of 801 pokemon images,AkshayAradhya,7,"Version 1,2017-11-28","games and toys
video games",Other,28 MB,CC0,"1,043 views",80 downloads,,0 topics,https://www.kaggle.com/dollarakshay/pokemon-images,"Context
I was searching far and wide, but I could not find any dataset that had a collection of all the pokemon Images. So i decided to write a python script. 15 Minutes later I had this.
Content
A collection of 801 transparent png pokemon images of size 215x215
Acknowledgements
Use at will
Inspiration
""I want to be the very best, like no one ever was..."""
ENEM 2016,"Data from ENEM 2016, the Brazilian High School National Exam.",Gustavo Bonesso,7,"Version 2,2017-11-30|Version 1,2017-11-29",education,CSV,1 GB,CC0,"1,017 views",119 downloads,3 kernels,,https://www.kaggle.com/gbonesso/enem-2016,"Context
This dataset was downloaded from INEP, a department from the Brazilian Education Ministry. It contains data from the applicants for the 2016 National High School Exam.
Content
Inside this dataset there are not only the exam results, but the social and economic context of the applicants.
Acknowledgements
The original dataset is provided by INEP (http://portal.inep.gov.br/microdados).
Inspiration
The objective is to explore the dataset to achieve a better understanding of the social and economic context of the applicants in the exams results."
Ukrainian Parliament Daily Agenda Results,Sessions of the Verkhovna Rada,Serhiy Subota,7,"Version 4,2017-10-18|Version 3,2017-10-18|Version 2,2017-10-18|Version 1,2017-10-18","politicians
politics",{}JSON,254 MB,CC0,633 views,32 downloads,,,https://www.kaggle.com/subota/ukrainian-parliament-daily-agenda-result,"Context
Data on parliamentary agendas and actions taken by the Verkhovna Rada (Верховна Рада), the Ukrainian parliament, in 2014 through 2017.
Content
For the period of 27th Nov 2014 through 17th Oct 2017:
List of deputies
List of parliamentary fractions
Session Days
Daily agenda results, including:
total voting result
individual deputy votes
speech authors and timings, no full text
registration performed as the session day starts
Acknowledgements
Sourced from: http://data.rada.gov.ua/open/data/ppz-skl8, on the Ukrainian government open data portal. Thanks to everyone who made this open data possible.
Photo by Illia Cherednychenko on Unsplash.
Inspiration
How are the Ukranian parliamentary factions structured?
Does parliamentary activity in this dataset reflect the ongoing political events in the country?"
Lithuanian parliament votes,All available votes made by members of Lithuanian parliament since 1997.,Mantas Zimnickas,7,"Version 1,2017-11-29","parties
law
politics",CSV,28 MB,ODbL,293 views,14 downloads,,0 topics,https://www.kaggle.com/sirexo/lithuanian-paliament-votes,"Context
In Lithuania, Lithuanian Parliament has sittings several times per week, during each sitting member of parliament vote for proposed questions. Some questions are related to law amendments other questions could be something like ""should we do a break before taking more questions"". In one they there can be several sittings, usually one in the morning and another in the evening.
All members of parliament (MPs) form parliamentary groups, there is a group having majority of MS's and other smaller groups.
In this dataset you will find all votes made by all MPs since 1997 up until 2017. So that is a lot of data and basically most of the political history of independent Lithuania.
Content
Each row in votes.csv file represents a vote mode by single MP on a single question. Also for each vote there is a number of meta data provided:
voting_id - unique voting id, can be a negative number. You can reconstruct URL to the voting page using this template: http://www.lrs.lt/sip/portal.show?p_r=15275&p_k=1&p_a=sale_bals&p_bals_id={voting_id}, replace {voting_id} with a voting id.
voter_id - unique MP id.
time - date and time when a vote was cast.
group - abbreviated name of a parliamentary group.
voter - full name of an MP.
question - question text, usually question sounds like ""do we accept this proposal or not"", unfortunately title and texts of proposed documents are not included in this dataset.
sitting_type - one of:
rytinis - in the morning
vakarinis - in the evening
neeilinis - additional sitting
nenumatytas - not planned
vote - vote value, one of:
1 - aye
-0.5 - abstain
-1 - against
n_eligible_voters - total number of eligible to vote MPs.
n_eligible_voters - number of MPs who voted in a voting.
Acknowledgements
All the data where scraped from Lithuanian Parliament web site. Code of web scraper can be found here:
https://github.com/sirex/databot-bots/blob/master/bots/lrs/balsavimai.py
Inspiration
One of the most interesting questions I would like to get is a way to automatically categorize all the voting by topic. Usually there are several major topics involving multiple documents and many voting sessions, some times these topics can last for years. Unfortunately there is no such field, with a topic, in order to discover a topic one would need to analyze content of documents and probably sitting transcripts to find a topic. But in order to do this, data of documents and sitting transcripts will be needed, I will provide this data some time later.
Other interesting things to look into is how different parliamentary groups or people relates to one another by their votes."
Stock Index,"Stock Index about eco10, Ibex 35, Ibex Divident, Igbm and Continuos market",Albert Costas,7,"Version 7,2017-11-08|Version 6,2017-10-29|Version 5,2017-10-27|Version 4,2017-10-25|Version 3,2017-10-23|Version 2,2017-10-16|Version 1,2017-10-13","finance
money",Other,322 KB,ODbL,"2,548 views",174 downloads,,0 topics,https://www.kaggle.com/acostasg/stock-index,"«Datasets per la comparació de moviments i patrons entre els principals índexs borsatils espanyols i les crypto-monedes»
Context
En aquest cas el context és detectar o preveure els diferents moviments que es produeixen per una serie factors, tant de moviment interns (compra-venda), com externs (moviments polítics, econòmics, etc...), en els principals índexs borsatils espanyols i de les crypto-monedes.
Hem seleccionat diferents fonts de dades per generar fitxers «csv», guardar diferents valors en el mateix període de temps. És important destacar que ens interessa més les tendències alcistes o baixes, que podem calcular o recuperar en aquests períodes de temps.
Content
En aquest cas el contingut està format per diferents csv, especialment tenim els fitxers de moviments de cryptomoneda, els quals s’ha generat un fitxer per dia del període de temps estudiat.
Pel que fa als moviments del principals índexs borsatils s’ha generat una carpeta per dia del període, en cada directori un fitxer amb cadascun del noms dels índexs. Degut això s’han comprimit aquests últims abans de publicar-los en el directori de «open data» kaggle.com.
Pel que fa als camps, ens interessà detectar els moviments alcistes i baixistes, o almenys aquelles que tenen un patró similar en les cryptomonedes i els índexs. Els camps especialment destacats són:
• Nom: Nom empresa o cryptomoneda;
• Preu: Valor en euros d’una acció o una cryptomoneda;
• Volum: En euros/volum 24 hores,acumulat de les transaccions diàries en milions d’euros
• Estat: Estat final en tancament en alta o baixa del dia.
• Var. Per cent: Variació en el moment del tancament amb tant per cent respecte el dia anterior
• Var. En euros: Variació en el moment del tancament amb euros respect el dia anterior.
• Capitalització: Valor de l’empra respecte les seves accions.
• PER: La ràtio preu-benefici
• Rent./Div: Rendibilitat de l’acció respecte el valor inicial de la acció.
Acknowledgements
En aquest cas les fonts de dades que s’han utilitzat per a la realització dels datasets corresponent a:
http://www.eleconomista.es
https://coinmarketcap.com
Per aquest fet, les dades de borsa i crypto-moneda estan en última instància sota llicència de les webs respectivament. Pel que fa a la terminologia financera podem veure vocabulari en renta4banco.
[https://www.r4.com/que-necesitas/formacion/diccionario]
Inspiration
Hi ha un estudi anterior on poder tenir primícies de com han enfocat els algoritmes:
https://arxiv.org/pdf/1410.1231v1.pdf
En aquest cas el «trading» en cryptomoneda és relativament nou, força popular per la seva formulació com a mitja digital d’intercanvi, utilitzant un protocol que garanteix la seguretat, integritat i equilibri del seu estat de compte per mitjà d’un entramat d’agents.
La comunitat podrà respondre, entre altres preguntes, a:
Està afectant o hi ha patrons comuns en les cotitzacions de cryptomonedes i el mercat de valors principals del país d'Espanya?
Els efectes o agents externs afecten per igual a les accions o cryptomonedes?
Hi ha relacions cause efecte entre les acciones i cryptomonedes?
Project repository
https://github.com/acostasg/scraping
Datasets
Els fitxers csv generats que componen el dataset s’han publicat en el repositori kaggle.com:
https://www.kaggle.com/acostasg/stock-index/
https://www.kaggle.com/acostasg/crypto-currencies
Per una banda, els fitxers els «stock-index» estan comprimits per carpetes amb la data d’extracció i cada fitxer amb el nom dels índexs borsatil. De forma diferent, les cryptomonedes aquestes estan dividides per fitxer on són totes les monedes amb la data d’extracció."
National Accounts,Global GDP & Government Expenditures since 1946,United Nations,7,"Version 1,2017-11-17",economics,CSV,34 MB,Other,901 views,98 downloads,,0 topics,https://www.kaggle.com/unitednations/national-accounts,"The Economic Statistics Branch of the United Nations Statistics Division (UNSD) maintains and annually updates the National Accounts Official Country Data database. This work is carried out in accordance with the recommendation of the Statistical Commission at its first session that the Statistics Division of the United Nations should publish regularly the most recent available data on national accounts for as many countries and areas as possible. The database contains detailed official national accounts statistics in national currencies as provided by the National Statistical Offices.
Data are available for most of the countries or areas of the world and form a valuable source of information on their economies. The database contains data as far back as 1946, up to the year t-1, with data for most countries available from the 1970s. The database covers not only national accounts main aggregates such as gross domestic product, national income, saving, value added by industry and household and government consumption expenditure and its relationships; but also detailed statistics for institutional sectors (including the rest of the world), comprising the production account, the generation of income account, the allocation of primary income account, the secondary distribution of income account, the use of disposable income account, the capital account and the financial account, if they are compiled by countries.
The statistics for each country or area are presented according to the uniform table headings and classifications as recommended in the United Nations System of National Accounts 1993 (1993 SNA). A summary of the 1993 SNA conceptual framework, classifications and definitions are included in the yearly publication “National Accounts Statistics, Main Aggregates and Detailed Tables”.
Acknowledgements
This dataset was kindly published by the United Nation on the UNData site. You can find the original dataset here.
License
Per the UNData terms of use: all data and metadata provided on UNdata’s website are available free of charge and may be copied freely, duplicated and further distributed provided that UNdata is cited as the reference."
DSL Corpus Collection (DSLCC),Multilingual collection of short excerpts of journalistic texts,VarDial,7,"Version 1,2017-11-21","languages
linguistics",Other,55 MB,Other,346 views,18 downloads,,0 topics,https://www.kaggle.com/vardial/dslcc,"This is a mirror of the data from original source of the DSLCC at http://ttg.uni-saarland.de/resources/DSLCC/
DSL Corpus Collection (DSLCC).
The DSLCC is a multilingual collection of short excerpts of journalistic texts. It has been used as the main data set for the DSL shared tasks organized within the scope of the workshop on NLP for Similar languages, Varieties and Dialects (VarDial). For more information, please check the DSL shared task reports (links below) or the website of past editions of VarDial workshop: VarDial 2017 at EACL, VarDial 2016 at COLING, LT4VarDial 2015 at RANLP, and VarDial 2014 at COLING.
So far, five versions of the DSLCC have been released. Languages included in each version of the DSLCC grouped by similarity are the table below. Click on the respective version to download the dataset.
Citing the Dataset
If you used the dataset we kindly ask you to refer to the corpus description paper where you can also find more information about the DSLCC:
Liling Tan, Marcos Zampieri, Nikola Ljubešić, Jörg Tiedemann (2014) Merging Comparable Data Sources for the Discrimination of Similar Languages: The DSL Corpus Collection. Proceedings of the 7th Workshop on Building and Using Comparable Corpora (BUCC). pp. 6-10. Reykjavik, Iceland. pdf bib
The DSL Reports
For the results obtained by the participants of the four editions of the DSL shared task, please see the shared task reports below. In 2017, the DSL shared task was part of the VarDial evaluation campaign.
2017 - Marcos Zampieri, Shervin Malmasi, Nikola Ljubešić, Preslav Nakov, Ahmed Ali, Jörg Tiedemann, Yves Scherrer, Noëmi Aepli (2017) Findings of the VarDial Evaluation Campaign 2017. Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial). pp. 1-15. Valencia, Spain. pdf bib
2016 - Shervin Malmasi, Marcos Zampieri, Nikola Ljubešić, Preslav Nakov, Ahmed Ali, Jörg Tiedemann (2016) Discriminating between Similar Languages and Arabic Dialect Identification: A Report on the Third DSL Shared Task. Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial). pp. 1-14. Osaka, Japan. pdf bib
2015 - Marcos Zampieri, Liling Tan, Nikola Ljubešić, Jörg Tiedemann, Preslav Nakov (2015) Overview of the DSL Shared Task 2015. Proceedings of the Joint Workshop on Language Technology for Closely Related Languages, Varieties and Dialects (LT4VarDial). pp. 1-9. Hissar, Bulgaria. pdf bib
2014 - Marcos Zampieri, Liling Tan, Nikola Ljubešić, Jörg Tiedemann (2014) A Report on the DSL Shared Task 2014. Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects (VarDial). pp. 58-67. Dublin, Ireland. pdf bib
Additional Datasets
The following datasets have been used in other shared tasks organized within the scope of the VarDial workshop.
Arabic Dialect Identification (ADI): A dataset containing four Arabic dialects: Egyptian, Gulf, Levantine, North African, and MSA.
German Dialect Identification (GDI): The ArchiMob corpus containing Swiss German dialects from Basel, Bern, Lucerne, and Zurich.
Cross-lingual Parsing (CLP): Datasets for parsing similar languages: Croatian - Slovenian, Slovak - Czech, Norwegian - Danish and Swedish.
Acknowledgements
Credits of the datasets goes to the original data creators and the VarDial workshop organizers.
Credits of the banner image goest to G. Crescoli on Unsplash"
Classic Literature in ASCII,Explore some of the most influential english language works,Myles O'Neill,7,"Version 1,2017-11-16","literature
internet",Other,124 MB,CC0,743 views,64 downloads,,0 topics,https://www.kaggle.com/mylesoneill/classic-literature-in-ascii,"On the internet of the 1980's everything was stored in ASCII text files. During these early days, many literary works were manually typed up and shared widely. TEXTFILES.COM is a website by Jason Scott dedicated to collecting and preserving text files from this internet of the past. This dataset is a small subset of his total collection - focussing exclusively on english literary works.
Each book is stored in its own ASCII text file and all are in English. How similar are the writing styles of so-called classic authors? Can you train a model to determine if a work is fictional or not? What words or phrases are the most popular in these books?"
Formspring data for Cyberbullying Detection,"A large unlabeled Formspring dataset, from a Summer 2010 crawl",SwetaAgrawal,7,"Version 4,2017-01-26|Version 3,2017-01-20|Version 2,2016-10-04|Version 1,2016-10-02",,CSV,4 MB,Other,"4,698 views",405 downloads,,,https://www.kaggle.com/swetaagrawal/formspring-data-for-cyberbullying-detection,"The data represented 50 ids from Formspring.me that were crawled in Summer 2010.
For each id, the profile information and each post (question and answer) was extracted.
Each post was loaded into Amazon's mechanical turk and labeled by three workers for cyberbullying content.
The data contains the following profile fields: BIO - profile biography created by owner of the id DATE - the date the id was crawled LOCATION - location provided by the owner of the id USERID - The actual id itself
The data contains the following information on each post TEXT - the question and answer (separated by a
) ASKER - the id of the person asking the question (blank if anonymous)
3 occurrences of LABELDATA:
    ANSWER - YES or NO as to whether the post contains cyberbullying
    CYBERBULLYINGWORK - word(s) or phrase(s) identified by the mechanical turk worker as the reason it was tagged as cyberbullying (n/a or blank if no cyberbullying detected)
    SEVERITY - cyberbullying severity from 0 (no bullying) to 10 
    OTHER - other comments from the mechanical turk worker
    WORKTIME - time needed to label the post (in seconds)
    WORKER - mechanical turk worker id
Information on how this data was used is available at:
Reynolds, K, A. Kontostathis and L. Edwards. 2011. Using Machine Learning to Detect Cyberbullying. In Proceedings of the 2011 10th International Conference on Machine Learning and Applications Workshops (ICMLA 2011). December 2011. Honolulu, HI.
This material is based upon work supported by the National Science Foundation under Grant No. 0916152.   Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.
I am currently working on detecting cyberbullying among teenagers in social media. In both physical and cyber worlds, Bullying has been recognized as a serious national health issue among adolescents. . The potential of sentiment analysis can help identify victims who pose high risk to themselves or others, and to enhance the scientific understanding of bullying overall."
Tumblr GIF Description Dataset,Animated GIF Description - Sequence to Sequence - Video Captioning,Yuncheng Li,7,"Version 1,2016-08-18",,Other,27 MB,Other,"3,167 views",91 downloads,,,https://www.kaggle.com/raingo/tumblr-gif-description-dataset,"Tumblr GIF (TGIF) dataset
The Tumblr GIF (TGIF) dataset contains 100K animated GIFs and 120K sentences describing visual content of the animated GIFs. The animated GIFs have been collected from Tumblr, from randomly selected posts published between May and June of 2015. We provide the URLs of animated GIFs in this release. The sentences are collected via crowdsourcing, with a carefully designed annotation interface that ensures high quality dataset. We provide one sentence per animated GIF for the training and validation splits, and three sentences per GIF for the test split. The dataset shall be used to evaluate animated GIF/video description techniques.
If you end up using the dataset, we ask you to cite the following paper: preprint
Yuncheng Li, Yale Song, Liangliang Cao, Joel Tetreault, Larry Goldberg, Alejandro Jaimes, Jiebo Luo. ""TGIF: A New Dataset and Benchmark on Animated GIF Description"", CVPR 2016
If you have any question regarding the dataset, please contact:
Yuncheng Li
License
This dataset is provided to be used for approved non-commercial research purposes. No personally identifying information is available in this dataset.
data
Contains URLs to download animated GIF files, sentence descriptions, train/test splits, baseline results and evaluation scripts.
tgif-v1.0.tsv
Animated GIF URLs and descriptions. Each row contains a URL and a sentence, tab-separated.
Examples:
https://38.media.tumblr.com/9f6c25cc350f12aa74a7dc386a5c4985/tumblr_mevmyaKtDf1rgvhr8o1_500.gif a man is glaring, and someone with sunglasses appears.
https://38.media.tumblr.com/9ead028ef62004ef6ac2b92e52edd210/tumblr_nok4eeONTv1s2yegdo1_400.gif a cat tries to catch a mouse on a tablet
https://38.media.tumblr.com/9f43dc410be85b1159d1f42663d811d7/tumblr_mllh01J96X1s9npefo1_250.gif a man dressed in red is dancing.
https://38.media.tumblr.com/9f659499c8754e40cf3f7ac21d08dae6/tumblr_nqlr0rn8ox1r2r0koo1_400.gif an animal comes close to another in the jungle
https://38.media.tumblr.com/9ed1c99afa7d71411884101cb054f35f/tumblr_mvtuwlhSkE1qbnleeo1_500.gif a man in a hat adjusts his tie and makes a weird face.
https://38.media.tumblr.com/9e437d26769cb2ac4217df14dbb20034/tumblr_npw7v7W07C1tmj047o1_250.gif someone puts a cat on wrapping paper then wraps it up and puts on a bow
https://38.media.tumblr.com/9e4ab65c0e7d4bb8aa6b5be854b83794/tumblr_mdlv9v6hE91qanrf2o1_r11_500.gif a brunette woman is looking at the man
https://38.media.tumblr.com/9ecd3483028290171dcb5e920ff4e3bb/tumblr_nkcmeflaVj1u26rdio1_500.gif a man on a bicycle is jumping over a fence.
https://38.media.tumblr.com/9f83754d20ce882224ae3392a8372ee8/tumblr_mkwd0y8Poo1qlnbq8o1_400.gif a group of men are standing and staring in the same direction.
https://38.media.tumblr.com/9e6fcb37722bf01996209bdf76708559/tumblr_np9xo74UgD1ux4g5vo1_250.gif a boy is happy parking and see another boy
splits
Contains train/test splits used in our CVPR 2016 paper. We include one sentence per GIF for training split, three sentence per GIF for test split.
Acknowledgement
We thank the Flickr vision team, including Gerry Pesavento, Huy Nguyen and others for their support and help in collecting descriptions via crowdsourcing.
Notes
Last edit: April 5, 2016"
League of Legends Summoner Ids and Data - 2016,480k summoner ids sorted by tier from October 2016. Game data from 130k players.,Chris Pierse,7,"Version 1,2017-04-27",video games,CSV,111 MB,CC4,"2,193 views",201 downloads,2 kernels,0 topics,https://www.kaggle.com/xenogearcap/league2016,"Context
The summoner ids, which are unique to each player, were collected and sorted by ranked tier around October of last year. The game data was collected from only gold-ranked summoner ids.
I originally collected this data to identify which champions top Quinn mains tend to play aside from Quinn. The strongest correlation I found was that top Quinn players tend to also play Graves in the top lane. I recently revisited this project to put together a simple recommender system with newer data, and that system can be found here.
I am sharing the 2016 data here because Riot's API seems to only provide a summoner's current rank, i.e. there is no rank history. This 2016 data could be useful for anyone interested in seeing how summoners have evolved over time. To get you started on working with champion data, I also added the 2016 game data I collected from Gold-ranked summoners when I was investigating top Quinn players.
Content
All data was collected through Riot Games' publicly available API.
SummIds2016 - 480421 summoner ids sorted by tier as of late October 2016.
GoldSummData2016 - Game data from 131552 Gold-ranked summoners. For each summoner, all champions that were played in the 2016 season at the time of collection are presented and sorted by role. The roles are those provided by Riot's API. The columns are separated by commas, while the champions in each role are separated by spaces.
ChampId2Name - Maps champion ids to champion names.
Acknowledgements
This data is only available because Riot provides a publicly accessible API. Thanks Riot! The banner image is also the property of Riot Games.
Inspiration
Given that the unique aspect of this data set is the rank of each summoner in 2016, it would be interesting to see how many summoners improved their performance from 2016 to 2017. Perhaps you can identify an underlying trend that can explain why some summoner's went up/down in rank, e.g. top Quinn players may have increased in rank due to the buffs to lethality.
Because League of Legends changes with each patch, it would also be interesting to see how someone can leverage year-old data to make recommendations that are still relevant."
Stack Overflow 2016 Dataset,Survey Answers from Developers,Jon Hong,7,"Version 1,2016-09-09",,CSV,67 MB,CC0,"4,242 views",304 downloads,14 kernels,,https://www.kaggle.com/jonmhong/stackoverflow2016,It's Stack Overflow's 2016 Dataset
Pokemon Go Gen II (251),"Factors that impact a Pokemon's CP, combat, defense and other variables",AaronMcKisic,7,"Version 1,2017-04-18",video games,CSV,31 KB,Other,"2,055 views",145 downloads,,0 topics,https://www.kaggle.com/aaronmckisic/pokemon-go-gen-ii-251,"Context
Pokemon has been around for the majority of my life. I obsessed over it as a child and enjoy seeing the success it carries still today. I figure that I can give the Pokemon Company a nod by applying my passion for data science to their datasets.
Content
This is compiled data from two websites that I will acknowledge soon. As for the data, the primary variables are id, name, attack, defense, health, and cp. Other variables include tankiness, potentialdamage, basicdps, attackbasicdps (damage per second with strongest basic attack*attack), chargedps, oneshotpotential (damage per second with strongest charge attack*attack), and various rankings in each category as well as growth rates for each ranking (this variable only makes sense when sorted by each variable's ranking).
Acknowledgements
I have to pay tribute to two websites:
serebii.net and pokemongodb.net // these two pages allowed me to find the data that helped best explore the Pokemon Go universe. Thank you very much.
Inspiration
The data makes it pretty clear what plays into a Pokemon's CP. But, I am curious to know what hidden gems you might find when going through this data. For example, is Dragonite really the powerhouse we think it is?"
Determine the pattern of Tuberculosis spread,Tuberculosis data (prevalence and mortality),Hena,7,"Version 1,2016-08-05",,CSV,851 KB,Other,"1,730 views",203 downloads,7 kernels,0 topics,https://www.kaggle.com/henajose/determine-the-pattern-of-tuberculosis-spread,"The data given here pertains to Tuberculosis spread across countries from 2007 to 2014. Aim of this exercise is to understand how well we can utilize or what insights we can derive from disease spread data collected by international organizations like WHO.
Data: Global Health Observatory data repository (WHO) : http://apps.who.int/gho/data/view.main.57020MP?lang=en
This data has been used in the past to create country wise distribution pattern."
San Francisco based Startups,1000+. From Angellist,AashaySachdeva,7,"Version 1,2016-09-11",,CSV,686 KB,Other,"2,448 views",179 downloads,,,https://www.kaggle.com/aashay96/san-francisco-based-startups,Startups from Angellist in the Bay Area
Grocery Store Data Set,This is a small data set consisting of 20 transactions.,Shazad Udwadia,7,"Version 1,2016-11-08",,CSV,478 B,CC0,"8,523 views","1,052 downloads",2 kernels,0 topics,https://www.kaggle.com/shazadudwadia/supermarket,"For my Data Mining lab where we had to execute algorithms like apriori, it was very difficult to get a small data set with only a few transactions. It was infeasible to run the algorithm with datasets containing over 10000 transactions. This dataset contains 11 items : JAM, MAGGI, SUGAR, COFFEE, CHEESE, TEA, BOURNVITA, CORNFLAKES, BREAD, BISCUIT and MILK."
Reddit Comments on the Presidential Inauguration,What was the world saying while Donald Trump was being sworn in?,"AndrewMalinow, PhD",7,"Version 2,2017-01-30|Version 1,2017-01-21","politics
internet",CSV,8 MB,CC4,"2,599 views",89 downloads,,,https://www.kaggle.com/amalinow/reddit-comments-on-presidential-inauguration,"The data was scraped from www.reddit.com on 1/20/17 using the query string: https://www.reddit.com/search?q=inauguration
Attributes in order (left to right):
Title (string)
Post (string)
Post Date (datetime)
Metadata (#points, #comments, author)- needs additional parsing (string)
Comments (string)
Post Location (string)
latlon (post location)
latlon (comments location)
If you are interested in having help with the analytics email me: amalinow1973@gmail.com"
National Footprint Accounts data set (1961-2013),National Footprint measure the ecological resource use and capacity of nations,LiamLarsen,7,"Version 1,2017-04-23","ecology
geography
international relations",CSV,13 MB,Other,"1,890 views",138 downloads,7 kernels,2 topics,https://www.kaggle.com/kingburrito666/national-footprint-accounts,"Content
EF- GDP
Column descriptions
Country
Ecological Footprint where record = ""EFConsTotGHA"" and year = 2013 | total ecological footprint
Ecological Footprint where record = ""EFConsTotGHA"" and year = 2009 | total ecological footprint
2013 GDP (total; based on value of 2010 US Dollar)
2009 GDP (total; based on value of 2010 US Dollar)
Difference of EF2013 - EF2009
Difference GDP2013 - GDP2009
GDPDelta_P - EFDelta_P
Percent change in EF from 2009 to 2013: ((EF2013 - EF2009) / EF2009) * 100
Percent change in GDP from 2009 to 2013: ((GDP2013 - GDP2009)/ GDP2009) * 100
GDPDelta_P - EFDelta_P
Ordinal ranking of DDelta_P (range 0-153)
Ordinal ranking of EFDelta_P (range 0-153)
Ordinal ranking of GDPDelta_P (range 0-153)
""Decoupled Flag"" 1 if (GDPDelta >= 0) & (EFDelta <= 0) else 0
GDP normalized to min-max scale 25-2500 (necessary for scaling size of markers properly in scatterplot).
NFA 2017
Column descriptions
Country
Year
country code
Record-type
The Ecological Footprint of cropland demand.
The Ecological Footprint of grazing land demand.
The Ecological Footprint of forest land demand.
The Ecological Footprint of fishing ground demand.
The Ecological Footprint of built-up land demand.
The Ecological Footprint of carbon demand.
The total Ecological Footprint of demand (sum of all land types)
Data quality score
View my kernel to see a sample of data"
Minneapolis Air Quality Survey,Air quality survey results for the city of Minneapolis spanning 2013-2014,Greg,7,"Version 1,2017-10-10","cities
environment
pollution
chemistry",CSV,777 KB,CC0,805 views,73 downloads,,0 topics,https://www.kaggle.com/gregnetols/minneapolis-air-quality-survey,"Context
Minneapolis air quality survey results
Content
Contained in the file are Minneapolis air quality survey results obtained between November 2013 and August 2014. The data set was obtained from http://opendata.minneapolismn.gov.
Inspiration
Visualizing air pollutants quantities over the city of Minneapolis may provide evidence for the source of certain air pollutants."
Country Socioeconomic Status Scores: 1880-2010,Population-weighted measures of SES,sdorius,7,"Version 1,2017-04-18","demographics
economics",CSV,116 KB,Other,"2,651 views",301 downloads,10 kernels,,https://www.kaggle.com/sdorius/globses,"This dataset contains estimates of the socioeconomic status (SES) position of each of 149 countries covering the period 1880-2010. Measures of SES, which are in decades, allow for a 130 year time-series analysis of the changing position of countries in the global status hierarchy. SES scores are the average of each country’s income and education ranking and are reported as percentile rankings ranging from 1-99. As such, they can be interpreted similarly to other percentile rankings, such has high school standardized test scores. If country A has an SES score of 55, for example, it indicates that 55 percent of the world’s people live in a country with a lower average income and education ranking than country A. ISO alpha and numeric country codes are included to allow users to merge these data with other variables, such as those found in the World Bank’s World Development Indicators Database and the United Nations Common Database.
See here for a working example of how the data might be used to better understand how the world came to look the way it does, at least in terms of status position of countries.
VARIABLE DESCRIPTIONS: UNID: ISO numeric country code (used by the United Nations) WBID: ISO alpha country code (used by the World Bank) SES: Socioeconomic status score (percentile) based on GDP per capita and educational attainment (n=174) country: Short country name year: Survey year SES: Socioeconomic status score (1-99) for each of 174 countries gdppc: GDP per capita: Single time-series (imputed) yrseduc: Completed years of education in the adult (15+) population popshare: Total population shares
DATA SOURCES: The dataset was compiled by Shawn Dorius (sdorius@iastate.edu) from a large number of data sources, listed below. GDP per Capita: 1. Maddison, Angus. 2004. 'The World Economy: Historical Statistics'. Organization for Economic Co-operation and Development: Paris. Maddison population data in 000s; GDP & GDP per capita data in (1990 Geary-Khamis dollars, PPPs of currencies and average prices of commodities). Maddison data collected from: http://www.ggdc.net/MADDISON/Historical_Statistics/horizontal-file_02-2010.xls. 2. World Development Indicators Database Years of Education 1. Morrisson and Murtin.2009. 'The Century of Education'. Journal of Human Capital(3)1:1-42. Data downloaded from http://www.fabricemurtin.com/ 2. Cohen, Daniel & Marcelo Cohen. 2007. 'Growth and human capital: Good data, good results' Journal of economic growth 12(1):51-76. Data downloaded from http://soto.iae-csic.org/Data.htm 3. Barro, Robert and Jong-Wha Lee, 2013, ""A New Data Set of Educational Attainment in the World, 1950-2010."" Journal of Development Economics, vol 104, pp.184-198. Data downloaded from http://www.barrolee.com/ Total Population 1. Maddison, Angus. 2004. 'The World Economy: Historical Statistics'. Organization for Economic Co-operation and Development: Paris. 13.
2. United Nations Population Division. 2009."
Oklahoma Earthquakes and Saltwater Injection Wells,Earthquakes in Oklahoma region and Oil and Gas fluid byproduct data.,ChrisM!,7,"Version 1,2017-09-27",geology,CSV,4 MB,CC0,"1,299 views",111 downloads,,3 topics,https://www.kaggle.com/ksuchris2000/oklahoma-earthquakes-and-saltwater-injection-wells,"Context
Beginning in 2009, the frequency of earthquakes in the U.S. State of Oklahoma rapidly increased from an average of fewer than two 3.0+ magnitude earthquakes per year since 1978 to hundreds per year in 2014, 2015, and 2016. Thousands of earthquakes have occurred in Oklahoma and surrounding areas in southern Kansas and North Texas since 2009. Scientific studies attribute the rise in earthquakes to the disposal of wastewater produced during oil extraction that has been injected deeply into the ground. (Wikipedia)
Injection wells are utilized to dispose of fluid created as a byproduct of oil and gas production activities. Likewise, hydraulic fracturing, ie ""fracking"", produces large byproducts of water. This byproduct is then injected deep back into the earth via disposal/injection wells.
Content
This dataset contains two data files. One detailing ""active"" saltwater injection wells in Oklahoma, as of September 2017. The second file lists earthquakes in the Oklahoma region (Oklahoma and surrounding states) since 1977.
Acknowledgements
Data was gathered from Oklahoma Corporation Commission and The United States Geological Survey.
Inspiration
Is there a correlation between earthquakes and injection well activity?
Can the data be used as a predictor of general proximity and/or time of future earthquakes ?"
Tom Cruise's Love Interest Age Gap,"As Tom Cruise Gets Older, His On-Screen Love Interests Stay the Same Age",Meg Shields,7,"Version 2,2017-11-02|Version 1,2017-11-02","film
gender
demographics",CSV,673 KB,CC0,"1,214 views",81 downloads,3 kernels,,https://www.kaggle.com/meghshields/tom-cruises-love-interest-age-gap,"I wrote an article a while back about how as Tom Cruise gets older his love interests stay the same age. While Cruise is by no means exceptional in this respect, his age gap seemingly mirrors and confirms the larger critique of Hollywood’s bias against older actresses (as gestured towards by the brilliant work of the folks at Time and The Pudding).
I am not a numbers person by any stretch and have no experience handling data let alone analyzing and visualizing it. But I did make this hilariously crude google doc and that's got to count for something. If you're curious about methodology, it's specified at the tail end of the article."
Gas sensor array under dynamic gas mixtures,"Time series data for Ethylene and Methane in air, and Ethylene and CO in air",UCI Machine Learning,7,"Version 2,2017-11-02|Version 1,2017-10-12",time series,Other,2 GB,ODbL,"1,087 views",84 downloads,,0 topics,https://www.kaggle.com/uciml/gas-sensor-array-under-dynamic-gas-mixtures,"Context
This data set contains the acquired time series from 16 chemical sensors exposed to gas mixtures at varying concentration levels. In particular, we generated two gas mixtures: Ethylene and Methane in air, and Ethylene and CO in air. Each measurement was constructed by the continuous acquisition of the 16-sensor array signals for a duration of about 12 hours without interruption.
The data set was collected in a gas delivery platform facility at the ChemoSignals Laboratory in the BioCircuits Institute, University of California San Diego. The measurement system platform provides versatility for obtaining the desired concentrations of the chemical substances of interest with high accuracy and in a highly reproducible manner.
The sensor array included 16 chemical sensors (Figaro Inc., US) of 4 different types: TGS-2600, TGS-2602, TGS-2610, TGS-2620 (4 units of each type). The sensors were integrated with customized signal conditioning and control electronics. The operating voltage of the sensors, which controls the sensorsâ€™ operating temperature, was kept constant at 5 V for the whole duration of the experiments. The sensorsâ€™ conductivities were acquired continuously at a sampling frequency of 100 Hz. The sensor array was placed in a 60 ml measurement chamber, where the gas sample was injected at a constant flow of 300 ml/min.
Each measurement was constructed by the continuous acquisition of the 16-sensor array signals while concentration levels changed randomly. For each measurement (each gas mixture), the signals were acquired continuously for about 12 hours without interruption.
The concentration transitions were set at random times (in the interval 80-120s) and to random concentration levels. The data set was constructed such that all possible transitions are present: increasing, decreasing, or setting to zero the concentration of one volatile while the concentration of the other volatile is kept constant (either at a fixed or at zero concentration level). At the beginning, ending, and approximately every 10,000 s, we inserted additional predefined concentration patterns with pure gas mixtures.
The concentration ranges for Ethylene, Methane, and CO were selected such that the induced magnitudes of the sensor responses were similar. Moreover, for gas mixtures, lower concentration levels were favored. Therefore, the multivariate response of the sensors to the presented set of stimuli is challenging since none of the configurations (single gas or mixture presentation) can be easily identified from the magnitude of sensorsâ€™ responses. In particular Ethylene concentration ranges from 0-20 ppm; 0-600 ppm for CO; and 0-300 ppm for Methane.
The primary purpose of making this data set freely accessible on-line is to provide extensive and continuous time series acquired from chemical sensors to the sensor and artificial intelligence research communities to develop and test strategies to solve a wide variety of tasks. In particular, the data set may be useful to develop algorithms for continuous monitoring or improve response time of sensory systems. Also, the repetition of the same type of sensors in the array will allow further investigation on sensor variability (reproducibility of sensors of the same kind). Other interesting topics may include sensor failure (to what extent system predictions degrade when sensors start failing) or calibration transfer (whether the model for one sensor can be extended to other sensors).
More information on the generated data set can be found in Fonollosa et al. 'Reservoir Computing compensates slow response of chemosensor arrays exposed to fast varying gas concentrations in continuous monitoring'; Sensors and Actuators B, 2015.
The data set can be used exclusively for research purposes. Commercial purposes are fully excluded.
Content
The data is presented in two different files: Each file contains the data from one mixture. The file ethylene_CO.txt contains the recordings from the sensors when exposed to mixtures of Ethylene and CO in air. The file ethylene_methane.txt contains the acquired time series induced by the mixture of Methane and Ethylene in air.
The structure of the files is the same: Data is distributed in 19 columns. First column represents time (in seconds), second column represents Methane (or CO) concentration set point (in ppm), third column details Ethylene concentration set point (in ppm), and the following 16 columns show the recordings of the sensor array.
Files include a header (one line) with the information of each column:
Time (seconds), Methane conc (ppm), Ethylene conc (ppm), sensor readings (16 channels)
The order of the sensors in the files is as follows: TGS2602; TGS2602; TGS2600; TGS2600; TGS2610; TGS2610; TGS2620; TGS2620; TGS2602; TGS2602; TGS2600; TGS2600; TGS2610; TGS2610; TGS2620; TGS2620
Sensors' readings can be converted to KOhms by 40.000/S_i, where S_i is the value provided in the text files.
Acknowledgements
This dataset is republished as-is from the UCI ML Data Repository, available here. The attribution thereof is:
Fonollosa et al. 'Reservoir Computing compensates slow response of chemosensor arrays exposed to fast varying gas concentrations in continuous monitoring'; Sensors and Actuators B, 2015
Creators:
Jordi Fonollosa (fonollosa '@'ucsd.edu)
BioCircutis Institute
University of California San Diego
San Diego, California, USA
Donors of the Dataset:
Jordi Fonollosa (fonollosa '@'ucsd.edu)
Ramon Huerta (rhuerta '@' ucsd.edu)"
Harvard Tuition,Tuition data from Harvard's College and graduate/professional schools since 1985,Harvard University,7,"Version 1,2016-11-11",education,CSV,11 KB,CC0,"2,855 views",292 downloads,4 kernels,0 topics,https://www.kaggle.com/harvard-university/harvard-tuition,"Harvard tuition data since 1985, for both the undergraduate College and the graduate and professional schools.
The Data
This dataset consists of two files: tuition_graduate.csv and undergraduate_package.csv, which contain the tuition and fees data for the graduate schools and undergraduate College, respectively.
tuition_graduate.csv contains the following fields:
academic.year: the academic year, between 1985 and 2017
school: the name of the graduate or professional school; one of GSAS, Business (MBA), Design, Divinity, Education, Government, Law, Medical/Dental, Public Health (1-Year MPH)
cost: the cost of tuition at a given school in a given year
undergraduate_package.csv contains the following fields:
academic.year: the academic year, between 1985 and 2017
component: the component of undergraduate fees; one of Tuition,*Health Services Fee*,Student Services Fee,*Room*,Board,*Total*
cost: the cost of the component; or, if the component is Total, the sum of the costs of the other components in that year
Acknowledgements
All of the data in this dataset comes from The Harvard Open Data Dataverse. Specific citations are as follows:
for the graduate tuition data:
Harvard Financial Aid Office, 2015, ""Harvard graduate school tuition"", doi:10.7910/DVN/LV0YSQ, Harvard Dataverse, V1
for the undergraduate tuition and fees data:
Harvard Financial Aid, 2015, ""Harvard College Tuition"", doi:10.7910/DVN/MSS2BE, Harvard Dataverse, V1 [UNF:6:FyXNny+KBTgLX+DzewzEfg==]"
"Vacation Rental Properties in Palm Springs, CA","A list of over 2,000 vacation rental properties in Palm Springs, CA",Datafiniti,7,"Version 1,2017-01-19","databases
home",CSV,21 MB,CC4,"2,103 views",172 downloads,5 kernels,0 topics,https://www.kaggle.com/datafiniti/palm-springs-vacation-rentals,"About This Data
This is a list of over 2,000 vacation rental properties in Palm Springs, CA provided by Datafiniti's Property Database.
The dataset includes property name, # beds, # bathrooms, price, and more. Note that each property will have an entry for each price found for it and so a single property may have multiple entries.
What You Can Do With This Data
A similar dataset was used to determine the most and least expensive cities for short-term vacation rentals in the US. E.g.:
What are the least and most expensive one-bed rentals?
What are the least and most expensive two-bed rentals?
What are the least and most expensive three-bed rentals?
What is the median price for short-term rental properties?
What is the variation in rental prices?
Data Schema
A full schema for the data is available in our support documentation.
About Datafiniti
Datafiniti provides instant access to web data. We compile data from thousands of websites to create standardized databases of business, product, and property information. Learn more.
Want More?
You can get more data like this by joining Datafiniti or requesting a demo."
NFL Arrests,Public records of police arrests at NFL stadiums,The Washington Post,7,"Version 1,2016-11-28","american football
crime",CSV,59 KB,CC4,"2,418 views",331 downloads,3 kernels,0 topics,https://www.kaggle.com/washingtonpost/nfl-arrests,"Context
This dataset consists of public records requests made by the Washington Post to police departments that oversee security at each NFL stadium.
Content
Twenty-nine of the 31 jurisdictions provided at least partial data, though reporting methods differed from agency to agency; Cleveland and New Orleans did not submit data. Certain data were omitted if found to be incomplete or unreliable. Among those jurisdictions sending partial arrest figures for home games between 2011 and 2015 were Buffalo, Miami and Oakland. St. Louis provided only year-by-year arrest data, rather than game-by-game numbers. Detroit, Minneapolis and Atlanta did not provide data for arrests that took place in stadium parking lots.
The main dataset includes fields such as the day of the week, which teams were playing on which home field, and the score of the game, between 2011 and 2015.
Inspiration
Which stadiums had the most arrests? The least?
Are arrests more likely when the home team lost a game? Does the score correlate with number of arrests? (For example, if the game ended in a narrow loss for the home team, does this correlate with more arrests?)
Are there any stadiums with consistent arrest rates, regardless of how the game ended?
Acknowledgements
Data was collected and reported by Kent Babb and Steven Rich of the Washington Post, and the original dataset can be found here."
NIPS17 Adversarial learning - Final results,"Scores, runtime statistics and intermediate results of the FINAL round.",Google Brain,7,"Version 1,2017-11-06",,CSV,220 KB,Other,"2,419 views",231 downloads,2 kernels,,https://www.kaggle.com/google-brain/nips17-adversarial-learning-final-results,"This dataset contains results of the final round of NIPS 2017 Adversarial learning competition.
Content
Matrices with intermediate results
Following matrices with intermediate results are provided:
accuracy_matrix.csv - matrix with number of correctly classified images for each pair of attack (targeted and non-targeted) and defense
error_matrix.csv - matrix with number of misclassified images for each pair of attack (targeted and non-targeted) and defense
hit_target_class_matrix.csv - matrix with number of times image was classified as specific target class for each pair of attack (targeted and non-targeted) and defense
In each of these matrices, rows correspond to defenses, columns correspond to attack. Also first row and column are headers with Kaggle Team IDs (or baseline ID).
Scores and run time statistics of submissions
Following files contain scores and run time stats of the submissions:
non_targeted_attack_results.csv - scores and run time statistics of all non-targeted attacks
targeted_attack_results.csv - scores and run time statistics of all targeted attacks
defense_results.csv - scores and run time statistics of all defenses
Each row of these files correspond to one submission. Columns have following meaning:
KaggleTeamId - either Kaggle Team ID or ID of the baseline.
TeamName - human readable team name
Score - raw score of the submission
NormalizedScore - normalized (to be between 0 and 1) score of the submission
MinEvalTime - minimum evaluation time of 100 images
MaxEvalTime - maximum evaluation time of 100 images
MedianEvalTime - median evaluation time of 100 images
MeanEvalTime - average evaluation time of 100 images
Notes about the data
Due to team mergers, team name in these files might be different from the leaderboard.
Not all attacks were used to compute scores of defenses and not all defenses were used to compute scores of attacks. Thus if you simply sum-up values in rows/columns of the corresponding matrix you won't obtain exact score of the submission (however number you obtain will be very close to actual score)."
High-Content Screening with C.Elegans,"A small, fully annotated dataset for getting starting with HCS analysis",Kevin Mader,7,"Version 3,2017-04-19|Version 2,2017-04-19|Version 1,2017-04-19","healthcare
diseases
microtechnology",Other,137 MB,Other,"1,639 views",100 downloads,7 kernels,0 topics,https://www.kaggle.com/kmader/high-content-screening-celegans,"About
This selection of images are controls selected from a screen to find novel anti-infectives using the roundworm C.elegans . The animals were exposed to the pathogen Enterococcus faecalis and either untreated or treated with ampicillin, a known antibiotic against the pathogen. The untreated (negative control) worms display predominantly the ""dead"" phenotype: worms appear rod-like in shape and slightly uneven in texture. The treated (ampicillin, positive control) worms display predominantly the ""live"" phenotype: worms appear curved in shape and smooth in texture. For more information, please see Moy et al. (ACS Chem Biol, 2009) [http://dx.doi.org/10.1021/cb900084v]
Images
One image per channel (Channel 1 = brightfield; channel 2 = GFP) was acquired at MGH on a Discovery-1 automated microscope (Molecular Devices). Original image size is 696 x 520 pixels. Images are available in 16-bit TIF.
Ground Truth
The 384 images are from a plate of positive and negative controls. The images are named using this format: <plate>_<wellrow>_<wellcolumn>_<wavelength>_<fileid>.tif Columns 1-12 are positive controls treated with ampicillin. Columns 13-24 are untreated negative controls.
We also provide human-corrected binary images of foreground/background segmentation. To address the problem of correctly segmenting individual worms also when they overlap or cluster, we provide one binary foreground/background segmentation ground truth image for each worm:
Acknowledgements
The data have been reposted from the original data taken from the Broad Institute. Please acknowledge the original source if this is used in other works. The original data can be found and downloaded here: https://data.broadinstitute.org/bbbc/BBBC010/
These images were originally acquired for a screen in Fred Ausubel's lab at MGH. Please contact aconery AT molbio.mgh.harvard.edu for more information.
Original Publication: http://dx.doi.org/10.1038/nmeth.1984
Inspiration"
Linux Kernel Git Revision History,Anonymized git commit log with detailed file information,Philipp Schmidt,7,"Version 2,2017-03-12|Version 1,2017-03-12","computing and society
programming",CSV,199 MB,CC4,"1,899 views",89 downloads,5 kernels,0 topics,https://www.kaggle.com/philschmidt/linux-kernel-git-revision-history,"This dataset contains commits with detailed information about changed files from about 12 years of the linux kernel master branch. It contains about 600.000 (filtered) commits and this breaks down to about 1.4 million file change records.
Each row represents a changed file in a specific commit, with annotated deletions and additions to that file, as well as the filename and the subject of the commit. I also included anonymized information about the author of each changed file aswell as the time of commit and the timezone of the author.
The columns in detail:
author_timestamp: UNIX timestamp of when the commit happened
commit_hash: SHA-1 hash of the commit
commit_utc_offset_hours: Extraced UTC offset in hours from commit time
filename: The filename that was changed in the commit
n_additions: Number of added lines
n_deletions: Number of deleted lines
subject: Subject of commit
author_id: Anonymized author ID.
I'm sure with this dataset nice visualizations can be created, let's see what we can come up with!
For everybody interested how the dataset was created, I've setup a github repo that contains all the required steps to reproduce it here.
If you have any questions, feel free to contact me via PM or discussions here."
COMPAS Recidivism Racial Bias,Racial Bias in inmate COMPAS reoffense risk scores for Florida (ProPublica),Dan Ofer,7,"Version 1,2017-06-29","crime
law
demographics",CSV,23 MB,ODbL,"1,693 views",116 downloads,,,https://www.kaggle.com/danofer/compass,"Context
COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) is a popular commercial algorithm used by judges and parole officers for scoring criminal defendant’s likelihood of reoffending (recidivism). It has been shown that the algorithm is biased in favor of white defendants, and against black inmates, based on a 2 year follow up study (i.e who actually committed crimes or violent crimes after 2 years). The pattern of mistakes, as measured by precision/sensitivity is notable.
Quoting from ProPublica: ""
Black defendants were often predicted to be at a higher risk of recidivism than they actually were. Our analysis found that black defendants who did not recidivate over a two-year period were nearly twice as likely to be misclassified as higher risk compared to their white counterparts (45 percent vs. 23 percent). White defendants were often predicted to be less risky than they were. Our analysis found that white defendants who re-offended within the next two years were mistakenly labeled low risk almost twice as often as black re-offenders (48 percent vs. 28 percent). The analysis also showed that even when controlling for prior crimes, future recidivism, age, and gender, black defendants were 45 percent more likely to be assigned higher risk scores than white defendants.
Black defendants were also twice as likely as white defendants to be misclassified as being a higher risk of violent recidivism. And white violent recidivists were 63 percent more likely to have been misclassified as a low risk of violent recidivism, compared with black violent recidivists.
The violent recidivism analysis also showed that even when controlling for prior crimes, future recidivism, age, and gender, black defendants were 77 percent more likely to be assigned higher risk scores than white defendants. ""
Content
Data contains variables used by the COMPAS algorithm in scoring defendants, along with their outcomes within 2 years of the decision, for over 10,000 criminal defendants in Broward County, Florida. 3 subsets of the data are provided, including a subset of only violent recividism (as opposed to, e.g. being reincarcerated for non violent offenses such as vagrancy or Marijuana).
Indepth analysis by ProPublica can be found in their data methodology article.
Acknowledgements
Data & original analysis gathered by ProPublica. Original Data methodology article: https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm
Original Article: https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing
Original data from ProPublica: https://github.com/propublica/compas-analysis
Additional ""simple"" subset provided by FairML, based on the proPublica data:
http://blog.fastforwardlabs.com/2017/03/09/fairml-auditing-black-box-predictive-models.html
Inspiration
Ideas:
Feature importance when predicting the COMPASS score itself, or recividism/crime risks.
Reweighting data to compensate for bias, e.g. subsetting for the violent offenders, or adjusting better for base risk.
Feature selection based on ""legal usage""/fairness (E.g. exclude race and see how well your model works. It worked for me)."
OECD Productivity Data,"Productivity, labour costs and GDP per capita for 32 OECD countries",Jessica Yung,7,"Version 1,2017-05-04",economics,CSV,26 MB,Other,"1,784 views",280 downloads,4 kernels,0 topics,https://www.kaggle.com/jessicayung/oecd-productivity-data,"Context
The data was obtained from the OECD website's Productivity Statistics section on 4 May 2017.
Productivity = output per units of input.
To be expanded on.
Content
To be filled in.
Acknowledgements
The data in this dataset is the property of the Organisation for Economic Co-operation and Development (the “OECD”).
The OECD makes data (the “Data”) available for use and consultation by the public. As stated in Section I(a) above, Data may be subject to restrictions beyond the scope of these Terms and Conditions, either because specific terms apply to those Data or because third parties may have ownership interests. It is the User’s responsibility to verify either in the metadata or source information whether the Data is fully or partially owned by third parties and/or whether additional restrictions may apply, and to contact the owner of the Data before incorporating it in your work in order to secure the necessary permissions. The OECD in no way represents or warrants that it owns or controls all rights in all Data, and the OECD will not be liable to any User for any claims brought against the User by third parties in connection with the use of any Data.
Permitted use Except where additional restrictions apply as stated above, You can extract from, download, copy, adapt, print, distribute, share and embed Data for any purpose, even for commercial use. You must give appropriate credit to the OECD by using the citation associated with the relevant Data, or, if no specific citation is available, You must cite the source information using the following format: OECD (year), (dataset name),(data source) DOI or URL (accessed on (date)). When sharing or licensing work created using the Data, You agree to include the same acknowledgment requirement in any sub-licenses that You grant, along with the requirement that any further sub-licensees do the same.
Inspiration: Research Questions
Why is productivity growth slowing down in many advanced and emerging economies?"
Olympic Track & Field Results,"Results from all Olympic Track & Field Events, 1896 - 2016",Jay Ravaliya,7,"Version 1,2017-05-26",olympic games,CSV,779 KB,Other,"2,673 views",375 downloads,11 kernels,,https://www.kaggle.com/jayrav13/olympic-track-field-results,"Context
In an effort to database all results from the Olympic Track & Field Events, this dataset is scraped from https://olympic.org/athletics.
Content
All column headers are provided below.
Inspiration
As a former (and hopefully future) runner, I'm inspired by the idea that data can help us better understand the progression of athletes over time at one of the largest global stages for the events, the Olympics!
Code
Scraper: https://github.com/jayrav13/olympics-athletics"
Nashville Housing Data,Home value data for the booming Nashville market,tmthyjames,7,"Version 1,2017-01-31",home,CSV,11 MB,CC0,"4,353 views",528 downloads,14 kernels,0 topics,https://www.kaggle.com/tmthyjames/nashville-housing-data,"Context
This is home value data for the hot Nashville market.
Content
There are 56,000+ rows altogether. However, I'm missing home detail data for about half. So if anyone wants to track that down then go for it! I'll be looking in the mean time. Enjoy.
Will add the Python file that retrieved this data once I clean it up.
Shameless plug:
visit this link for my latest project, a SQL magic function for IPython Notebook."
Bank Loan Status Dataset,Future Loan Status prediction via classification models,Zaur Begiev,7,"Version 1,2017-04-09",,CSV,20 MB,Other,"3,936 views",494 downloads,,,https://www.kaggle.com/zaurbegiev/my-dataset,"Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
Micro-Loans,Micro-Loans dataset. Main goal to forecast whether a client will payback or not,bielrv,7,"Version 1,2017-07-04",,CSV,1 MB,Other,834 views,228 downloads,,,https://www.kaggle.com/bielrv/microloans,"Context
We do have a dataset with given loans and its arrears rate which allow a supervised machine learning.
Content
This is a dataset of given loans with its default rate"
Armenian Pub Survey,Data from online survey questionnaire about Armenian pubs,ErikHambardzumyan,7,"Version 2,2017-03-17|Version 1,2017-03-16",food and drink,CSV,32 KB,Other,"3,936 views",312 downloads,36 kernels,2 topics,https://www.kaggle.com/erikhambardzumyan/pubs,"The dataset collected from an online survey questionnaire includes behavioral, psychographic, geographic and demographic information about Armenian pubs.
The data has been intended for an independent project organized by the students of the American University of Armenia solely for educational purposes.
This data is unique as the pubs sector in Armenia has not been reasearched so far."
March Madness Forecasts - Men & Women's,2017 March Madness forecasts from FiveThirtyEight,tylerfuller,7,"Version 2,2017-03-15|Version 1,2017-03-13",basketball,CSV,19 KB,Other,"3,190 views",286 downloads,6 kernels,2 topics,https://www.kaggle.com/tylerfuller/march-madness-forecasts,"Use FiveThirtyEight's March Madness (men's basketball) forecasts to make the perfect bracket.
Round-by-Round probability for each team. Be sure to account for upsets!
Huge thanks to FiveThirtyEight for allowing public access to this data.
Who can make the perfect bracket?"
Opendata AIG Brazil,Aircraft Accidents in Brazil,Nosbielcs,7,"Version 5,2017-10-03|Version 4,2017-10-03|Version 3,2017-05-01|Version 2,2017-04-13|Version 1,2017-04-13","brazil
aviation",CSV,5 MB,ODbL,"1,880 views",122 downloads,7 kernels,,https://www.kaggle.com/nosbielcs/opendataaigbrazil,"Opendata AIG Brazil
Sobre o Projeto
Download dos Dados
OCORRÊNCIAS AERONÁUTICAS
AERONAVES ENVOLVIDAS
FATORES CONTRIBUINTES
RECOMENDAÇÕES DE SEGURANÇA
Notas Técnicas
Os textos dentro das colunas estão denotados por aspas duplas ("""").
As colunas das tabelas estão separadas por til (~).
As tabelas contém cabeçalhos que identificam suas colunas.
Em cada tabela existe uma coluna contendo a informação sobre a data de extração dos dados.
Outras Informações ""For Dummies""
Os relatórios finais podem ser consultados no site do CENIPA - Relatórios.
As recomendações de segurança podem ser consultadas no site do CENIPA - Recomendações.
Artigos científicos sobre o tema podem ser encontrados / publicados na Revista Conexão SIPAER.
Outros Recursos
Outras bases de dados para consultas:
NTSB
BEA
RISCO DA FAUNA
RAIO LASER
RISCO BALOEIRO
AERÓDROMOS BRASILEIROS
AEROVIAS BRASILEIRAS
Dicas para melhor aproveitamento dos recursos
Antes de fazer o download dos dados, leia com calma todo o texto desta página. Este recurso irá guiá-lo(a) para um adequado entendimento sobre os relacionamentos entre os conjuntos de dados disponíveis (ocorrencia, aeronave envolvida, fator_contribuinte e recomendações de segurança).
Para aprofundar-se no tema, visite o site do CENIPA e confira as LEGISLAÇÕES que norteiam a investigação e prevenção de acidentes aeronáuticos no Brasil.
Conheça o Manual de Investigação do SIPAER. Nos anexos deste documento você encontrará uma tabela de domínios (taxonomia) para algumas das variáveis disponíveis nos conjuntos de dados.
Devido ao dinamismo dos trabalhos de investigação e preocupação do CENIPA com a agilidade na disponibilização dos dados, os conjuntos de dados estarão sujeitos a modificações sempre que forem atualizados. Portanto, sempre que possível, utilize a ""data de extração"" dos conjuntos de dados para justificar/referenciar os seus estudos e análises.
Saiba como trabalhar com dados no formato CSV. Clique aqui para aprender
Dúvidas
Se persistirem dúvidas, por gentileza me enviem uma Issue (relatar problema). Clique aqui para relatar um Problema"
"Linux Gamers Survey, Q1 2016",Who are Linux gamers and what do they want?,Ekianjo,7,"Version 1,2017-02-02","video games
sociology
computing and society",CSV,856 KB,CC4,"1,839 views",142 downloads,2 kernels,0 topics,https://www.kaggle.com/sanqualis/linuxgamerssurvey,"Context
The following is the results of an online survey conducted by BoilingSteam.com among the Linux Gamers' Community (n=560, sharing only here answers where respondents explicitly agreed to have their answers made public, i.e. total n size was higher) in end of Q1 2016, to better understand their hardware, usage habits and reactions to several of Valve's Steam Initiatives. Most of the answers are coming from members of the r/Linux_Gaming and r/Linux subreddits, so you need to take in account that this may not be representative of your typical Linux user.
Content
There are many variables in this data set, with both numerical, free text and categorical answers. Every line corresponds to an individual response. Note that answers are anonymous. The first row is the coding you can use for your analysis (that should save a bit of time), the second row is the actual question asked (you can erase it), and the data starts from the third row.
Questions cover some of the following attributes (there are much more in the actual datasheet):
Demographics / Geography
Family Situation
OS used for Work and at Home
Linux Usage experience
Linux Gaming Experience
Type of Gamer (Hardcore or not)
Playing Exclusively on Linux or not
Time spent Playing per week
Budget spent on Linux Games per month
Games played recently
Games Bought recently
Hardware GPU for Gaming
Hardware GPU Model
General Hardware at Home using Linux
Usage of Resellers (Steam, GOG, HumbleBundle)
Satisfaction of different Resellers
Awareness of Steam Machines
Awareness of Steam Controller, Steam Link
Intent of Purchase of Steam Machines
Intent of Building Steam Machine DIY
SteamOS and opinion towards it
General feeling towards future of Linux
Stance about DRM
Stance about WINE
WINE usage and satisfaction
and much more...
Acknowledgements
The questionnaire was designed by Ekianjo at BoilingSteam.com. If you have suggestions for improvements of future surveys of the same kind, please reach us on Kaggle or on our contact page: http://boilingsteam.com/about-boiling-steam/
Past Research
You can see some analysis done a previous iteration of this survey (previous data can not be made public however) - this may serve as a good benchmark to measure changes: http://boilingsteam.com/the-three-kinds-of-linux-gamers/
Inspiration
Feel free to play with the data, and share what insights you may find. We are big proponents of making data free in general for transparency purposes, so if your analysis can help generate a better understanding of who are Linux Gamers, this would be a great outcome."
Residential Energy Consumption Survey,The Residential Energy Consumption Survey is a national energy survey,LiamLarsen,7,"Version 1,2017-03-29",energy,CSV,26 MB,Other,"3,902 views",444 downloads,2 kernels,,https://www.kaggle.com/kingburrito666/residential-energy-consumption-survey,"Context
This 2009 version represents the 13th iteration of the RECS program. First conducted in 1978, the Residential Energy Consumption Survey is a national sample survey that collects energy-related data for housing units occupied as a primary residence and the households that live in them. Data were collected from 12,083 households selected at random using a complex multistage, area-probability sample design. The sample represents 113.6 million U.S. households, the Census Bureau's statistical estimate for all occupied housing units in 2009 derived from their American Community Survey (ACS)
The csv data file is accompanied by a corresponding ""Layout file"", which contains descriptive labels and formats for each data variable. The ""Variable and response codebook"" file contains descriptive labels for variables, descriptions of the response codes, and indicators for the variables used in each end-use model."
My Chess Games,Played on the Lichess app,Gabriel Forsythe y Korzeniewicz,7,"Version 2,2017-04-06|Version 1,2017-03-16",board games,CSV,2 MB,CC4,"3,192 views",178 downloads,2 kernels,4 topics,https://www.kaggle.com/gabfyk/chess-games,"Context
Since downloading the Lichess app on July 10, 2016 (and as of March 16, 2017) I have played 2237 games of chess on my phone. Lichess allows one to play chess against other players or computers with a variety of time formats and rule variations. It is my main phone addiction and occasional cause of procrastination. Upon discovering Kaggle, I decided to explore the site by uploading the data of these games and playing around with them.
Content
Most of the data is contained in 20170316_lichess_games.csv, accompanied by two separate files for white and black featuring move times (in tenths of seconds) for each game. The main files includes data on:
Unique identifiers (including game id, url, date/time created, date/time of last move)
Game parameters (including rated or not, rule variant, clock settings)
Game results (including total playing time, total turns, last player to move, winning color, method of winning [checkmate/resigns/etc])
Game play content (including move list, opening names)
Player specific information (including player id, ELO rating [before game], ELO rating change [if rated], total playing time, and min/max move times)
Acknowledgements
All data was downloaded via the Lichess API.
Inspiration
Besides the personal interests of seeing what variables are most predictive of my winning or what areas in which I have the most opportunity to improve, it may be interesting to explore how to create a chess coach for individual players."
Exercise Pattern Prediction,What does your exercise pattern fall into?,Athni,7,"Version 1,2017-04-06",exercise,CSV,12 MB,Other,"4,984 views",376 downloads,10 kernels,0 topics,https://www.kaggle.com/athniv/exercisepatternpredict,"Context
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. Our goal here will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).
Content
The dataset contains about 160 predictors (most of which are not required) and classifiers column is 'classe' and the exercise pattern is classified into 5 types- A, B, C, D, E
Acknowledgements
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.
Read more: http://groupware.les.inf.puc-rio.br/har#wle_paper_section#ixzz4dPxKFugX
Inspiration
What better ways of cleaning up the data? Which model will fit it best and how to go about handling it in R"
Prosper Loan Data,prosper p2p lending platform,Joshua Schnessl,7,"Version 1,2017-03-28",,CSV,82 MB,Other,"4,662 views",407 downloads,2 kernels,0 topics,https://www.kaggle.com/jschnessl/prosperloans,"Context
This data comes from the Prosper p2p lending platform. I came across the data during the Udacity Data Analyst Nanodegree. All credit goes to Prosper and Udacity. A link to the data is here: https://s3.amazonaws.com/udacity-hosted-downloads/ud651/prosperLoanData.csv. A variable dictionary can be found here: https://docs.google.com/spreadsheets/d/1gDyi_L4UvIrLTEC6Wri5nbaMmkGmLQBk-Yx3z0XDEtI/edit#gid=0.
Content
The data contains a wide variety of information concerning loans on the Prosper p2p lending platform. More information can be found in the variable dictionary linked to above.
Acknowledgements
Once again, all credit for this data goes to Prosper and to Udacity. I would be happy to remove it if either party has any qualms with it being shared here.
Inspiration
There are a wide variety of questions to be investigated with this fantastic dataset."
2014 World Cup Forecasts and Scores,How well did FiveThirtyEight's algorithm predict match scores?,FiveThirtyEight,7,"Version 1,2017-01-26",association football,CSV,279 KB,CC4,"3,241 views",347 downloads,,0 topics,https://www.kaggle.com/fivethirtyeight/world-cup,"Content
FiveThirtyEight’s World Cup forecasting model used ESPN’s Soccer Power Index (SPI) — a system that combines game and player ratings to estimate a team’s overall skill level — to calculate odds of each country’s performance during the two stages of the World Cup. The probabilities, based on 10,000 simulations, were updated at the end of each match and aggregated into one file with the prediction date and time.
Acknowledgements
The 2014 prediction data was featured in the FiveThirtyEight article It's Brazil's World Cup to Lose and the interactive infographic 2014 World Cup Predictions. The World Cup match scores were scraped from the FIFA archive."
Presidential Approval Ratings,Public opinion survey methodology and results since 2008,The Huffington Post,7,"Version 2,2017-03-01|Version 1,2017-02-14",politics,CSV,979 KB,Other,"2,639 views",200 downloads,3 kernels,,https://www.kaggle.com/huffingtonpost/presidential-approval,"Context
HuffPost Pollster (originally Pollster.com) aims to report the results of every public poll that claims to provide a representative sample of the population or electorate. We have included polls of varying methodology, including automated or recorded voice telephone polls and online surveys using non-probability internet samples.
As of 2010, we require all polls from a new organization (or one new to us) to meet all of the minimal disclosure requirements of the National Council on Public Polling. We have always excluded polls that fail to disclose survey dates, sample size, and sponsorship; however, we may now choose in our editorial discretion to exclude polls or pollsters that do not provide sufficient methodological information for us or our readers to determine their quality.
Content
This dataset lists results from public opinion surveys regarding the president's job approval since 2008.
Acknowledgements
The presidential approval ratings were provided by the organization listed and aggregated by HuffPost Pollster.
Inspiration
How does public approval of the president change over time? How do current events and presidential responses impact approval ratings? Can you predict how Trump's approval rating will change over the course of his presidency?"
Academic Scores for NCAA Athletic Programs,Survey of team's program eligibility and retention by year and institution,NCAA,7,"Version 1,2017-02-16","sports
education",CSV,2 MB,Other,"3,874 views",514 downloads,4 kernels,3 topics,https://www.kaggle.com/ncaa/academic-scores,"Context
College presidents across the nation recognized a need to track how student-athletes are doing academically prior to graduation. Starting in 2003, colleges and universities in NCAA Division I — the largest and highest profile athletics programs — implemented a comprehensive academic reform package designed to improve the academic success and graduation of all student-athletes. The centerpiece of the academic reform package was the development of a real-time academic measurement for sports teams, known as the Academic Progress Rate (APR).
The APR includes student-athlete eligibility, retention and graduation as factors in a formula that yields a single number, providing a much clearer picture of the current academic culture on each Division I sports team in the country. Since its inception, the APR has become an important measure of student-athlete academic success. For high APR scores, the NCAA recognizes member institutions for ensuring that student-athletes succeed in the classroom. If, however, low APR scores are earned consistently, member institutions can be subjected to penalties including scholarship reductions and the loss of eligibility to compete in championships.
Content
This study was created, by the National Collegiate Athletic Association (NCAA), to provide public access to team-level APR scores, eligibility rates, retention rates, and athlete counts on Division I athletic programs starting with the 2003-2004 season through the 2013-2014 season
Inspiration
Which sport or school has the highest academic score? Which schools' scores have increased or decreased significantly in the past decade? Are men's or women's team academic performance better? What about public and private colleges?"
The Metropolitan Museum of Art Open Access,"Explore information on more than 420,000 historic artworks",The Metropolitan Museum of Art,7,"Version 1,2017-04-07","culture and humanities
museums",CSV,216 MB,CC0,"2,139 views",187 downloads,4 kernels,0 topics,https://www.kaggle.com/metmuseum/the-metropolitan-museum-of-art-open-access,"Dataset source: https://github.com/metmuseum/openaccess
The Metropolitan Museum of Art presents over 5,000 years of art from around the world for everyone to experience and enjoy. The Museum lives in three iconic sites in New York City—The Met Fifth Avenue, The Met Breuer, and The Met Cloisters. Millions of people also take part in The Met experience online.
Since it was founded in 1870, The Met has always aspired to be more than a treasury of rare and beautiful objects. Every day, art comes alive in the Museum's galleries and through its exhibitions and events, revealing both new ideas and unexpected connections across time and across cultures.
The Metropolitan Museum of Art provides select datasets of information on more than 420,000 artworks in its Collection for unrestricted commercial and noncommercial use. To the extent possible under law, The Metropolitan Museum of Art has waived all copyright and related or neighboring rights to this dataset using Creative Commons Zero. This work is published from: The United States Of America. You can also find the text of the CC Zero deed in the file LICENSE in this repository. These select datasets are now available for use in any media without permission or fee; they also include identifying data for artworks under copyright. The datasets support the search, use, and interaction with the Museum’s collection.
At this time, the datasets are available in CSV format, encoded in UTF-8. While UTF-8 is the standard for multilingual character encodings, it is not correctly interpreted by Excel on a Mac. Users of Excel on a Mac can convert the UTF-8 to UTF-16 so the file can be imported correctly.
Additional usage guidelines
Images not included
Images are not included and are not part of the dataset. Companion artworks listed in the dataset covered by the policy are identified in the Collection section of the Museum’s website with the Creative Commons Zero (CC0) icon.
For more details on how to use images of artworks in The Metropolitan Museum of Art’s collection, please visit our Open Access page.
Documentation in progress
This data is provided “as is” and you use this data at your own risk. The Metropolitan Museum of Art makes no representations or warranties of any kind. Documentation of the Museum’s collection is an ongoing process and parts of the datasets are incomplete.
We plan to update the datasets with new and revised information on a regular basis. You are advised to regularly update your copy of the datasets to ensure you are using the best available information.
Pull requests
Because these datasets are generated from our internal database, we do not accept pull requests. If you have identified errors or have extra information to share, please email us at openaccess@metmuseum.org and we will forward to the appropriate department for review.
Attribution
Please consider attributing or citing The Metropolitan Museum of Art's CC0 select datasets, especially with respect to research or publication. Attribution supports efforts to release other datasets in the future. It also reduces the amount of ""orphaned data,"" helping to retain source links.
Do not misrepresent the dataset
Do not mislead others or misrepresent the datasets or their source. You must not use The Metropolitan Museum of Art’s trademarks or otherwise claim or imply that the Museum or any other third party endorses you or your use of the dataset.
Whenever you transform, translate or otherwise modify the dataset, you must make it clear that the resulting information has been modified. If you enrich or otherwise modify the dataset, consider publishing the derived dataset without reuse restrictions.
The writers of these guidelines thank the The Museum of Modern Art, Tate, Cooper-Hewitt, and Europeana."
Alpha-Numeric Handwritten Dataset,Mini version of HASYv2 Dataset,Sumit Kothari,7,"Version 1,2017-08-15",,Other,5 MB,CC0,"1,634 views",175 downloads,,0 topics,https://www.kaggle.com/usersumit/alphanumeric-handwritten-dataset,"Context
While searching for Alpha-Numeric handwritten dataset I came across HASYv2 dataset but it contains lot of other classes too. So I filtered the dataset which now contains only Alpha-Numeric handwritten data.
Content
The dataset contains 2 numpy files and 1 csv file:
alphanum-hasy-data-X.npy: Contains images data-set with size (4658, 32, 32)
alphanum-hasy-data-y.npy : Contains corresponding labels data-set with size (4658,)
symbols.csv : Contains mapping between symbol_id and its symbol (i.e digit/char)
More information at AlphaNum-HASYv2 Github repository.
Acknowledgements
HASYv2: https://arxiv.org/abs/1701.08380"
New Orlean's Slave Sales,"A dataset of 15,377 slave sales from 1856 - 1861",Chris Crawford,7,"Version 1,2017-07-14","slaves
history",Other,4 MB,Other,"1,818 views",238 downloads,,,https://www.kaggle.com/crawford/new-orleans-slave-sales,"Context
Abraham Lincoln's election produced Southern secession, war, and abolition. This dataset was used to study connections between news and slave prices for the period 1856-1861. By August 1861, slave prices had declined by roughly one-third from their 1860 peak. That decline was similar for all age and sex cohorts and thus did not reflect expected emancipation without compensation. The decision to secede reflected beliefs that the North would not invade and that emancipation without compensation was unlikely. Both were encouraged by Lincoln's conciliatory tone before the attack on Fort Sumter, and subsequently dashed by Lincoln's willingness to wage all-out war.
Calomiris, Charles W., and Jonathan Pritchett. 2016. ""Betting on Secession: Quantifying Political Events Surrounding Slavery and the Civil War."" American Economic Review, 106(1): 1-23.
Content
Data description: by Jonathan Pritchett These data were collected from the office of the Orleans parish Civil Clerk of Court. The sample includes all slave sales recorded by the register of conveyance from October 1856 to August 1861. The construction of the dataset is similar to that employed previously by Fogel and Engerman (1976). The unit of observation is the individual with the exception of children who were bundled with their mothers. Fields are defined as follows:
ID number: Unique observation number.
Conveyance: Number of Conveyance volume.
Page: Page number of transaction.
Researcher: Initials of research assistant who transcribed transaction.
Notary First Name: First name of public notary who recorded transaction.
Notary Last Name: Last name of public notary.
Sales Date: Sales date of transaction (MM/DD/YYYY format). This is NOT the date the sale was recorded in conveyance office.
Sellers First Name: Seller’s First Name.
Sellers Last Name: Seller’s Last Name.
Sellers County of Origin: Seller’s county (or city) of origin.
Sellers State of Origin: Seller’s state of origin.
Representing Seller: Name of agent representing seller if seller is not present at time of sale (normally blank).
Relationship to Seller: Agent’s relationship to seller.
Buyers First Name: Buyer’s First Name.
Buyers Last Name: Buyer’s Last Name.
Buyers County of Origin: Buyer’s County (or city) of Origin.
Buyers State of Origin: Buyer’s State of Origin.
Representing Buyer: Name of agent representing buyer if buyer is not present at time of sale (normally blank).
Relationship to Buyer: Agent’s relationship to buyer.
Slave Name: Slave’s first name; rarely last name also listed.
Sex: Male (M) or female (F). Gender often inferred from sale record, such as Negro vs. Negress, mulatto vs. mulattress, etc.
Age: Age in years.
Color: Description of slave’s skin color, but may also indicate ancestry (such as mulatto).
Occupation: Slave’s occupation – often blank.
Family Relationship: Describes family relationship of slaves (if any) sold in groups
Name Child 1: Name of child 1 (sold with mother). Blank when slaves are listed separately and/or with separate prices.
Sex Child 1: Gender of child 1 (sold with mother).
Age Child 1: Age in years of child 1 (sold with mother). Decimal equivalents for age in months.
Name Child 2: Name of child 2 (sold with mother). Blank when slaves are listed separately and/or with separate prices.
Sex Child 2: Gender of child 2 (sold with mother).
Age Child 2: Age in years of child 2 (sold with mother). Decimal equivalents for age in months.
Name Child 3: Name of child 3 (sold with mother). Blank when slaves are listed separately and/or with separate prices.
Sex Child 3: Gender of child 3 (sold with mother).
Age Child 3: Age in years of child 3 (sold with mother). Decimal equivalents for age in months.
Name Child 4: Name of child 4 (sold with mother). Blank when slaves are listed separately and/or with separate prices.
Sex Child 4: Gender of child 4 (sold with mother).
Age Child 4: Age in years of child 4 (sold with mother). Decimal equivalents for age in months.
Name Child 5: Name of child 5 (sold with mother). Blank when slaves are listed separately and/or with separate prices
Sex Child 5: Gender of child 5 (sold with mother).
Age Child 5: Age in years of child 5 (sold with mother). Decimal equivalents for age in months.
Name Child 6: Name of child 6 (sold with mother). Blank when slaves are listed separately and/or with separate prices
Sex Child 6: Gender of child 6 (sold with mother).
Age Child 6: Age in years of child 6 (sold with mother). Decimal equivalents for age in months.
Name Child 7: Name of child 7 (sold with mother). Blank when slaves are listed separately and/or with separate prices.
Sex Child 7: Gender of child 7 (sold with mother).
Age Child 7: Age in years of child 7 (sold with mother). Decimal equivalents for age in months.
Name Child 8: Name of child 8 (sold with mother). Blank when slaves are listed separately and/or with separate prices.
Sex Child 8: Gender of child 8 (sold with mother).
Age Child 8: Age in years of child 8 (sold with mother). Decimal equivalents for age in months.
Guaranteed: Yes or No. Most conveyance records omit this information (missing value).
Notes on Guarantee: Description of guarantee/flaw; reason why slave doesn’t have full guarantee.
Number of Total Slaves: Total number of slaves listed in transaction.
Number of Adult Slaves: Total number of “principal” slaves – corresponds to the number of separate entries for each transaction. Recall that children (especially those under 10 years) were bundled with mothers.
Number of Child Slaves: Total number of children listed in transaction.
Number of Prices: Total number of prices listed in transaction.
Price: Price associated with slaves. For group sales with a single price, only one price is listed for first slave, and for other slaves, entry is blank.
Payment Method: Cash or Credit. Sometimes “Cash and Credit” for credit sales with down payment. Also slave exchange, typical result of redhibition claim.
Payment flag: Description of payment schedule for credit sales.
DUMMY credit: 0 or 1 indicator for credit sales.
Down Payment: Number value of cash down payment for credit sales.
mthcred: Maximum length of credit (in months).
Interest Rate: Annual interest rate charged on credit sales.
Discount Rate: Calculated monthly discount rate.
predicted rate: Predicted monthly interest rate for credit sales without explicit interest rates.
Calculations: Intermediate calculation – please ignore.
Ratio: Intermediate calculation – please ignore.
PresentValue: Present value calculation for credit sales; blank for cash sales.
DUMMY omission: 0 or 1 indicator for omitting observation.
Reason for Omission: Reason for excluding observation from working sample.
Comments: Description of unusual characteristics for observation.
DUMMY Estate Sale: 0 or 1 indicator for estate sale.
Acknowledgements
Calomiris, Charles W., and Jonathan Pritchett. 2016. ""Betting on Secession: Quantifying Political Events Surrounding Slavery and the Civil War."" American Economic Review, 106(1): 1-23. DOI: 10.1257/aer.20131483
This dataset was converted from XLSX to CSV
Inspiration
As a principal port, New Orleans played a major role during the antebellum era in the Atlantic slave trade. The authors of ""Betting on Secession: Quantifying Political Events Surrounding Slavery and the Civil War."" did a fantastic job of putting this dataset together to learn more about the country's connections between slave trade and the American Civil War."
Jester Online Joke Recommender,"Over 11 million ratings of 150 jokes from 79,681 users",Chris Crawford,7,"Version 1,2017-07-15",humor,Other,29 MB,Other,"1,190 views",128 downloads,,0 topics,https://www.kaggle.com/crawford/jester-online-joke-recommender,"Context
Jester is a joke recommender system developed at UC Berkeley to study social information filtering. Users of the system are presented a joke and then they rate them. This dataset is a collection of those ratings.
http://eigentaste.berkeley.edu/
Eigentaste: A Constant Time Collaborative Filtering Algorithm. Ken Goldberg, Theresa Roeder, Dhruv Gupta, and Chris Perkins. Information Retrieval, 4(2), 133-151. July 2001.
Content
Notes from the source:
Each row is a user (Row 1 = User #1)
Each column is a joke (Column 1 = Joke #1)
Ratings are given as real values from -10.00 to +10.00
99 corresponds to a null rating
As of May 2009, the jokes 7, 8, 13, 15, 16, 17, 18, 19 are the ""gauge set"" (as discussed in the Eigentaste paper)
Acknowledgements
Thanks go to Dr. Ken Golberg's group for putting this super cool data together and for permission to share it with the Kaggle community!
Eigentaste: A Constant Time Collaborative Filtering Algorithm. Ken Goldberg, Theresa Roeder, Dhruv Gupta, and Chris Perkins. Information Retrieval, 4(2), 133-151. July 2001.
The original data file was converted to a CSV format before uploading.
The original list of jokes was converted from DAT to TSV.
Original files can be found here: http://eigentaste.berkeley.edu/dataset/
Inspiration
Take a look at the Eigentaste paper to learn more about how the data was used. See if you can recreate the study or glean some new insight!"
Fine-grained Context-sensitive Lexical Inference,Annotations for semantic relations between words within context sentences,Vered Shwartz,7,"Version 1,2017-08-12",,CSV,3 MB,GPL,571 views,31 downloads,,0 topics,https://www.kaggle.com/vered1986/context-lexinf,"Context
Recognizing lexical inference is an essential component in natural language understanding. In question answering, for instance, identifying that broadcast and air are synonymous enables answering the question ""When was 'Friends' first aired?"" given the text ""'Friends' was first broadcast in 1994"". Semantic relations such as synonymy (tall, high) and hypernymy (cat, pet) are used to infer the meaning of one term from another, in order to overcome lexical variability. This inference should typically be performed within a given context, considering both the term meanings in context and the specific semantic relation that holds between the terms.
Content
This dataset provides annotations for fine-grained lexical inferences in-context. The dataset consists of 3,750 term pairs, each given within a context sentence, built upon a subset of terms from PPDB. Each term pair is annotated to the semantic relation that holds between the terms in the given contexts.
Files:
full_dataset.csv - the full dataset is provided, as well as the train-test-validation split.
train.csv, test.csv, validation.csv - A split of the dataset to 70% train, 25% test, and 5% validation sets. Each of the sets contains different term-pairs, to avoid overfitting for the most common relation of a term-pair in the training set.
File Structure: comma-separated file
Fields:
x: the first term
y: the second term
context_x: the sentence in which x appears (highlighted by x)
context_y: the sentence in which y appears (highlighted by y)
semantic_relation: the (directional) semantic relation that holds between x and y: equivalence, forward_entailment, reverse_entailment, alternation, other-related and independence.
confidence: the relation annotation confidence (percentage of annotators that selected this relation), in a scale of 0-1
Acknowledgements
If you use this dataset, please cite the following paper:
Adding Context to Semantic Data-Driven Paraphrasing.
Vered Shwartz and Ido Dagan. *SEM 2016.
Inspiration
I hope that this dataset will motivate the development of context-sensitive lexical inference methods, which have been relatively overlooked, although they are crucial for applications."
Student Survey,University level Student Survey for Academic quality enhancement,Razib Mustafiz,7,"Version 1,2017-07-27","schools and traditions
education",CSV,169 KB,CC4,"2,988 views",377 downloads,,,https://www.kaggle.com/razibmustafiz/student-survey,"This Data set was collected by a survey conducted by Google forms for a Bangladeshi University in order to examine their current academic situation and also to improve on them. This Survey was part of the Institutional Quality Assurance Program, initiated by University Grant Commission, Bangladesh and funded by World Bank.
To meet the globalization challenges raising higher education quality to the world standard is essential. Bangladesh Govt. has taken initiatives to develop the quality of tertiary education. Govt. plans to prepare university graduates in such way that they can successfully compete in the context of international knowledge society.
Accordingly, the Ministry of Education, with the assistance of the World Bank, has undertaken a Higher Education Quality Enhancement Project (HEQEP). The project aims at improving the quality of teaching-learning and research capabilities of the tertiary education institutions through encouraging both innovation and accountability and by enhancing the technical and institutional capacity of the higher education sector.
The University Grants Commission of Bangladesh is the implementing agency of the project. A HEQEP Unit has been established in UGC for implementation, management, monitoring and evaluation of the activities.
The Data set contains 500 rows and they are Timestamped showing the exact time of data collection process. The survey was conducted on Undergraduate and Postgraduate level students of a Bangladeshi Private University. Among the various columns of data, the GPA columns contains important linear data with strong correlation. These data can be used to predict other GPA columns. So that we can predict a student's GPA in advance and can take necessary steps to improve his or her score.
Some acronyms that might help understand the Data set: S.S.C- Secondary School Certificate ( 10th Class public exam) H.S.C- Higher Secondary School Certificate ( 12th Class public exam) Area of Evaluation - There are 1 to 5 points. Where 5 being the best and 1 being the worst.
My heartfelt Acknowledgement goes to the Students who helped me sharing their data and time to make this survey a success. Without their help this could not be possible for me."
Fracking Well Chemical Disclosure Datasets,Datasets of fracking well chemical disclosures and toxicities,Fracking Analysis,7,"Version 1,2017-06-01","energy
mining",CSV,547 MB,CC4,"1,374 views",142 downloads,6 kernels,,https://www.kaggle.com/frackinganalysis/fracking-well-chemical-disclosure-datasets,"Context
These datasets are extractions provided by FrackingData.org of the SQL Server 2012 backup file obtained on a monthly basis from FracFocus.org's ""FracFocus Data Download"" web page. As FracFocus.org's SQL Server 2012 backup file is inconvenient to ingest for most citizen-scientists or data analysts, FrackingData.org ingests the database and outputs both CSV and SQLite files more readily suitable for analysis. The files in question, available herein, are also available at FracFocus.org's ""FracFocus Data"" web page.
Content
Fracking well chemical disclosures, the ""Registry"" files, hierarchy as follows:
RegistryUpload, this is the header file.
RegistryPurpose, this is an intermediate file between the header and the ingredients.
RegistryIngredients, this is the detail file of the chemical ingredients used in each well.
Chemical health effects and toxicities by Chemical Abstract Society Registry Number (CASRN).
Acknowledgements
FrackingData.org for the ingestion and conversion of the FracFocus Registry database.
FracFocus.org for the collection of the fracking well chemical disclosures.
Scorecard Chemical Health Effects for the compilation of the various chemicals' health effects and toxicities.
Inspiration
Start by associating each fracking well chemical disclosure to its chemical health effects using the CASNumber or CASRN, as appropriate."
ACB 1994-2016 Spanish Basketball League Results,Results and stats for the Spanish Basketball League from 1994 to 2016,Antonio Javier González Ferrer,7,"Version 3,2017-07-01|Version 2,2017-06-29|Version 1,2017-06-17",basketball,SQLite,24 MB,Other,"1,177 views",77 downloads,,,https://www.kaggle.com/jgonzalezferrer/acb-spanish-basketball-league-results,"acb-database
The aim of this project is to collect information about all the games of the ACB Spanish Basketball League from 1994 to 2016. A SQLite database has been used for this purpose. The code can be found in https://github.com/jgonzalezferrer/acb-database-scraping.
Why from 1994? In 1994, the ACB changed to the current modern league format, which consists of a regular season and a playoff competition between the best 8.
Content
This dataset includes statistics about the games, teams, players and coaches. It is divided in the following tables:
Game: basic information about the game such as the venue, the attendance, the kickoff, the involved teams and the final score.
Participant: a participant is a player, coach or referee that participates in a game. A participant is associated to a game, an actor and a team. Each row contains information about different stats such as number of points, assists or rebounds.
Actor: an actor represents a player or a coach. It contains personal information about them, such as the height, position or birthday. With this table we can track the different teams that a player has been into.
Team: this class represents a team.
TeamName: the name of a team can change between seasons (and even within the same season).
In summation, this database contains the stats from games such as http://www.acb.com/fichas/LACB61295.php"
Ground Parrot Vocalisation Dataset,University of Wollongong ground parrot vocalisation dataset,DucThanhNguyen,7,"Version 8,2017-07-07|Version 7,2017-07-07|Version 6,2017-07-07|Version 5,2017-07-07|Version 4,2017-07-07|Version 3,2017-07-07|Version 2,2017-07-07|Version 1,2017-07-06","australia
animals
acoustics",Other,2 MB,CC0,844 views,42 downloads,2 kernels,,https://www.kaggle.com/ducthanhnguyen/ground-parrot-vocalisation-dataset,"Context
The ability to monitor species in their natural habitat is useful when determining how the species respond to changes in environment. This may be particularly important when the species being studied are endangered and a population decline will trigger management action. The Eastern Ground Parrot (Pezoporous wallicus wallicus) is found only in Australia, where it is officially listed as a rare and threatened species. Furthermore, it is difficult to directly observe, as it lives in dense vegetation and rarely flies. As such, the chance of successful monitoring depends on locating the vocalisation or calls. One of the techniques that has been commonly used to monitor Ground Parrots (GPs) in their natural habitat is for experienced ecologists to listen to the calls and manually locate and plot the perceived distribution of the birds. More recently, surveys have also used techniques based on audio signal processing and pattern recognition, where records of acoustic signals are analysed and specific vocalisations are detected. Such detection can be used for the purpose of recognition and estimation of the number of GPs in the habitat.
To facilitate research on bird call detection using pattern recognition techniques, we provide here a dataset of GP vocalisation. The dataset contains 4 audio sequences of different lengths sampled at 16kHz using Song Meter SM2 devices. The dataset was recorded at dusk and dawn times on different days and at four different locations in the Barren Grounds Nature Reserve, a protected nature park located in the Southern Highlands region of New South Wales, Australia. The sequences include overlapping GP calls and sounds from sources other than GP (e.g. wind, spurious noise, and vocalisations of other species). All the GP calls in the dataset are manually annotated and used as the ground truth for evaluation of detection algorithms.
Content
This is the dataset used for Ground Parrot Call detection project. The folder Data includes 1) 4 audio sequences formatted in wav files and named Sq1, Sq2, Sq3, and Sq4. 2) Training. This folder contains the training data and includes 2 sub-folders: GroundParrot (ground parrot calls) and Others (sounds other than ground parrot calls). Each folder contains 25 wav files and each file is 1 second length. 3) GroundTruth. This folder includes 4 files. Each file contains the ground-truth of the ground parrot calls of one of the 4 sequences, e.g. Sq1_gt.txt is the ground-truth of the sequence Sq1.wav. The ground-truth files are written in the following format, - The first line contains the number of ground parrot calls. - Each following line describes a call with the start and end time stamp. The time stamps are represented in hours:minutes:seconds
Acknowledgements
This work was supported by funding from the Office of Environment and Heritage, NSW Australia."
Eurovision YouTube Comments,YouTube comments on entries from the 2003-2008 Eurovision Song Contests,Rachael Tatman,7,"Version 1,2017-07-20","languages
music
linguistics
internet",CSV,365 KB,CC4,"1,559 views",98 downloads,,,https://www.kaggle.com/rtatman/eurovision-youtube-comments,"Context:
The Eurovision Song Contest, which originated in 1956, is present on YouTube through uploads of songs performed in the Contest. Any user can freely comment on these songs. This dataset is made of up a collection of comments made on four YouTube videos of Eurovision entries by Belgium. The comments are in a number of languages.
Content:
The YouTube online forums associated with the Eurovision Song Contest have a large number of users from varied linguistic backgrounds who, because of their interests in song performance, are particularly attentive to language-related issues, such as the accent of the performers and the choice of language of the songs. Commentaries are made by forum participants from disparate locations on a variety of topics, one of the most prominent being language, including language features and perceptions of language use.
Acknowledgements:
This dataset was collected by Dejan Ivković for the purpose of linguistic research. If you made use of this data, please cite the following article:
Ivković, D. (2013). The Eurovision Song Contest on YouTube: A corpus-based analysis of language attitudes. Language@Internet, 10, article 1. (urn:nbn:de:0009-7-35977)
Inspiration:
This dataset contains multiple languages. Can you identify and the language of each comment?
Can you automatically find positive and negative comments about different country’s songs?
Are some commenters more positive or more negative than others?"
Corpus of bilingual children's speech,Transcribed natural speech from 25 bilingual children,Rachael Tatman,7,"Version 1,2017-07-22","languages
children
linguistics",Other,934 KB,CC4,"1,904 views",474 downloads,3 kernels,0 topics,https://www.kaggle.com/rtatman/corpus-of-bilingual-childrens-speech,"Project Description
The Paradis corpus consists of naturalistic language samples from 25 children learning English as a second language (English language learners or learners of English as an additional language). Transcription is in English orthography only; phonetic transcription was not included in this research. Any real names of people or places in the transcripts have been replaced with pseudonyms. The participants are identified with four letter codes.
Content
The data in this corpus was collected in 2002 in Edmonton, Canada. Children were video-­‐taped in conversation with a student research assistant in their homes for approximately 45 minutes. During this time, the research assistant had a list of “interview” questions to ask. If the child introduced his or her own topics and the conversation moved forward, the questions were not asked. This dataset only includes data from the first stage of data collection, in 2002. The full longituinal corpus may be found on the CHILDES website, here: http://childes.talkbank.org/access/Biling/Paradis.html
These data are in .cha files, which are intended for use with the program CLAN (http://alpha.talkbank.org/clan/). However, you may also treat these files as raw text files, with one speech snippet per line. Lines starting with @ are metadata.
File format information:
*EXP: Experimenter speaking
*CHI: Child speaking
%[some text]: These lines contain non-linguistic information
Biographical data
Participants in this study were children from newcomer (immigrant and refugee) families to Canada. The children started to learn English as a second language (L2) after their first language (L1) had been established, at 4 years 11 months on average. In the table below, “AOA” refers to the “age of arrival” of the child when the family immigrated. The number “1” indicates children who were Canadian born. The column “AOE” refers to the age of onset of English acquisition. All ages are in months. Each child’ s L1 and gender is also listed in the table below.
For more information about the participants and procedures in this research, see the following:
Paradis, J. (2005). Grammatical morphology in children learning English as a second language: Implications of similarities with Specific Language Impairment. Language, Speech and Hearing Services in the Schools, 36, 172-187. Golberg, H., Paradis, J. & Crago, M. (2008). Lexical acquisition over time in minority L1 children learning English as a L2. Applied Psycholinguistics, 29, 1-25.
Inspiration:
Does children’s first language affect what English words they use? How many words?
Do some children pause (marked as (.) or (..)) more often than others?
Do children at different ages interrupt/overlap their speech more often? (Marked by <> around text.)
Does a children’s age of first exposure to English affect how often then say “um”? (Transcribed as“&-um”.)
Related datasets:
When do children learn words?
Diagnosing specing language impairment in children"
Independence days,Independence days around the world,Rachael Tatman,7,"Version 2,2017-07-25|Version 1,2017-07-17","traditions
anthropology
cultural studies",CSV,22 KB,CC4,"1,171 views",96 downloads,4 kernels,,https://www.kaggle.com/rtatman/independence-days,"Context:
An Independence Day is an annual event commemorating the anniversary of a nation's independence or statehood, usually after ceasing to be a group or part of another nation or state; more rarely after the end of a military occupation. Most countries observe their respective independence days as national holidays.
Content:
This dataset is a collection of information about 184 independence celebrations in countries around the world.
Acknowledgements:
This dataset was taken from the Wikipedia article “List of national independence days” on July 17, 2017.
Inspiration:
Are there seasonal clusters of independence day celebrations?
How popular is it for an independence celebration’s name to include the name of the country?
Can you create an interactive visualization that shows when/where independence days are celebrated?"
Discourse Acts on Reddit,"Annotated discourse acts fom 10,000 posts and replies",Rachael Tatman,7,"Version 1,2017-07-25","languages
linguistics",{}JSON,52 MB,Other,934 views,59 downloads,,0 topics,https://www.kaggle.com/rtatman/discourse-acts-on-reddit,"Context/background
Discourse acts are the different types of things you can do in a conversation, like agreeing, disagreeing or elaborating. This dataset contains annotations of the discourse acts of different Twitter comments. The discourse acts labeled here are “coarse” in the sense that they’re labelled broadly (for the whole Reddit comment) rather than for individual sentences or phrases, not in the sense of being vulgar. The discourse act of each post has been annotated by multiple annotators.
Content
A large corpus of discourse annotations and relations on ~10K forum threads. Please refer to the following paper for an in depth analysis and explanation of the data: Characterizing Online Discussion Using Coarse Discourse Sequences (ICWSM '17).
Explanation of fields
Thread fields
URL - reddit URL of the thread
title - title of the thread, as written by the first poster
is_self_post - True if the first post in the thread is a self-post (text addressed to the reddit community as opposed to an external link)
subreddit - the subreddit of the thread
posts - a list of all posts in the thread
Post fields
id - post ID, reddit ID of the current post
in_reply_to - parent ID, reddit ID of the parent post, or the post that the current post is in reply to
post_depth - the number of replies the current post is from the initial post
is_first_post - True if the current post is the initial post
annotations - a list of all annotations made to this post (see below)
majority_type - the majority annotated type, if there is a majority type between the annotators, when considering only the main_type field
majority_link - the majority annotated link, if there is a majority link between the annotators
Annotation fields
annotator - an unique ID for the annotator
main_type - the main discourse act that describes this post
secondary_type - if a post contains more than one discourse act in sequence, this is the second discourse act in the post
link_to_post - the post that this post is linked to
Data sampling and pre-processing
Selecting Reddit threads
This data was randomly sampled from the full Reddit dataset starting from its inception to the end of May 2016, which is made available publicly as a dump on Google BigQuery. This dataset was subsampled from the larger dataset and does not include posts with fewer than two comments, not in English, which contain pornographic material or from Subreddits focused on trading. Further, the number of replies to a single thread was limited to 40.
Annotation
Three annotators were assigned to each thread and were instructed to annotate each comment in the thread with its discourse act (main_type) as well as the relation of each comment to a prior comment (link_to_post), if it existed. Annotators were instructed to consider the content at the comment level as opposed to sentence or paragraph level to make the task simpler.
Authors
Amy X. Zhang, MIT CSAIL, Cambridge, MA, USA. axz@mit.edu
Ka Wong, Google, Mountain View, CA, USA. kawong@google.com
Bryan Culbertson, Calthorpe Analytics, Berkeley, CA, USA. bryan.culbertson@gmail.com
Praveen Paritosh, Google, Mountain View, CA, USA. pkp@google.com
Citation Guidelines
If you are using this data towards a research publication, please cite the following paper.
Amy X. Zhang, Bryan Culbertson, Praveen Paritosh. Characterizing Online Discussion Using Coarse Discourse Sequences. In Proceedings of the International AAAI Conference on Weblogs and Social Media (ICWSM '17). Montreal, Canada. 2017.
Bibtex: @inproceedings{coarsediscourse, title={Characterizing Online Discussion Using Coarse Discourse Sequences}, author={Zhang, Amy X. and Culbertson, Bryan and Paritosh, Praveen}, booktitle={Proceedings of the 11th International AAAI Conference on Weblogs and Social Media}, series={ICWSM '17}, year={2017}, location = {Montreal, Canada} }
License
CC-by
Inspiration
Can you visualize which discourse acts are used to in replies to each kind of discourse act?
Are threads more likely to be made up of a single type of discourse act or multiple discourse acts?
Are certain discourse acts more closely associated with specific subreddits?"
"ACLED African Conflicts, 1997-2017",Details on 165k Conflicts Across Africa Over Twenty Years,Jacob Boysen,7,"Version 2,2017-08-08|Version 1,2017-08-08",war,CSV,59 MB,CC0,"1,131 views",94 downloads,,,https://www.kaggle.com/jboysen/african-conflicts,"Context:
The Armed Conflict Location and Event Data Project is designed for disaggregated conflict analysis and crisis mapping. This dataset codes the dates and locations of all reported political violence and protest events in dozens of developing countries in Africa. Political violence and protest includes events that occur within civil wars and periods of instability, public protest and regime breakdown. The project covers all African countries from 1997 to the present.
Content:
These data contain information on:
Dates and locations of conflict events;
Specific types of events including battles, civilian killings, riots, protests and recruitment activities;
Events by a range of actors, including rebels, governments, militias, armed groups, protesters and civilians;
Changes in territorial control; and
Reported fatalities.
Event data are derived from a variety of sources including reports from developing countries and local media, humanitarian agencies, and research publications. Please review the codebook and user guide for additional information: the codebook is for coders and users of ACLED, whereas the brief guide for users reviews important information for downloading, reviewing and using ACLED data. A specific user guide for development and humanitarian practitioners is also available, as is a guide to our sourcing materials.
Acknowledgements:
ACLED is directed by Prof. Clionadh Raleigh (University of Sussex). It is operated by senior research manager Andrea Carboni (University of Sussex) for Africa and Hillary Tanoff for South and South-East Asia. The data collection involves several research analysts, including Charles Vannice, James Moody, Daniel Wigmore-Shepherd, Andrea Carboni, Matt Batten-Carew, Margaux Pinaud, Roudabeh Kishi, Helen Morris, Braden Fuller, Daniel Moody and others. Please cite:
Raleigh, Clionadh, Andrew Linke, Håvard Hegre and Joakim Karlsen. 2010. Introducing ACLED-Armed Conflict Location and Event Data. Journal of Peace Research 47(5) 651-660.
Inspiration:
Do conflicts in one region predict future flare-ups? How do the individual actors interact across time? Do some sources report more often on certain actors?"
KCBS Barbeque Competitions,"Competition context and results from 1,559 KCBS Barbeque Competitions",73805,7,"Version 2,2017-07-21|Version 1,2017-07-20",food and drink,CSV,11 MB,CC0,"1,776 views",139 downloads,3 kernels,,https://www.kaggle.com/jaysobel/kcbs-bbq,"Context
This data set is the aggregate of 1,559 KCBS competitions from July 2013 through December 2016. The Kansas City Barbeque Society (KCBS) is ""world's largest organization of barbeque and grilling enthusiasts with over 20,000 members worldwide."" The data set was constructed by scraping the KCBS events page
A Standard Competition
At a standard KCBS BBQ Competition, 30 certified barbeque judges (CBJs) blindly judge the BBQ served by 36 teams. Judges are broken up into tables of 6. There are four categories of meat: chicken, pork ribs, pork and brisket. Each judge receives six samples representing six different teams.
Scoring
Samples are scored across three characteristics: appearance, taste and tenderness. Scores range from 0 to 9, with a 9 being perfect, a 6 corresponding to average, and a 0 given as part of a DQ or other official sanction. A team's score within a category is calculated by a weighted sum of the six judges' scores. The KCBS scoring weights were last changed in July 2013 which aligns with the start date of this data set. The maximum score in a category is 180 (9 points * 3 characteristics * 6 judges). The maximum overall score in a standard 4-category competition is 720 (180 points * 4 categories). Not all competitions in this data set are standard 4-category; see the is_standard feature.
Content
Competition Data Features:
This CSV file contains 1,559 rows of competition data. Scoring and placing resultsare stored separately across five category-specific files. The scoring results are linked to their competitions by a foreign key. There are many results for each competition in this file.
contest_key - the primary key used to link rows of results to a competition
date - the date of the competition
title - the name of the competition
location_str - the full string of the location ie New Palestine, IN
city - the city extracted from location_str
state - the state abbreviation extracted from location_str
state_full - the full name of the state extrapolated from state
prize - the total prize money awarded (I believe it's total, and not just 1st place)
cbj_percentage - the percentage of judges that are certified barbeque judges (CBJs)
is_championsip - a boolean indicating if the competition is a ""state championship"" (note: each state has more than one per year)
is_standard - a boolean indicating if the competition consists of and only of the four categories: chicken, pork ribs, pork and brisket. Some competitions include extra categories like dessert and these are considered non-standard (note: overall scores in a non-standard competition may be greater that 4 * 180)
Results Data Features:
There are five separate tables of results; one for each of the standard categories. The features are the same across each of these tables.
contest_key - the foreign key linking a result row to its competition
place - the place earned by a competing team (1st 2nd 3rd as 1, 2, 3)
score - the total score (0-180 in a chicken, ribs, pork and brisket, 0-720 in overall-- assuming a standard competition)
team_name - the name of the team (usually clever, and good for a word cloud!)
Acknowledgements
I'd like to thank the KCBS for providing fun opportunities to taste great BBQ, and recording event data on the website in an fairly accessible and light-weight manner. In the future I'd love to experiment with anonymized judge scoring data. A judge can view their scoring history and how it compared sample-by-sample with other judges' scores at the table.
Inspiration
I've been a CBJ for 4 years!"
"US Trademark Case Files, 1870-2016",Over 8 million Trademark case files and their owners,US Patent and Trademark Office,7,"Version 2,2017-11-10|Version 1,2017-08-10","government agencies
united states
product",Other,4 GB,CC0,"1,027 views",112 downloads,,,https://www.kaggle.com/uspto/us-trademark-case-files-18702016,"Context:
A trademark is a brand name. A trademark or service mark includes any word, name, symbol, device, or any combination, used or intended to be used to identify and distinguish the goods/services of one seller or provider from those of others, and to indicate the source of the goods/services.
Content:
The Trademark Case Files Dataset contains detailed information on 8.6 million trademark applications filed with or registrations issued by the USPTO between January 1870 and January 2017. It is derived from the USPTO main database for administering trademarks and includes data on mark characteristics, prosecution events, ownership, classification, third-party oppositions, and renewal history.
This dataset is a partial version of the full dataset, made up of only the case files and owner information. For the full dataset and additional information, please see the USPTO website.
Inspiration:
Which owner has filed for multiple trademarks with the longest break in between?
Who is the most prolific trademarker?
How has the volume of trademarks changed since 1870?
Which US city has produced the most trademark owners?"
Fortune 500 Diversity,Detailed diversity metrics for the Fortune 500 companies,Fortune,7,"Version 1,2017-06-27",demographics,CSV,460 KB,Other,"2,300 views",269 downloads,3 kernels,,https://www.kaggle.com/fortune-inc/f500-diversity,"Context
Workforce diversity is an increasingly salient issue, but it can be difficult to easily check how a specific company is performing. This dataset was created by Fortune to show what was discoverable by someone considering employment with one of the Fortune 500 firms and curious about their commitment to diversity and inclusion could find.
Content
This dataset contains the name of each firm, its rank in the 2017 Fortune 500, a link to its diversity and inclusion page or equal opportunity statement, and whether the company releases full, partial, or no data about the gender, race, and ethnicity of its employees. Additional detail is included where it was available. As there are over 200 fields in this dataset; please consult the data dictionary for details about specific features.
Acknowledgements
This dataset was assembled by Fortune.com data reporter Grace Donnelly. The details of her data preparation process can be found here.
Inspiration
Are the companies that release the most information more or less diverse than their peers? Are there any particular industries that stand out?"
Stanford Open Policing Project - Washington State,Data on Traffic and Pedestrian Stops by Police in Washington,Stanford Open Policing Project,7,"Version 2,2017-07-11|Version 1,2017-07-11","crime
law",Other,2 GB,Other,806 views,91 downloads,,0 topics,https://www.kaggle.com/stanford-open-policing/stanford-open-policing-project-washington-state,"Context:
On a typical day in the United States, police officers make more than 50,000 traffic stops. The Stanford Open Policing Project team is gathering, analyzing, and releasing records from millions of traffic stops by law enforcement agencies across the country. Their goal is to help researchers, journalists, and policymakers investigate and improve interactions between police and the public.
If you'd like to see data regarding other states, please go to https://www.kaggle.com/stanford-open-policing.
Content:
This dataset includes 2gb of stop data from Washington state. Please see the data readme for the full details of the available fields.
Acknowledgements:
This dataset was kindly made available by the Stanford Open Policing Project. If you use it for a research publication, please cite their working paper: E. Pierson, C. Simoiu, J. Overgoor, S. Corbett-Davies, V. Ramachandran, C. Phillips, S. Goel. (2017) “A large-scale analysis of racial disparities in police stops across the United States”.
Inspiration:
How predictable are the stop rates? Are there times and places that reliably generate stops?
Concerns have been raised about jurisdictions using civil forfeiture as a funding mechanism rather than to properly fight drug trafficking. Can you identify any jurisdictions that may be exhibiting this behavior?"
OpenAddresses - Asia and Oceania,Addresses and geolocations for Asia and Oceania,OpenAddresses,7,"Version 1,2017-08-04",,CSV,4 GB,Other,"1,019 views",153 downloads,,0 topics,https://www.kaggle.com/openaddresses/openaddresses-asia-and-oceania,"Context
OpenAddresses's goal is to connect the digital and physical worlds by sharing geographic coordinates, street names, house numbers and postal codes.
Content
This dataset contains one data file for each of these countries in Asia and Oceania.
United Arab Emirates - arab_emirates.csv
Australia - australia.csv
China - china.csv
Iceland - iceland.csv
Israel - israel.csv
Japan - japan.csv
Kazakhstan - kazakhstan.csv
Kuwait - kuwait.csv
New Caldonia - new_caldonia.csv
New Zealand - new_zealand.csv
Qatar - qatar.csv
Saudi Arabia - saudiarabia.csv
Singapore - singapore.csv
South Korea - south_korea.csv
Taiwan - taiwan.csv
Field descriptions:
LON - Longitude
LAT - Latitude
NUMBER - Street number
STREET - Street name
UNIT - Unit or apartment number
CITY - City name
DISTRICT - ?
REGION - ?
POSTCODE - Postcode or zipcode
ID - ?
HASH - ?
Acknowledgements
Data collected around 2017-07-25 by OpenAddresses (http://openaddresses.io).
Address data is essential infrastructure. Street names, house numbers and postal codes, when combined with geographic coordinates, are the hub that connects digital to physical places.
Data licenses can be found in LICENSE.txt.
Data source information can be found at https://github.com/openaddresses/openaddresses/tree/9ea72b079aaff7d322349e4b812eb43eb94d6d93/sources
Inspiration
Use this dataset to create maps in conjunction with other datasets to map weather, crime, or how your next canoing trip."
Foreign Exchange (FX) Prediction - USD/JPY,Jan 2017 Martket Data(Lightweight CSV),Team AI,7,"Version 1,2017-08-13","time series
finance",CSV,1 MB,CC0,"1,672 views",122 downloads,3 kernels,0 topics,https://www.kaggle.com/team-ai/foreign-exchange-fx-prediction-usdjpy,"Context
Coming Soon
Content
Coming Soon
Acknowledgements
Special Thanks to http://www.histdata.com/download-free-forex-data/
Inspiration
実際の取引にこの情報を使うときは十分ご注意ください。弊社およびコミュニティメンバーは損失の責任を取ることができません。"
Shanghai PM2.5 Air Pollution Historical Data,Let's hack enviromental protection!,Team AI,7,"Version 3,2017-08-15|Version 2,2017-08-15|Version 1,2017-08-15",environment,CSV,3 MB,CC0,"2,665 views",173 downloads,2 kernels,2 topics,https://www.kaggle.com/team-ai/shanghai-pm25-air-pollution-historical-data,"Context
Coming soon.
Content
Historical PM2.5 data in Shanghai.
Acknowledgements
Special Thanks to UCI https://archive.ics.uci.edu/ml/datasets/PM2.5+Data+of+Five+Chinese+Cities
Inspiration
We need care more about environment. Let's use data science for social good."
Kaggle Movie League Results,Movie box office earnings,Myles O'Neill,7,"Version 1,2018-01-03",film,CSV,5 KB,CC0,86 views,2 downloads,,0 topics,https://www.kaggle.com/mylesoneill/kaggle-movie-league-results,The results of Kaggle's box office prediction league.
Q & A Discussed in Parliament of India,88000+ Questions & answers discussed in Rajya Sabha from 2009 till date.,Rajanand Ilangovan / இராஜ்ஆனந்த் இளங்கோவன்,7,"Version 4,2017-10-02|Version 3,2017-10-02|Version 2,2017-09-28|Version 1,2017-09-22","india
politicians
government
+ 2 more...",CSV,159 MB,CC4,"1,429 views",137 downloads,2 kernels,0 topics,https://www.kaggle.com/rajanand/rajyasabha,"Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks.
Context
The Rajya Sabha has published the questions and answers that were discussed in each session on their webiste. But one has to search by question/session/ministry wise to get the detail. There were no direct way to get all the quesions and answers in structured way(i.e, csv) to do an analysis and find insights. So I've written a web scrapper in R to scrape the data from the Rajya Sabha website and created csv files for each year.
Content
This dataset helps one to understand what was being discussed in Parliament (Rajya Sabha) of India. There are over 88000+ questions and answers that were discussed in Rajya sabha from 2009 till date (Sep'2017).
Variables detail:
id - Unique identifier
answer_date - Answer date
ministry - Ministry name
question_type - Type of question (Starred or Unstarred)*
question_no - Question number. (This question no. is unique per session)
question_by - Minister who has raised the question.
question_title - Discussion title
question_description - Detailed question.
answer - Detailed answer to the above question.
*Starred Questions : These are Questions to which answers are desired to be given orally on the floor of the House during the Question Hour. These are distinguished in the printed lists by asterisks. 15 such questions are listed each day.
Unstarred Questions: These are Questions to which written answers are given by Ministers which are deemed to have been laid on the Table of the House at the end of the Question Hour. Upto 160 such questions are listed each day in a separate list.
source
Acknowledgements
Thanks to Rajya Sabha for making the question and answers searchable.
Inspiration
The below are some of the questions can be answered from this dataset.
Which state or district names mentioned most in question/answer?
Sentiment analysis
No. of questions ministry wise
No. of questions/answers per day. Typically it should be 175 per day. How many days were less productive?
Who has raised more question? etc."
NYC taxi trip durations,Data augmentation using OSRM,Marouane Benmeida,7,"Version 2,2017-09-11|Version 1,2017-09-10",,CSV,473 MB,CC0,"1,640 views",200 downloads,2 kernels,,https://www.kaggle.com/atmarouane/nyc-taxi-trip-noisy,"Context
As a participant to NYC Taxi Trip Duration, I'm providing additional data, to help extracting many new usefull features.
To do so I'm using a high performance routing engine designed to run on OpenStreetMap data.
Having the whole blind test data, I decided also to share a small amount concerning erroneous samples (less than 0.15%), so competitors can focus matching real world data and to not try to fit randomness.
Note: The steps files are big so I split them into two parts. Part 1, Part2
Content
Description of different tables used.
Train/Test augmented
id: Record id
distance: Route distance (m)
duration: OSRM trip duration (s)
motorway, trunk, primary, secondary, tertiary, unclassified, residential:
The proportion spent on different kind of roads (% of total distance)
nTrafficSignals: The number of traffic signals.
nCrossing: The number of pedestrian crossing.
nStop: The number of stop signs.
nIntersection: The number of intersections, if you are OSRM user, intersection have different meaning than the one used in OSRM.
*Intersection can be crossroad, but not a highway exit...
srcCounty, dstCounty: Pickup/Dropoff county.
NA: Not in NYC
1: Brooklyn
2: Queens
3: Staten Island
4: Manhattan
5: Bronx
Train/Test steps
For each trip we saved all the ways (route portion).
id: train/test id.
wayId: Way id, you can check the way using www.openstreetmap.org/way/wayId
portion: The proportion of the total distance
Polylines table
It contains encoded nodes (lon/lat coordinates), of the used ways.
wayId: The way identification.
polyline: Encoded polylines.
Train/Test bugs (0.15% of total data)
id: Same as original data.
bug: kind of the bug (0=none)
Trip duration higher than 1 day;
Drop off on the day after pickup, and trip duration higher than 6h;
Drop off time at 00:00:00 and vendor_id eq 2.
trip_duration: Taxi trip duration
Credits
Real-time routing with OpenStreetMap data (http://project-osrm.org/)
OSM (http://www.openstreetmap.org)
Banner (Photo by Nicolai Berntsen on Unsplash)"
NSE Listed 1000+ Companies' Historical Data,Indian stock data from the NSE,Abhishek,7,"Version 2,2018-02-19|Version 1,2018-02-19","business
finance
money
+ 2 more...",Other,63 MB,CC0,378 views,56 downloads,,0 topics,https://www.kaggle.com/abhishekyana/nse-listed-1384-companies-data,"Context
The Dataset here is the CSV (Comma Separated Value) formatted data of 1000+ Indian companies' historical stock data which are listed on NSE web scrapped using python. This data helps the community to dive into algorithmic trading using the ML techniques and can be used for any task. Hope this will be of great use for everyone.
Content
This dataset(.zip) is a collection of numerous CSV formatted files that are in format of ['Date','Open','high','low','close','adj close','volume']. I've acquired this data using the yahoo finance v7 server using the python requests and a bit of pre-processing.
Maruti_data.csv is the sample data of Maruti stock data from 2003-07 to till data (updated on 18-Feb-2018) .
Companies_dict.d is the python pickle dictionary variable to get company name from the SYMBOL or name if the csv file. You can load this using the pickle library and get the actual company SYMBOL to Legal Name. e.g.Python Code
Symbol2Name = pickle.load(open('company_symbol_name_dict.d','rb')) print(Symbol2Name['MARUTI']) #Will give you Maruti_Suzuki_India_Ltd
Acknowledgements
I would like to thank this githubrepo for making the python file this script of mine is based on.
Inspiration
I would love to see many people like me to get their hands dirty with this data and use it effectively to correlate the inter relationships among the companies."
UK Traffic Counts,Details of traffic in England and Wales,Sohier Dane,7,"Version 1,2017-08-30",road transport,CSV,988 MB,Other,"1,997 views",265 downloads,,0 topics,https://www.kaggle.com/sohier/uk-traffic-counts,"Data are available for each junction to junction link on the major road network (motorways and A roads). Data are also available for the sample of points on the minor road network (B, C and unclassified roads) that are counted each year, and these counts are used to produce estimates of traffic growth on minor roads.
The data are produced for every year, and are in three formats: a) the raw manual count data collected by trained enumerators; b) Annual Average Daily Flows (AADFs) for count points on major roads and minor roads; and c) traffic figures for major roads only. Explanatory notes (metadata) are available for each dataset, and in one combined note.
A description of how annual road traffic estimates are produced is available at https://www.gov.uk/government/uploads/system/uploads/attachment_data/file/270083/contents-page.pdf
This dataset was kindly released by the British Department of Transportation. You can find the original dataset here."
The Bank of England’s balance sheet,Annual data from 1696 to 2014,Sohier Dane,7,"Version 1,2017-09-16",economics,CSV,190 KB,CC0,"1,558 views",184 downloads,,0 topics,https://www.kaggle.com/sohier/the-bank-of-englands-balance-sheet,"This dataset contains an annual summary of the assets and liabilities from the bank's founding in 1696 through 2014.
Content
The csv is a condensed version of the original spreadsheet. Some notes, disclaimers, and a small portion of the data have been discarded to enable the format conversion.
Acknowledgements
This dataset was kindly made available by the Bank of England. You can find the original dataset here.
Inspiration
Can you back out key moments in history from this dataset using time series analysis?"
The Tate Collection,"Metadata for 70,000 artworks from the Tate",Rachael Tatman,7,"Version 1,2017-08-19","museums
visual arts
europe",CSV,26 MB,CC0,"1,216 views",99 downloads,3 kernels,0 topics,https://www.kaggle.com/rtatman/the-tate-collection,"Context
""Tate is an institution that houses the United Kingdom's national collection of British art, and international modern and contemporary art. It is a network of four art museums: Tate Britain, London (until 2000 known as the Tate Gallery, founded 1897), Tate Liverpool (founded 1988), Tate St Ives, Cornwall (founded 1993) and Tate Modern, London (founded 2000), with a complementary website, Tate Online (created 1998). Tate is not a government institution, but its main sponsor is the UK Department for Culture, Media and Sport.
""The name 'Tate' is used also as the operating name for the corporate body, which was established by the Museums and Galleries Act 1992 as 'The Board of Trustees of the Tate Gallery'.
""The gallery was founded in 1897, as the National Gallery of British Art. When its role was changed to include the national collection of modern art as well as the national collection of British art, in 1932, it was renamed the Tate Gallery after sugar magnate Henry Tate of Tate & Lyle, who had laid the foundations for the collection. The Tate Gallery was housed in the current building occupied by Tate Britain, which is situated in Millbank, London. In 2000, the Tate Gallery transformed itself into the current-day Tate, or the Tate Modern, which consists of a federation of four museums: Tate Britain, which displays the collection of British art from 1500 to the present day; Tate Modern, which is also in London, houses the Tate's collection of British and international modern and contemporary art from 1900 to the present day. Tate Liverpool has the same purpose as Tate Modern but on a smaller scale, and Tate St Ives displays modern and contemporary art by artists who have connections with the area. All four museums share the Tate Collection. One of the Tate's most publicised art events is the awarding of the annual Turner Prize, which takes place at Tate Britain.""
-- Tate. (n.d.). In Wikipedia. Retrieved August 18, 2017, from https://en.wikipedia.org/wiki/Plagiarism. Text reproduced here under a CC-BY-SA 3.0 license.
Content
This dataset contains the metadata for around 70,000 artworks that Tate owns or jointly owns with the National Galleries of Scotland as part of ARTIST ROOMS. Metadata for around 3,500 associated artists is also included.
The metadata here is released under the Creative Commons Public Domain CC0 licence. Images are not included and are not part of the dataset.
This dataset contains the following information for each artwork:
Id
accession_number
artist
artistRole
artistId
title
dateText
medium
creditLine
year
acquisitionYear
dimensions
width
height
depth
units
inscription
thumbnailCopyright
thumbnailUr
url
You may also like
Museum of Modern Art Collection: Title, artist, date, and medium of every artwork in the MoMA collection
The Metropolitan Museum of Art Open Access: Explore information on more than 420,000 historic artworks"
SF Street Trees,188k Street Trees Around SF,Jacob Boysen,7,"Version 1,2017-09-01","geography
nature
climate
civil engineering",CSV,48 MB,CC0,823 views,62 downloads,,0 topics,https://www.kaggle.com/jboysen/sf-street-trees,"Context:
Prop. E was a measure on the November 8, 2016 San Francisco ballot regarding responsibility for maintaining street trees and surrounding sidewalks. Voters were asked if the City should amend the City Charter to transfer responsibility from property owners to the City for maintaining trees on sidewalks adjacent to their property, as well for repairing sidewalks damaged by the trees. Prop E passed with almost 80% of the voters’ support. As part of this proposition, a SF tree census was conducted by SF Public Works.
Content:
18 columns of data includes address (including lat/longs), caretaker details, legal details, size of plot, time of planting, surrounding site context, and species.
Acknowledgements:
This data was collected by SF Public Works. You can use Kernels to analyze, share, and discuss this data on Kaggle, but if you’re looking for real-time updates and bigger data, check out the data on BigQuery, too.
Inspiration:
SF is notorious for it’s microclimates--can you identify zones from particular trees that thrive there?
Combine this data with the NYC Tree Census--which species are most common? least?"
Nashville Meetup Network,Teaching Dataset for NashNetX Presentation (PyTN),Stephen Bailey,7,"Version 5,2018-02-10|Version 4,2018-02-10|Version 3,2018-02-09|Version 2,2018-02-09|Version 1,2018-02-09","social groups
networks
tutorial
network analysis",CSV,12 MB,CC0,532 views,37 downloads,4 kernels,0 topics,https://www.kaggle.com/stkbailey/nashville-meetup,"Context
meetup.com is a website for people organizing and attending regular or semi-regular events (""meet-ups""). The relationships amongst users—who goes to what meetups—are a social network, ideal for graph-based analysis.
This dataset was generated for a talk titled Principles of Network Analysis with NetworkX, embedded online here (or with notebooks, etc. on Github). It forms the basis for a series of tutorials I presented on at PyNash and PyTennessee. In them, we work through the basics of graph theory and how to use NetworkX, a popular open-source Python package. We then apply this knowledge to extract insights about the social fabric of Tennessee MeetUp groups.
Content
Graph data
member-to-group-edges.csv: Edge list for constructing a member-to-group bipartite graph. Weights represent number of events attended in each group.
group-edges.csv: Edge list for constructing a group-to-group graph. Weights represent shared members between groups.
member-edges.csv: Edge list for constructing a member-to-member graph. Weights represent shared group membership.
rsvps.csv: Raw member-to-event attendance data, which was aggregated to form member-to-group-edges.csv.
Metadata
meta-groups.csv: Information for each group, including name and category. group_id can serve as index.
meta-members.csv: Information for each member, including name and location. member_id can serve as index.
meta-events.csv: Information for each event, including name and time. event_id can serve as index.
Acknowledgements
I'd like to acknowledge the folks at MeetUp.com, who have made their database publicly available via a convenient REST API. Even newbies like myself can access and enjoy!"
Alcohol and Drug Consumption of German Teens,Do modern teens prefer weed over cigarettes?,Fabiola,7,"Version 3,2017-09-15|Version 2,2017-09-15|Version 1,2017-09-15","alcohol
public health
children
+ 2 more...",CSV,14 KB,Other,"3,037 views",444 downloads,,,https://www.kaggle.com/fabiolabusch/alcohol-and-drug-consumption-of-german-teens,"Context
The data was collected by social science research institutes:
1973 bis 1993: Institut für Jugendforschung GmbH (IJF), München; 1997: GfM-GETAS/WBA GmbH, Hamburg 2001 bis 2015: forsa Gesellschaft für Sozialforschung und statistische Analysen mbH, Dortmund und Berlin.
Long term studies regarding the alcohol and (illegal) drug consumption habits of 12 to 25 year old German teens were taken. The motivations and influences in drug consumption were studied. The aim was to develop preventional means and ways of communication with drug consuming teenagers.
Content
Young Germans between 12 and 25 years were asked about their drug consumptions in the last 12 months. The survey was taken from the 70's until today. Different datasets were provided and for some years features are missing.
Acknowledgements
The data is provided by German Bundeszentrale für gesundheitliche Aufklärung (Ministry of Health Education) and can be accessed at http://www.gbe-bund.de.
Photo by Stas Svechnikov on Unsplash.
Inspiration
While growing up in the late 2000's in Germany I had the impression that teenagers smoke less cigarettes and drink less alcohol than they used to in the 90's. Instead, they consumed more cannabis. I wonder if I was right ..."
Solar generation and demand Italy 2015-2016,Two-years hourly-recorded data of solar power generation and electricity demand,Ariel Cedola,7,"Version 1,2018-01-17","time series
renewable energy",CSV,537 KB,Other,"1,014 views",127 downloads,2 kernels,0 topics,https://www.kaggle.com/arielcedola/solar-generation-and-demand-italy-20152016,"Context
This dataset summarizes valuable data about recent total solar power generation and total electricity demand in a specific european country like Italy. Data are time series with hourly resolution, and values represent average of real-time power (generated and used) per market time unit (*). Data correspond to years 2015 and 2016. They are useful to analyze, for instance, the variation of solar generation with time in the four seasons of the year, the change of electricity demand depending on the day of the week, or in summer/winter holidays. Use of historical weather data could help to visualize the variation of solar power generation with climate conditions, extremely useful excercise for solar power generation forecasting. Studies of load forecasting could be also conducted by making use of the present dataset.
(*) Detailed data descriptions
Content
The two files include time series data of solar generation and total electricity consumption in Italy during the years 2015 and 2016, with hourly resolution. CSV files are structured in three columns: 1. Date and Time 2. Load 3. Solar Generation The time is expressed in Coordinated Universal Time (UTC), and the format of Date and Time is ""%Y-%m-%dT%H%M%SZ"". Solar generation and load are floating point numbers, which represent power expressed in MW (Mega Watts) units. Solar generation is the total solar power generated in Italy in 2015 and 2016, calculated by adding the generation in the different italian bidding zones (6 geographical regions: Nord, Centro Nord, Centro Sud, Sud, Sardegna and Sicilia, and 4 poles: Brindisi, Foggia, Priolo and Rossano). Load represents the total demand of power in the same periods. Note: the 2015 file presents a few missing data.
Acknowledgements
Data has been extracted from ""Open Power System Data. 2017. Data Package Time series. Version 2017-07-09 (https://data.open-power-system-data.org/time_series/2017-07-09).""
The primary data source is ENTSO-E Transparency, the central data platform of the European transmission system operators (https://transparency.entsoe.eu).
Inspiration
Do you think we could improve the Day-ahead load forecasting? Navigate for instance the ENTSO-E Transparency website, it shows up-to-date comparisons between Day ahead total load forecast and Actual total load, by bidding zones or countries. As you will see, sometimes the differences may be significant.
Important: The ENTSO-E Platform is a great repository of energy data. Measured data and forecasts are provided to the Platform by the Primary Data Owners (see Terms and Conditions at https://transparency.entsoe.eu)."
Faulty Steel Plates,Steel plate faults classified into seven types,UCI Machine Learning,7,"Version 1,2017-09-06","business
engineering
civil engineering
+ 2 more...",CSV,291 KB,Other,"2,231 views",207 downloads,3 kernels,0 topics,https://www.kaggle.com/uciml/faulty-steel-plates,"Context
This dataset comes from research by Semeion, Research Center of Sciences of Communication. The original aim of the research was to correctly classify the type of surface defects in stainless steel plates, with six types of possible defects (plus ""other""). The Input vector was made up of 27 indicators that approximately [describe] the geometric shape of the defect and its outline. According to the research paper, Semeion was commissioned by the Centro Sviluppo Materiali (Italy) for this task and therefore it is not possible to provide details on the nature of the 27 indicators used as Input vectors or the types of the 6 classes of defects.
Content
There are 34 fields. The first 27 fields describe some kind of steel plate faults seen in images. Unfortunately, there is no other information that I know of to describe these columns.
X_Minimum
X_Maximum
Y_Minimum
Y_Maximum
Pixels_Areas
X_Perimeter
Y_Perimeter
Sum_of_Luminosity
Minimum_of_Luminosity
Maximum_of_Luminosity
Length_of_Conveyer
TypeOfSteel_A300
TypeOfSteel_A400
Steel_Plate_Thickness
Edges_Index
Empty_Index
Square_Index
Outside_X_Index
Edges_X_Index
Edges_Y_Index
Outside_Global_Index
LogOfAreas
Log_X_Index
Log_Y_Index
Orientation_Index
Luminosity_Index
SigmoidOfAreas
The last seven columns are one hot encoded classes, i.e. if the plate fault is classified as ""Stains"" there will be a 1 in that column and 0's in the other columns. If you are unfamiliar with one hot encoding, just know that the last seven columns are your class labels.
Pastry
Z_Scratch
K_Scatch
Stains
Dirtiness
Bumps
Other_Faults
Acknowledgements
MetaNet: The Theory of Independent Judges (PDF Download Available). Available from: https://www.researchgate.net/publication/13731626_MetaNet_The_Theory_of_Independent_Judges [accessed Sep 6, 2017].
Dataset provided by Semeion, Research Center of Sciences of Communication, Via Sersale 117, 00128, Rome, Italy. www.semeion.it
Lichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science."
World Bank's Major Contracts,The largest supply contracts awarded by the World Bank,World Bank,7,"Version 1,2017-09-14","supply chain
international relations",CSV,51 MB,CC0,"1,827 views",186 downloads,,0 topics,https://www.kaggle.com/theworldbank/world-banks-major-contracts,"This set of contract awards includes data on commitments against contracts that were reviewed by the Bank before they were awarded (prior-reviewed Bank-funded contracts) under IDA/IBRD investment projects and related Trust Funds. This dataset does not list all contracts awarded by the Bank, and should be viewed only as a guide to determine the distribution of major contract commitments among the Bank's member countries. ""Supplier Country"" represents place of supplier registration, which may or not be the supplier's actual country of origin. Information does not include awards to subcontractors nor account for cofinancing. The Procurement Policy and Services Group does not guarantee the data included in this publication and accepts no responsibility whatsoever for any consequences of its use. The World Bank complies with all sanctions applicable to World Bank transactions.
Acknowledgements
This dataset was kindly made available by the World Bank. You can find the original dataset here.
Inspiration
How do the contract awards compare to each nations's voting rights? Are there any unexpected, consistent preferences?"
Indian Hotels on Goibibo,"4,000 Indian hotels on Goibibo",PromptCloud,7,"Version 1,2017-09-15","india
hotels
internet",CSV,9 MB,CC4,"1,476 views",205 downloads,2 kernels,0 topics,https://www.kaggle.com/PromptCloudHQ/hotels-on-goibibo,"Context
This is a pre-crawled dataset, taken as subset of a bigger dataset (more than 33344 hotels) that was created by extracting data from goibibo.com, a leading travel site from India.
Content
This dataset has following fields:
address
area - The sub-city region that this hotel is located in, geographically.
city
country - Always India.
crawl_date
guest_recommendation - How many guests that stayed here have recommended this hotels to others on the site.
hotel_brand - The chain that owns this hotel, if this hotel is part of a chain.
hotel_category
hotel_description - A hotel description, as provided by the lister.
hotel_facilities -
hotel_star_rating - The out-of-five star rating of this hotel.
image_count - The number of images provided with the listing.
latitude
locality
longitude
pageurl
point_of_interest - Nearby locations of interest.
property_name
property_type - The type of property. Usually a hotel.
province
qts - Crawl timestamp.
query_time_stamp - Copy of qts.
review_count_by_category - Reviews for the hotel, broken across several different categories.
room_area
room_count
room_facilities
room_type
similar_hotel
site_review_count - The number of reviews for this hotel left on the site by users.
site_review_rating - The overall rating for this hotel by users.
site_stay_review_rating
sitename - Always goibibo.com
state
uniq_id
Acknowledgements
This dataset was created by PromptCloud's in-house web-crawling service.
Inspiration
Try exploring some of the amenity categories. What do you see?
Try applying some natural language processing algorithms to the hotel descriptions. What are the some common words and phrases? How do they relate to the amenities the hotel offers?
What can you discover by drilling down further into hotels in different regions?"
Bestseller books on Paytm,1500 bestseller books on Paytm,PromptCloud,7,"Version 1,2017-09-16","books
internet",CSV,2 MB,CC4,"2,316 views",273 downloads,,0 topics,https://www.kaggle.com/PromptCloudHQ/bestseller-books-on-paytm,"Context
This is a pre-crawled dataset, taken as subset of a bigger dataset (more than 16,000 books) that was created by extracting data from paytm.com, a leading eCommerce store in India.
Content
This dataset has following fields:
amtsave
brand
breadcrumbs
country
desc
discount
domain
gallery
image
insertedon
list_price
model
name
other_sellers
payment_methods_supported
productcode
selling_price
specifications
type
uniq_id
url
weight
Acknowledgements
This dataset was created by PromptCloud's in-house web-crawling service.
Inspiration
Analyses of pricing, discount, specifications and authors can be performed."
Chicago Red Light Violations,309k Records of Violations in Safety Zones,Chicago Police Department,7,"Version 1,2017-09-13","government agencies
government
law",CSV,46 MB,CC0,"1,432 views",122 downloads,2 kernels,,https://www.kaggle.com/chicagopolice/chicago-red-light-violations,"Context:
This dataset reflects the daily volume of violations created by the City of Chicago Red Light Program for each camera. The data reflects violations that occurred from July 1, 2014 until present, minus the most recent 14 days. This data may change due to occasional time lags between the capturing of a potential violation and the processing and determination of a violation. The most recent 14 days are not shown due to revised data being submitted to the City of Chicago during this period. The reported violations are those that have been collected by the camera system and reviewed by two separate City contractors. In some instances, due to the inability the registered owner of the offending vehicle, the violation may not be issued as a citation. However, this dataset contains all violations regardless of whether a citation was actually issued, which provides an accurate view into the Red Light Program. Because of occasional time lags between the capturing of a potential violation and the processing and determination of a violation, as well as the occasional revision of the determination of a violation, this data may change.
Content:
More information on the Red Light Program can be found here.
Data covers July 1, 2014 to Sept 7, 2017. Rows include:
Intersection: Intersection of the location of the red light enforcement camera(s). There may be more than one camera at each intersection.
Camera ID: A unique ID for each physical camera at an intersection, which may contain more than one camera.
Address: The address of the physical camera (CAMERA ID). The address may be the same for all cameras or different, based on the physical installation of each camera.
Violation Date: The date of when the violations occurred. NOTE: The citation may be issued on a different date.
Violations: Number of violations for each camera on a particular day.
X Coordinate: The X Coordinate, measured in feet, of the location of the camera. Geocoded using Illinois State Plane East (ESRI:102671).
Y Coordinate: The Y Coordinate, measured in feet, of the location of the camera. Geocoded using Illinois State Plane East (ESRI:102671).
Latitude: The latitude of the physical location of the camera(s) based on the ADDRESS column. Geocoded using the WGS84.
Longitutde: The longitude of the physical location of the camera(s) based on the ADDRESS column. Geocoded using the WGS84.
Location: The coordinates of the camera(s) based on the LATITUDE and LONGITUDE columns. Geocoded using the WGS84.
Acknowledgements:
Dataset compiled by City of Chicago here.
Inspiration:
Which intersections have the most violations?
When do most violations occur?"
Punkt Sentence Tokenizer Models,Kiss and Strunk (2006) Unsupervised Multilingual Sentence Boundary Detection,NLTK Data,7,"Version 2,2017-08-17|Version 1,2017-08-17",linguistics,Other,35 MB,Other,"1,223 views",37 downloads,,0 topics,https://www.kaggle.com/nltkdata/punkt,"Context
The punkt.zip file contains pre-trained Punkt sentence tokenizer (Kiss and Strunk, 2006) models that detect sentence boundaries. These models are used by nltk.sent_tokenize to split a string into a list of sentences.
A brief tutorial on sentence and word segmentation (aka tokenization) can be found in Chapter 3.8 of the NLTK book.
The punkt.zip file contents:
README: contains the information of how and what the models are trained on.
PY3/: contains the below pickled files as above for Python3
czech.pickle
danish.pickle
dutch.pickle
english.pickle
estonian.pickle
finnish.pickle
french.pickle
german.pickle
greek.pickle
italian.pickle
norwegian.pickle
polish.pickle
portuguese.pickle
slovene.pickle
spanish.pickle
swedish.pickle
turkish.pickle
Citations
Kiss, Tibor and Strunk, Jan (2006): Unsupervised Multilingual Sentence Boundary Detection.
Computational Linguistics 32: 485-525."
TSA Claims Database,Property and injury claims filed from 2002 - 2015,Terminal Security Agency,7,"Version 1,2017-08-22",government agencies,CSV,34 MB,CC0,"1,287 views",153 downloads,3 kernels,0 topics,https://www.kaggle.com/terminal-security-agency/tsa-claims-database,"Context
Did you know that claims can be filed against TSA? Sometimes US Terminal Security Agency (TSA) makes mistakes. People can get hurt and property can be damaged, lost, or stolen. Claims are generally filed against TSA for personal injuries and lost or damaged property during screenings and they keep records of every claim!
Content
The dataset includes claims filed between 2002 through 2015.
Claim Number
Date Received
Incident Date
Airport Code
Airport Name
Airline Name
Claim Type
Claim Site
Item
Claim Amount
Status
Close Amount
Disposition
Acknowledgements
File modifications: - Excel format to TSV - commas to semicolons - TSV to CSV
Original data can be found here: https://www.dhs.gov/tsa-claims-data
Inspiration
I took a quick look at these data and discovered that the most claims are filed against TSA at John F. Kennedy International. I also discovered four claims against Wrongful Death!"
Elevators in New York City,All registered elevators in New York City,City of New York,7,"Version 1,2017-09-16",cities,CSV,13 MB,CC0,"1,471 views",100 downloads,2 kernels,,https://www.kaggle.com/new-york-city/nyc-elevators,"Context
This is a dataset of every registered elevator in New York City. It was generated by the NYC Department of Buildings in September 2015 in response to a journalistic Freedom of Information Law request, and contains information on elevator type, status, and function provided in the city's BISweb interface.
Content
The addresses, locations, and statuses of elevators in New York City.
Acknowledgements
This data is republished as-is from its public source on GitHub. That data in turn came from the NYC Department of Buildings.
Inspiration
Where are the elevators, how many of them are functional, and what floors do they go to?"
ResNet-152,ResNet-152 Pre-trained Model for PyTorch,PyTorch,7,"Version 1,2017-12-14","machine learning
pre-trained model",Other,214 MB,CC0,513 views,8 downloads,,0 topics,https://www.kaggle.com/pytorch/resnet152,"ResNet-152
Deep Residual Learning for Image Recognition
Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity.
An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.
The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.
Authors: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun
https://arxiv.org/abs/1512.03385
Architecture visualization: http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006
What is a Pre-trained Model?
A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.
Why use a Pre-trained Model?
Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it."
notMNIST,A dataset for a more challenging MNIST-like classification task,jwjohnson314,7,"Version 2,2018-02-15","image processing
deep learning
image data
multiclass classification",Other,359 MB,CC0,686 views,62 downloads,3 kernels,0 topics,https://www.kaggle.com/jwjohnson314/notmnist,"Context
The MNIST dataset is one of the best known image classification problems out there, and a veritable classic of the field of machine learning. This dataset is more challenging version of the same root problem: classifying letters from images. This is a multiclass classification dataset of glyphs of English letters A - J.
This dataset is used extensively in the Udacity Deep Learning course, and is available in the Tensorflow Github repo (under Examples). I'm not aware of any license governing the use of this data, so I'm posting it here so that the community can use it with Kaggle kernels.
Content
notMNIST _large.zip is a large but dirty version of the dataset with 529,119 images, and notMNIST_small.zip is a small hand-cleaned version of the dataset, with 18726 images. The dataset was assembled by Yaroslav Bulatov, and can be obtained on his blog. According to this blog entry there is about a 6.5% label error rate on the large uncleaned dataset, and a 0.5% label error rate on the small hand-cleaned dataset.
The two files each containing 28x28 grayscale images of letters A - J, organized into directories by letter. notMNIST_large.zip contains 529,119 images and notMNIST_small.zip contains 18726 images.
Acknowledgements
Thanks to Yaroslav Bulatov for putting together the dataset."
Upvoted Kaggle Dataset,2885 Kaggle datasets with at least one vote,Canggih P Wibowo,7,"Version 1,2018-02-22","databases
information",CSV,1 MB,CC0,116 views,3 downloads,2 kernels,0 topics,https://www.kaggle.com/canggih/voted-kaggle-dataset,"Context
Kaggle dataset becomes a popular growing place to share datasets. Almost every day there will be new datasets uploaded. I am curious to explore what can be extracted from the information of each dataset.
Content
This dataset consists 2885 datasets information in 15 columns:
Title
Subtitle
Owner
Vote
Last update
Tags
Datatype
Size
License
Views
Downloads
Kernels
Topics
URL
Description
Acknowledgements
All data were taken from Kaggle website. Collected on 21 Feb 2018
Inspiration
With this dataset, we may try to predict the upcoming datasets uploaded, including its topics, number of votes, number of downloads, etc. Data visualization involving clustering may be performed also."
Food 101,Pictures of 101 types of food,DanB,6,"Version 1,2018-01-04",food and drink,Other,5 GB,Other,396 views,62 downloads,,0 topics,https://www.kaggle.com/dansbecker/food-101,"Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
This is the Food 101 dataset, also available from https://www.vision.ee.ethz.ch/datasets_extra/food-101/
It contains images of food, organized by type of food. It was used in the Paper ""Food-101 – Mining Discriminative Components with Random Forests"" by Lukas Bossard, Matthieu Guillaumin and Luc Van Gool. It's a good (large dataset) for testing computer vision techniques.
Acknowledgements
The Food-101 data set consists of images from Foodspotting [1] which are not property of the Federal Institute of Technology Zurich (ETHZ). Any use beyond scientific fair use must be negociated with the respective picture owners according to the Foodspotting terms of use [2].
[1] http://www.foodspotting.com/ [2] http://www.foodspotting.com/terms/"
GOSU.AI Dota 2 Game Chats,Anonymized chats from Dota 2 match replays,Peter Romov,6,"Version 2,2018-02-19|Version 1,2018-02-15","video games
text data",CSV,313 MB,Other,498 views,16 downloads,,0 topics,https://www.kaggle.com/romovpa/gosuai-dota-2-game-chats,"Dataset
This dataset contains chat messages from Dota 2 — video game by Valve, one of the most popular eSport discipline. The dataset was used to train Roflan bot. It contains chats of almost 1M matches from public matchmaking (when players are selected by the game server at random with about the same skill level).
Caution and Disclaimer
Important, please read. This dataset is completely Not Safe For Work.
In Dota 2 the players communicate with each other in a very specific way. For instance, you may found a lot of abbreviations and game-specific terms. For Dota 2 player it is typical to blame teammates and opponents for failure in the game. Unfortunately, many messages may contain coarse insults, humiliation of another player's family, expressions of racism and other awful things. We provide the messages ""as is"" without any filters and censorship and we are not responsible for offensive content inside the data.
Our goal is to give researchers an opportunity to explore players community by diving into a real dialogs. We want to draw an attention to the problem of outstanding toxicity of the most Dota 2 players, we consider this behaviour of players unhealthy.
Usage of the dataset
See rough explanations on how do we learn our Roflan bot intended to mirror typical player's chat behaviour. You can apply your own language models on this dataset and make alternative chat bot or just compare performance of learning.
Look at this arXiv paper with analysis of esport spectators' chats. You can apply similar analysis to game participants chats."
Tatoeba,Crowd-source Example Sentence and Translations,Liling Tan,6,"Version 4,2018-01-05|Version 3,2018-01-02|Version 2,2018-01-02|Version 1,2017-12-29","languages
linguistics",CSV,639 MB,CC3,699 views,30 downloads,2 kernels,0 topics,https://www.kaggle.com/alvations/tatoeba,"Context
Tatoeba is a crowd-sourced dataset made up of example sentences and their translations.
This dump uploaded on Kaggle is downloaded some time in Oct/Nov 2017. The latest dumps can be downloaded from https://tatoeba.org/eng/downloads
Acknowledgements
Credits goes to the maintainers and the crowd on https://tatoeba.org
Credits of the banner image goes to Patrick Tomasso
License
The official license is CC BY-SA 2.0"
Cargo 2000 Dataset,A Transport and Logistics Case Study Data Set,Chris Crawford,6,"Version 2,2018-02-09|Version 1,2018-02-09","road transport
shipping",Other,324 KB,CC0,709 views,87 downloads,,0 topics,https://www.kaggle.com/crawford/cargo-2000-dataset,"Transport and Logistics Case Study Data Set (Cargo 2000)
Cargo 2000 is an initiative of IATA, the International Air Transport Association (Cargo 2000 has been re-branded as Cargo iQ in 2016). It aims at delivering a new quality management system for the air cargo industry. Cargo 2000 allows for unprecedented transparency in the supply chain. Stakeholders involved in the transport process can share agreed Cargo 2000 messages, comprising transport planning, replanning and service completion events. Cargo 2000 is based on the following key principles: (1) Every shipment gets a plan (called a route map) describing predefined monitoring events. (2) Every service used during shipment is assigned a predefined milestone with a planned time of achievement. (3) Stakeholders receive alerts when a milestone has failed and notifications upon milestone completion, which include the effective time the milestone has been achieved.
Content
The case study data comprises tracking and tracing events from a forwarding company’s Cargo 2000 system for a period of five months. From those Cargo 2000 messages, we reconstructed execution traces of 3,942 actual business process instances, comprising 7,932 transport legs and 56,082 service invocations. Each execution trace includes planned and effective durations (in minutes) for each of the services of the business process (introduced in Section II), as well as airport codes for the DEP (“departure”) and RCF (“arrival”) services. Due to the fact that handling of transport documents along the business process differs based on whether the documents are paper-based or electronic, we focus on the flow of physical goods, as our data set did not allow us to discern the different document types.
The reconstruction process involved data sanitation and anonymization. We filtered overlapping and incomplete Cargo 2000 messages, removed canceled transports (i.e., deleted route maps), sanitized for exceptions from the C2K system (such as events occurring before route map creation) and homogenized the way information was represented in different message types. Finally, due to confidentiality reasons, message fields which might exhibit business critical or customer-related data (such as airway bill numbers, flight numbers and airport codes) have been eliminated or masked.
Each of the transport legs involves the following physical transport services:
• RCS: Check in freight at departure airline. Shipment is checked in and a receipt is produced at departure airport.
• DEP: Confirm goods on board. Aircraft has departed with shipment on board.
• RCF: Accept freight at arrival airline. Shipment is checked in according to the documents and stored at arrival warehouse.
• DLV: Deliver freight. Receipt of shipment was signed at destination airport.
Acknowledgements
A. Metzger, P. Leitner, D. Ivanovic, E. Schmieders, R. Franklin, M. Carro, S. Dustdar, and K. Pohl, “ Comparing and combining predictive business process monitoring techniques,” IEEE Trans. on Systems Man Cybernetics: Systems, 2015.
A. Metzger, R. Franklin, and Y. Engel, “ Predictive monitoring of heterogeneous service-oriented business networks: The transport and logistics case,” in Service Research and Innovation Institute Global Conference (SRII 2012), ser. Conference Publishing Service (CPS), R. Badinelli, F. Bodendorf, S. Towers, S. Singhal, and M. Gupta, Eds. IEEE Computer Society, 2012.
Z. Feldmann, F. Fournier, R. Franklin, and A. Metzger, “Industry article: Proactive event processing in action: A case study on the proactive management of transport processes,” in Proceedings of the Seventh ACM International Conference on Distributed Event-Based Systems, DEBS 2013, Arlington, Texas, USA, S. Chakravarthy, S. Urban, P. Pietzuch, E. Rundensteiner, and S. Dietrich, Eds. ACM, 2013."
Ramen Ratings,Over 2500 ramen ratings,Aleksey Bilogur,6,"Version 1,2018-01-11","food and drink
asia",CSV,155 KB,Other,259 views,21 downloads,2 kernels,0 topics,https://www.kaggle.com/residentmario/ramen-ratings,"Context
The Ramen Rater is a product review website for the hardcore ramen enthusiast (or ""ramenphile""), with over 2500 reviews to date. This dataset is an export of ""The Big List"" (of reviews), converted to a CSV format.
Content
Each record in the dataset is a single ramen product review. Review numbers are contiguous: more recently reviewed ramen varieties have higher numbers. Brand, Variety (the product name), Country, and Style (Cup? Bowl? Tray?) are pretty self-explanatory. Stars indicate the ramen quality, as assessed by the reviewer, on a 5-point scale; this is the most important column in the dataset!
Note that this dataset does not include the text of the reviews themselves. For that, you should browse through https://www.theramenrater.com/ instead!
Acknowledgements
This dataset is republished as-is from the original BIG LIST on https://www.theramenrater.com/.
Inspiration
What ingredients or flavors are most commonly advertised on ramen package labels?
How do ramen ratings compare against ratings for other food products (like, say, wine)?
How is ramen manufacturing internationally distributed?"
Mobile Price Classification,classify mobile price range,Abhishek Sharma,6,"Version 1,2018-01-28","business
classification",CSV,182 KB,Other,529 views,98 downloads,,,https://www.kaggle.com/iabhishekofficial/mobile-price-classification,"Context
Bob has started his own mobile company. He wants to give tough fight to big companies like Apple,Samsung etc.
He does not know how to estimate price of mobiles his company creates. In this competitive mobile phone market you cannot simply assume things. To solve this problem he collects sales data of mobile phones of various companies.
Bob wants to find out some relation between features of a mobile phone(eg:- RAM,Internal Memory etc) and its selling price. But he is not so good at Machine Learning. So he needs your help to solve this problem.
In this problem you do not have to predict actual price but a price range indicating how high the price is"
Cannabis Strains,Marijuana strain dataset,LiamLarsen,6,"Version 9,2017-12-17|Version 8,2017-12-17|Version 7,2017-12-17|Version 6,2017-12-17|Version 5,2017-12-17|Version 4,2017-12-17|Version 3,2017-12-17|Version 2,2017-12-17|Version 1,2017-12-17","healthcare
botany",CSV,415 KB,Other,684 views,78 downloads,2 kernels,,https://www.kaggle.com/kingburrito666/cannabis-strains,"Context
Cannabis Strains
Content
strain name: Given name of strain
type of strain: indica, sativa, hybrid
rating: user ratings averaged
effects: different effects optained
taste: taste of smoke
description: backround, etc
Acknowledgements
leafly.com
Inspiration
Marijuana may get a bad rep in the media as far as the decriminalization debate goes, but its health benefits can no longer go unnoticed. With various studies linking long-term marijuana use to positive, health-related effects, there are more than just a few reasons to smoke some weed every day.
A study done by the Boston Medical Center and the Boston University of Medicine, examined 589 drug users—more than 8 out of 10 of whom were pot smokers. It determined that “weed aficionados” were no more likely to visit the doctor than non-drug users. If an increased risk of contracting ailments is what’s preventing you from smoking more weed, it looks like you’re in the clear!
One of the greatest medicinal benefits of marijuana is its pain relieving qualities, which make it especially effective for treating chronic pain. From menstruation cramps to nerve pain, as little as three puffs of bud a day can help provide the same relief as synthetic painkillers. Marijuana relieves pain by “changing the way the nerves function,” says Mark Ware, MD and assistant professor of anesthesia and family medicine at McGill University.
Studies have found that patients suffering from arthritis could benefit from marijuana use. This is because naturally occurring chemicals in cannabis work to activate pathways in the body that help fight off joint inflammation."
Top 23 Users in Kernel Ranking,Kaggle User Data,Kaan Can,6,"Version 1,2018-01-01",,Other,80 KB,CC0,203 views,15 downloads,,,https://www.kaggle.com/kanncaa1/top-23-user-in-kernel-ranking,"Context
Data includes one month informatation of top 23 users in kernel rank. Recording data in each day takes my 20 minutes so I only collect 1 month information of users.
Content
There are 21 features(columns). date: Date of the record. From 20.11.2017 to 17.12.2017. (day.month.year), name: Name of the users, kernel_gold: Number of gold medal that is won at kernel ranking, kernel_silver: Number of silver medal that is won at kernel ranking,
kernel_bronze: Number of bronze medal that is won at kernel ranking,
kernel_points: Number of kernel points in kernel ranking, followers: Number of followers of users, following: Number of following, run_activity: Number of daily run activity, comment_activity: Number of daily comment activity, datasets: Number of published datasets, kernel_rank: Number of kernel rank, kernel_level: Kernel level like kernel master or kernel expert, discussion_level: Discussion level like discussion master or discussion expert, kernel_number: Number of published kernel,
discussion_number: Number of discussion discussion_rank: Discussion rank
discussion_gold: Number of gold medal that is won at discussion ranking, discussion_silver: Number of silver medal that is won at discussion ranking,
discussion_bronze: Number of bronze medal that is won at discussion ranking,
competition_rank: Competition rank
Acknowledgements
I take records at almost 22:00 according to Istanbul clock therefore due to kaggle update time there can be inconsistency between discussion number and daily comment_activity.
Inspiration
I collect this data because of my curiosity. However, 1 month information is not enough for detailed analysis. Therefore, maybe more detailed public information of users can be published by kaggle team."
Oil and Gas,,Baking Pi,6,"Version 1,2017-12-14",natural resources,CSV,1012 KB,Other,661 views,78 downloads,,0 topics,https://www.kaggle.com/raspberrypie/oil-and-gas,"Context
The Global dataset of oil and natural gas production, prices, exports, and net exports.
Content
Oil production and prices data are for 1932-2014 (2014 data are incomplete); gas production and prices are for 1955-2014; export and net export data are for 1986-2013. Country codes have been modified from earlier versions to conform to Correlates of War (COW) and Quality of Government (QOG) standards
Acknowledgements
Ross, Michael; Mahdavi, Paasha, 2015, ""Oil and Gas Data, 1932-2014"", doi:10.7910/DVN/ZTPW0Y, Harvard Dataverse
Inspiration
How has the price varied from 1900s to 2000s?"
elemental_properties,,Chris Bartel,6,"Version 1,2017-12-23",,CSV,2 KB,CC0,166 views,638 downloads,,0 topics,https://www.kaggle.com/cbartel/elemental-properties,This dataset does not have a description yet.
2016 U.S. Presidential Campaign Texts and Polls,Debate and Speech Transcripts & Voter Group Polls,Alan Du,6,"Version 1,2017-12-30","politics
demographics
linguistics
political science",Other,2 MB,CC0,765 views,90 downloads,,0 topics,https://www.kaggle.com/alandu20/2016-us-presidential-campaign-texts-and-polls,"Context
This is an aggregate of the data I studied for my thesis titled, ""Data Mining in Presidential Debates and Speeches: How Campaign Rhetoric Shaped Voter Opinion in the 2016 U.S. Presidential Race"". The goal of my thesis was to use NLP techniques to understand how Donald Trump’s rhetoric impacted the opinions of various voter groups throughout his campaign. Here is a summary of my findings:
Trump’s words were typically more common in an American English corpus and more extreme on both ends of the sentiment spectrum
Trump not only used rhetorical devices for persuasion but also adeptly coupled these devices with the right talking points based on the composition of his audience
Precise execution of the above strategy garnered him an unexpectedly large number of votes from the white female and Hispanic demographics
I hope that others can use this dataset to answer questions of their own about the 2016 presidential campaign.
Content
Collection of data from the 2016 U.S. Presidential Election Campaign containing:
Transcripts of the three presidential debates, divided into separate Trump and Clinton text files
Transcripts of Trump's 64 speeches delivered after the RNC and Clinton's 35 speeches delivered after the DNC
Transcripts of select speeches delivered by candidates during the primary campaigns
USC Dornsife/LA Times Presidential Election Poll, with daily breakdown by voter groups
Five Thirty Eight Election Poll, containing daily data from numerous pollsters
Acknowledgements
Debate and speech texts scraped from the American Presidency Project website."
Medical Cost Personal Datasets,Insurance Forecast by using Linear Regression,Miri Choi,6,"Version 1,2018-02-21","healthcare
finance",CSV,54 KB,ODbL,600 views,112 downloads,,0 topics,https://www.kaggle.com/mirichoi0218/insurance,"Context
Machine Learning with R by Brett Lantz is a book that provides an introduction to machine learning using R. As far as I can tell, Packt Publishing does not make its datasets available online unless you buy the book and create a user account which can be a problem if you are checking the book out from the library or borrowing the book from a friend. All of these datasets are in the public domain but simply needed some cleaning up and recoding to match the format in the book.
Content
Columns - age: age of primary beneficiary - sex: insurance contractor gender, female, male - bmi: Body mass index, providing an understanding of body, weights that are relatively high or low relative to height, objective index of body weight (kg / m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9 - children: Number of children covered by health insurance / Number of dependents - smoker: Smoking - region: the beneficiary's residential area in the US, northeast, southeast, southwest, northwest. - charges: Individual medical costs billed by health insurance
Acknowledgements
The dataset is available on GitHub here.
Inspiration
Can you accurately predict insurance costs?"
fastText English Word Vectors Including Sub-words,"Word vectors trained on Wikipedia 2017, UMBC webbase corpus, and statmt.org",Facebook,6,"Version 1,2018-01-11","machine learning
pre-trained model",Other,988 MB,CC3,498 views,33 downloads,,0 topics,https://www.kaggle.com/facebook/fasttext-english-word-vectors-including-subwords,"English Word Vectors with sub-word information
About fastText
fastText is a library for efficient learning of word representations and sentence classification. One of the key features of fastText word representation is its ability to produce vectors for any words, even made-up ones. Indeed, fastText word vectors are built from vectors of substrings of characters contained in it. This allows you to build vectors even for misspelled words or concatenation of words.
About the vectors
These pre-trained vectors contain 1 million word vectors learned with subword information on Wikipedia 2017, the UMBC webbase corpus and the statmt.org news dataset. In total, it contains 16B tokens.
The first line of the file contains the number of words in the vocabulary and the size of the vectors. Each line contains a word followed by its vectors, like in the default fastText text format. Each value is space separated. Words are ordered by descending frequency.
Acknowledgements
These word vectors are distributed under the Creative Commons Attribution-Share-Alike License 3.0.
P. Bojanowski*, E. Grave*, A. Joulin, T. Mikolov, Enriching Word Vectors with Subword Information
A. Joulin, E. Grave, P. Bojanowski, T. Mikolov, Bag of Tricks for Efficient Text Classification
A. Joulin, E. Grave, P. Bojanowski, M. Douze, H. Jégou, T. Mikolov, FastText.zip: Compressing text classification models

(* These authors contributed equally.)"
"Beer, Liquor, and Wine Reviews","A list of 2,000 reviews of beer, liquor, and wine sold online.",Datafiniti,6,"Version 1,2018-01-17","databases
food and drink
alcohol
+ 2 more...",CSV,741 KB,CC4,370 views,37 downloads,,0 topics,https://www.kaggle.com/datafiniti/wine-beer-and-liquor-reviews,"About this Data
This is a list of over 2,000 reviews for beer, liquor, and wine sold online provided by Datafiniti's Product Database. The dataset includes text and title of the review, the name and manufacturer of the product, reviewer metadata, and more.
What You Can Do With This Data
You can use this data to discover insights into how consumers review alcoholic beverages. E.g.:
Which brands have the best reviews?
Does white wine or red wine get better reviews?
What words are most commonly associated with each beverage type?
Data Schema
A full schema for the data is available in our support documentation.
About Datafiniti
Datafiniti provides instant access to web data. We compile data from thousands of websites to create standardized databases of business, product, and property information. Learn more.
Want More?
You can get more data like this by joining Datafiniti or requesting a demo."
Grammar and Online Product Reviews,"A list of 71,045 online reviews from 1,000 different products.",Datafiniti,6,"Version 1,2018-02-16","databases
grammar
product
+ 2 more...",CSV,9 MB,CC4,573 views,48 downloads,,0 topics,https://www.kaggle.com/datafiniti/grammar-and-online-product-reviews,"About This Data
This is a list of over 71,045 reviews from 1,000 different products provided by Datafiniti's Product Database. The dataset includes the text and title of the review, the name and manufacturer of the product, reviewer metadata, and more.
What You Can Do With This Data
You can use this data to assess how writing quality impacts positive and negative online product reviews. E.g.:
Do reviewers use punctuation correctly?
Does the number of spelling errors differ by rating?
What is the distribution of star ratings across products?
How does review length differ by rating?
How long is the typical review?
What is the frequency of words with spelling errors by rating?
What is the number of reviews that don’t end sentences with punctuation?
What is the proportion of reviews with spelling errors?
Data Schema
A full schema for the data is available in our support documentation.
About Datafiniti
Datafiniti provides instant access to web data. We compile data from thousands of websites to create standardized databases of business, product, and property information. Learn more.
Want More?
You can get more data like this by joining Datafiniti or requesting a demo."
Eclipse Megamovie,Citizen science covering the 2017 eclipse,Eclipse MegaMovie,6,"Version 1,2017-12-07","astronomy
bigquery",BigQuery,77 MB,CC0,827 views,0 downloads,2 kernels,0 topics,https://www.kaggle.com/eclipse-megamovie/eclipse-megamovie,"This first-of-its-kind citizen science project is a collection of photos submitted by a group of dedicated volunteers from locations across the United States during the August 21, 2017 total solar eclipse.
The bigquery tables include metadata for the photos, links to the photos, and astronomical measurements extracted from the photos.
Acknowledgements
This dataset was kindly prepared by Google, UC Berkeley, and thousands of volunteers. Please see https://eclipsemega.movie/ for more information.
Inspiration
Can you map out the locations of the contributors to the project? How many of them were outside the path of totality?"
ResNet-50,ResNet-50 Pre-trained Model for PyTorch,PyTorch,6,"Version 1,2017-12-14","machine learning
pre-trained model",Other,91 MB,CC0,568 views,4 downloads,,0 topics,https://www.kaggle.com/pytorch/resnet50,"ResNet-50
Deep Residual Learning for Image Recognition
Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity.
An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.
The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.
Authors: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun
https://arxiv.org/abs/1512.03385
Architecture visualization: http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006
What is a Pre-trained Model?
A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.
Why use a Pre-trained Model?
Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it."
ResNet-101,ResNet-101 Pre-trained Model for PyTorch,PyTorch,6,"Version 1,2017-12-14","machine learning
pre-trained model",Other,159 MB,CC0,459 views,7 downloads,,0 topics,https://www.kaggle.com/pytorch/resnet101,"ResNet-101
Deep Residual Learning for Image Recognition
Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity.
An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.
The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.
Authors: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun
https://arxiv.org/abs/1512.03385
Architecture visualization: http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006
What is a Pre-trained Model?
A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.
Why use a Pre-trained Model?
Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it."
FourSquare - NYC Restaurant Check-Ins,Check-ins for New York City restaurants over 4 months,Dan Ofer,6,"Version 1,2017-09-05",,CSV,1 MB,Other,"1,229 views",93 downloads,,0 topics,https://www.kaggle.com/danofer/foursquare-nyc-rest,"Context
Dataset includes check-in, tip and tag data of restaurant venues in NYC collected from Foursquare from 24 October 2011 to 20 February 2012. It contains 3,112 users, 3,298 venues with 27,149 check-ins and 10,377 tips.
Content
NY_Restauraunts_checkins.csv {Originally dataset_ubicomp2013_checkins.txt} has two columns. Each line represents a check-in event. The first column is user ID, while the second column is venue ID.
NY_Restauraunts_tips.csv {Originally dataset_ubicomp2013_tips.txt} has three columns. Each line represents a tip/comment a user left on a venue. The first and second columns are user ID and venue ID, repsectively. The third column is tip text.
NY_Restauraunts_tags.csv {Originally dataset_ubicomp2013_tags.txt} has two columns. Each line represents the tags users added to a venue. The first column is venue ID while the second column is tag set of the corresponding venues. Empty tag sets may exist for a venue if no user has ever added a tag to it.
Acknowledgements
Columns headers, details and file formats added manually.
Source: Scraped from Foursquare and downloaded from: https://sites.google.com/site/yangdingqi/home/foursquare-dataset
Dingqi Yang, Daqing Zhang, Zhiyong Yu and Zhiwen Yu, Fine-Grained Preference-Aware Location Search Leveraging Crowdsourced Digital Footprints from LBSNs. In Proceeding of the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp 2013), September 8-12, 2013, in Zurich, Switzerland.
Dingqi Yang, Daqing Zhang, Zhiyong Yu and Zhu Wang, A Sentiment-enhanced Personalized Location Recommendation System. In Proceeding of the 24th ACM Conference on Hypertext and Social Media (HT 2013), 1-3 May, 2013, Paris, France. Dingqi Yang, Daqing Zhang, Zhiyong Yu, Zhiwen Yu, Djamal Zeghlache.
SESAME: Mining User Digital Footprints for Fine-Grained Preference-Aware Social Media Search. ACM Trans. on Internet Technology, (TOIT), 14(4), 28, 2014.
Original README included (note that columns were added).
Inspiration
Interesting questions:
Linkage to additional data.
Sentiment analysis.
Recommender systems, prediction of checkins to related venues or tags.
Use for augmenting other datasets with geospatial or geotemporal data (for that period)."
Telstra Competition Dataset,Great competition revisited,Yifan Xie,6,"Version 1,2017-09-24","education
telecommunications",CSV,3 MB,Other,783 views,45 downloads,10 kernels,0 topics,https://www.kaggle.com/yifanxie/telstra-competition-dataset,"Context
This is the dataset that is used in the Telstra Network Disruptions competition that ran between Nov 2015 and Feb 2016 This competition provided a very nice and small dataset that allows many aspects of predictive modelling:
relational data between different entities of the disruption data
clean dataset that provides consistent and reliable feedback
ideal for practices for many parts of the predictive modelling pipelin: feature engineering, cross-validation, stacking, etc
Magic Feature! see forum thread for more details :)
This dataset is re-uploaded since the original competition did not feature kernels, and it is made available here give people a chance to practice their data science/predictive modelling skill with this nice little dataset
Content
The goal of the problem is to predict Telstra network's fault severity at a time at a particular location based on the log data available. Each row in the main dataset (train.csv, test.csv) represents a location and a time point. They are identified by the ""id"" column, which is the key ""id"" used in other data files.
Fault severity has 3 categories: 0,1,2 (0 meaning no fault, 1 meaning only a few, and 2 meaning many).
Different types of features are extracted from log files and other sources: event_type.csv, log_feature.csv, resource_type.csv, severity_type.csv.
Note: “severity_type” is a feature extracted from the log files (in severity_type.csv). Often this is a severity type of a warning message coming from the log. ""severity_type"" is categorical. It does not have an ordering. “fault_severity” is a measurement of actual reported faults from users of the network and is the target variable (in train.csv).
Acknowledgements
This dataset is made available entirely for educational use only, it is shared by Telstra and Kaggle for the original competition, and is subjected to their permission of usage.
Inspiration
How far up can you get in the post-deadline LB? :)"
World Soccer - archive of soccer results and odds,"20+ countries, 16 seasons, 30+ leagues, 130k+ matches with bookies odds",sash,6,"Version 9,2017-12-04|Version 8,2017-11-10|Version 7,2017-10-30|Version 6,2017-10-16|Version 5,2017-09-30|Version 4,2017-09-29|Version 3,2017-09-19|Version 2,2017-09-12|Version 1,2017-09-11",sports,SQLite,14 MB,Other,"1,542 views",209 downloads,2 kernels,4 topics,https://www.kaggle.com/sashchernuh/european-football,"Context
We have a few soccer datasets are already uploaded to Kaggle platform. However, after playing with some hypothesis, we found that we need to have updates more often to complete started framework.
Main difference between well-known European Soccer Database are:
added not only odds per match, but Under Over, Asian Handicaps.
more seasons, leagues and matches;
up to date data and regular updates of DB
Since, that is first release of dataset, we will fix all bugs to have good training history.
Content
Data Dictionary is described fully here - http://www.football-data.co.uk/notes.txt
Acknowledgements
Thanks to
football-data.co.uk for collected results, fixtures and odds
phoebet.com - for technical consultancies and helping to generate the dataset
Inspiration
Is it possible to get statistical advantage in long game?
When better to use different financial strategies?
What is more promising Ordinar's Or Accumulator Systems?
PS. We are not encourage to play bets or have bonuses from advertisements! We just focused to share and open knowledge to community about this domain as particular case of Arbitrage"
Crop Nutrient Database,USDA data about crop nutrients in the U.S.,Chris Crawford,6,"Version 1,2017-08-19","food and drink
science and culture
united states
+ 2 more...",CSV,281 KB,CC0,"1,790 views",162 downloads,,0 topics,https://www.kaggle.com/crawford/crop-nutrient-database,"Context
The PLANTS Database provides standardized information about the vascular plants, mosses, liverworts, hornworts, and lichens of the U.S. and its territories. It includes names, plant symbols, checklists, distributional data, species abstracts, characteristics, images, plant links, references, and crop information, and automated tools.
This particular dataset is the Crop Nutrient Database.
Content
These are the fields included in the dataset. I'll be honest, I have no idea what some of them mean:
Crop
ScientificName
Symbol
NuContAvailable
PlantPartHarvested
CropCategory
YieldUnit
AvYieldUnitWeight(lb)
AvMoisture%
AvN%(dry)
AvP%(dry)
AvK%(dry)
YieldUnitWeight(lb)_set
YieldUnitWeight(lb)_Bau
YieldUnitWeight(lb)_Joh
YieldUnitWeight(lb)_Roberts
YieldUnitWeight(lb)_WEEP
YieldUnitWeight(lb)_Men
YieldUnitWeight(lb)_Guy
YieldUnitWeight(lb)_Mc
YieldUnitWeight(lb)_Mah
YieldUnitWeight(lb)_Sha
YieldUnitWeight(lb)_Sch
YieldUnitWeight(lb)_Atu
YieldUnitWeight(lb)_Zim
YieldUnitWeight(lb)_Scu
YieldUnitWeight(lb)_John
YieldUnitWeight(lb)_Arc
DryMatter%_M-FF
DryMatter%_NAS
DryMatter%_F&L
DryMatter%_F&N
DryMatter%_Alb
DryMatter%_Est1
DryMatter%_Est2
DryMatter%_Est3
DryMatter%_Est4
DryMatter%_Est5
DryMatter%_Est6
DryMatter%_M&R
DryMatter%_M&L
DryMatter%_Sun
DryMatter%_Gro
DryMatter%_AgH8-9
DryMatter%_AgH8-12
DryMatter%_B788
AvDryMatter%
N%(dry)_NAS
N%(dry)_F&L
N%(dry)_F&N
N%(dry)_Swa
N%(dry)_Chapko
N%(dry)_Hill
N%(dry)_Bru
N%(dry)_AgH8-9
N%(dry)_AgH8-12
N%(dry)_B788
N%(dry)_M&L
N%(dry)_M-FF
N%(dry)_M&R
N%(dry)_Foster
N%(dry)_Rob1
N%(dry)_Rob2
N%(dry)_Coa
N%(dry)_And
N%(dry)_Gol1
N%(dry)_Gol2
N%(dry)_Wol
N%(dry)_Pete
N%(dry)_Col
N%(dry)_Alb
N%(dry)_Arc
N%(dry)_Bis
N%(dry)_Gar
N%(dry)_Heg
N%(dry)_Flo
N%(dry)_Feil
N%(dry)_Bre
N%(dry)_Burns
N%(dry)_Coc
P%(dry)_M-FF
P%(dry)_NAS
P%(dry)_F&L
P%(dry)_F&N
P%(dry)_AgH8-9
P%(dry)_AgH8-12
P%(dry)_B788
P%(dry)_M&L
P%(dry)_L&V
P%(dry)_Foster
P%(dry)_Rob1
P%(dry)_Rob2
P%(dry)_Coa
P%(dry)_And
P%(dry)_Gol1
P%(dry)_Gol2
P%(dry)_Sims
P%(dry)_Wol
P%(dry)_Pete
P%(dry)_Col
P%(dry)_Alb
P%(dry)_Arc
P%(dry)_Swa
P%(dry)_Rei
K%(dry)_M-FF
K%(dry)_NAS
K%(dry)_F&L
K%(dry)_F&N
K%(dry)_AgH8-9
K%(dry)_AgH8-12
K%(dry)_B788
K%(dry)_Foster
K%(dry)_Rob1
K%(dry)_Rob2
K%(dry)_Coa
K%(dry)_And
K%(dry)_Gol1
K%(dry)_Gol2
K%(dry)_Sims
K%(dry)_Wol
K%(dry)_Pete
K%(dry)_Col
K%(dry)_Alb
K%(dry)_Arc
K%(dry)_Swa
K%(dry)_Rei
Moisture%_M&R
Moisture%_M&L
Moisture%_Sun
Moisture%_Gro
gWater/100g_AgH8-9
gWater/100g_AgH8-12
gWater/100g_B788
Protein%(dry)_NAS
Protein%(dry)_F&L
Protein%(dry)_F&N
Protein%(dry)_Swa
Protein%(dry)_Chapko
Protein%(dry)_Hill
Protein%(dry)_Bru
Protein%(dry)_Bis
Protein%(dry)_Gar
Protein%(dry)_Heg
Protein%(dry)_Flo
Protein%(dry)_Feil
Protein%(dry)_Bre
Protein%(dry)_Burns
gProtein/100g(wet)_AgH8-9
gProtein/100g(wet)_AgH8-12
gProtein/100g(wet)_B788
Protein%(wet)_M&L
N%(wet)_M-FF
P%(wet)_M-FF
gP/100g(wet)_AgH8-9
gP/100g(wet)_AgH8-12
gP/100g(wet)_B788
P%(wet)_M&L
K%(wet)_M-FF
gK/100g(wet)_AgH8-9
gK/100g(wet)_AgH8-12
gK/100g(wet)_B788"
Seattle Police Department 911 Incident Response,1.4 million responses from 2009 onwards,Sohier Dane,6,"Version 1,2017-08-30",crime,CSV,362 MB,CC0,"1,467 views",141 downloads,,0 topics,https://www.kaggle.com/sohier/seattle-police-department-911-incident-response,"This dataset records police responses to 911 calls in the city of Seattle.
Acknowledgements
This dataset was kindly made available by the City of Seattle. They update the data daily; you can find the original version here.
Inspiration
The study discussed in this Atlantic article reviewing 911 calls in Milwaukuee found that that incidents of police violence lead to large drops in the number of 911 calls. Does this hold true for Seattle as well? This dataset technically only contains the responses to 911 calls rather than the calls themselves, but it should be feasible to use the responses as a decent proxy for calls."
Allen-Unger Global Commodity Prices,Price Time Series Between 965 and 1983,Sohier Dane,6,"Version 1,2017-09-23","history
economics",CSV,30 MB,CC0,779 views,70 downloads,2 kernels,0 topics,https://www.kaggle.com/sohier/allenunger-global-commodity-prices,"Context
Are you tired of hearing your elders talk about how much cheaper things were back in their day? Would you like to one-up them by talking about how much cheaper goods were a thousand years before they were born? Of course you would!
You might also have an interest in an unusually comprehensive set of historic prices that cover long time spans. English port wine prices, for example, stretch from 1209 through 1869.
Content
Each commodity contains prices in the local currency and standardized silver units that allow for broader comparisons.
Please note that this dataset has been consolidated into a single file from the original thousand or so csvs, so the format is slightly different.
Acknowledgements
This dataset was kindly made available by Robert Allen and Richard Unger. You can find the original dataset here.
Inspiration
Can you identify goods or locations that remained largely unaffected by the industrial revolution?
If you like
If you enjoyed this dataset, you might also like the millennium of macroeconomic data dataset."
Historical American Lynching,Information on 2806 lynchings in the United States,Rachael Tatman,6,"Version 1,2017-08-17","united states
death
crime
violence",CSV,185 KB,CC4,847 views,46 downloads,,0 topics,https://www.kaggle.com/rtatman/historical-american-lynching,"Context:
""Lynching"" historically includes not only Southern lynching but frontier lynching and vigilantism nationwide and many labor-related incidents. Persons of any race or ethnicity and either gender may have been either perpetrators or victims of lynching. The lynchings in this dataset follow an NAACP definition for including an incident in the inventory of lynchings:
There must be evidence that someone was killed;
The killing must have occurred illegally;
Three or more persons must have taken part in the killing; and
The killers must have claimed to be serving justice or tradition.
Content:
The original data came from the NAACP Lynching Records at Tuskegee Institute, Tuskegee, Alabama. Stewart Tolnay and E.M. Beck examined these records for name and event duplications and other errors with funding from a National Science Foundation Grant and made their findings available to Project HAL in 1998. Project HAL is inactive now, but it’s original purpose was to build a data set for researchers to use and to add to.
The dataset contains the following information for each of the 2806 reported lynchings:
State: State where the lynching took place
Year: Year of the lynching
Mo: Month
Day: Day
Victim: Name of the victim
County: County where the lynching occurred (keep in mind that county names have changed & boundaries redrawn)
Race: Race of the victim
Sex: Sex of the victim
Mob: Information on the mob
Offense: Victim’s alleged offense
Note: Note (if any)
2nd Name: Name of the 2nd victim (if any)
3rd Name: Name of the 3rd victim (if any)
Comments: Comments (if any)
Source: Source of the information (if any)
Acknowledgements:
This dataset was compiled by Dr. Elizabeth Hines and Dr. Eliza Steelwater. If you use this dataset in your work, please include the following citation:
Hines, E., & Steelwater, E. (2006). Project Hal: Historical American Lynching Data Collection Project. University of North Carolina, http://people.uncw.edu/hinese/HAL/HAL%20Web%20Page.htm
You may also like:
Bryan Stevenson’s Equal Justice Initiative, EJI posts lynching information and stories and is currently quite active: https://www.eji.org/
First Person Narratives of the American South: Personal accounts of Southern life between 1860 and 1920
Inspiration:
Can you use the county-level data in this dataset to create a map of lynchings in the US?
What demographic qualities were most associated with lynching victims?
How did patterns of lynching change over time?"
Twitter vs. Newsletter Impact,Which format is best for getting the word out?,Rachael Tatman,6,"Version 2,2017-09-19|Version 1,2017-09-07","marketing
internet",CSV,3 KB,CC4,"1,675 views",173 downloads,,0 topics,https://www.kaggle.com/rtatman/twitter-vs-newsletter,"Context:
There are lots of really cool datasets getting added to Kaggle every day, and as part of my job I want to help people find them. I’ve been tweeting about datasets on my personal Twitter accounts @rctatman and also releasing a weekly newsletter of interesting datasets.
I wanted to know which method was more effective at getting the word out about new datasets: Twitter or the newsletter?
Content:
This dataset contains two .csv files. One has information on the impact of tweets with links to datasets, while the other has information on the impact of the newsletter.
Twitter:
The Twitter .csv has the following information:
month: The month of the tweet (1-12)
day: The day of the tweet (1-31)
hour: The hour of the tweet (1-24)
impressions: The number of impressions the tweet got
engagement: The number of total engagements
clicks: The number of URL clicks
Fridata Newsletter:
The Fridata .csv has the following information:
date: The Date the newsletter was sent out
month: The Month the newsletter was sent out (1-12)
day: The day the newsletter was sent out (1-31)
# of dataset links: How many links were in the newsletter
recipients: How many people received the email with the newsletter
total opens: How many times the newsletter was opened
unique opens: How many individuals opened the newsletter
total clicks: The total number of clicks on the newsletter
unique clicks: (unsure; provided by Tinyletter)
notes: notes on the newsletter
Acknowledgements:
This dataset was collected by the uploader, Rachael Tatman. It is released here under a CC-BY-SA license.
Inspiration:
Which format receives more views?
Which format receives more clicks?
Which receives more clicks/view?
What’s the best time of day to send a tweet?"
World Color Survey,Speakers of Unwritten Languages Name Color Chips,Jacob Boysen,6,"Version 1,2017-08-24",linguistics,Other,12 MB,Other,"1,598 views",129 downloads,,0 topics,https://www.kaggle.com/jboysen/color-survey,"Context:
In the WCS investigation, an average of 24 native speakers of each of 110 unwritten languages were asked
to name each of 330 Munsell chips, shown in a constant, random order,
exposed to a palette of these chips and asked to to pick out the best example(s) (""foci"") of the major terms elicited in the naming task.
See the original WCS Instructions to Fieldworkers for further information on the data elicitation method. The files in this archive display the results of that investigation.
Content:
Demographics on speakers, color chip coordinates, terms used, and the WCS and the Munsell coordinates, as well as the CIEL*a*b* coordinates. Further details and resources at source.
Acknowledgements:
This material is based upon work supported by the National Science Foundation under Grant No. 0130420. Richard Cook1, Paul Kay2, and Terry Regier3
University of California at Berkeley
International Computer Science Institute, Berkeley, CA
University of Chicago
In any published work based on these data, please cite these archives. URL: http://www.icsi.berkeley.edu/wcs/data.html"
London Fire Brigade Calls,32k Calls to London Fire Brigade,Jacob Boysen,6,"Version 1,2017-09-01","government agencies
government",CSV,11 MB,CC0,"1,380 views",146 downloads,,0 topics,https://www.kaggle.com/jboysen/london-fire,"Context:
London's fire and rescue service is the busiest in England and one of the largest firefighting and rescue organisations in the world. In the aftermath of the Grenfell Tower fire, it is critical that firefighting resources are accurately and appropriately deployed.
Content:
This data covers Jan 01-April 30 2017, consisting of 32 columns containing information on time, type, and address of call, as well the home station, stay duration, and arrival time of attending pumps.
Acknowledgements:
This dataset was compiled by the City of London. You can use Kernels to analyze, share, and discuss this data on Kaggle, but if you’re looking for real-time updates and bigger data, check out the data on BigQuery, too.
Inspiration:
Which boroughs have the shortest average call response? Longest?
Which boroughs have the greatest volume of calls?"
UK fleet and foreign fleet landings by port,Data from 2008-2015 on fishing vessels either from the UK or landing in the UK,The Flying Munkey,6,"Version 4,2017-08-24|Version 3,2017-08-24|Version 2,2017-08-24|Version 1,2017-08-24","government agencies
fishing
business",CSV,81 MB,Other,827 views,55 downloads,,0 topics,https://www.kaggle.com/theflyingmunkey/uk-fleet-landings,"Context
Data taken from the Marine Management Organisation on all UK vessels landing in ports, or foreign vessels landing in UK ports. This dataset contains data on the catch, also the weight and value (£) of the catch.
Content
Year year of landing, 2008-2015
Month calendar months, 1-12
Port of Landing name of port
Port Nationality nationality of the port of landing
Vessel Nationality nationality of the vessel
Length Group length of the vessel, 10m or under/Over 10m
Gear Category gear carried by the vessel
Species Code three letter code of the catch
Species Name name of the catch
Species as shown in publication name of the catch with fewer subcategories
Species Group catch species
Live Weight (tonnes) weight of live catch
Landed Weight (tonnes) landed weight of catch
Value (£) the value of the catch in GBP, most likely without any inflation adjustment
Points to note
Data on Port Nationality and Vessel Nationality for 2008 were supplied as 3 letter codes, not all of which matched standard country codes. I've tried to clean these up as best I can to match with standard country codes but ones that couldn't be matched have been left as-is.
Species Code and Species Name were not supplied for 2008. You may be able to infer some from the 2009-2015 data.
Inspiration
Which UK ports see the greatest activity?
Which foreign ports see the greatest number of UK vessels?
Has the price-paid per tonne of goods varied during the data collection period?
Has there been any major changes in activity for particular species?
Acknowledgements
Data taken from the Office of National Statistics and is part of the UK Sea Fisheries Annual Statistics. Data are available under a Open Government Licence v3.0."
NASA Facilities,A dataset of NASA facility names and locations,NASA,6,"Version 2,2017-08-23|Version 1,2017-08-23","space
organizations
spaceflight",CSV,101 KB,CC0,899 views,66 downloads,2 kernels,0 topics,https://www.kaggle.com/nasa/nasa-facilities,"Context
NASA has something like 400 different facilities across the United States! This dataset is a collection of those facilities and their locations.
Content
Center: Name of the ""Center"", a collection facilities
Center Search Status: Public or...?
Facility: Name of the facility
FacilityURL
Occupied
Status
URL Link
Record Date
Last Update
Country
Location
City
State
Zipcode
Acknowledgements
This dataset was downloaded from https://data.nasa.gov/Management-Operations/NASA-Facilities/gvk9-iz74. The original file was modified to remove contact information for each facility."
Space walking,A record of Russian and U.S. extra-vehicular actvity,NASA,6,"Version 3,2017-08-28|Version 2,2017-08-23|Version 1,2017-08-22","space
astronauts
aerospace engineering
spaceflight",CSV,92 KB,CC0,"1,189 views",89 downloads,2 kernels,,https://www.kaggle.com/nasa/space-walking-russian-and-us-evas,"Context
Extra-vehicular activities are activities done by an astronaut or cosmonaut outside a spacecraft beyond the Earth's appreciable atmosphere. I like to just call it space walking :) It's unclear if this is a complete record of spacewalks. So keep that in mind.
Content
EVA #
Country
Crew: Crew members, separated with |
Vehicle: Space craft, space ship, space station, etc. If multiple vehicles, they are separated with |
Date
Duration
Purpose: Description of the EVA. Some of these have internal commas and are enclosed with double quotes ("")
Acknowledgements
These data were collected from here
The original CSV was modified slightly to remove extra spaces"
The National Summary of Meats,USDA's data on beef and mutton production since the 1930s,US Department of Agriculture,6,"Version 1,2017-08-23","government
agriculture",CSV,63 KB,Other,"1,019 views",94 downloads,,0 topics,https://www.kaggle.com/usda/the-national-summary-of-meats,"Beef. Lamb. Veal. We might not all eat them, but they are the meats whose grades the US Department of Agriculture has seen fit to publish. This dataset contains records on meat production and quality as far back as 1930.
After meat and poultry are inspected for wholesomeness, producers and processors may request that they have products graded for quality by a licensed Federal grader. The USDA's Agricultural Marketing Service (http://www.ams.usda.gov) is the agency responsible for grading meat and poultry. Those who request grading must pay for the service. Grading for quality means the evaluation of traits related to tenderness, juiciness, and flavor of meat; and, for poultry, a normal shape that is fully fleshed and meaty and free of defects.
USDA grades are based on nationally uniform Federal standards of quality. No matter where or when a consumer purchases graded meat or poultry, it must have met the same grade criteria. The grade is stamped on the carcass or side of beef and is usually not visible on retail cuts. However, retail packages of beef, as well as poultry, will show the U.S. grade mark if they have been officially graded.
To better understand the available fields: - All fields labeled 'pounds' are really in units of indicate millions of pounds. - You can find a helpful explanation of what the different grades mean here.
Acknowledgements
This data was kindly released by the US Department of Agriculture. You can find their most recent meat updates here.
Inspiration
This is a good dataset for anyone looking to do basic data cleanup. I've converted it into a properly formed CSV, but there are still numerous missing values, footnoted fields, and exceptions.
This is a good candidate for regression analysis, especially in conjunction with other datasets. Can you identify correlates for the amount of beef produced? Validate how well cattle futures predict annual yields?
2015 was a banner year for beef production. What happened?"
BRFSS 2001-2010,Behavioral Risk Factor Surveillance System for 2001-2010,Centers for Disease Control and Prevention,6,"Version 1,2017-08-24","mental health
public health",CSV,4 GB,CC0,"1,301 views",214 downloads,,0 topics,https://www.kaggle.com/cdc/brfss-20012010,"The objective of the BRFSS is to collect uniform, state-specific data on preventive health practices and risk behaviors that are linked to chronic diseases, injuries, and preventable infectious diseases in the adult population. Factors assessed by the BRFSS include tobacco use, health care coverage, HIV/AIDS knowledge or prevention, physical activity, and fruit and vegetable consumption. Data are collected from a random sample of adults (one per household) through a telephone survey.
The Behavioral Risk Factor Surveillance System (BRFSS) is the nation's premier system of health-related telephone surveys that collect state data about U.S. residents regarding their health-related risk behaviors, chronic health conditions, and use of preventive services. Established in 1984 with 15 states, BRFSS now collects data in all 50 states as well as the District of Columbia and three U.S. territories. BRFSS completes more than 400,000 adult interviews each year, making it the largest continuously conducted health survey system in the world.
Content
Each year contains a few hundred columns. Please see one of the annual code books for complete details.
These CSV files were converted from a SAS data format using pandas; there may be some data artifacts as a result.
If you like this data, you might also enjoy the 2011-2015 batch. Please note that those years use a different format.
Acknowledgements
This dataset was released by the CDC. You can find the original dataset, manuals, and additional years of data here."
Federal Firearm Licences,Active firearm sales licenses in the United States,Department of Justice,6,"Version 3,2017-09-15|Version 2,2017-09-15|Version 1,2017-09-15",government,CSV,11 MB,CC0,"1,182 views",110 downloads,2 kernels,,https://www.kaggle.com/doj/federal-firearm-licensees,"Context
Firearms sold in the United States must be licensed by the US Department of Justice Bureau of Alcohol, Tobacco, Firearms and Explosives. This dataset is a record of every firearm license which was still current as of July 2017.
Content
This dataset contains the names, license types, expiration dates, and locations of all Federal Firearms License (FFL) holders in the United States. The possible license types are:
01 Dealer in Firearms Other Than Destructive Devices (Includes Gunsmiths)
02 Pawnbroker in Firearms Other Than Destructive Devices
03 Collector of Curios and Relics
06 Manufacturer of Ammunition for Firearms
07 Manufacturer of Firearms Other Than Destructive Devices
08 Importer of Firearms Other Than Destructive Devices
09 Dealer in Destructive Devices
10 Manufacturer of Destructive Devices
11 Importer of Destructive Devices
Acknowledgements
This data is published online in a tab-separated format by the Department of Justice Bureau of Alcohol, Tobacco, Firearms, and Explosives. It has been lightly retouched into a CSV file before publication here.
Inspiration
Can you geocode this data to determine where licensed gun shops are distributed?
What is the distribution of gun licenses across different types?"
United States Code,The general and permanent laws of the United States,U.S. Government Publishing Office,6,"Version 1,2017-09-06","government
law",Other,591 MB,CC0,731 views,53 downloads,,0 topics,https://www.kaggle.com/us-gpo/united-states-code,"The United States Code (""Code"") contains the general and permanent laws of the United States, arranged into 54 broad titles according to subject matter. The organization of the Code was originally established by Congress in 1926 with the enactment of the act of June 30, 1926, chapter 712. Since then, 27 of the titles, referred to as positive law titles, have been restated and enacted into law by Congress as titles of the Code. The remaining titles, referred to as non-positive law titles, are made up of sections from many acts of Congress that were either included in the original Code or subsequently added by the editors of the Code, i.e., the Office of the Law Revision Counsel, and its predecessors in the House of Representatives. Positive law titles are identified by an asterisk on the Search & Browse page. For an explanation of the meaning of positive law, see the Positive Law Codification page.
Each title of the Code is subdivided into a combination of smaller units such as subtitles, chapters, subchapters, parts, subparts, and sections, not necessarily in that order. Sections are often subdivided into a combination of smaller units such as subsections, paragraphs, subparagraphs, clauses, subclauses, and items. In the case of a positive law title, the units are determined by Congress in the laws that enact and later amend the title. In the case of a non-positive law title, the organization of the title since 1926 has been determined by the editors of the Code and has generally followed the organization of the underlying acts 1 as much as possible. For example, chapter 7 of title 42 sets out the titles, parts, and sections of the Social Security Act as corresponding subchapters, parts, and sections of the chapter.
In addition to the sections themselves, the Code includes statutory provisions set out as statutory notes, the Constitution, several sets of Federal court rules, and certain Presidential documents, such as Executive orders, determinations, notices, and proclamations, that implement or relate to statutory provisions in the Code. The Code does not include treaties, agency regulations, State or District of Columbia laws, or most acts that are temporary or special, such as those that appropriate money for specific years or that apply to only a limited number of people or a specific place. For an explanation of the process of determining which new acts are included in the Code, see the About Classification page.
The Code also contains editorially created source credits, notes, and tables that provide information about the source of Code sections, their arrangement, the references they contain, and their history.
The law contained in the Code is the product of over 200 years of legislating. Drafting styles have changed over the years, and the resulting differences in laws are reflected in the Code. Similarly, Code editorial styles and policies have evolved over the 80-plus years since the Code was first adopted. As a result, not all acts have been handled in a consistent manner in the Code over time. This guide explains the editorial styles and policies currently used to produce the Code, but the reader should be aware that some things may have been done differently in the past. However, despite the evolution of style over the years, the accuracy of the information presented in the Code has always been, and will always remain, a top priority.
Content
This dataset is a snapshot of the XML version of the United States Code. It is not a suitable for any form of legal work and is intended for research purposes only.
The data are stored in a large json dictionary, indexed by the title of the code.
Acknowledgements
This dataset was released by the United States Government Publishing Office. You can find the original dataset here."
NYC Filming Permits,Information on ~40k Filming Locations,City of New York,6,"Version 1,2017-09-20","government agencies
visual arts",CSV,13 MB,CC0,681 views,49 downloads,,0 topics,https://www.kaggle.com/new-york-city/nyc-filming-permits,"Context:
Dataset is a list of film and television permits received from the Mayor's Office of Media and Entertainment in response to a series of FOIL requests in 2015. The permits stretch from October 2011 through September 2015.
Content:
ProjectTitle: The title of the film/television project.
EventName: A shorthand name for the specific shoot/event being permitted, e.g. SunsetPark-010815.
EventType: One of the following: Scouting Permit, Rigging Permit, Shooting Permit, Film Shoot / Production, DCAS Prep/Shoot/Wrap Permit, Grid Request, or Red Carpet Premiere. According to the MOME, Shooting Permit and Film Shoot / Production are interchangeable.
EventStartDate and EventEndDate: The start and end date and time of the permit.
Location: One or more locations covered by the permit.
Boro: What borough the listed locations are in.
ProjectId: An internal identifier.
CategoryName: Film or Television.
SubCategoryName: Includes values such as Pilot, Student Film, Variety, Reality, etc. This probably isn't a reliable classification for TV shows: it's chosen by the permit applicant on the online form and is not vetted by the Mayor's Office. It also includes overlapping subcategories. For example, a show could be both a Morning Show and a Talk Show but would have to choose one or the other.
CompanyName: The supplied production company name, which can be useful in connecting a working title to an actual film/show.
The project title is sometimes a variation on the actual title (e.g. Mozart in the Jungle S1 or The Wolf of Wall Street ReShoots) or a working title (e.g. Untitled Female Buddy Cop Movie instead of The Heat, St James Place instead of Bridge of Spies).
In some cases, the locations listed actually span multiple boroughs, and the Boro field only represents the primary borough, or the borough of the first listed location. In some cases, the Boro field is blank.
A given shooting permit can have any number of locations listed for a single day. According to the guidelines, the locations are supposed to be listed in the order they're used on that day. Most locations are either an address or a range of blocks in the format of STREET 1 between STREET 2 and STREET 3.
Permits are generally required when asserting the exclusive use of city property, like a sidewalk, a street, or a park. A shooting permit on a street doesn't necessarily mean there is exterior shooting on the street. It may just mean, for example, that something is being shot indoors and the crew needs special parking privileges for trucks. See ""When a Permit is Required"".
Shooting on Department of Citywide Administrative Services (DCAS) property, like in a city courthouse, involves an additional permitting process.
Shooting on MTA property or on state/federal property is subject to a different permitting process.
A shooting permit is typically, but not always, for a single day or a single overnight period.
Acknowledgements:
Data was FOIL’d by WNYC Data Journalism team and hosted originally on GitHub here. Check out these great related resources:
General MOME Permit Info
The Made in NY Location Library
DCAS Managed Public Buildings
Metrocosm's NYC Film Permits Map
2015 BCG Report on Media and Entertainment in NYC
Inspiration:
Where do most films occur in the city?
When is the most common filming time?
Who films the most in the city?"
2016 VOTER Survey Data Set,"Unique Longitudinal Data Set on ~8k Voters, 2011-16",Democracy Fund,6,"Version 1,2017-09-23",,Other,60 MB,CC4,"1,704 views",254 downloads,,,https://www.kaggle.com/democracy-fund/2016-voter-survey,"Context
The Democracy Fund Voter Study Group is using a unique longitudinal data set that most recently surveyed 8,000 adults (age 18+) in December 2016 via YouGov. Participants were identified from a pool of respondents who participated in a similar survey in December 2011, as well as a second pre-election interview in 2012, and a third interview following the 2012 presidential election. For these 8,000 respondents, we have measures of their political attitudes, values, and affinities in 2011 as well as self-reports of their turnout and vote choice in November 2012.
Content
The VOTER (Views of the Electorate Research) Survey was conducted by the survey firm YouGov. In total, 8,000 adults (age 18+) with internet access took the survey online between November 29 and December 29, 2016. The estimated margin of error is plus or minus 2.2 percent. YouGov also supplied measures of primary voting behavior from the end of the primary period (July 2016), when these respondents had been contacted as part of a different survey project.
These respondents were originally interviewed by YouGov in 2011 to 2012 as part of the 2012 Cooperative Campaign Analysis Project (CCAP). In that survey, 45,000 respondents were first interviewed in December 2011 and were interviewed a second time in one of the 45 weekly surveys between January 1 and November 8, 2012. After the November election, 35,408 respondents were interviewed a third time. We invited 11,168 panelists from the 2012 CCAP. Of those invited, 8,637 (77 percent) completed the 2016 VOTER Survey.
The 2012 CCAP was constructed using YouGov’s sample matching procedure. A stratified sample is drawn from YouGov’s panel, which consists of people who have agreed to take occasional surveys. The strata are defined by the combination of age, gender, race, and education, and each stratum is sampled in proportion to its size in the U.S. population. Then, each element of this sample is matched to a synthetic sampling frame that is constructed from the U.S. Census Bureau’s American Community Survey, the Current Population Survey Voting and Registration Supplement, and other databases. The matching procedure finds the observation in the sample from YouGov’s panel that most closely matches each observation in the synthetic sampling frame on a set of demographic characteristics. The resulting sample is then weighted by a set of demographic and non-demographic variables.
Information on variables can be found in the ""Guide to the 2016 Voter Survey"" and included in the dataset.
Acknowledgements
The Democracy Fund, a charitable foundation committed to the protection and enhancement of democratic values, found the rise of these movements and candidates worthy of analysis. In May 2016, the Democracy Fund chose to begin a rigorous project that would supply hard data to test proposed theories to explain these phenomenon. Working with Henry Olsen of the Ethics and Public Policy Center and John Sides of George Washington University, the Democracy Fund assembled a diverse group of scholars and analysts representing political viewpoints from all angles.
Additional information on participants can be found here.
Data was originally published here.
To reference the VOTER survey, please use this protocol: Democracy Fund Voter Study Group. VIEWS OF THE ELECTORATE RESEARCH SURVEY, December 2016. [Computer File] Release 1: August 28, 2017. Washington DC: Democracy Fund Voter Study Group [producer] https://www.voterstudygroup.org/.
Inspiration
The Democracy Fund Voter Study Group has made available a series of insights on the dataset--read them for further data inspiration:
Executive Summary
Political Divisions in 2016 and Beyond
Race, Religion, and Immigration in 2016
The Story of Trump's Appeal
The Five Types of Trump Voters
Methodology
Read the latest news and updates from (and about) the Democracy Fund Voter Study Group."
General Election Results,"November 8, 2016 General Election for the State of Arizona",Arizona Secretary of State,6,"Version 3,2017-09-23|Version 2,2017-09-23|Version 1,2017-09-22",politics,CSV,65 MB,CC0,"1,351 views",102 downloads,,,https://www.kaggle.com/arizonaSecofState/2016-statewide-general-election-results,"Context
The 2016 Statewide General Election results for Arizona.
Arizona's 15 counties are required by statute to publish tabulated General Election results by precinct. This file represents a standardized and aggregated version of all 15 files. Please note that while the file is mostly standardized, many of the attributes are relatable accross counties via a fuzzy match (the keyword ""Congress"" etc...).
Content
County: Abbreviation of Arizona's 15 counties
AP: Apache
CH: Cochise
CN: Coconino
GI: Gila
GM: Graham
GN: Greenlee
LP: La Paz
MC: Maricopa
MO: Mohave
NA: Navajo
PM: Pima
PN: Pinal
SC: Santa Cruz
YA: Yavapai
YU: Yuma
PrecinctID: Precinct identification number designated by the counties. County shorthand has been added.
PrecinctName: Precinct Name designated by the counties. This is directly passed from the tabulation files.
ContestID: Contest Identification number designated by the counties. This is directly passed from the tabulation files and may not be standardized across counties.
ContestTitle: Title of race as designated by counties. This is directly passed from the tabulation files and may not be standardized across counties.
CandidateID: Candidate identification number designated by the counties. This is directly passed form the tabulation files and may not be standardized across counties.
CandidateName: Name of Candidate as desingated by the counties. This is directly passed form the tabulation files and may not be standardized across counties.
TotalVotes: Vote Total aggregated from the attributes ""PollVotes, EarlyVotes, Provisionals, LatePoll, LateEarly"".
PollVotes: Total votes tabulated at a designated precinct location on election day.
EarlyVotes: Total votes tabulated by the counties during the 29 day early voting period.
Provisionals: Total votes tabulated at a designated precinct location that were deemed acceptable provisional ballots. 12: LateEarly: Total votes tabulated by the counties of early votes that were dropped off at designated polling locations rather than received in the mail. (Note: only a few counties separated this number from EarlyVote in their tabulation files).
Registered: The number of registered voters at the time of the election in each designated precinct.
Undervote: The number of ballots that did note cast the allowed number of votes for any given race. (Example: voters are allowed to ""vote for 2"" in the Arizona House of Representatives race, in this case these ballots were either left blank or only voted for 1)
ContestTotal: Total votes cast in a given contest.
CandidateParty: Party of candidate in a given contest.
REP: Republican
DEM: Democrat
NP: No Party
GRN: Green
LBT: Libertarian
TotalTurnout: Total turnout for a designated precinct.
EDTurnout: Total turnout for a designated precinct on election day.
EarlyTurnout: Total turnout for a designated precinct during the 29 day early voting period. (Note, this number will include early ballots dropped off at the designated polling location.)
Final Note: There are certain records in the file that are not part of any contest. They are normally designated by a contest ID that begins with a ""999"" These are records that the tabulators append to every file to provide background on each of the designated precincts."
Austin Weather,"Historical temperature, precipitation, humidity, and windspeed for Austin, Texas",GrubenM,6,"Version 3,2017-08-15|Version 2,2017-08-15|Version 1,2017-08-13",,CSV,103 KB,GPL,"1,496 views",245 downloads,8 kernels,,https://www.kaggle.com/grubenm/austin-weather,"Context
This dataset is meant to complement the Austin Bikesharing Dataset.
Content
Contains the:
Date (YYYY-MM-DD)
TempHighF (High temperature, in Fahrenheit)
TempAvgF (Average temperature, in Fahrenheit)
TempLowF (Low temperature, in Fahrenheit)
DewPointHighF (High dew point, in Fahrenheit)
DewPointAvgF (Average dew point, in Fahrenheit)
DewPointLowF (Low dew point, in Fahrenheit)
HumidityHighPercent (High humidity, as a percentage)
HumidityAvgPercent (Average humidity, as a percentage)
HumidityLowPercent (Low humidity, as a percentage)
SeaLevelPressureHighInches (High sea level pressure, in inches)
SeaLevelPressureAvgInches (Average sea level pressure, in inches)
SeaLevelPressureLowInches (Low sea level pressure, in inches)
VisibilityHighMiles (High visibility, in miles)
VisibilityAvgMiles (Average visibility, in miles)
VisibilityLowMiles (Low visibility, in miles)
WindHighMPH (High wind speed, in miles per hour)
WindAvgMPH (Average wind speed, in miles per hour)
WindGustMPH (Highest wind speed gust, in miles per hour)
PrecipitationSumInches (Total precipitation, in inches) ('T' if Trace)
Events (Adverse weather events. ' ' if None)
This dataset contains data for every date from 2013-12-21 to 2017-07-31.
Acknowledgements
This dataset was obtained from WeatherUnderground.com, at the Austin KATT station.
Inspiration
Can we use this dataset to explain some of the variation in the Austin Bikesharing Dataset?"
South Africa Stock Market Data,"Price, financials and rates",NeilS,6,"Version 6,2017-07-02|Version 5,2017-07-02|Version 4,2017-06-26|Version 3,2017-06-26|Version 2,2017-06-26|Version 1,2017-06-26",finance,CSV,4 MB,CC0,"2,702 views",321 downloads,4 kernels,,https://www.kaggle.com/neilslab/south-africa-stock-market-data,"South African Stock Price Predictions
Welcome to SA Stock Market Data :)
The dataset contains information for the largest 34 companies in South Africa by market cap as well as data for the SA40 Futures.
The Price data (P.csv) is in the format: Close, Open, High, Low, Vol, Change
The Financials data (F.csv) is in the format: Revenue, Cost of Sales, Gross profit, Net Profit, Issue of shares, Share repurchase, Non-current assets, Current assets,
Non-current liabilities, Current liabilities, Net cash inflow/outflow from operating activities
Interest rate data (R.csv) has a single column: rates
Symbol reference:
BTIJ = British American Tobacco PLC
BILJ = BHP Billiton PLC
BGAJ = Barclays Africa Group Ltd
CFRJ = Compagnie Financiere Richemont SA DRC
CCOJ = Capital & Counties Properties PLC
AGLJ = Anglo American PLC
MTNJ = MTN Group Ltd
NPNJn = Naspers Ltd
SOLJ = Sasol Ltd
SBKJ = Standard Bank Group Ltd
VODJ = Vodacom Group Ltd
KIOJ = Kumba Iron Ore Ltd
FSRJ = Firstrand Ltd
OMLJ = Old Mutual PLC
SLMJ = Sanlam Ltd
SHPJ = Shoprite Holdings Ltd
REMJ = Remgro Ltd
NEDJ = Nedbank Group Ltd
APNJ = Aspen Pharmacare Holdings Ltd
BVTJ = The Bidvest Group Ltd
ANGJ = Anglogold Ashanti Ltd
IMPJ = Impala Platinum Holdings Ltd
WHLJ = Woolworths Holdings Ltd
TBSJ = Tiger Brands Ltd
EXXJ = Exxaro Resources Ltd
RMHJ = RMB Holdings Ltd
ITUJ = Intu Properties PLC
GRTJ = Growthpoint Properties Ltd
MNDJ = Mondi Ltd
SNHJ = Steinhoff International Holdings Ltd
INPJ = Investec PLC
LHCJ = Life Healthcare
REIJ = Reinet
DSYJ = Discovery Holdings Ltd
IPLJ = Imperial Holdings Ltd
ARIJ = African Rainbow Minerals Ltd
SA = FTSE/JSE Top 40 Futures
If you are interested in how I put everything together to build a single model you can look at the following Github link, please note this contains a lot of copy-pasta and needs to be simplified with some loops and functions before I put it on Kaggle: https://github.com/Nlabbert/SA-Stock-Market"
NBA 16-17 regular season shot log,Shot log data grouped by NBA teams for the 16-17 regular season,wh0801,6,"Version 1,2017-06-03",,Other,19 MB,Other,"1,156 views",150 downloads,2 kernels,3 topics,https://www.kaggle.com/wh0801/NBA-16-17-regular-season-shot-log,"This is the shot log for NBA in 16-17 regular season, grouped by teams. Inside the dataset includes the shot type, shot distance, shot angle, shot player, shot time, team name, etc. This data is scraped down from https://www.mysportsfeeds.com/
I came across these two great articles which discusses about the shooting rationality of individual NBA players: https://www.kaggle.com/selfishgene/kobe-bryant-shot-selection/psychology-of-a-professional-athlete https://www.kaggle.com/drgilermo/irrational-shot-selection The basic idea is that if an NBA player makes one shot, the next shot he takes tends to be more difficult and further away from the basket. So I was wondering, will this be the case for a team? If a team makes one shot, will they take a further and more difficult shot for the next one? Individuals tend to be less rational after making one shot, but how about a team as a group? Will the decision making from a group compensate for the irrationality of individuals?
I wrote a blog about my analysis here: https://haowang204.wordpress.com/2017/06/03/shooting-rationality-of-nba-teams/
The python code is uploaded to: https://github.com/wh0801/NBA-shooting-rationality-2016-17-Regular-Season Let me know your thoughts!"
"Primetime Emmy Awards, 1949-2017",Which television show has won the most Emmy Awards?,Paul Magda,6,"Version 2,2017-09-19|Version 1,2017-09-14","actors
entertainment
telecommunications",CSV,2 MB,CC0,"1,265 views",118 downloads,,,https://www.kaggle.com/pmagda/primetime-emmy-awards,"Context
We are working on a project relating to predicting and voting for Academy Award. With the Primetime Emmy Awards coming up this week, I thought it would be interesting to see if I could integrated those. I couldn't find too many well organized datasets relating to those awards. I decided to spend the afternoon and build my own. We probably won't use this information this year, but it might be something we could use in the future.
Content
I created a simple web parser in Go, and parsed the data from the Emmy Awards Website. The data is a representation of Primetime Emmy Nominees from the first Emmy Awards (1949)... to the current ones that will air Sunday September 17th, 2017. After this date, the winner will have to be updated. In work we've done with Academy Awards, we used movie title and name as the main structure for the data. I kind of felt this was a little inconsistent as certain awards focus on one or the other. With the Emmy Nominees, I made it more general with nominee and additional detail, I believe this will make the data more consistent and easier to manipulate.
Acknowledgements
I based the structure of the data from the Kaggle dataset of the Academy Awards . I would also like to acknowledge the Academy of Television Arts & Sciences for providing the data on their website.
Inspiration
Who won the most Emmys for Outstanding Comedy Series? I think it would be cool, if we could answer: Who will win the Emmy for Outstanding Comedy Series in 2018? But, I think we more than just historical data."
New Zealand Migration,Migration numbers to and from New Zealand from 1979 to 2016,Timo Bozsolik,6,"Version 1,2017-06-07","countries
demographics",CSV,4 MB,Other,"2,278 views",257 downloads,24 kernels,3 topics,https://www.kaggle.com/timoboz/migration-nz,"Context
*This dataset shows the migration to and from New Zealand by country and citizenship from 1979 to 2016. *
Content
The columns in this dataset are:
Measure: The signal type given in this row, one of: ""Arrivals"", ""Departures"", ""Net""
Country: Country from where people arrived into to New Zealand (for Measure = ""Arrivals"") or to where they left (for Measure = ""Departures""). Contains special values ""Not Stated"" and ""All countries"" (grand total)
Citizenship: Citizenship of the migrants, one of: ""New Zealand Citizen"", ""Australian Citizen"", ""Total All Citizenships""
Year: Year of the measurement
Value: Number of migrants
Permanent and long-term arrivals include overseas migrants who arrive in New Zealand intending to stay for a period of 12 months or more (or permanently), plus New Zealand residents returning after an absence of 12 months or more. Permanent and long-term departures include New Zealand residents departing for an intended period of 12 months or more (or permanently), plus overseas visitors departing New Zealand after a stay of 12 months or more. For arrival series, the country of residence is the country where a person arriving in New Zealand last lived for 12 months or more (country of last permanent residence). For departure series, the country of residence is the country where a person departing New Zealand intends to live for the next 12 months or more (country of next permanent residence).
Acknowledgements
Curated data by figure.nz, original data from Stats NZ. Dataset licensed under Creative Commons 4.0 - CC BY 4.0.
Inspiration
A good challenge would be to explain New Zealand migration flows as a function of the economic performance of New Zealand or other countries (combine with other datasets). The data could be possibly linked up with other data sources to predict general migration to/from countries based on external factors."
Nominal € GDP per capita of Spain (by regions),Series from the years 2000 to 2016,XavierMartinezBartra,6,"Version 5,2017-08-10|Version 4,2017-08-04|Version 3,2017-08-02|Version 2,2017-08-01|Version 1,2017-08-01","business
social sciences
economics",CSV,2 KB,CC0,"3,487 views",126 downloads,2 kernels,,https://www.kaggle.com/xavier14/nominal-gdp-per-capita-of-spain-by-regions,"Context
Some countries have a very divergent GDP per capita between its regions. Sometimes a given country's regions tend to converge over time, while in other cases the disparity between the poorer and the richer regions is kept over the decades. In this DataSet we can examine the Spanish case. Which has been the evolution of the nominal GDP per capita by regions in Spain since the year 2000 ? Have the regions converged ? Which is the spread between regions ? Can we make a cluster analysis of the regions ?
Content
We have a DataFrame of the evolution of the nominal GDP per capita across the 19 Spanish regions (autonomous communities & cities) since 2000 to 2016.
** Acknowledgements **
The Data used has been compiled by the INE (spanish institute of statistics). http://www.ine.es/"
Paranormal Romance Novel Titles,4000 paranormal romance novel titles,Rachael Tatman,6,"Version 1,2017-07-18",literature,Other,91 KB,CC4,"1,004 views",46 downloads,3 kernels,0 topics,https://www.kaggle.com/rtatman/paranormal-romance-novel-titles,"Context:
Paranormal Romance is a subgenre of romance that combines fantasy and romance elements. Notable examples are the Twilight series and The Southern Vampire Mysteries, which the T.V. show True Blood was based on.
Content:
This is a list of 4000 paranormal romance novel titles, scraped from the web by Mark Riedl. Some longer titles have been truncated, and end with ellipses (...).
Inspiration:
Can you generate new titles using a Markov chain text generator?
Some novel titles are truncated. Can you generate the complete version?
Can you cluster novels into sub genres based on their titles?"
Colonia Corpus of Historical Portuguese,A 5.1 million word corpus of historical Portuguese,Rachael Tatman,6,"Version 1,2017-07-29","languages
literature
brazil
+ 2 more...",CSV,76 MB,CC0,601 views,37 downloads,,0 topics,https://www.kaggle.com/rtatman/colonia-corpus-of-historical-portuguese,"Context:
Portuguese is a romance language that is the native language of over 215 million speakers worldwide. Like Spanish, English and French, it was the language of both its country of origin and also that country’s colonial possessions. This corpus contains examples of historical Portuguese written between 1500 and 1936, both in Portugal and Brazil.
Content:
The corpus contains complete Portuguese manuscripts published from 1500 to 1936 divided into 5 sub-corpora per century (summarized in the table below). The part of speech (POS) of words in this corpus was tagged using TreeTagger. You can find more information on this corpus on the Colonia homepage.
Century Texts Tokens 16th 13 399,245 17th 18 709,646 18th 14 425,624 19th 38 2,490,771 20th 17 1,132,696 Total 100 5,157,982
Texts are balanced in terms of the variety, consisting of 48 European Portuguese texts and 52 Brazilian Portuguese texts. You can find more information in the paper that describes the corpus. The complete inventory of texts is here and more detail regarding annotation can be found here.
Part of Speech (POS) Tags
The works in this corpus have been automatically tagged for their part of speech (POS). The tagset used to annotate the corpus is presented in the table below. It contains not only the classic POS tags (e.g. V, DET, N) but also a couple of compound tags, such as the combination of preposition plus determiner as (PREP+DET) or verb plus pronoun (V+P). The tool used to annotate the corpus was TreeTagger.
Category POS Example Adjective ADJ bonita Adverb ADV muita Determiner DET os Cardinal CARD primeiro Noun NOM mesa Pronoun P eles Preposition PRP de Verb V fazer Interjection I Oh! Commas VIRG , Punctuation SENT .
Studies report that TreeTagger achieves performance higher than 95% accuracy in attributing the correct POS tag and lemma of a token.
Acknowledgements:
If you use this corpus in your work, please cite this paper:
Zampieri, M. and Becker, M. (2013) Colonia: Corpus of Historical Portuguese. In: ZSM Studien, Special Volume on Non-Standard Data Sources in Corpus-Based Research. Volume 5. Shaker.
Inspiration:
What changes have occurred in Portuguese over time? Have words changed? Syntactic structures? How grammatical agreement is expressed?
Can you create a classifier which can classify the era and unseen work is from?
Using the part of speech tags in this tagger, can you train a new tagger and run it over the Brazilian Portuguese Literature Corpus linked below?
You may also like:
A 3.7 million word literary corpus of Brazilian Portugese"
Dictionary of American Regional English (DAREDS),American words and where to find them,Rachael Tatman,6,"Version 1,2017-08-08","languages
united states
linguistics",{}JSON,643 KB,CC4,864 views,51 downloads,,0 topics,https://www.kaggle.com/rtatman/dictionary-of-american-regional-english-dareds,"Context:
Languages tend to be used differently in different places. One of the ways that language use varies is that different words are used in different areas. For example, in Philadelphia you might hear someone refer to something they’ve forgotten the name for as a “jawn”, and in Maine and the northeast, hedgehogs are sometimes called “quill-pigs”. This dataset contains information on many regionally-specific words of American English and where they are used..
Content:
DAREDS is a dataset of words found in American English, created from the Dictionary of American Regional English (DARE). DARE collects information on dialect regions, which terms are used in them and the meaning of those terms. It is based on dialectal surveys from different rege In order to construct a dataset based on DARE, the web version of DARE was downloaded. It was then cleaned, removing both multiword expressions and very common words. Dialect regions that didn’t correspond to a single state or set of cities (e.g. South) were mapped to the most populous cities within each region. For example, within the Pacific Northwest dialect region, the most populous cities (Seattle, Tacoma, Portland, Salem, Eugene) were added to this dataset as subregions. The resulting dataset (DAREDS) consists of around 4.3k dialect terms from 99 U.S. dialect regions.
Acknowledgements:
This dataset was compiled by Afshin Rahimi, Trevor Cohn and Timothy Baldwin. If you use this dataset, please cite both their paper and the Dictionary of American Regional English.
Rahimi, Afshin, Cohn, Trevor and Baldwin, Timothy. ""A Neural Model for User Geolocation and Lexical Dialectology."" ACL 2017 (2017): 209.
Cassidy, F. G. (1985). Dictionary of American Regional English. Belknap Press of Harvard University Press.
Inspiration:
This dataset was originally composed to help validate the geographic origin of Twitter data (if someone uses a word that’s only used in a small geographic area, then it’s more likely that they are from there). But there are lots of other interesting questions you can ask with this data!
How many of these words can you find in Twitter datasets like this one of celebrity tweets? How accurately can you guess the location of these celebrities given this dataset?
Can you train word vectors based on these words? Do words tend to cluster by regional origin?
Many of the words in this dataset have become less popular over time. Can you tell which ones are still popular, perhaps by using Google’s n-gram viewer?"
ATP Tennis Dataset,All pro tour matches compiled with multiple features from Jan. 2012 to Jul. 2017,V.A. Freeman,6,"Version 9,2017-08-07|Version 8,2017-08-07|Version 7,2017-08-05|Version 6,2017-08-04|Version 5,2017-08-03|Version 4,2017-08-03|Version 3,2017-08-03|Version 2,2017-07-26|Version 1,2017-07-26",tennis,CSV,2 MB,CC4,"1,421 views",194 downloads,,0 topics,https://www.kaggle.com/m3financial/atp-tennis-data-from-201201-to-201707,"Context
Need to maintain a database of ATP Tour data for forecasting and predictions, etc.
Content
Retrieved data from online, but compiled file from 2012 to 2017 through 07/20/2017. Note that there are errors in some 2012 dates, but I revised the initial dataset as most recent version. Also, the W_Odds and L_Odds columns are averages of Odds taken from several online gaming sites including BetOnline365 and others.
Most recent data is usually updated weeks prior to all major grand slam events.
Acknowledgements
Compiled all useful data from the following link: http://tennis-data.co.uk/alldata.php
Inspiration
Building dataset for match prediction"
"National Employment, Hours, and Earnings","Estimates of employment, hours, and earnings information on a national basis",US Bureau of Labor Statistics,6,"Version 1,2017-06-29","government agencies
business
demographics
economics",CSV,1 GB,CC0,"1,044 views",160 downloads,,0 topics,https://www.kaggle.com/bls/employment,"Context:
The Current Employment Statistics (CES) program provides estimates of employment, hours, and earnings information on a national basis and in considerable industry detail. The Bureau of Labor Statistics collects payroll data each month from a sample of business and government establishments in all nonfarm activities.
Employment data include series for total employment, number of women employees, and number of production or nonsupervisory employees. Estimates of average hourly earnings, average weekly hours, average weekly earnings, and average weekly overtime hours are produced for both all employees and for production or nonsupervisory employees. Overtime hours are produced for manufacturing industries only.
A sample of approximately 147,000 businesses and government agencies representing approximately 634,000 worksites throughout the United States is utilized for this monthly survey. The sample contains about 300,000 employer units.
All employment, hours and earnings series are classified according to the 2012 North American Industry Classification System (NAICS). The industry code used in the survey corresponds to the NAICS code, except in those cases where multiple industries have been combined.
Content:
Please refer to ce.txt for a description of how to parse and use the unique identifiers.
This dataset was collected on June 27th, 2017 and may not be up-to-date.
Summary of Data Available: For all employees, women employees, and production or nonsupervisory employees, CES publishes about 4,300 monthly series. The series for all employees cover more than 900 industries on both a seasonally adjusted and not seasonally adjusted basis.
For private-sector industries, nearly 7,500 series are published each month for average weekly earnings, average hourly earnings, average weekly hours, and, in manufacturing, average weekly overtime hours. Hours and earnings data for all employees are available for about 620 industries and for production or nonsupervisory employees about 550 industries.
From the employment, hours, and earnings series, CES produces about 7,500 derivative series, such as indexes and real earnings series.
Most employment series begin in 1990, although some series, including industry supersectors, are available from 1939. Supersectors include: mining and logging; construction; manufacturing; trade, transportation, and utilities; information; financial activities; professional and business services, education and health services; leisure and hospitality, other services, and government.
Frequency of Observations: Data series are monthly in most cases; quarterly averages are available for total employment, average weekly hours, and average overtime hours, seasonally adjusted (datatypes 19, 20, 25, 36, and 37).
Annual averages are available for all series that are not adjusted for seasonality (except for the 12-month diffusion index series).
Data Characteristics: Earnings are measured in dollars and are published to the nearest cent (two decimal places). Average weekly and overtime hours are measured in hours and are published to the nearest tenth of an hour (one decimal place).
Employment is measured in thousands of workers and is stored with no decimal place for all supersectors and for both durable goods and nondurable goods in manufacturing. Employment for all other industries are stored to one decimal place.
Special characteristics of the data are footnoted where necessary. For example; I indicates that the seasonally adjusted series is independently seasonally adjusted and not used in aggregating to higher summary industries. For all employees, higher summary series, such as total nonfarm, are aggregated up from the 3-digit NAICS level.
Each year with the release of January estimates in February, CES data are re-anchored to universe counts of nonfarm employment or benchmarks for the most recent March. For example, CES introduced March 2016 benchmark counts with the release of January 2017 first preliminary estimates in February 2017. On a not seasonally adjusted basis, all series are subject to revision back to the prior year’s benchmarked data (21 months of data), while seasonally adjusted estimates may be revised back 5 years (or more).
References: BLS Handbook of Methods, Chapter 2, ""Employment, hours, and earnings from
the establishment survey"", https://www.bls.gov/opub/hom/pdf/homch2.pdf
Acknowledgements:
This dataset was taken directly from the U.S. Bureau of Labor Statistics website at http://www.bls.gov/data/ and converted to CSV format. Originally, there were 55 data files; one for each major sector of industry. Those files have been combined into one one file. An additional column has been added to record the original file name from which each row came from.
Inspiration:
The Bureau of Labor Statistics has done a great job of providing this source of information for the public to explore. With this dataset you can explore which industries make the most money or or work the most. If you’re feeling frisky, you might stretch those data science skills and combine this dataset with the BLS - Consumer Price Index dataset to find out which cities pay the most for snacks per hours worked!"
Amending America,"11,000+ Proposed Amendments to the United States Constitution from 1787 to 2014",U.S. National Archives and Records Administration,6,"Version 1,2017-06-23","history
politics",CSV,5 MB,CC0,852 views,51 downloads,3 kernels,,https://www.kaggle.com/national-archives/amending-america,"Context:
Article Five of the United States Constitution describes a process that allows for alteration of the federal Constitution. So far, 27 amendments have been fully added to the federal Constitution but there have been a lot of proposals that didn’t make it through. This dataset contains information about a whopping 11,797 Constitutional amendments that have been proposed to Congress from 1787 to 2014!
Content:
This dataset consists of 11,797 rows with 17 fields and was compiled by NARA volunteers that transcribed information from written records that were issued by Congress. Because a lot of the information comes from written records, the dataset may not be a complete record of all amendment proposals. Proposals before 1973 were taken from various government publications and proposals after 1973 are publicly available on https://www.congress.gov.
Identifier: Unique code generated by NARA that serves as a unique identifier.
Source_code: Code generated by NARA that represents the government publication or website where the information was transcribed.
Source_citation: Bibliographic citation of the government publication or website from which the information was transcribed.
Source_index_number: An index number listed within the source publication.
Title_or_description_from_source: Title or description of the proposed amendment, transcribed from the source publication or website.
Date_approximation: When the date is estimated, the term includes “circa”, otherwise it is left blank.
Year: Year that the amendment was proposed in Congress (YYYY format).
Month: Month that the amendment was proposed in Congress (blank if unknown).
Day: Day that the amendment was proposed in Congress (blank if unknown).
Congress: The number for the congress in which the amendment was proposed.
Congressional_session: The number or designation of the session of Congress in which the amendment was proposed.
Joint_resolution_chamber: The chamber of Congress in which the amendment was proposed.
Joint_resolution_number: The number assigned to the proposed amendment by Congress. (Amendments are submitted as joint resolutions)
Sponsor_name: The name of the member of Congress or group who proposed the amendment. (Blank if unknown).
Sponsor_state_or_territory: The U.S. state or territory represented by the amendment’s sponsoring member of Congress.
Committee_of_referral: The committee to which the amendment was referred for further consideration, following formal introduction to Congress.
Last_modified: the timestamp of the most recent modification made on the data contained within the particular row.
Acknowledgements:
The National Archives and Records Administration created this dataset as part of the Amending America initiative. To prepare for the 2016 ""Amending America"" exhibition at the National Archives Museum in Washington, D.C., NARA volunteers and staff transcribed and edited over 11,000 entries representing proposed amendments to the U.S. Constitution, as recorded by Congress. http://www.archives.gov/amending-america
Inspiration:
This is an interesting dataset because it contains a lot of history that isn’t necessarily reflected in the Constitution. You could use it to glean insight into the political climates at different times in the history of the United States. For instance, what kind of amendments were being proposed during the Civil War? Who proposed the most amendments? Historically, have there been more proposals during any particular time of year?"
Stanford Open Policing Project - California,Data on Traffic and Pedestrian Stops by Police in California,Stanford Open Policing Project,6,"Version 1,2017-07-11","government agencies
crime
law",CSV,2 GB,Other,"1,030 views",119 downloads,2 kernels,0 topics,https://www.kaggle.com/stanford-open-policing/stanford-open-policing-project-california,"Context:
On a typical day in the United States, police officers make more than 50,000 traffic stops. The Stanford Open Policing Project team is gathering, analyzing, and releasing records from millions of traffic stops by law enforcement agencies across the country. Their goal is to help researchers, journalists, and policymakers investigate and improve interactions between police and the public.
If you'd like to see data regarding other states, please go to https://www.kaggle.com/stanford-open-policing.
Content:
This dataset includes over 2gb of stop data from California, covering all of 2013 onwards. Please see the data readme for the full details of the available fields.
Acknowledgements:
This dataset was kindly made available by the Stanford Open Policing Project. If you use it for a research publication, please cite their working paper: E. Pierson, C. Simoiu, J. Overgoor, S. Corbett-Davies, V. Ramachandran, C. Phillips, S. Goel. (2017) “A large-scale analysis of racial disparities in police stops across the United States”.
Inspiration:
How predictable are the stop rates? Are there times and places that reliably generate stops?
Concerns have been raised about jurisdictions using civil forfeiture as a funding mechanism rather than to properly fight drug trafficking. Can you identify any jurisdictions that may be exhibiting this behavior?"
Stanford Open Policing Project - Bundle 2,Data on Traffic and Pedestrian Stops by Police in many states,Stanford Open Policing Project,6,"Version 1,2017-08-01","government agencies
crime
law",CSV,1 GB,Other,678 views,120 downloads,,0 topics,https://www.kaggle.com/stanford-open-policing/stanford-open-policing-project-bundle-2,"Context:
On a typical day in the United States, police officers make more than 50,000 traffic stops. The Stanford Open Policing Project team is gathering, analyzing, and releasing records from millions of traffic stops by law enforcement agencies across the country. Their goal is to help researchers, journalists, and policymakers investigate and improve interactions between police and the public.
If you'd like to see data regarding other states, please go to https://www.kaggle.com/stanford-open-policing.
Content:
This dataset includes stop data from MS, MT, ND, NH, NJ, NV, OR, RI, SD, TN, VA, V, WI, and WY. Please see the data readme for the full details of the available fields.
Acknowledgements:
This dataset was kindly made available by the Stanford Open Policing Project. If you use it for a research publication, please cite their working paper: E. Pierson, C. Simoiu, J. Overgoor, S. Corbett-Davies, V. Ramachandran, C. Phillips, S. Goel. (2017) “A large-scale analysis of racial disparities in police stops across the United States”.
Inspiration:
How predictable are the stop rates? Are there times and places that reliably generate stops?
Concerns have been raised about jurisdictions using civil forfeiture as a funding mechanism rather than to properly fight drug trafficking. Can you identify any jurisdictions that may be exhibiting this behavior?"
Current Properati Listing Information,Property attributes of 1.5 million Latin American listings,Properati Data,6,"Version 4,2017-07-04|Version 3,2017-06-27|Version 2,2017-06-26|Version 1,2017-06-21",home,CSV,464 MB,Other,"2,669 views",85 downloads,,,https://www.kaggle.com/properati-data/properties,"Properati is a competitive marketplace for Latin American real estate that strives to assist consumers in purchasing and renting homes, while simultaneously providing location-specific property databases for the public.
Properati currently lists over 1.5 million properties through hubs in Argentina, Mexico, and Brazil. Listings include property name, type, price, listing date, surface area, coordinates, description, and photos. Most of these premises are located in urban settings, with few straying into suburban or rural areas. Innovators at Properati understand that a house is usually the most important and costly purchase an individual makes in his or her lifetime. Properati has developed tools, such as Preciómetro, to help people make well-educated property investments.
Properati also creates data analyses based on over 1.5 million properties to better understand the real estate market. Through these reports, Properati uncovers social trends and urban processes that help characterize current and future listings. Visit Properati’s blog and website to join them in their advanced market knowledge.
Properati invites you to utilize their location based datasets from current and past listings in Big Query https://bigquery.cloud.google.com/dataset/properati-data-public:properties_ar https://bigquery.cloud.google.com/dataset/properati-data-public:properties_br https://bigquery.cloud.google.com/dataset/properati-data-public:properties_mx"
OpenAddresses - U.S. Northeast,Addresses and geo locations for the U.S. Northeast,OpenAddresses,6,"Version 1,2017-07-28",,CSV,2 GB,ODbL,824 views,137 downloads,,0 topics,https://www.kaggle.com/openaddresses/openaddresses-us-northeast,"Context
OpenAddresses's goal is to connect the digital and physical worlds by sharing geographic coordinates, street names, house numbers and postal codes.
Content
This dataset contains one datafile for each state in the U.S. Northeast region.
States included in this dataset:
Connecticut - ct.csv
Massachusetts - ma.csv
Maine - me.csv
New Hampshite - nh.csv
New Jersey - nj.csv
New York - ny.csv
Pennsylvania - pa.csv
Rhode Island - ri.csv
Vermont - vt.csv
Field descriptions:
LON - Longitude
LAT - Latitude
NUMBER - Street number
STREET - Street name
UNIT - Unit or apartment number
CITY - City name
DISTRICT - ?
REGION - ?
POSTCODE - Postcode or zipcode
ID - ?
HASH - ?
Acknowledgements
Data collected around 2017-07-25 by OpenAddresses (http://openaddresses.io).
Address data is essential infrastructure. Street names, house numbers and postal codes, when combined with geographic coordinates, are the hub that connects digital to physical places.
Data licenses can be found in LICENSE.txt.
Data source information can be found at https://github.com/openaddresses/openaddresses/tree/9ea72b079aaff7d322349e4b812eb43eb94d6d93/sources
Inspiration
Use this dataset to create maps in conjunction with other datasets for crime or weather"
OpenAddresses - U.S. South,Addresses and geo locations for the U.S. South,OpenAddresses,6,"Version 1,2017-08-02",,CSV,3 GB,CC0,730 views,129 downloads,,0 topics,https://www.kaggle.com/openaddresses/openaddresses-us-south,"Context
OpenAddresses's goal is to connect the digital and physical worlds by sharing geographic coordinates, street names, house numbers and postal codes.
Content
This dataset contains one datafile for each state in the U.S. South region (although some are arguably not in the South).
States included in this dataset:
Alabama - al.csv
Arkansas - ar.csv
Washington D.C. - dc.csv
Delaware - de.csv
Florida - fl.csv
Georgia - ga.csv
Kentucky - ky.csv
Louisiana - la.csv
Maryland - md.csv
Mississippi - ms.csv
North Carolina - nc.csv
Oklahoma - ok.csv
South carolina - sc.csv
Tennessee - tn.csv
Texas - tx.csv
Virginia - va.csv
West Virginia - wv.csv
Field descriptions:
LON - Longitude
LAT - Latitude
NUMBER - Street number
STREET - Street name
UNIT - Unit or apartment number
CITY - City name
DISTRICT - ?
REGION - ?
POSTCODE - Postcode or zipcode
ID - ?
HASH - ?
Acknowledgements
Data collected around 2017-07-25 by OpenAddresses (http://openaddresses.io).
Address data is essential infrastructure. Street names, house numbers and postal codes, when combined with geographic coordinates, are the hub that connects digital to physical places.
Data licenses can be found in LICENSE.txt.
Data source information can be found at https://github.com/openaddresses/openaddresses/tree/9ea72b079aaff7d322349e4b812eb43eb94d6d93/sources
Inspiration
Use this dataset to create maps in conjunction with other datasets for crime or weather"
OpenAddresses - U.S. Midwest,Addresses and geo locations for the U.S. Midwest,OpenAddresses,6,"Version 1,2017-08-02",,CSV,2 GB,Other,684 views,104 downloads,,0 topics,https://www.kaggle.com/openaddresses/openaddresses-us-midwest,"Context
OpenAddresses's goal is to connect the digital and physical worlds by sharing geographic coordinates, street names, house numbers and postal codes.
Content
This dataset contains one datafile for each state in the U.S. Midwest region (although some are arguably not in the Midwest).
States included in this dataset:
Iowa - ia.csv
Illinois - il.csv
Indiana - in.csv
Kansas - ks.csv
Michigan - mi.csv
Minnesota - mn.csv
Missouri - mo.csv
North Dakota - nd.csv
Nebraska - ne.csv
Ohio - oh.csv
South Dakota - sd.csv
Wisconsin -wi.csv
Field descriptions:
LON - Longitude
LAT - Latitude
NUMBER - Street number
STREET - Street name
UNIT - Unit or apartment number
CITY - City name
DISTRICT - ?
REGION - ?
POSTCODE - Postcode or zipcode
ID - ?
HASH - ?
Acknowledgements
Data collected around 2017-07-25 by OpenAddresses (http://openaddresses.io).
Address data is essential infrastructure. Street names, house numbers and postal codes, when combined with geographic coordinates, are the hub that connects digital to physical places.
Data licenses can be found in LICENSE.txt.
Data source information can be found at https://github.com/openaddresses/openaddresses/tree/9ea72b079aaff7d322349e4b812eb43eb94d6d93/sources
Inspiration
Use this dataset to create maps in conjunction with other datasets for housing prices, crime, or weather!"
Parkinson Disease Spiral Drawings,Hand drawing data using Digitized Graphics Tablet Data Set,Team AI,6,"Version 1,2017-08-15",,Other,16 MB,CC0,"1,903 views",155 downloads,,0 topics,https://www.kaggle.com/team-ai/parkinson-disease-spiral-drawings,"Context
More than 10 million people worldwide are living with Parkinson's disease. Improving machine learning model which identifies Parkinson's disease will lead to helping patients with early dialogs and reduction of treatment cost.
Content
Handwriting database consists of 62 PWP(People with Parkinson) and 15 healthy individuals. The data was collected in 2009.
Number of instances: 77, Number of attributes: 7
Acknowledgements
Source: https://archive.ics.uci.edu/ml/datasets/Parkinson+Disease+Spiral+Drawings+Using+Digitized+Graphics+Tablet
Citation:
1.Isenkul, M.E.; Sakar, B.E.; Kursun, O. . 'Improved spiral test using digitized graphics tablet for monitoring Parkinson's disease.' The 2nd International Conference on e-Health and Telemedicine (ICEHTM-2014), pp. 171-175, 2014.
2.Erdogdu Sakar, B., Isenkul, M., Sakar, C.O., Sertbas, A., Gurgen, F., Delil, S., Apaydin, H., Kursun, O., 'Collection and Analysis of a Parkinson Speech Dataset with Multiple Types of Sound Recordings', IEEE Journal of Biomedical and Health Informatics, vol. 17(4), pp. 828-834, 2013."
Robocall Complaints,Consumer complaints filed with the FCC,Federal Communications Commission,6,"Version 1,2017-09-13",telecommunications,CSV,152 MB,CC0,841 views,57 downloads,,0 topics,https://www.kaggle.com/fcc/robocall-complaints,"Individual informal consumer complaint data detailing complaints filed with the Consumer Help Center beginning October 31, 2014. This data represents information selected by the consumer. The FCC does not verify the facts alleged in these complaints.
This dataset contains everything you need to analyze the jerk companies who call during dinner: time, mode of communication, logged phone number (of the hassler), and what they were trying to do.
Acknowledgements
This dataset was kindly made available by the FCC. You can find the original dataset here."
"Build Bridges, Not Walls",Data describing 600k bridges from the United States National Bridge Inventory,Brian Roach,6,"Version 1,2017-02-12",transport,CSV,298 MB,CC0,"1,928 views",109 downloads,,,https://www.kaggle.com/broach/build-bridges-not-walls,"The United State Federal Highway Administration (FHWA) collects and updates information on the nation's bridges that are located on public roads, including both interstate and US highways, state and county roads, and publicly accessible bridges on federal land. This collection of information is known as the National Bridge Inventory (NBI), and it has been captured electronically since 1972. While parts of the data were first made available to the public in 1997, it wasn't until 2007 that the FWHA decided to make all elements of the NBI database publicly available.
This NBI data set contains 135 variables describing over 600,000 bridges. Variables describe the location, structure, maintenance, usage, status, and other aspects of the bridges. An extremely detailed (124 page) pdf guide to the variables, codes, and other metadata on the NBI can be found here:
https://www.fhwa.dot.gov/bridge/mtguide.pdf
Acknowledgements
The Department of Transportation FHWA collects and provides the NBI data as authorized by statue 23, U.S.C. 151
This data set was downloaded from the Homeland Infrastructure Foundation here:
https://hifld-dhs-gii.opendata.arcgis.com/datasets/94c41e96db0d4b85b9eb622923e0a0e8_0
Inspiration
This data set contains variables describing the cost of bridge (ITEM94) and roadway (ITEM95) improvement in thousands of dollars. How many improvement projects could be completed for $20B?
Use of the NBI data also enables FHWA to satisfy its requirements under 23 U.S.C. 144, which mandate the inventory, classification, cost estimates for replacement or rehabilitation, and assignment of replacement or rehabilitation priorities for all highway bridges on all public roads. Can you come up with better cost estimates and classifications? For example, in the absence of additional information, FHWA recommends using 10% of the bridge cost as a roadway improvement cost estimator.
Using Latitude and Longitude data and operational status (ITEM41), can you find any bridges to nowhere?"
UK Constituency Results,2015 General Election and 2016 Referendum results by constituency,Tiago Vinhoza,6,"Version 2,2017-06-13|Version 1,2017-04-28",politics,CSV,105 KB,CC0,"1,211 views",98 downloads,4 kernels,0 topics,https://www.kaggle.com/tiagotvv/uk-constituency-results,"Background
On April 18, 2017, prime minister Theresa May announced that she was seeking a general election to be held on June 8. The day after, the MPs voted to dissolve Parliament and a new election is confirmed. It would be interesting to do some analysis of past election results on the 650 constituencies in to find out:
The most vulnerable seats from each party.
The seats targeted by each party.
Can the Brexit referendum result affect the outcome of this election in a particular region of the UK?
How a swing of x% from party A to party B affect the seat distribution?
among many other relevant questions.
Dataset Contents
This dataset consists of a mixture between two tables that were scrapped from Wikipedia.
1) 2015 general election results by constituency: https://en.wikipedia.org/wiki/Results_of_the_United_Kingdom_general_election,_2015_by_parliamentary_constituency
2) 2016 referendum result by constituency: https://en.wikipedia.org/wiki/Results_of_the_United_Kingdom_European_Union_membership_referendum,_2016#List_of_constituency_results This table used results estimated by Chris Hanretty in [1].
The tables were joined and cleaned. Some imputation errors were found and corrected. A couple of columns was created (or removed). The following set of columns was obtained:
Constituency: name of constituency
Region: region where the constituency is located
Con: Conservative party votes
Lab: Labour party votes
UKIP: UKIP votes
LD: Liberal Democrat votes
SNP: Scottish National Party votes
Grn: Green party votes
DUP: Democratic Unionist Party votes
PC: Plaid Cymru votes
SF: Sinn Fein votes
UUP: Ulster Unionist Party votes
SDLP: Social Democratic Labour Party votes
Alliance: Alliance party votes
Ind: Independent votes
Spk: Speaker votes
Others: Other parties votes
ValidVotes: sum of votes received by all parties
WinningParty: party that holds the seat
SecondPlace: party that finished in 2nd place
WinningVotes: number of votes received by the winning party
SecondPlaceVotes: number of votes received by the 2nd placed party
WinningPct: percentage of votes received by winner
Majority: WinningVotes - SecondPlaceVotes
MajorityPct: Majority as a percentage of ValidVotes
TurnoutPct2015: turnout in the last general election
RemainPct: percentage of Remain votes in 2016 referendum
LeavePct: percentage of Leave votes in 2016 referendum
LeaveMajority: LeavePct - RemainPct
Reference
[1] https://medium.com/@chrishanretty/final-estimates-of-the-leave-vote-or-areal-interpolation-and-the-uks-referendum-on-eu-membership-5490b6cab878
Disclaimer
Cover picture is from Chensiyuan and made availabe under a Creative Commons Attribution-Share Alike 4.0 International, 3.0 Unported, 2.5 Generic, 2.0 Generic and 1.0 Generic license. https://commons.wikimedia.org/wiki/File:1_westminster_palace_panorama_2012_dusk.jpg"
The Apnea-ECG database,Data for development and evaluation of ECG-based apnea detectors,ecerulm,6,"Version 1,2017-05-24",,Other,581 MB,Other,"1,648 views",124 downloads,4 kernels,,https://www.kaggle.com/ecerulm/apneaecg,"Context
This is a ECG/EKG dataset from the Data for development and evaluation of ECG-based apnea detectors
Content
It contains 70 Electrocardiography records:
Training set
a01.dat to a20.dat
b01.dat to b05.dat
c01.data to c10.dat
Test set
x01.dat to x25.dat
The .dat files contain the digitized ECGs (16 bits per sample, least significant byte first in each pair, 100 samples per second, nominally 200 A/D units per millivolt).
For each a.dat file in the training set there is .apn file that is annotation (by human experts) stating for each minute of recording if there was apnea or not
Acknowledgements
This database is described in T Penzel, GB Moody, RG Mark, AL Goldberger, JH Peter. The Apnea-ECG Database. Computers in Cardiology 2000;27:255-258.
Goldberger AL, Amaral LAN, Glass L, Hausdorff JM, Ivanov PCh, Mark RG, Mietus JE, Moody GB, Peng C-K, Stanley HE. PhysioBank, PhysioToolkit, and PhysioNet: Components of a New Research Resource for Complex Physiologic Signals. Circulation 101(23):e215-e220 [Circulation Electronic Pages; http://circ.ahajournals.org/content/101/23/e215.full]; 2000 (June 13).
Links
https://physionet.org/physiobank/database/apnea-ecg/
https://physionet.org/faq.shtml
License
ODC Public Domain Dedication and Licence (PDDL) https://opendatacommons.org/licenses/pddl/1.0/ According to PhysioNet FAQ https://physionet.org/faq.shtml the redistribution of the data is allowed"
Cars Data,Data for predicting the car make,Abineshkumar K,6,"Version 1,2017-03-11",,CSV,9 KB,CC0,"6,149 views",603 downloads,4 kernels,0 topics,https://www.kaggle.com/abineshkumark/carsdata,"Cars Data has Information about 3 brands/make of cars. Namely US, Japan, Europe. Target of the data set to find the brand of a car using the parameters such as horsepower, Cubic inches, Make year, etc.
A decision tree can be used create a predictive data model to predict the car brand."
The VidTIMIT Audio-Video Dataset,Dataset is comprised of video and corresponding audio recordings of 35 people.,Akshay Babbar,6,"Version 1,2017-02-16",,Other,73 MB,Other,"2,027 views",114 downloads,6 kernels,,https://www.kaggle.com/akshay4/speakerrecognition,"Context
This Dataset is conducive for various types of Audio-Video Analysis.(Just want to mention one thing here,the files in the data for Video Analysis I am not Uploading courtesy Size issues but for those interested can download Here)
Content
The VidTIMIT dataset is comprised of video and corresponding audio recordings of 35 people(though the original data contains the data for 43 People but some links were missing), reciting short sentences. It can be useful for research on topics such as automatic lip reading, multi-view face recognition, multi-modal speech recognition and person identification. The dataset was recorded in 3 sessions, with a mean delay of 7 days between Session 1 and 2, and 6 days between Session 2 and 3. The sentences were chosen from the test section of the TIMIT corpus. There are 10 sentences per person. The first six sentences (sorted alpha-numerically by filename) are assigned to Session 1. The next two sentences are assigned to Session 2 with the remaining two to Session 3.
The first two sentences for all persons are the same, with the remaining eight generally different for each person.
The corresponding audio is stored as a mono, 16 bit, 32 kHz WAV file.
Acknowledgements
The VidTIMIT dataset is Copyright © 2001 Conrad Sanderson. For more details refer here
Inspiration
There are many reasons for uploading the data as Fetching audio-video data free of cost (and even with cost) is relatively hard and this data can be used to build models like Speaker_recognition,Person Verification and much more."
2017 #Oscars Tweets,"29,000+ tweets about the 2017 Academy Awards",Madhur Inani,6,"Version 3,2017-03-18|Version 2,2017-03-18|Version 1,2017-03-13","film
linguistics
internet",CSV,16 MB,Other,"3,190 views",235 downloads,7 kernels,,https://www.kaggle.com/madhurinani/oscars-2017-tweets,"Hi, I have extracted the Tweets related to Oscar 2017.
The timeframe is from Feb 27th,2017 to March 2nd,2017.
The number ofTweets is 29498.
The whole idea of extraction to know how people reacted in general about Oscars and also after the Best Picture mix up."
Linux Operating System Code Commits,The public code committed for the Operating System of Linux,Chase Willden,6,"Version 1,2017-03-08","computing and society
programming",CSV,1 MB,Other,"1,834 views",89 downloads,8 kernels,,https://www.kaggle.com/chasewillden/linuxcommits,"Context
I pulled data from the Github Repo for linux. https://github.com/torvalds/linux. This is looking at the source code committed by the public for the Linux Operating System.
Content
The content contains 6 columns: - Date: The date of the code commit - Commits: The total number of commits on that day by that user - Additions: The total additions made to the code. Essentially added characters - Deletions: The total deletions made to the code. - UserId: The unique user who pushed the commit. - StartOfWeek: I don't completely know.
Acknowledgements
GitHub and Linux"
Overwatch,Who are the best heroes?,EdoardoPiccari,6,"Version 1,2017-02-26",video games,CSV,2 KB,CC0,"3,628 views",172 downloads,,2 topics,https://www.kaggle.com/edopic/overwatch,"Overwatch is a team-based multiplayer first-person shooter video game developed and published by Blizzard Entertainment.
Overwatch puts players into two teams of six, with each player selecting one of several pre-defined hero characters with unique movement, attributes, and abilities; these heroes are divided into four classes: Offense, Defense, Tank and Support. Players on a team work together to secure and defend control points on a map and/or escort a payload across the map in a limited amount of time. Players gain cosmetic rewards that do not affect gameplay, such as character skins and victory poses, as they continue to play in matches. The game was launched with casual play, while Blizzard added competitive ranked play about a month after launch. Additionally, Blizzard has developed and added new characters, maps, and game modes post-release, while stating that all Overwatch updates will remain free, with the only additional cost to players being microtransactions to earn additional cosmetic rewards. ( Wikipedia - https://en.wikipedia.org/wiki/Overwatch_(video_game) )"
League of Legends MatchID dataset V2.0,+100 000 match ids. For further use with Riot API and League game analysis,LanVukušič,6,"Version 2,2017-02-27|Version 1,2017-02-19",,CSV,3 MB,CC0,"6,431 views",246 downloads,,,https://www.kaggle.com/lanls1/matchidv1,"League of Legends MatchID dataset V2.0
As people who like data analysis , but young enough to still like gaming, we thought that League of legends would be a great game to analyze. Due to competitive play some statistics and predictions were quite welcome. There are of course a lot of websites that offer that by them selves, but we think that League community needed an open dataset to work with, as there was none that offered some real volume of data. There came the idea for a bigger dataset which would offer other people to drive their projects without the struggle of long lasting process of parsing matches with Riot API (which has a limit of 500 calls per 10 minutes...so yea)
This is NOT the finished project, but more like a post along the way. The dataset only consists of one column and its basically useless by it self. The file consists of 223 715 match IDs of ranked games . Each column represents the MatchId of a single match played in League, which can be than accessed with Riot API The purpose is only to allow others like us, to continue the research with Riot API with some pre gathered data and save them some precious time that way.
The final dataset ""League of Legends MatchesDataset V1.0"" we will be posting, consists of 100 000 matches in JSON which will be directly suitable for data analysis.
Link to the dataset: WIP
We are also open sourcing the data gathering program (written in python)
GitHub link: GitHub program
This project has been posted by me (Lan Vukušič) as data scientist but the main credit goes to lead programmer Matej Urbas who is responsible for the data gathering in this project and without whom the project would not exist.
We are happy to give the dataset out for free, to let the comunity use that dataset. We would love to see what people are going to create. We know that we are ""rookies"" in that field but would still like to contribute to evergrowing field of data science. So if there is really anything that should be changed in upcoming updates please feel free to message us and tell us your thoughts.
Contacts : leaguedataset@gmail.com
Best regards
League of Legends MatchID dataset V1.0 and League of Legends MatchID dataset V2.0 aren't endorsed by Riot Games and doesn't reflect the views or opinions of Riot Games or anyone officially involved in producing or managing League of Legends. League of Legends and Riot Games are trademarks or registered trademarks of Riot Games, Inc. League of Legends © Riot Games, Inc."
HR Dataset for Analytics,HR employee dataset for analytics,John Traavis,6,"Version 2,2017-02-27|Version 1,2017-02-27",,Other,336 KB,Other,"4,235 views",708 downloads,3 kernels,0 topics,https://www.kaggle.com/mvsk93/hr-dataset-for-analytics,"Context
This data contains employee's terminated and hired data.
Content
File contains 4 sheets- Terminated, Active, Complete Data and sheet4 with appendix Each Sheet contains country, gender hired on date, terminated date, Action and grades"
Train.CSV,CSV FOR MACHINE LEARNING,Shivam Patel,6,"Version 1,2017-03-01",,CSV,60 KB,CC0,"5,152 views",895 downloads,6 kernels,0 topics,https://www.kaggle.com/shivamp629/traincsv,"Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
The Bachelor & Bachelorette Contestants,"Includes name, age, occupation, hometown, and week eliminated",Brian Gonzalez,6,"Version 4,2017-03-08|Version 3,2017-03-08|Version 2,2017-03-07|Version 1,2017-03-04",popular culture,CSV,46 KB,CC0,"3,390 views",250 downloads,5 kernels,,https://www.kaggle.com/brianbgonz/the-bachelorette-contestants,"Context
I thought it might be neat to do some simple analytics on The Bachelor/The Bachelorette contestant data.
Content
Contestant info from seasons 1, 2, 5, 9-21 of ABC's The Bachelor and 1-12 of The Bachelorette -- very incomplete.
Acknowledgements
I just compiled this from Wikipedia. Some data on The Bachelor contestants comes from Reddit user u/nicolee314.
Inspiration
Are there any predictors for success on reality dating shows? Has the typical contestant changed over the years? Are certain qualities under/over-represented in these contestants?"
Sberbank Russian Housing Market Data Fix,Removed noise and errors for the Sberbank competition from the data set.,Matthew Anderson,6,"Version 2,2017-05-07|Version 1,2017-05-06",,CSV,42 MB,Other,"1,201 views",135 downloads,24 kernels,,https://www.kaggle.com/matthewa313/sberbankdatafix,"Upon reviewing the train data for the Sberbank Russian Housing Market competition, I noticed noise & errors. Obviously, neither of these should be present in your training set, and as such, you should remove them. This is the updated train set with all noise & errors I found removed.
Data was removed when:
full_sq-life_sq<0 full_sq-kitch_sq<0 life_sq-kitch_sq<0 floor-max_floor<0
I simply deleted the row from the dataset, and did not really use anything special other than that."
White House Salaries,Trump vs. Obama Administration,def love(x):,6,"Version 1,2017-07-10","income
politics",CSV,388 KB,CC0,"2,520 views",409 downloads,5 kernels,,https://www.kaggle.com/adamschroeder/white-house-salaries,"Context
For democracy to function, we need transparency. Part of the transparency is given to us through government salaries and names of employees. Since 1996, the White House has been required by Congress to disclose a list of staff and their salaries.
Content
The Obama_staff_salaries covers salaries under the Obama administration from 2009-2016 (by Julianna Langston). The White_house_2017_salaries was released by the White House as a 16-page PDF, detailing the salaries of Trump Administration's employees. This is a CSV scraped from the PDF using Tabula (by Carl V. Lewis).
Acknowledgements
I would like to thank @julilangston and @carblewis at data.world for providing these datasets.
Inspiration
How do WH staff salaries compare to Trump staff salaries?
How much did each administration spend on Immigration advisor/assistant staff?
Who has more executive assistants?
How did salaries under the Obama's administration change from 2009-2016?"
Restaurants That Sell Tacos and Burritos,"A list of 19,000+ businesses offering ""burrito"" or ""taco"" menu items in the U.S.",Datafiniti,6,"Version 2,2017-07-12|Version 1,2017-07-12","databases
food and drink
business",CSV,48 MB,CC4,"1,692 views",129 downloads,,,https://www.kaggle.com/datafiniti/restaurants-burritos-and-tacos,"About this Data
This is a list of 19,439 restaurants and similar businesses with menu items containing ""burrito"" or ""taco"" in their names as provided by Datafiniti's Business Database.
The dataset includes the category, cuisine, restaurant information, and more for a menu item. Note that each row corresponds to a single menu item from the restaurant, and the entirety of each restaurant's menu is not listed. Only burrito or taco items are listed.
What You Can Do With This Data
You can use this data to discover which parts of the country offer the most for Mexican food aficionados. E.g.:
What is the ratio of burritos and tacos on restaurant menus from each city?
What is the ratio of burritos and tacos on restaurant menus from cities with the most restaurants per capita (10,000 residents)?
What is the ratio of cities with the most authentic Mexican restaurants per capita (10,000 residents)?
Which cities have the most authentic Mexican restaurants?
Which cities have the most Mexican restaurants?
Which Mexican restaurants have the most locations nationally?
Data Schema
A full schema for the data is available in our support documentation.
About Datafiniti
Datafiniti provides instant access to web data. We compile data from thousands of websites to create standardized databases of business, product, and property information. Learn more.
Want More?
You can get more data like this by joining Datafiniti or requesting a demo."
Childhood Blood Lead Surveillance,"National and state-level surveillance data, 1997 to 2015",Centers for Disease Control and Prevention,6,"Version 1,2017-05-01","healthcare
epidemiology
health",CSV,171 KB,CC0,"1,673 views",200 downloads,2 kernels,,https://www.kaggle.com/cdc/childhood-blood-lead-surveillance,"CDC began collecting childhood blood lead surveillance data in April 1995. The national surveillance system is composed of data from state and local health departments.
States maintain their own child-specific databases so they can identify duplicate test results or sequential test results on individual children. These databases contain follow-up data on children with elevated blood lead levels including data on medical treatment, environmental investigations, and potential sources of lead exposure. States extract fields from their child-specific surveillance databases and transfer them to CDC for the national database.
State child-specific databases contain follow-up data on children with elevated blood lead levels including data on medical treatment, environmental investigations, and potential sources of lead exposure. Surveillance fields for CDC's national database are extracted from state child-specific databases and transferred to CDC.
State surveillance systems are based on reports of blood lead tests from laboratories. Ideally, laboratories report results of all blood lead tests, not just elevated values, to state health departments. States determine the reporting level for blood lead tests and decide which data elements should accompany the blood lead test result.
These data were collected for program management purposes. The data have limitations, and we cannot compare across states or counties because data collection methods vary across grantees. Data are not generalizable at the national, state, or local level."
2014 Public Libraries Survey,What state has the most library books per capita?,Institute of Museum and Library Services,6,"Version 1,2017-01-26",libraries,CSV,3 MB,CC0,"1,904 views",261 downloads,5 kernels,0 topics,https://www.kaggle.com/imls/public-libraries,"Content
The Public Libraries Survey (PLS) is conducted annually by the Institute of Museum and Library Services under the mandate in the Museum and Library Services Act of 2010. The data file includes all public libraries identified by state library administrative agencies in the 50 states and the District of Columbia. The reporting unit for the survey is the administrative entity, defined as the agency that is legally established under local or state law to provide public library service to the population of a local jurisdiction. The FY 2014 PLS collected state characteristics data, including the state total population estimate, number of central and branch libraries, and the total library visits and circulation transactions, and data on each public library, such as its name and location, population of legal service area, print and digital collections, full-time-equivalent staff, and operating revenue and expenditures.
Acknowledgements
The U.S. Census Bureau is the data collection agent for IMLS Public Libraries Survey."
"Museums, Aquariums, and Zoos","Name, location, and revenue for every museum in the United States",Institute of Museum and Library Services,6,"Version 1,2017-03-07","museums
animals",CSV,7 MB,CC0,"2,653 views",335 downloads,17 kernels,,https://www.kaggle.com/imls/museum-directory,"Content
The museum dataset is an evolving list of museums and related organizations in the United States. The data file includes basic information about each organization (name, address, phone, website, and revenue) plus the museum type or discipline. The discipline type is based on the National Taxonomy of Exempt Entities, which the National Center for Charitable Statistics and IRS use to classify nonprofit organizations.
Non-museum organizations may be included. For example, a non-museum organization may be included in the data file because it has a museum-like name on its IRS record for tax-exempt organizations. Museum foundations may also be included.
Museums may be missing. For example, local municipal museums may be undercounted because original data sources used to create the compilation did not include them.
Museums may be listed multiple times. For example, one museum may be listed as both itself and its parent organization because it was listed differently in each original data sources. Duplicate records are especially common for museums located within universities.
Information about museums may be outdated. The original scan and compilation of data sources occurred in 2014. Scans are no longer being done to update the data sources or add new data sources to the compilation. Information about museums may have changed since it was originally included in the file.
Acknowledgements
The museum data was compiled from IMLS administrative records for discretionary grant recipients, IRS records for tax-exempt organizations, and private foundation grant recipients.
Inspiration
Which city or state has the most museums per capita? How many zoos or aquariums exist in the United States? What museum or related organization had the highest revenue last year? How does the composition of museum types differ across the country?"
Newspaper Endorsements of Presidential Candidates,Candidate endorsements and presidential election results since 1980,WNYC,6,"Version 1,2017-02-04","news agencies
politics",CSV,19 KB,Other,992 views,79 downloads,2 kernels,0 topics,https://www.kaggle.com/wnyc/candidate-endorsements,"Content
This dataset includes presidential candidates endorsed by the top 100 American newspapers by circulation for every election since 1980.
Acknowledgements
The historical candidate endorsements were compiled and documented by Github user veltman, and the presidential election results were provided by the Federal Election Commission."
"Freedom of the Press, 2001-2015","Scores on legal, political, and economic freedom in all countries",Freedom House,6,"Version 1,2017-02-04","news agencies
law",CSV,44 KB,Other,"1,824 views",219 downloads,7 kernels,0 topics,https://www.kaggle.com/freedomhouse/press-freedom,"Content
The 2016 edition of Freedom of the Press, which provides analytical reports and numerical scores for 199 countries and territories, continues a process conducted by Freedom House since 1980. Each country and territory is given a total press freedom score from 0 (best) to 100 (worst) on the basis of 23 methodology questions divided into three subcategories. The total score determines the status designation of Free, Partly Free, or Not Free. The scores and reports included in Freedom of the Press 2016 cover events that took place between January 1, 2015, and December 31, 2015.
The level of press freedom in each country and territory is evaluated through 23 methodology questions divided into three broad categories: the legal environment, the political environment, and the economic environment. For each methodology question, a lower number of points is allotted for a more free situation, while a higher number of points is allotted for a less free environment. A country or territory’s final score (from 0 to 100) represents the total of the points allotted for each question. A total score of 0 to 30 results in a press freedom status of Free; 31 to 60 results in a status of Partly Free; and 61 to 100 indicates a status of Not Free.
The legal environment category encompasses an examination of both the laws and regulations that could influence media content, and the extent to which they are used in practice to enable or restrict the media’s ability to operate. We assess the positive impact of legal and constitutional guarantees for freedom of expression; the potentially negative aspects of security legislation, the penal code, and other statutes; penalties for libel and defamation; the existence of and ability to use freedom of information legislation; the independence of the judiciary and official regulatory bodies; registration requirements for both media outlets and journalists; and the ability of journalists’ organizations to operate freely.
Under the political environment category, we evaluate the degree of political influence in the content of news media. Issues examined include the editorial independence of both state-owned and privately owned outlets; access to information and sources; official censorship and self-censorship; the vibrancy of the media and the diversity of news available within each country or territory; the ability of both foreign and local reporters to cover the news in person without obstacles or harassment; and reprisals against journalists or bloggers by the state or other actors, including arbitrary detention, violent assaults, and other forms of intimidation.
Our third category examines the economic environment for the media. This includes the structure of media ownership; transparency and concentration of ownership; the costs of establishing media as well as any impediments to news production and distribution; the selective withholding of advertising or subsidies by the state or other actors; the impact of corruption and bribery on content; and the extent to which the economic situation in a country or territory affects the development and sustainability of the media."
Montreal bike lanes,Use of bike lanes in Montreal city in 2015,PabloMonleon,6,"Version 1,2016-12-22",cycling,CSV,30 KB,ODbL,"1,390 views",46 downloads,5 kernels,,https://www.kaggle.com/pablomonleon/montreal-bike-lanes,The dataset contains information about the number of bicycles that used certain bicycle lanes in Montreal in the year 2015.
Obama White House,"A set of 20,000+ documents released on the Obama White House website",Jay Ravaliya,6,"Version 2,2017-04-11|Version 1,2017-04-09",politics,CSV,191 MB,Other,"2,434 views",158 downloads,6 kernels,,https://www.kaggle.com/jayrav13/obama-white-house,"Why?
Inspired by an interest in scraping and organizing data released by the federal government, this is an easy-to-navigate CSV of all of the documents released by the Obama White House, now found on obamawhitehouse.archives.gov.
This includes the full content of each of the documents released (wherever possible), along with link references to each document.
Further, each of them is broken down by document type as well."
Project Gutenberg's Top 20 Books,The 20 Most Popular Books from Project Gutenberg,Currie32,6,"Version 1,2017-03-23",,Other,13 MB,Other,889 views,141 downloads,,0 topics,https://www.kaggle.com/currie32/project-gutenbergs-top-20-books,"Context
I thought Kaggle could use more datasets for Natural Language Processing projects, so what better way to provide some data than to use some of the most popular books of all time!
Content
Each file contains the full book from Project Gutenberg.
Acknowledgements
I really want to thank the people who have created Project Gutenberg for making such a wide selection of books available! It will be great to see what projects we can create from their work!
Inspiration
Use one or more of these books for a text generation project.
Build an interesting visual using word vectors.
Compare the vocabulary used between authors."
Weather in Szeged 2006-2016,"Hourly/daily summary with temperature, pressure, wind speed and more",NorbertBudincsevity,6,"Version 1,2017-01-08",climate,CSV,16 MB,CC4,"2,943 views",628 downloads,229 kernels,0 topics,https://www.kaggle.com/budincsevity/szeged-weather,"Context
This is a dataset for a larger project I have been working on. My idea is to analyze and compare real historical weather with weather folklore.
Content
The CSV file includes a hourly/daily summary for Szeged, Hungary area, between 2006 and 2016.
Data available in the hourly response:
time
summary
precipType
temperature
apparentTemperature
humidity
windSpeed
windBearing
visibility
loudCover
pressure
Acknowledgements
Many thanks to Darksky.net team for their awesome API."
Technology Price Index 2016,Where can you buy an iPhone cheaper?,Irina Kalatskaya,6,"Version 1,2017-01-03","finance
technology forecasting",CSV,9 KB,Other,"5,034 views",603 downloads,3 kernels,,https://www.kaggle.com/ikalats/TechnologyPriceIndex,"Context
Linio.com is Latin America’s leading eCommerce platform. It have recently released the 2016-17 Technology Price Index comparing the cost of 14 popular electronic devices and brands, across 72 countries. Linio conducted a pretty impressive research study for better understanding the global economic trends in the price of most popular electronic devices. From http://www.outsourcingportal.eu/en/linio-com-compares-the-cost-of-technology-products-in-72-countries: To conduct the research Linio looked at the costs of all products in the study from several brick and mortar chain stores and smaller retailers in all major cities in each country. The study also took into account average costs from at least three reputable online outlets in each country. Taxes and other associated purchasing costs, minus delivery, were also accounted for.
Content
This dataset contains the cost of 14 different devices including smartphones, laptops, game consoles, tablets, smart devices, and other gadgets, across 72 different countries. ranking the countries on the average cost of all products researched. The dataset was downloaded from Linio.com in December 2016.
Acknowledgements
The dataset was taken from here: https://www.linio.com.mx/sp/technology-price-index-2016 without any modifications.
Inspiration
Is it possible to predict the cost of my favorite iPhone based on 13 other devices?
What device has the most unpredictable trend?
What products have the highest variability in price and whether it was caused by the same countries?"
Smogon 6v6 Pokemon Tiers,Tiering of Smogon 6v6 Format (12/29/16),Gibs,6,"Version 1,2016-12-30",video games,CSV,35 KB,CC0,"1,937 views",96 downloads,2 kernels,0 topics,https://www.kaggle.com/notgibs/smogon-6v6-pokemon-tiers,"This dataset stems from Alberto Barradas' popular Pokemon with stats dataset by listing the tiered Pokemon in Smogon 6v6.
Smogon 6v6 is one of the most popular formats for competitive Pokemon. Although it is not the ""official"" competitive format, there is still a significant number of people who play the format. There are a number of 'tiers' of Pokemon in which people can play, the most popular being OU. This dataset seeks to display both a Pokemon's stats and corresponding tier for easier competitive analysis.
In addition to the addition of the 'Tier' variable, there are several other changes I made to the set:
Classified Mythical Pokemon as 'Legendary'
Changed the naming convention of Mega Evolutions and some form changes
Addition of 'Mega' tier to signify Mega Evolutions
Note that this dataset includes only Pokemon tiered from PU to AG. NFE and LC Pokemon are not included unless they appear in Smogon's list. List of which Pokemon are in which tier was found here.
Thank you to Alberto Barradas for his comprehensive Pokemon dataset."
Alaska Airport Data,Visualizing Airfields in the Last Frontier,James Tollefson,6,"Version 1,2017-04-21",aviation,CSV,1 MB,CC0,"1,561 views",158 downloads,2 kernels,0 topics,https://www.kaggle.com/jamestollefson/alaskaairfields,"This listing includes four datasets which, when combined, thoroughly describe the existing airfields in the State of Alaska. The challenge for this dataset, for those who care to attempt it, is to create an interactive graphical representation of this information that provides as much of this data on demand as possible.
This dataset was pulled from the faa.gov and is current as of 30 March 2017.
No research has yet been done on this dataset. Creating a usable graphical representation of this data (i.e. a map that shows all airfields and provides detailed information when each airfield is selected) would prove a very useful planning tool for emergency response planning in the state of Alaska. The intent of posting this dataset is to seek feedback and analysis along those lines.
Above all it is important that any code used to transform this dataset be reusable, since the FAA regularly updates their information on these airfields. An ideal solution would allow these datasets to be fed in one end and spit out a beautiful, intuitive, user-friendly product at the other end.
All the data files are provided in .csv format. The dictionary that contains the definitions for the various fields is provided in Excel because it contains multiple spreadsheets (one to describe each .csv file). It is recommended that you download the Excel file so you can refer to it in Excel while you work on the .csv files."
Stock Pricing,Google Stock Pricing,ShivinderKapil,6,"Version 1,2017-01-18",,CSV,402 KB,Other,"3,667 views",345 downloads,13 kernels,0 topics,https://www.kaggle.com/shivinder/googlestockpricing,"This data set contains google stock pricing from the year 2004 up to 2017 which includes opening,closing, high ,low and adjusted stock prices.
The data set contains following columns: Date Open High Low Close Volume Ex-Dividend Split Ratio Adj. Open
Adj. High
Adj. Low Adj. Close
Adj. Volume"
Type Allocation Code (TAC),Type Allocation Code for mobile and wireless devices,Richard Nagyfi,6,"Version 1,2017-04-18","networks
telecommunications",CSV,6 MB,ODbL,"8,332 views",367 downloads,,0 topics,https://www.kaggle.com/sedthh/typeallocationtable,"Context
The Type Allocation Code (TAC) is the initial eight-digit portion of the 15-digit IMEI and 16-digit IMEISV codes used to uniquely identify wireless devices. (Wikipedia)
TAC numbers can be used to identify devices connected to networks. Complete TAC databases are hard to find and cost a furtune, as maintaining them and keeping them up to date is labour intensive due to the large amount of devices being released every day.
Content
The dataset contains information about the devices' TAC number, manufacturer, model, aliases of the model, operating system, year of release and LTE compatibility.
Some devices may have multiple TAC numbers, as they are different subversions of the same hardware. This TAC database is nowhere near complete, and some of the data provided here could be incorrect, as even manufacturers sometimes share inaccurate information about their own devices. LTE capability is the worst offender in this case, as companies often release TAC information with contradictory stats about LTE compatibility.
Acknowledgements
This is a merged and cleaned dataset based on free TAC databases found on the internet, including:
http://tacdb.osmocom.org/
https://www.mulliner.org/tacdb/feed/
Inspiration
This database is useful for anyone who works with telecommunication networks and wants to identify their users."
"Los Angeles Crime Data, 2012 to 2016","Combined raw crime data for 2012 through 2016 in Los Angeles, California",LiamLarsen,6,"Version 1,2017-03-30",crime,CSV,184 MB,Other,"3,419 views",344 downloads,7 kernels,,https://www.kaggle.com/kingburrito666/los-angeles-crime,"Content
This is the combined raw crime data for 2012 through 2016. Please note that it is missing a few weeks in both December 2015 and December 2016 (Winter break)."
"Ebola Cases, 2014 to 2016","Ebola data in record format with indicator, country, date and value",LiamLarsen,6,"Version 1,2017-04-24","healthcare
diseases
epidemiology",CSV,1 MB,ODbL,"2,365 views",318 downloads,4 kernels,,https://www.kaggle.com/kingburrito666/ebola-cases,"from Aug 29, 2014 - Mar 23, 2016
Content
indicator: 36 DISTINCT
Country: 12 DISTINCT
Date: 259 DISTINCT
Value: > 1,000
Acknowledgements
Original source https://data.humdata.org/dataset/ebola-cases-2014"
"Over 13,000 Steam Games","Title, release date, price of video games from Steam",LiamLarsen,6,"Version 1,2017-04-27",,CSV,527 KB,Other,"2,245 views",108 downloads,,2 topics,https://www.kaggle.com/kingburrito666/over-13000-steam-games,"Context
Game title
Game release date
Game current price
Problem(s)
In the Price column, if a game is on sale it shows both prices for example:
$39.99$30.99
Possible kernels
Games per year
Average price per game per year
Most occurring names in games (wordcloud)
Date -> Price
Highest price games, oldest, etc
Enjoy!"
"Crime in Bulgaria, 2000 to 2014",Percentage of resolved cases per region,Rumen Manev,6,"Version 1,2017-04-18",crime,CSV,132 KB,Other,"1,454 views",155 downloads,2 kernels,,https://www.kaggle.com/rmanev/crime-in-bulgaria,"Context
I've processed the data, freely available from the Bulgarian Ministry of Interior Affairs, and decided to provide it to see what interesting visualizations you might come up with. It would also be interesting to see a forecast of the same crime types for futures years based on the available data.
Content
The dataset contains information about various types of criminal activity. It shows the percentage of resolved crimes between 2000 and 2014 in all 28 regions of Bulgaria.
Acknowledgements
Data is taken from the reports found on the homepage of the Bulgarian Ministry of Interior Affairs."
Narrativity in Scientific Publishing,Is scientific research written in a narrative style more influential?,Crowdflower,6,"Version 1,2017-04-07","research tools and topics
research
science and culture",CSV,11 MB,CC4,"1,221 views",82 downloads,,0 topics,https://www.kaggle.com/crowdflower/narrativity-in-scientific-publishing,"CrowdFlower was used in a recent study published in PLOS One on how narrative style affects the way scientific findings are cited and shared. Refer to the article’s supplementary materials for more information.
The dataset contains abstracts from peer-reviewed studies on climate change that were labeled using the CrowdFlower platform. Abstracts were each assessed by multiple raters (n = 7) for their narrativity. Narrativity includes whether the abstract appeals to the reader, has a narrative perspective, using sensory language, and other factors. The dataset also contains additional information about the studies including citation rate, journal identity, and number of authors.
Past research
From the study abstract:
Peer-reviewed publications focusing on climate change are growing exponentially with the consequence that the uptake and influence of individual papers varies greatly. Here, we derive metrics of narrativity from psychology and literary theory, and use these metrics to test the hypothesis that more narrative climate change writing is more likely to be influential, using citation frequency as a proxy for influence. From a sample of 732 scientific abstracts drawn from the climate change literature, we find that articles with more narrative abstracts are cited more often. This effect is closely associated with journal identity: higher-impact journals tend to feature more narrative articles, and these articles tend to be cited more often. These results suggest that writing in a more narrative style increases the uptake and influence of articles in climate literature, and perhaps in scientific literature more broadly.
Inspiration
Can you replicate the authors' findings? Are abstracts with narrative qualities cited more often? What other variables are associated with narrativity in scientific abstracts about climate change? Examine relationships between citation rate, abstract length, abstract authors, and more.
Acknowledgements
This dataset is made available via CrowdFlower's ""Data for Everyone"" collection which hosts open data jobs that have come through the crowdsourced labeling platform."
SF Historic Secured Property Tax Rolls,SF secured property tax roll spanning from 2007 to 2015,DataSF,6,"Version 2,2017-01-07|Version 1,2017-01-05","cities
finance
politics",CSV,421 MB,Other,"1,446 views",78 downloads,,0 topics,https://www.kaggle.com/datasf/sf-historic-secured-property-tax-rolls,"Context
This data set includes the Office of the Assessor-Recorder’s secured property tax roll spanning from 2007 to 2015 (~1.6M). It includes all legally disclosable information, including location of property, value of property, the unique property identifier, and specific property characteristics. The data is used to accurately and fairly appraise all taxable property in the City and County of San Francisco. The Office of the Assessor-Recorder makes no representation or warranty that the information provided is accurate and/or has no errors or omissions.
Potential question(s) to get started with!
Can the effects of Prop 13 been seen in the historic property tax rolls?
Fields
There are 48 fields in this dataset.
A full data dictionary can be found here.
We have included the following commonly used geographic shapefiles:
Analysis Neighborhoods
Supervisor Districts as of April 2012
Acknowledgements
Data provided by the San Francisco Office of the Assessor-Recorder via the San Francisco Open Data Portal at https://data.sfgov.org/d/wv5m-vpq2 PDDL 1.0 ODC Public Domain Dedication and Licence (PDDL)
Photo from Flickr via Rebecca Morgan (CC BY-NC-SA 2.0)"
SF Restaurant Inspection Scores,SF Health Department records for restaurant inspections,DataSF,6,"Version 2,2017-01-07|Version 1,2017-01-05",food and drink,CSV,12 MB,Other,"4,378 views",355 downloads,8 kernels,0 topics,https://www.kaggle.com/datasf/sf-restaurant-inspection-scores,"Context:
The SF Health Department has developed an inspection report and scoring system. After conducting an inspection of the facility, the Health Inspector calculates a score based on the violations observed. Violations can fall into:high risk category: records specific violations that directly relate to the transmission of food borne illnesses, the adulteration of food products and the contamination of food-contact surfaces.moderate risk category: records specific violations that are of a moderate risk to the public health and safety.low risk category: records violations that are low risk or have no immediate risk to the public health and safety.The score card that will be issued by the inspector is maintained at the food establishment and is available to the public in this dataset.
Potential question(s) to get started with!
What are some predictors of health scores? What relevant outside data can you bring to bear on the question, including restaurant reviews, sentiment analysis, demographic data, etc?
Fields:
San Francisco's LIVES restaurant inspection data leverages the LIVES Flattened Schema (https://goo.gl/c3nNvr), which is based on LIVES version 2.0, cited on Yelp's website (http://www.yelp.com/healthscores).
Please refer to https://goo.gl/c3nNvr for detailed data dictionary.
Further info on the Food Safety Program can be found here.
We have included the following commonly used geographic shapefiles:
Analysis Neighborhoods
Supervisor Districts as of April 2012
Acknowledgements:
Data provided by the San Francisco Health Department via the San Francisco Open Data Portal at https://data.sfgov.org/d/pyih-qa8i License: PDDL 1.0 ODC Public Domain Dedication and Licence (PDDL)
Photo via Flickr Rob Hyndman Attribution-NonCommercial-ShareAlike 2.0 Generic (CC BY-NC-SA 2.0)"
The Global Avian Invasions Atlas,A database of alien bird distributions worldwide,figshare,6,"Version 1,2017-04-08",animals,CSV,966 MB,CC4,833 views,74 downloads,,0 topics,https://www.kaggle.com/figshare/the-global-avian-invasions-atlas,"This comma-separated text file contains the 27,723 alien bird records that form the core of the Global AVian Invasions Atlas (GAVIA) project. These records represent 971 species, introduced to 230 countries and administrative areas across all eight biogeographical realms, spanning the period 6000 BCE – AD 2014. The data comprises taxonomic (species-level), spatial (geographic location, realm, land type) and temporal (dates of introduction and spread) components, as well as details relating to the introduction event (how and why the species was introduced, whether or not it is established). Each line of data consists of an individual record concerning a specific alien bird species introduced to a specific location. The data derives from both published and unpublished sources, including atlases, country species lists, peer-reviewed articles, websites and via correspondence with in-country experts.
Acknowledgements
Dyer, Ellie; Redding, David; Blackburn, Tim (2016): Data from: The Global Avian Invasions Atlas - A database of alien bird distributions worldwide. figshare."
Open Flood Risk by Postcode,English postcodes with Environment Agency flood risk,GetTheData,6,"Version 1,2017-03-21",geography,CSV,85 MB,Other,"1,883 views",128 downloads,,0 topics,https://www.kaggle.com/getthedata/open-flood-risk-by-postcode,"Context
This dataset takes the Environment Agency's Risk of Flooding from Rivers and Sea, and places English postcodes in their appropriate flood risk area, allowing you to look up flood risk from postcode.
Content
Risk of Flooding from Rivers and Sea consists of geographical areas within England which are at risk of flooding from rivers and sea. Each area is assigned a flood risk within a banding:
High
Medium
Low
Very low
None
Open Flood Risk by Postcode takes postcodes as point locations (from Open Postcode Geo) and places the postcode in the appropriate flood risk area. It is important to note that actual properties within a specific postcode may have a slightly different point location and therefore be in a different flood risk area. Generally speaking the point location of a postcode is the point location of the central property in that postcode.
For a full field list and explanations of values see the Open Flood Risk by Postcode documentation.
Acknowledgements
Open Flood Risk by Postcode is derived from two open datasets:
Risk of Flooding from Rivers and Sea
Open Postcode Geo
Both of these datasets are licensed under the OGL.
The following attribution statements are required:
Contains OS data © Crown copyright and database right 2017
Contains Royal Mail data © Royal Mail copyright and database right 2017
Contains National Statistics data © Crown copyright and database right 2017
Contains Environment Agency data licensed under the Open Government Licence v3.0.
The dataset is maintained by GetTheData.
The latest version and full documentation is available here.
Inspiration
Example application:
Lookup or drill down to individual English postcodes to see a map of that postcode and its flood risk, alongside surrounding postcodes and their flood risks:
Application: Flood Map by Postcode
Example postcode: RG9 2LP"
Freedom of Information Act Requests,Which agency or department receives the most FOIA requests?,Department of Justice,6,"Version 1,2017-01-21","information
politics",CSV,101 KB,CC0,"1,449 views",80 downloads,7 kernels,0 topics,https://www.kaggle.com/doj/foia-requests,"Context
Since 1967, the Freedom of Information Act (FOIA) has provided the public the right to request access to records from any federal agency. It is often described as the law that keeps citizens in the know about their government. Federal agencies are required to disclose any information requested under the FOIA unless it falls under one of nine exemptions which protect interests such as personal privacy, national security, and law enforcement. As Congress, the President, and the Supreme Court have all recognized, the Act is a vital part of our democracy.
A FOIA request can be made for any agency record. You can also specify the format in which you wish to receive the records (for example, printed or electronic form). The Act does not require agencies to create new records or to conduct research, analyze data, or answer questions when responding to requests. Each federal agency handles its own records in response to requests. There are currently one hundred agencies subject to the FOIA with several hundred offices that process FOIA requests.
Content
Annual Freedom of Information Act Reports are submitted to Congress and published by FOIA.gov to promote agency accountability for the administration of the Act."
2017 Iditarod Trail Sled Dog Race,"What strategy won the 1,000 mile sled dog race across Alaska this year?",Iditarod Trail Committee,6,"Version 1,2017-03-22","sports
animals",CSV,139 KB,Other,"1,378 views",75 downloads,4 kernels,0 topics,https://www.kaggle.com/iditarod/iditarod-race,"Context
The dog sled race covers 1000 miles of the roughest, most beautiful terrain Mother Nature has to offer from Anchorage, in south central Alaska, to Nome on the western Bering Sea coast. She throws jagged mountain ranges, frozen river, dense forest, desolate tundra and miles of windswept coast at the mushers and their dog teams. Add to that temperatures far below zero, winds that can cause a complete loss of visibility, the hazards of overflow, long hours of darkness and treacherous climbs and side hills, and you have the Iditarod — the Last Great Race on Earth!
The race route is alternated every other year, one year going north through Cripple, Ruby and Galena and the next year south through Iditarod, Shageluk, Anvik.
Content
This dataset provides a record for mushers and dog teams at all seventeen checkpoints on the Iditarod trail, including the musher's name and bib number, status in the race, country of origin, checkpoint name and location, distance in miles from the last checkpoint, time in hours from departure at the last checkpoint, average speed in miles per hour, date and time of arrival and departure, layover time at the checkpoint, and the number of dogs at arrival and departure.
Acknowledgements
The data on mushers, checkpoints, and race standings was scraped from the Iditarod website.
Inspiration
Which competitor had the highest average speed during the race? Did race veterans outpace rookies on the trail? How did their strategies differ? Did the competitors' speed slow with less dogs on their team?"
Kabaddi World Cup 2016,"2016 Kabaddi World Cup, the third standard-style Kabaddi World Cup dataset",MANOJKUMAR PARMAR,6,"Version 3,2017-10-02|Version 2,2016-12-05|Version 1,2016-12-04",sports,CSV,6 KB,CC4,"3,180 views",156 downloads,6 kernels,,https://www.kaggle.com/parmarmanojkumar/kabaddi-world-cup-2016,"Context
Kabaddi is a contact sport that originated in ancient India. more information
The standard style Kabaddi World Cup, is an indoor international kabaddi competition conducted by the International Kabaddi Federation (IKF),contested by men's and women's national teams. The competition has been previously contested in 2004, 2007 and 2016. All the tournaments have been won by India. more information
The 2016 Kabaddi World Cup, the third standard-style Kabaddi World Cup, was an international kabaddi tournament governed by the International Kabaddi Federation, contested from 7 to 22 October 2016 in Ahmedabad, India. Twelve countries had competed in the tournament. more information
30 league matches played between teams. teams were deivided in 2 pools with 6 team in each pool. Top 2 teams from each team were qualifid for semifinals and winner of semifianls played in finals.
This dataset contains data for all 33 matches at granualirity level of attack, defense, allout and extra points. Data set also includes toss results, super tackle count and all out count along with match results.
Content
This dataset was manually prepared from taking necessary statistics from Kabaddi world cup site. Points acquired as per rules are main statistics.
This dataset contains necessary statistics in today format and details of all variables are as per following.
gameNo : Match number. Sequential {Integer}
team : Team name {Factor}
oppTeam : Opposition team name {Factor}
matchStage : Tournament stage at which match was played. (0 - League, 1 - SemiFinal, 2 - Final ) {Factor}
tossResult : Results of toss to select either side or raid (0 - Loss, 1 - Win) {Factor}
alloutRec : No. of time team was all out yielding 2 point {Integer}
alloutGiv : No. of time opposition team was all out yielding 2 point {Integer}
sTackleRec : No. of times super tackle by team yielding 2 point {Integer}
sTackleGiv : No. of times super tackle by opposition team yielding 2 point {Integer}
touchPntsRec : No. of times player in raid touched opposition team player yiedling 1 point for every touch {Integer}
touchPntsGiv : No. of times opposition player in raid touched team player yiedling 1 point for every touch {Integer}
bonusPntsRec : No. of times player in raid crossed bonus line yiedling 1 point for every raid {Integer}
bonusPntsGiv : No. of times opposition player in raid crossed bonus line yiedling 1 point for every raid {Integer}
raidPntsRec : No. of total raid (attack) points by team, sum of touch points and bonus points {Integer}
raidPntsGiv : No. of total raid (attack) points by opposition team, sum of touch points and bonus points {Integer}
tacklePntsRec : No. of tackle (defense) points received by team yielding 1 point for normal tackle and 2 points for super tackle {Integer}
tacklePntsGiv : No. of tackle (defense) points received by opposition team yielding 1 point for normal tackle and 2 points for super tackle {Integer}
alloutPntsRec : No. of all out points received by team yielding 2 points per allout {Integer}
alloutPntsGiv : No. of all out points received by opposition team yielding 2 points per allout {Integer}
extraPntsRec : No. of extra (technical, penalty) points received by team {Integer}
extraPntsGiv : No. of extra (technical, penalty) points received by opposition team {Integer}
totalPntsRec : No. of total points received by team, sum of raid points, tackle points, allout points & extra points {Integer}
totalPntsGiv : No. of total points received by opposition team, sum of raid points, tackle points, allout points & extra points {Integer}
touchPntsDiff : No. of touch points difference from opposition team {Integer}
bonusPntsDiff : No. of bonus points difference from opposition team {Integer}
raidPntsDiff : No. of raid points difference from opposition team {Integer}
tacklePntsDiff : No. of tackle points difference from opposition team {Integer}
alloutPntsDiff : No. of allout points difference from opposition team {Integer}
extraPntsDiff : No. of extra points difference from opposition team {Integer}
totalPntsDiff : No. of total points difference from opposition team {Integer}
matchResults : Results of the match (0 - Loss, 1 - Win) {Factor}
Acknowledgements
I would like to thank Kabaddi World Cup site for providing this data.
Inspiration
This dataset was prepared for my research paper which aims to answer following questions
Is attack is better than defence?
Does bonus point lead to victory?
What is the role of all out points on determining strength of attack?
Can we build predictive model for winning?
How strong establish teams are compared to new teams?"
Wine Quality,Modeling wine preferences by data mining from physicochemical properties.,Daniel S. Panizzo,6,"Version 1,2017-10-30",,CSV,383 KB,ODbL,"1,875 views",307 downloads,2 kernels,0 topics,https://www.kaggle.com/danielpanizzo/wine-quality,"Citation Request: This dataset is public available for research. The details are described in [Cortez et al., 2009]. Please include this citation if you plan to use this database:
P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553. ISSN: 0167-9236.
Available at: [@Elsevier] http://dx.doi.org/10.1016/j.dss.2009.05.016 [Pre-press (pdf)] http://www3.dsi.uminho.pt/pcortez/winequality09.pdf [bib] http://www3.dsi.uminho.pt/pcortez/dss09.bib
Title: Wine Quality
Sources Created by: Paulo Cortez (Univ. Minho), Antonio Cerdeira, Fernando Almeida, Telmo Matos and Jose Reis (CVRVV) @ 2009
Past Usage:
P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553. ISSN: 0167-9236.
In the above reference, two datasets were created, using red and white wine samples. The inputs include objective tests (e.g. PH values) and the output is based on sensory data (median of at least 3 evaluations made by wine experts). Each expert graded the wine quality between 0 (very bad) and 10 (very excellent). Several data mining methods were applied to model these datasets under a regression approach. The support vector machine model achieved the best results. Several metrics were computed: MAD, confusion matrix for a fixed error tolerance (T), etc. Also, we plot the relative importances of the input variables (as measured by a sensitivity analysis procedure).
Relevant Information:
The two datasets are related to red and white variants of the Portuguese ""Vinho Verde"" wine. For more details, consult: http://www.vinhoverde.pt/en/ or the reference [Cortez et al., 2009]. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).
These datasets can be viewed as classification or regression tasks. The classes are ordered and not balanced (e.g. there are munch more normal wines than excellent or poor ones). Outlier detection algorithms could be used to detect the few excellent or poor wines. Also, we are not sure if all input variables are relevant. So it could be interesting to test feature selection methods.
Number of Instances: red wine - 1599; white wine - 4898.
Number of Attributes: 11 + output attribute
Note: several of the attributes may be correlated, thus it makes sense to apply some sort of feature selection.
Attribute information:
For more information, read [Cortez et al., 2009].
Input variables (based on physicochemical tests): 1 - fixed acidity (tartaric acid - g / dm^3) 2 - volatile acidity (acetic acid - g / dm^3) 3 - citric acid (g / dm^3) 4 - residual sugar (g / dm^3) 5 - chlorides (sodium chloride - g / dm^3 6 - free sulfur dioxide (mg / dm^3) 7 - total sulfur dioxide (mg / dm^3) 8 - density (g / cm^3) 9 - pH 10 - sulphates (potassium sulphate - g / dm3) 11 - alcohol (% by volume) Output variable (based on sensory data): 12 - quality (score between 0 and 10)
Missing Attribute Values: None
Description of attributes:
1 - fixed acidity: most acids involved with wine or fixed or nonvolatile (do not evaporate readily)
2 - volatile acidity: the amount of acetic acid in wine, which at too high of levels can lead to an unpleasant, vinegar taste
3 - citric acid: found in small quantities, citric acid can add 'freshness' and flavor to wines
4 - residual sugar: the amount of sugar remaining after fermentation stops, it's rare to find wines with less than 1 gram/liter and wines with greater than 45 grams/liter are considered sweet
5 - chlorides: the amount of salt in the wine
6 - free sulfur dioxide: the free form of SO2 exists in equilibrium between molecular SO2 (as a dissolved gas) and bisulfite ion; it prevents microbial growth and the oxidation of wine
7 - total sulfur dioxide: amount of free and bound forms of S02; in low concentrations, SO2 is mostly undetectable in wine, but at free SO2 concentrations over 50 ppm, SO2 becomes evident in the nose and taste of wine
8 - density: the density of water is close to that of water depending on the percent alcohol and sugar content
9 - pH: describes how acidic or basic a wine is on a scale from 0 (very acidic) to 14 (very basic); most wines are between 3-4 on the pH scale
10 - sulphates: a wine additive which can contribute to sulfur dioxide gas (S02) levels, wich acts as an antimicrobial and antioxidant
11 - alcohol: the percent alcohol content of the wine
Output variable (based on sensory data): 12 - quality (score between 0 and 10)"
Adult income dataset,A widely cited KNN dataset as a playground,1251,6,"Version 2,2016-10-06|Version 1,2016-10-06",,CSV,5 MB,Other,"12,671 views","1,244 downloads",13 kernels,,https://www.kaggle.com/wenruliu/adult-income-dataset,"An individual’s annual income results from various factors. Intuitively, it is influenced by the individual’s education level, age, gender, occupation, and etc.
This is a widely cited KNN dataset. I encountered it during my course, and I wish to share it here because it is a good starter example for data pre-processing and machine learning practices.
Fields The dataset contains 16 columns Target filed: Income -- The income is divide into two classes: <=50K and >50K
Number of attributes: 14 -- These are the demographics and other features to describe a person
We can explore the possibility in predicting income level based on the individual’s personal information.
Acknowledgements This dataset named “adult” is found in the UCI machine learning repository http://www.cs.toronto.edu/~delve/data/adult/desc.html
The detailed description on the dataset can be found in the original UCI documentation http://www.cs.toronto.edu/~delve/data/adult/adultDetail.html"
Press Release by Govt. of India,10000 records of press release from 2003.,Rajanand Ilangovan / இராஜ்ஆனந்த் இளங்கோவன்,6,"Version 1,2017-10-02","india
government
linguistics",CSV,19 MB,CC4,952 views,55 downloads,,0 topics,https://www.kaggle.com/rajanand/press-release,"Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks.
I have used a web scrapper written in R to scrape the data from pib.nic.in website.
Acknowledgements
This dataset was scrapped from Press Information Bureau, Government of India's website. Banner Photo by Patrick Tomasso on Unsplash."
Generating chromosome overlapps,How to generate a representative dataset.,Jeanpat,6,"Version 1,2016-09-26",human genetics,Other,6 MB,CC0,"2,027 views",133 downloads,6 kernels,0 topics,https://www.kaggle.com/jeanpat/metaphase,The dataset contains two images taken at two wavelength (blue and red) of human metaphasic chromosomes (DAPI stained) hybridized with a Cy3 labelled telomeric probe. The two images can be combined into a color image. The previous dataset of overlapping chromosomes was generated chromosomes belonging to this metaphase.
Datasets for ISRL,For the labs specified in An Introduction to Statistical Learning,ishaan,6,"Version 1,2016-11-03",,CSV,569 KB,CC0,"5,605 views",679 downloads,5 kernels,,https://www.kaggle.com/ishaanv/ISLR-Auto,From http://www-bcf.usc.edu/~gareth/ISL/data.html for the purpose of conducting the labs
Missing People,Taken from National Crime Records Bureau,ArjoonnSharma,6,"Version 1,2017-10-24","india
crime",CSV,333 KB,CC0,"2,501 views",197 downloads,2 kernels,,https://www.kaggle.com/arjoonn/missing-people,"Missing Persons India
Taken from the pdf available at National Crime Records Bureau. Since the original was a PDF this is a table extracted from the original PDF using scripts.
Some level of noise is present in the data partly due to the original source and partly due to the extraction scripts."
EEG Analysis,Sleep Pattern detection,Jean-MarcBouvier,6,"Version 2,2016-09-22|Version 1,2016-09-09",,CSV,19 MB,CC0,"4,408 views",289 downloads,5 kernels,,https://www.kaggle.com/jbouv27/eeg,"The goal of this dataset is to detect a certain type of pattern (called “spindle”) inside Electroencephalogram (EEG) waves.
Data come from Garches Hospital Sleep Studies Lab (France, Paris area)
They are extracted from a single EEG, mostly from C3-A1 and C4-A1 electrodes
We have used 2 kinds of inputs to read these waves :
The raw difference in potential measured by each captor
The frequencies and power spectral densities issued from GeneCycle package periodogram function applied on the result of Fast Fourier Transformation of differences in potential:
raw Difference in potential -> FFT -> periodogram -> frequencies and power spectral densities The use of frequencies and densities gives much better results.
We know that the spindles must occur in the middle of a certain type of waves (theta waves), so we take as a training set a mixture of spindles and theta waves, and associate the outcome value 1 (as factor) for spindles and value 0 for theta waves. The 1st part of the algorithm already identified the “spindle candidates” from windows of 1 second in the EEG raw waves and sent these candidates to the “authentication” program, which is the one described. So I used R neural networks (nnet) and Kernel Support Vector Machine (KSVM) for the authentication.
Here is the program with neural net.
In version 2, I added raw EEg data of 30 mn of sleep (extrait_wSleepPage01.csv) and expected patterns (spindles.csv) manually extracted. If one of you guys has a method to more directly identify spindles from the sleep page, you are welcome !"
Kung Fu Panda,Movie Script/Screenplay,Zeeshan-ul-hassan Usmani,6,"Version 2,2017-11-08|Version 1,2017-11-07",video games,CSV,168 KB,CC0,869 views,15 downloads,,,https://www.kaggle.com/zusmani/kung-fu-panda,"Context
Do you know what is common among Kung Fu Panda, Alvin and the Chipmunks, Monster Trucks, Trolls, Spongebob Movie and Monster Vs Aliens? They all were scripted by the same authors - Jonathan Aibel and Glenn Berger.
Kung Fu Panda is a 2008 animated movie by DreamWorks Production. It has made $631 million and its one of the most successful film on the box office from DreamWorks.
There is much talk and discussions on this movie beyond cinema-goers. Some like to learn leadership lessons from it and few others try to link it with Christianity, Taoism, Mysticism and Islam.
I was wondering if we can see the script from data science perspective and can answer some of the questions with significant implications in movie and other industries.
I welcome you all to do Data Science Martial Arts with Kung-fu-Panda and see who survives
Content
It’s a complete script of Kung Fu Panda 1 and 2 in CSV format with all background narrations, scene settings and movie dialogues by characters (Po, Master Shufy, Tai Lung, Tigress, Monkey, Viper, Oogway, Mr. Ping, Mantis and Crane).
Acknowledgements
Kung Fu Panda is a production by DreamWorks Studios. All scripts were gathered from online public sources like this and this.
Inspiration
Some ideas worth exploring:
• Can we train the neural network to recognize the character by dialogue? For example, if I give any line from the script, your algorithm will be able to tell who’s more likely to say this in movie?
• Can we make the word cloud for each character (and perhaps compare it with other movie characters by same authors and see who is similar to who)
• Can we train a chat bot for Oogway to Po so kids can talk to it and it would respond the same way as Oogway or Po would
• Can we calculate the average length or dialogue
• Can we estimate the difficulty level of vocabulary being used and perhaps compare it with movies of other genre
• Can we compare the script with some religious text and find out similarities"
Fatigue striations marked on SEM photos,840 fracture SEM images alongside masks that marks the fatigue striations,Roi Shikler,6,"Version 2,2016-12-01|Version 1,2016-11-27",,Other,5 GB,CC4,"1,767 views",61 downloads,,,https://www.kaggle.com/roishik/fatigue-striations-marked-on-sem-photos,"9 fatigue experiments done, on Aluminium specimens. For every specimen, two sets (each 25 photos) of images in X200 and X400 were taken. The resolution of the photos is 1024X1280 pixels. The ground-truth database was built manually, using a GUI (Graphical User Interface) that was specially created. The database contains 840 SEM images, each has a correlated binary 'mask' image. The masks are binary images, at the same size of the SEM photos, which composed of ones and zeros, where all the pixels that containing striations are 1, and the rest are 0. The images are saved as row vectors, that stacks one upon the other in .csv files."
European Soccer Database,"25k+ matches, players & teams attributes for European Professional Football",paosheng,6,"Version 1,2016-11-23",,CSV,6 KB,ODbL,"3,072 views",375 downloads,2 kernels,,https://www.kaggle.com/paosheng/european-soccer-database,歐洲足球資料庫 背景:歐洲足球 內容:歐洲足球分析
Norwegian Development Funds 2010-2015,Predict aid based on OECD markers,HenrikHeggland,6,"Version 2,2016-11-22|Version 1,2016-11-16",finance,CSV,50 MB,CC0,"1,835 views",86 downloads,5 kernels,,https://www.kaggle.com/henrikheggland/norwegian-development-funds,"This data set gives you all funds given to development countries from Norway in the time period 2010-2015. The dataset includes OECD markers.
Some inspiration:
Predict aid based on on OECD markers.
Predict implementation partner based on the available features."
Traffic Violations in USA,Traffic Violations in USA,Felix Gutierrez,6,"Version 1,2017-10-31",demographics,CSV,352 MB,CC0,"1,860 views",228 downloads,2 kernels,2 topics,https://www.kaggle.com/felix4guti/traffic-violations-in-usa,"Traffic violations followed the invention of the automobile: the first traffic ticket in the United States was allegedly given to a New York City cab driver on May 20, 1899, for going at the breakneck speed of 12 miles per hour. Since that time, countless citations have been issued for traffic violations across the country, and states have reaped untold billions of dollars of revenue from violators.
Traffic violations are generally divided into major and minor types of violations. The most minor type are parking violations, which are not counted against a driving record, though a person can be arrested for unpaid violations.
The most common type of traffic violation is a speed limit violation. Speed limits are defined by state."
State Election Results 1971 - 2012,"Results for 72,000 elections",Sohier Dane,6,"Version 1,2017-10-03","politics
political science",CSV,2 MB,CC0,"1,385 views",157 downloads,3 kernels,,https://www.kaggle.com/sohier/state-election-results-1971-2012,"Context
This dataset contains results of general elections to the lower house of the state legislatures in the United States over the last fifty years, up to 2012. This dataset was created by the Princeton Gerrymandering Project as part of their effort to analyze and combat partisan gerrymandering. The Supreme Court will be hearing a very important case on this issue on October 3rd 2017. Regardless of who wins, this dataset will be of interest to anyone hoping to defeat (or achieve!) a gerrymandering attempt.
Content
Each row represents one election, from the perspective of the winner. For example, the first row of the data should be read as a victory for a Democrat who was not the incumbent.
Acknowledgements
This dataset was kindly made available by the Princeton Gerrymandering Project. You can find their copy, detailed discussion of the data, and their code here."
Arrest Related Violence in California,"Records on use of force incidents, officer deaths, and deaths in prison",Sohier Dane,6,"Version 1,2017-11-04",crime,CSV,20 MB,CC0,"1,171 views",163 downloads,,0 topics,https://www.kaggle.com/sohier/arrest-related-violence-in-california,"Context
This dataset provides information on violence in California's criminal justice system, whether the victim was a civilian or an officer or if the incident occurred during the arrest or while a subject was in custody. It's composed of a few related datasets:
Use of force incidents: The use of force (URSUS) incidents that result in serious bodily injury or death or involved the discharge of a firearm are reported annually from LEAs and other entities throughout the state that employ peace officers. The URSUS data is narrowly defined and does not represent the totality of use of force incidents that occur in California. LEAs are only required to report use of force incidents that result in serious bodily injury or death of either the civilian or the officer and all incidents where there is a discharge of a firearm. As such, caution must be used when using the data for comparisons or in calculating rates.
Law enforcement officers killed: Law Enforcement Officer's Killed or Assaulted (LEOKA) data are reported as part of the Federal Uniform Crime Reporting (UCR) Program by LEAs throughout the state. LEOKA data are summary data, meaning it is a collection of information describing the totality of incidents, not a collection at the detailed, incident level. LEOKA is a federally mandated collection. From the 1960's until 1990, the CJSC did not retain any of the LEOKA data; the forms were passed along to the Federal Bureau of Investigation (FBI). In 1990, the DOJ began to collect and retain the data from the LEOKA form for statistical purposes, but it wasn't until 2000, that full retention at the State level was defined and standardized.
Death in Custody & Arrest-Related Deaths: State and local law enforcement agencies and correctional facilities report information on deaths that occur in custody or during the process of arrest in compliance with Section 12525 of the California Government Code. Contributors include: California law enforcement agencies, county probation departments, state hospitals, and state correctional facilities. Data are subject to revision as reports are received by the California Department of Justice (DOJ); figures in previous and current releases may not match.
Citizens' Complaints Against Peace Officers: State and local law enforcement agencies that employ peace officers provide Citizens' Complaints Against Peace Officers (CCAPO) data via an annual summary. The information includes the number of criminal and non-criminal complaints reported by citizens and the number of complaints sustained. Assembly Bill 953 (2015) modified the reporting requirements to expand the types of findings and also include complaints based upon racial and identity profiling claims. 2016 was the first year of collection under the new reporting requirements.
Acknowledgements
This dataset was made available by the state of California's open justice program.
Photo by Meric Dagli."
The Works of Charles Darwin,Collected from Project Gutenberg [text],FuzzyFrogHunter,6,"Version 3,2017-10-14|Version 2,2017-10-14|Version 1,2017-10-14","books
biology
scientists",Other,20 MB,Other,"1,001 views",65 downloads,,0 topics,https://www.kaggle.com/fuzzyfroghunter/darwin,"Context
This is a collection of all the works of Charles Darwin that are available through Project Gutenberg.
This dataset is subject to the Project Gutenberg license.
It is very possible that I have missed some of his works. Please add them and update the ""Last updated"" date below if you get the chance. Otherwise, if you leave a comment on this dataset, I will try and do so myself.
Content
Last updated: October 13, 2017
This dataset consists of the following works in TXT format:
""On the Origin of Species by Means of Natural Selection or the Preservation of Favoured Races in the Struggle for Life."" - pg22764.txt
""The Formation of Vegetable Mould through the action of worms with observations on their habits"" - 2355-0.txt
""The Descent of Man and Selection in Relation to Sex, Vol. I (1st edition)"" - 34967-0.txt
""The Descent of Man and Selection in Relation to Sex Volume II (1st Edition)"" - 36520-0.txt
""A Monograph on the Sub-class Cirripedia (Volume 1 of 2)"" - pg31558.txt
""A Monograph on the Sub-class Cirripedia (Volume 2 of 2)"" - 46408-0.txt
""The Foundations of the Origin of Species"" - pg22728.txt
""Coral Reefs"" - pg2690.txt
""More Letters of Charles Darwin Volume II"" - pg2740.txt
""The Variation of Animals and Plants under Domestication Volume I"" - pg2871.txt
""The Variation of Animals and Plants under Domestication Volume II"" - pg2872.txt
""South American Geology"" or ""Geological Observations On South America"" - pg3620.txt
""The Different Forms of Flowers on Plants of the Same Species"" - pg3807.txt
""The Effects of Cross & Self-Fertilisation in the Vegetable Kingdom"" - pg4346.txt
""The Movement and Habits of Climbing Plants"" - 2485-0.txt
""The Expression of Emotion in Man and Animals"" - pg1227.txt
""The Life and Letters of Charles Darwin, Volume I (of II)"" - pg2087.txt
""The Life and Letters of Charles Darwin, Volume II (of II)"" - pg2088.txt
""More Letters of Charles Darwin Volume I (of II)"" - pg2739.txt
""A Naturalist's Voyage Round the World The Voyage Of The Beagle"" - pg3704.txt
""Charles Darwin: His Life in an Autobiographical Chapter, and in a Selected Series of His Published Letters"" - pg38629.txt
""Insectivorous Plants"" - pg5765.txt
Acknowledgements
Content taken from Project Gutenberg
Image taken from Wikimedia Commons
Inspiration
Making this data available for any kind of textual analysis. I intend for this to be part of a series."
HanziDB,List of simplified Chinese characters ordered by frequency rank.,Rudd Fawcett,6,"Version 2,2017-10-03|Version 1,2017-10-03","languages
china
linguistics",CSV,540 KB,ODbL,903 views,71 downloads,,,https://www.kaggle.com/ruddfawcett/hanzidb,"Content
A ranked list (by frequency) of over 9k simplified Chinese characters.
Acknowledgements
All data scraped from HanziDB.org, which is based on Jun Da's Modern Chinese Character Frequency List.
Inspiration
Some possible questions:
What is the distribution of radicals through the 100 most popular characters? 500? 1,000?
Does stroke count affect usage?
Is there an association between the number of strokes and the HSK level of characters?"
Employment in Manufacturing,Percent change of employment from the same quarter a year ago,US Bureau of Labor Statistics,6,"Version 1,2016-11-07",,CSV,1 KB,Other,"1,346 views",89 downloads,,0 topics,https://www.kaggle.com/bls/employment-in-manufacturing,"Context
This dataset lists the percent change of employment in U.S. manufacturing from the same quarter a year ago. It includes data collected on a quarterly basis from March 31, 1995 through June 30, 2016.
Content
Data includes the date of the quarterly collection and the percent change of employment in manufacturing from the same quarter a year ago.
Inspiration
What is the general trend of employment in manufacturing?
What points in time had the largest negative and positive changes? Do they correlate with historical events or policy changes?
The BLS Productivity database includes other datasets about manufacturing, such as unit labor costs and compensation. Do any of these factors correlate with the change of employment?
Acknowledgement
This dataset is part of the US Department of Labor Bureau of Labor Statistics Datasets (the BLS Productivity database), and the original source can be found here."
National Wetlands Inventory,Location and Type of Wetlands and Deepwater Habitats in the United States,ArcGIS Open Data,6,"Version 1,2016-11-17",ecology,CSV,80 MB,Other,"1,872 views",91 downloads,4 kernels,0 topics,https://www.kaggle.com/arcgisopendata/national-wetlands-inventory,"Context
The data delineate the areal extent of wetlands and surface waters as defined by Cowardin et al. (1979). Certain wetland habitats are excluded from the National mapping program because of the limitations of aerial imagery as the primary data source used to detect wetlands. These habitats include seagrasses or submerged aquatic vegetation that are found in the intertidal and subtidal zones of estuaries and near shore coastal waters. Some deepwater reef communities (coral or tuberficid worm reefs) have also been excluded from the inventory. These habitats, because of their depth, go undetected by aerial imagery. By policy, the Service also excludes certain types of ""farmed wetlands"" as may be defined by the Food Security Act or that do not coincide with the Cowardin et al. definition. Contact the Service's Regional Wetland Coordinator for additional information on what types of farmed wetlands are included on wetland maps.
Content
The dataset includes:
OBJECTID
ATTRIBUTE
WETLAND_TYPE
ACRES
GLOBALID
ShapeSTArea
ShapeSTLength
Acknowledgement
The original dataset and metadata can be found here.
Inspiration
Can you visualizes the differences in the wetlands shape by type?"
Shinzo Abe (Japanese Prime Minister) Twitter NLP,Let's analyze Japanese politician,Team AI,6,"Version 2,2017-10-27|Version 1,2017-10-24","politicians
politics
twitter",CSV,59 KB,CC3,"1,593 views",60 downloads,10 kernels,,https://www.kaggle.com/team-ai/shinzo-abe-japanese-prime-minister-twitter-nlp,"Context
This dataset contains Japanese Prime Minister Tweet. Japanese culture, diplomatic problem ( North Korea and Tramp etc), time of disaster, economics... For example,14.April 2014 ""Removing radiation contaminated water in all weather, 365/24 at Fukushima. I am deeply thankful for dedication and commitment of our peers."" Maybe if you analyze his tweets about Japanese economy this data will be useful for stock price forecasting etc.
Content
This dataset contains following the data:
url
Full name show
user name dir
tweet nav
tweet nav_link
tweet text size block
tweet text size link
tweet text size Link_link
Profile tweet 1
Profile tweet 2
Profile tweet 3
replay
re tweet
like
Inspiration
Inspired by Trump vs Clinton NLP"
Food 101,Pictures of 101 types of food,DanB,6,"Version 1,2018-01-04",food and drink,Other,5 GB,Other,398 views,62 downloads,,0 topics,https://www.kaggle.com/dansbecker/food-101,"Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
This is the Food 101 dataset, also available from https://www.vision.ee.ethz.ch/datasets_extra/food-101/
It contains images of food, organized by type of food. It was used in the Paper ""Food-101 – Mining Discriminative Components with Random Forests"" by Lukas Bossard, Matthieu Guillaumin and Luc Van Gool. It's a good (large dataset) for testing computer vision techniques.
Acknowledgements
The Food-101 data set consists of images from Foodspotting [1] which are not property of the Federal Institute of Technology Zurich (ETHZ). Any use beyond scientific fair use must be negociated with the respective picture owners according to the Foodspotting terms of use [2].
[1] http://www.foodspotting.com/ [2] http://www.foodspotting.com/terms/"
Tatoeba,Crowd-source Example Sentence and Translations,Liling Tan,6,"Version 4,2018-01-05|Version 3,2018-01-02|Version 2,2018-01-02|Version 1,2017-12-29","languages
linguistics",CSV,639 MB,CC3,701 views,30 downloads,2 kernels,0 topics,https://www.kaggle.com/alvations/tatoeba,"Context
Tatoeba is a crowd-sourced dataset made up of example sentences and their translations.
This dump uploaded on Kaggle is downloaded some time in Oct/Nov 2017. The latest dumps can be downloaded from https://tatoeba.org/eng/downloads
Acknowledgements
Credits goes to the maintainers and the crowd on https://tatoeba.org
Credits of the banner image goes to Patrick Tomasso
License
The official license is CC BY-SA 2.0"
Cannabis Strains,Marijuana strain dataset,LiamLarsen,6,"Version 9,2017-12-17|Version 8,2017-12-17|Version 7,2017-12-17|Version 6,2017-12-17|Version 5,2017-12-17|Version 4,2017-12-17|Version 3,2017-12-17|Version 2,2017-12-17|Version 1,2017-12-17","healthcare
botany",CSV,415 KB,Other,687 views,78 downloads,2 kernels,,https://www.kaggle.com/kingburrito666/cannabis-strains,"Context
Cannabis Strains
Content
strain name: Given name of strain
type of strain: indica, sativa, hybrid
rating: user ratings averaged
effects: different effects optained
taste: taste of smoke
description: backround, etc
Acknowledgements
leafly.com
Inspiration
Marijuana may get a bad rep in the media as far as the decriminalization debate goes, but its health benefits can no longer go unnoticed. With various studies linking long-term marijuana use to positive, health-related effects, there are more than just a few reasons to smoke some weed every day.
A study done by the Boston Medical Center and the Boston University of Medicine, examined 589 drug users—more than 8 out of 10 of whom were pot smokers. It determined that “weed aficionados” were no more likely to visit the doctor than non-drug users. If an increased risk of contracting ailments is what’s preventing you from smoking more weed, it looks like you’re in the clear!
One of the greatest medicinal benefits of marijuana is its pain relieving qualities, which make it especially effective for treating chronic pain. From menstruation cramps to nerve pain, as little as three puffs of bud a day can help provide the same relief as synthetic painkillers. Marijuana relieves pain by “changing the way the nerves function,” says Mark Ware, MD and assistant professor of anesthesia and family medicine at McGill University.
Studies have found that patients suffering from arthritis could benefit from marijuana use. This is because naturally occurring chemicals in cannabis work to activate pathways in the body that help fight off joint inflammation."
Oil and Gas,,Baking Pi,6,"Version 1,2017-12-14",natural resources,CSV,1012 KB,Other,663 views,78 downloads,,0 topics,https://www.kaggle.com/raspberrypie/oil-and-gas,"Context
The Global dataset of oil and natural gas production, prices, exports, and net exports.
Content
Oil production and prices data are for 1932-2014 (2014 data are incomplete); gas production and prices are for 1955-2014; export and net export data are for 1986-2013. Country codes have been modified from earlier versions to conform to Correlates of War (COW) and Quality of Government (QOG) standards
Acknowledgements
Ross, Michael; Mahdavi, Paasha, 2015, ""Oil and Gas Data, 1932-2014"", doi:10.7910/DVN/ZTPW0Y, Harvard Dataverse
Inspiration
How has the price varied from 1900s to 2000s?"
elemental_properties,,Chris Bartel,6,"Version 1,2017-12-23",,CSV,2 KB,CC0,168 views,638 downloads,,0 topics,https://www.kaggle.com/cbartel/elemental-properties,This dataset does not have a description yet.
2016 U.S. Presidential Campaign Texts and Polls,Debate and Speech Transcripts & Voter Group Polls,Alan Du,6,"Version 1,2017-12-30","politics
demographics
linguistics
political science",Other,2 MB,CC0,767 views,90 downloads,,0 topics,https://www.kaggle.com/alandu20/2016-us-presidential-campaign-texts-and-polls,"Context
This is an aggregate of the data I studied for my thesis titled, ""Data Mining in Presidential Debates and Speeches: How Campaign Rhetoric Shaped Voter Opinion in the 2016 U.S. Presidential Race"". The goal of my thesis was to use NLP techniques to understand how Donald Trump’s rhetoric impacted the opinions of various voter groups throughout his campaign. Here is a summary of my findings:
Trump’s words were typically more common in an American English corpus and more extreme on both ends of the sentiment spectrum
Trump not only used rhetorical devices for persuasion but also adeptly coupled these devices with the right talking points based on the composition of his audience
Precise execution of the above strategy garnered him an unexpectedly large number of votes from the white female and Hispanic demographics
I hope that others can use this dataset to answer questions of their own about the 2016 presidential campaign.
Content
Collection of data from the 2016 U.S. Presidential Election Campaign containing:
Transcripts of the three presidential debates, divided into separate Trump and Clinton text files
Transcripts of Trump's 64 speeches delivered after the RNC and Clinton's 35 speeches delivered after the DNC
Transcripts of select speeches delivered by candidates during the primary campaigns
USC Dornsife/LA Times Presidential Election Poll, with daily breakdown by voter groups
Five Thirty Eight Election Poll, containing daily data from numerous pollsters
Acknowledgements
Debate and speech texts scraped from the American Presidency Project website."
Eclipse Megamovie,Citizen science covering the 2017 eclipse,Eclipse MegaMovie,6,"Version 1,2017-12-07","astronomy
bigquery",BigQuery,77 MB,CC0,830 views,0 downloads,2 kernels,0 topics,https://www.kaggle.com/eclipse-megamovie/eclipse-megamovie,"This first-of-its-kind citizen science project is a collection of photos submitted by a group of dedicated volunteers from locations across the United States during the August 21, 2017 total solar eclipse.
The bigquery tables include metadata for the photos, links to the photos, and astronomical measurements extracted from the photos.
Acknowledgements
This dataset was kindly prepared by Google, UC Berkeley, and thousands of volunteers. Please see https://eclipsemega.movie/ for more information.
Inspiration
Can you map out the locations of the contributors to the project? How many of them were outside the path of totality?"
ResNet-50,ResNet-50 Pre-trained Model for PyTorch,PyTorch,6,"Version 1,2017-12-14","machine learning
pre-trained model",Other,91 MB,CC0,571 views,4 downloads,,0 topics,https://www.kaggle.com/pytorch/resnet50,"ResNet-50
Deep Residual Learning for Image Recognition
Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity.
An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.
The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.
Authors: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun
https://arxiv.org/abs/1512.03385
Architecture visualization: http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006
What is a Pre-trained Model?
A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.
Why use a Pre-trained Model?
Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it."
ResNet-101,ResNet-101 Pre-trained Model for PyTorch,PyTorch,6,"Version 1,2017-12-14","machine learning
pre-trained model",Other,159 MB,CC0,462 views,7 downloads,,0 topics,https://www.kaggle.com/pytorch/resnet101,"ResNet-101
Deep Residual Learning for Image Recognition
Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity.
An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.
The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.
Authors: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun
https://arxiv.org/abs/1512.03385
Architecture visualization: http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006
What is a Pre-trained Model?
A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.
Why use a Pre-trained Model?
Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it."
Brazil's Parliamentary Quota - Cota Parlamentar,Data about the donations and quotas for the parliamentary people in Brazil.,Anderson Chaves,5,"Version 2,2017-12-08|Version 1,2017-12-07","brazil
politics",CSV,77 MB,Other,184 views,14 downloads,,,https://www.kaggle.com/apachaves/brazil-parliamentary-quota,"Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
This dataset contains the data from the donations and quotas of the parliamentary people in Brazil from 2009 to 2017. The original files were separated and can be found in this link here.
IMPORTANT: This is suppose to be a first version, and better ones will come with more time spent in preparation. For the moment the job I had was to translate the headers and join the files. I would like to provide a data dictionary of the variables too, but even the brazilian government did not. Hopefully most variables can be understood by its name.
Acknowledgements
Thank you everyone in advance for the suggestions and feedbacks. This is my first attempt of uploading a dataset here and I might make mistakes. Be gentle, please.
Inspiration
I would like governments to be more transparent and that people could audit more their data."
Football striker performance,Data on the performance of football strikers and wingers.,giim,5,"Version 1,2017-12-14","association football
sports",CSV,211 KB,CC0,235 views,41 downloads,,0 topics,https://www.kaggle.com/gimunu/football-striker-performance,"Context
The aim of this dataset is to offer in a relatively small number of columns (~30) data to compare the performance of some football players, or to compare the efficiency of strikers in-between different European leagues.
Content
Inside the dataset are some performance indicators (goals, assists, minutes played, games played) for football strikers over (up to) the last 5 years.
Acknowledgements
The data was extracted from https://www.transfermarkt.co.uk"
Ben Hamner's Tweets,A complete Twitter timeline from Kaggle's CTO,Megan Risdal,5,"Version 1,2017-12-12","linguistics
twitter
internet",CSV,791 KB,Other,318 views,10 downloads,,0 topics,https://www.kaggle.com/mrisdal/ben-hamners-tweets,"Context
I was looking for something Ben Hamner, Kaggle's CTO, tweeted a while back and it turned out just using R's TwitteR package was easier than scrolling through his timeline. Since I collected all of his tweets, I figured I would share them here as well.
Content
What you get: All of Ben Hamner's tweets current through today (12 December 2017).
What's inside: The text from his tweets plus metadata like favorites, retweets, timestamps, etc. You can even see whether or not I've personally favorited or retweeted his tweets.
Acknowledgements
Thanks to Ben for his insightful tweets! Check out his tweets at @benhamner on Twitter."
Emotions Sensor Data Set,"Words Classified Statistically Into 7 Basic Emotions (happy,sad,anger,surprise)",jon.bill,5,"Version 4,2017-12-29|Version 3,2017-12-27|Version 2,2017-12-27|Version 1,2017-12-27","emotion
robotics
artificial intelligence",CSV,114 KB,ODbL,689 views,84 downloads,,0 topics,https://www.kaggle.com/iwilldoit/emotions-sensor-data-set,"Context
Emotions Detection Is an Interesting Blend of Psychology and Technology.
-This Technology Helps to Build a Companion Robots,This Robots Can be Friendly and Have The Ability to Recognize Users’ Emotions and Needs, and to Act Accordingly.
-It’s Essentially a Way to Determine How your Consumers are Reacting to Your Website, Social Media Posts, and Other Forms of Your Online Content This Helps to Transform the Face of Marketing and Advertising by Reading Human Emotions and Then Adapting Consumer Experiences to These Emotions in Real Time.
Content
Now Emotions Sensor DataSet Helps To Detect Emotions In Text or Voice Speech and You Can Easily Build a Sentiment Analysis Bot In few simple steps.
Emotions Sensor Data Set Contain Top 1100 English Words Classified Statistically Into 7 Basic Emotion Disgust, Surprise ,Neutral ,Anger ,Sad ,Happy and Fear.
The Words Have Been Manually and Automatically Labeled Using Andbrain Engine From over 1.185.540 Classified Words Blogs,Twitters and Sentences .
-You Can See How To Build Sentiment Analysis In Few Steps Using Emotional Sensor Data Set
Questions or Get full Emotions Sensor Data Set ?
Contact us lakadsens@gmail.com"
Diabetes,"Prevalence of obesity, diabetes, and other cardiovascular risk factors.",qizheng,5,"Version 1,2017-12-30","healthcare
demographics",Other,30 KB,CC0,734 views,116 downloads,,,https://www.kaggle.com/yuqizheng/diabetes,"Context
This is the data for my Project 2 from STA 108 with Prof. Chen Fall '17 at U.C. Davis.
Content
Data description: The data “diabetes.txt” contains 16 variables on 366 subjects who were interviewed in a study to understand the prevalence of obesity, diabetes, and other cardiovascular risk factors in central Virginia for African Americans. We will consider building regression models with glyhb as the response variable as Glycosolated Hemoglobin > 70 is often taken as a positive diagnostics of diabetes. The goal is to ﬁnd the “best” model for later use.
Acknowledgements
I had a great deal of help with this project from my tutor and also from the TA Cody Carroll.
Inspiration
I enjoyed using this data set, and thought that it'd be nice to share with the community."
VGG-19,VGG-19 Pre-trained Model for Keras,Keras,5,"Version 2,2017-12-12|Version 1,2017-12-07","machine learning
pre-trained model",Other,580 MB,CC0,915 views,23 downloads,,0 topics,https://www.kaggle.com/keras/vgg19,"VGG19
Very Deep Convolutional Networks for Large-Scale Image Recognition
In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.
Authors: Karen Simonyan, Andrew Zisserman
https://arxiv.org/abs/1409.1556
VGG Architectures
What is a Pre-trained Model?
A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.
Why use a Pre-trained Model?
Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it."
InceptionV3,InceptionV3 Pre-trained Model for Keras,Keras,5,"Version 2,2017-12-13|Version 1,2017-12-07","machine learning
pre-trained model",Other,162 MB,CC0,663 views,26 downloads,2 kernels,0 topics,https://www.kaggle.com/keras/inceptionv3,"InceptionV3
Rethinking the Inception Architecture for Computer Vision
Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error on the validation set (3.6% error on the test set) and 17.3% top-1 error on the validation set.
Authors: Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna
https://arxiv.org/abs/1512.00567
InceptionV3 Architecture
What is a Pre-trained Model?
A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.
Why use a Pre-trained Model?
Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it."
AlexNet,AlexNet Pre-trained Model for PyTorch,PyTorch,5,"Version 1,2017-12-13","machine learning
pre-trained model",Other,216 MB,CC0,720 views,40 downloads,,0 topics,https://www.kaggle.com/pytorch/alexnet,"AlexNet
ImageNet Classification with Deep Convolutional Neural Networks
We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.
Authors: Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton
https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks
Top of the image is cut-off even in the original paper :D
What is a Pre-trained Model?
A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.
Why use a Pre-trained Model?
Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it."
DenseNet-201,DenseNet-201 Pre-trained Model for PyTorch,PyTorch,5,"Version 1,2017-12-13","machine learning
pre-trained model",Other,73 MB,CC0,341 views,3 downloads,,0 topics,https://www.kaggle.com/pytorch/densenet201,"DenseNet-201
Densely Connected Convolutional Networks
Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and models are available at this https URL.
Authors: Gao Huang, Zhuang Liu, Kilian Q. Weinberger, Laurens van der Maaten
https://arxiv.org/abs/1608.06993
DenseNet Architectures
What is a Pre-trained Model?
A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.
Why use a Pre-trained Model?
Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it."
InceptionV3,InceptionV3 Pre-trained Model for PyTorch,PyTorch,5,"Version 1,2017-12-13","machine learning
pre-trained model",Other,96 MB,CC0,342 views,8 downloads,,0 topics,https://www.kaggle.com/pytorch/inceptionv3,"InceptionV3
Rethinking the Inception Architecture for Computer Vision
Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error on the validation set (3.6% error on the test set) and 17.3% top-1 error on the validation set.
Authors: Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna
https://arxiv.org/abs/1512.00567
InceptionV3 Architecture
What is a Pre-trained Model?
A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.
Why use a Pre-trained Model?
Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it."
ResNet-34,ResNet-34 Pre-trained Model for PyTorch,PyTorch,5,"Version 1,2017-12-14","machine learning
pre-trained model",Other,77 MB,CC0,476 views,8 downloads,,0 topics,https://www.kaggle.com/pytorch/resnet34,"ResNet-34
Deep Residual Learning for Image Recognition
Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity.
An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.
The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.
Authors: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun
https://arxiv.org/abs/1512.03385
Architecture visualization: http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006
What is a Pre-trained Model?
A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.
Why use a Pre-trained Model?
Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it."
International T20 Cricket,Structured ball-by-ball data for International T20 matches,sna,5,"Version 1,2017-10-17","cricket
sports",CSV,32 MB,CC0,875 views,175 downloads,,0 topics,https://www.kaggle.com/shoaib/t20-international-cricket-data,"Context
I know, I am not only one who got inspired after watching/reading Moneyball and dreamt of making great contributions to what could've later transformed into cricket's version of Sabermetrics. Like many out there, I also started my journey by building web scrapers for collecting ODI cricket match data and have spend a considerable amount of time, trying to make sense out of it. My current research interests involves alternate player metrics/statistics which can explain players' ability to contribute to a match win.
The source data for the shared dataset is not my own (see the acknowledgement section), as I didn't want to deal with the headache of figuring out all the possible permissions and copyright issues that may arise if I released it online.
Content
This dataset has ball by ball data for T20 international cricket matches. The data is available for matches played between 2005 and 2017.
The columns included are:
Match_Id - Unique ID for the matches
team - Team 1 (the numbering has no significance other than to differentiate between the teams)
team2 - Team 2 (the numbering has no significance other than to differentiate between the teams)
season - Season information
date - Date of the match
series - The name of the series. A series is a collection of matches, generally played between two or more countries
match_number - The number as part of the series
venue - Name of the cricket stadium/ground where the match was played
city - Name of the city where the match was played
toss_winner - The team who won the toss
toss_decision - The decision made by the team who won the toss
player_of_match - Man of the match, generally the player with the most significant contribution with bat or ball or both
umpire, reserve_umpire, tv_umpire, match_referee - The Umpiring team
winner - The team which won the match
winner_wickets - The number of wickets by which the team won if applicable
winner_runs - The number of runs by which the team won if applicable
neutralvenue - Binary field indicating whether the match was played at a neutral venue
method - Indicates whether the D/L method was invoked during the match
outcome - Indicates special outcome like 'no result' (matching getting cancelled) or 'tie'
bowl_out - A non-missing value indicates that the match went into a bowl out in order to break the tie
competition - The name of the competition
eliminator - A non-missing value indicates the use of eliminator method for breaking a tied result
Innings - The current inning
Overs - Overs
Batting_Team - The team which is batting
Striker - The batsman who faced the ball
Non_Striker - The batsman at the non-strikers end
Bowler - The player who is bowling
Run_Scored - Run scored
Extras - Extras scored
Dismissal - How the batsman got out, if applicable
Dismissed - The batsman who got dismissed
Acknowledgements
The source data for shared dataset comes from the https://cricsheet.org website. Thanks to cricsheet team for making the data available for use.
The code used for consolidating the source dataset is available at my GitHub page: https://github.com/shoaibnajeeb/cricsheet-data-preparation.
The image used:
England v Sri Lanka, Twenty20 International - Thursday 15 June 2006
Source: https://www.flickr.com/photos/badgerswan/173466044
Licensed under the Creative Commons Attribution 2.0 Generic license.
Inspiration
How would you like it, if you can predict the probability of the team batting second winning the match?
What if I tell you that, you can start making these predictions right from the start of the second innings?
I have always been fascinated by the idea of making predictions about which team is going to win the match or how much the teams would score at the end of each innings. The availability of ball by ball data opens up a lot of possibilities for creating different kinds of aggregations at the player, match, team and other levels. I intend to add more datasets featuring aggregations built on top of the ball by ball data."
English Premier League in-game match data,Detailed player data from the top tier of English football,ShubhamPawar,5,"Version 1,2017-12-29","association football
sports",Other,2 MB,CC4,875 views,126 downloads,,2 topics,https://www.kaggle.com/shubhmamp/english-premier-league-match-data,"This dataset was obtained as part of my project to rate player performances in a game and use it to model game outcomes. I was looking for an open dataset which included important in-game stats for players but couldn't find one. Hence I ended up scraping data myself. Subsequently, it has been successfully used to predict player performances in future games and build an optimum fantasy league team. I would be updating the dataset monthly to include newer games of the current season.
The dataset includes 2 JSON files. One of the files describes in-game match stats for every match of the past 4 seasons (current season included) like player touches, passes, shots, yellow cards, saves etc. Some of the stats are available as aggregate stats for the entire team and some of them are player specific. Second, file describes general match outcomes like the full time and half-time score etc.
Data snapshot --
{
    ""1190174"":{
        ""13"":{
            ""team_details"":{
                ""team_id"":""13"",
                ""team_name"":""Arsenal"",
                ""team_rating"":""7.30714285714286"",
                ""date"":""11/08/2017""
            },
            ""aggregate_stats"":{
                ""fk_foul_lost"":""9"",
                ""won_contest"":""16"",
                ""possession_percentage"":""70"",
                ""total_throws"":""21"",
                 .............
             },
            ""Player_stats"":{
                ""Petr Cech"":{
                    ""player_details"":{
                        ""player_id"":""6775"",
                        ""player_name"":""Petr Cech"",
                        ""player_position_value"":""1"",
                        ""player_position_info"":""GK"",
                        ""player_rating"":""5.78""
                    },
                    ""Match_stats"":{
                        ""good_high_claim"":""1"",
                        ""touches"":""27"",
                        ""total_tackle"":""1"",
                        ""total_pass"":""20"",
                        ""formation_place"":""1"",
                        ""accurate_pass"":""16""
                    },
This dataset could be used to predict player performances and how a particular player/team plays against another. Can a game outcome be modeled on the player composition of the participating teams? Are goals the most important factor that determines season outcomes or something other than historical goals be used to predict the future team performance in the league?"
BITCOIN,,JohnnyHa,5,"Version 1,2017-10-21",,CSV,226 KB,CC0,"2,565 views",339 downloads,2 kernels,0 topics,https://www.kaggle.com/johnnyjai/bitcoin,This dataset does not have a description yet.
Canada National Justice Survey 2016,Canadian government justice system survey results,Aleksey Bilogur,5,"Version 1,2017-10-11","government agencies
crime
politics",CSV,4 MB,Other,801 views,82 downloads,,,https://www.kaggle.com/residentmario/national-justice-survey-2016,"Context
This dataset is the anonymized result of responses submitted to a survey collected by the Canadian Department of Justice in 2016. This survey ""...focuses on the criminal justice system (CJS) to inform the current criminal justice system review...[this] involved a traditional public opinion research survey, in informed choice survey and in person and online focus groups...this work was undertaken to support reforms and new initiatives in this area.""
This dataset is the survey component of this review.
Content
Respondents were asked over 50 questions on their perception of how the Canadian Justice system works at large. This dataset was published in a typical survey output format, in that most questions are 1-10 rating scales or 0-1 True/False questions, with some free-text responses intermixed. To understand the fields, please see the attached data dictionary, or otherwise access it here.
Acknowledgements
This data was published as-is by the Government of Canada, here. It is licensed under the Open Government License - Canada.
Inspiration
In a time of increasingly invective dialogue between police forces and the people they police, this dataset provides a window on the general level of satisfaction and concern that Canadian government citizens have with their country's justice systems. These results are mostly generalizable to the developed world as a whole."
Handwritten Letters 2,Images of Russian Letters,Olga Belitskaya,5,"Version 1,2017-12-31","languages
photography
classification
deep learning",Other,58 MB,Other,"1,044 views",86 downloads,,0 topics,https://www.kaggle.com/olgabelitskaya/handwritten-letters-2,"History
I made the database from my own photos of Russian lowercase letters written by hand.
Content
The GitHub repository with examples
Handwritten Letters on GitHub
The main dataset (letters3.zip)
6600 (200x33) color images (32x32x3) with 33 letters and the file with labels letters3.txt.
Photo files are in the .png format and the labels are integers and values.
Additional letters3.csv file.
The file LetterColorImages3.h5 consists of preprocessing images of this set: image tensors and targets (labels)
The data can be combined with the database ""Classification of Handwritten Letters""
Letter Symbols => Letter Labels
а=>1, б=>2, в=>3, г=>4, д=>5, е=>6, ё=>7, ж=>8, з=>9, и=>10, й=>11, к=>12, л=>13, м=>14, н=>15, о=>16, п=>17, р=>18, с=>19, т=>20, у=>21, ф=>22, х=>23, ц=>24, ч=>25, ш=>26, щ=>27, ъ=>28, ы=>29, ь=>30, э=>31, ю=>32, я=>33
Background Images => Background Labels
striped=>0, gridded=>1, background=>2, graph paper=>3
Acknowledgements
As an owner of this database, I have published it for absolutely free using by any site visitor.
Usage
Classification, image generation, etc. in a case of handwritten letters with a small number of images are useful exercises.
Improvement
There are lots of ways for increasing this set and the machine learning algorithms applying to it. For example: add the same images but written by other person or add capital letters."
Porn Data,,zelhassn,5,"Version 1,2017-11-13",,CSV,20 MB,CC0,"1,030 views",88 downloads,,0 topics,https://www.kaggle.com/ljlr34449/porn-data,This dataset does not have a description yet.
Donald Trump Forbes 400 Rankings,Estimated net wealth of the president of the USA since 1985,DaveRosenman,5,"Version 1,2017-10-19",,CSV,3 KB,Other,996 views,55 downloads,,0 topics,https://www.kaggle.com/daverosenman/donald-trump-forbes-400-rankings-1985-to-2017,"Context
Donald Trump's 'Forbes Richest 400 Americans' rankings and estimated net worth from 1985 to 2017.
Content
Forbes Magazine's yearly Richest 400 Americans list was first published in 1982. Trump was on the list in 1982, 1983, and 1984, which are the only three years that I haven't been able to find his ranking. I left those years off the list. In 1982, according to ""TrumpNation: The Art of Being the Donald"" by Timothy O'Brien, ""Forbes gave Donald an undefined share of a family fortune the magazine estimate at $200 million - at at time when all Donald owned personally was a half interest in the Grand Hyatt and a share of the yet-to-be completed Trump Tower. 1983- Wealth: Share of Fred's estimated $400 million fortune...1984- Wealth: Fred has $200 million, Donald has $400 million... 1985-Rank:51 Wealth: $600 million. Donald becomes a solo Forbes 400 act; Fred disappears from list.""
The ""Worth"" column contains Trump's estimated net worth in billions. Years when his ranking and net worth are ""NA"" are years when he did not make the Forbes 400 list (1990-1995)."
Candidatos Deputado Federal e Estadual 2014,"Dados pessoais, bens declarados e doações recebidas",Eliezer Bourchardt,5,"Version 2,2018-01-24|Version 1,2018-01-23","brazil
government
politics",CSV,6 MB,CC0,235 views,18 downloads,2 kernels,,https://www.kaggle.com/eliezerfb/candidatos-deputado-federal-e-estadual-2014,"Context
Dataset criado para realizar o projeto de conclusão do curso de Engenheiro de Machine Learning pela Udacity.
Content
Dados de candidatos a deputados estadual e federal eleições 2014. Características dos candidatos (sexo, idade, raça/cor), valores dos bens declarados e doações recebidas.
Acknowledgements
Agradeço a Felipe Antunes por disponibilizar o dataset de doações aos candidatos. Também agradeço Heitor Gomes, revisor de meu projeto pelas ótimas sugestões de melhorias.
Inspiration
Como os dados dos candidatos podem ser utilizados para realizar uma previsão se o candidato será ou não eleito melhorando o resultado obtido neste projeto?"
Developers and programming languages,Skills of 17.000 github.com users.,Jaime Valero,5,"Version 2,2017-12-03|Version 1,2017-11-30",programming languages,CSV,6 MB,CC0,424 views,56 downloads,4 kernels,0 topics,https://www.kaggle.com/jaimevalero/developers-and-programming-languages,"Context
Sample of 17.000 github.com developers, and programming language they know - or want to -.
Content
I acquired the data listing the 1.000 most starred repos dataset, and getting the first 30 users that starred each repo. Cleaning the dupes. Then for each of the 17.000 users, I calculate the frequency of each of the 1.400 technologies in the user and forked repositories metadata.
Acknowledgements
Thanks to Jihye Sofia Seo, because their dataset Top 980 Starred Open Source Projects on GitHub is the source for this dataset.
Inspiration
I am using this dataset for my github recommendation engine, I use it to find similar developers, to use his stared repositories as recommendation. Also, I use this dataset to categorize developer types, trying to understand the weight of a developer in a team, specially when a developer leaves the company, so It is possible to draw the talent lost for the team and the company."
DACA Recipients,Aggregate data as of Sept 4 2017,Anu,5,"Version 2,2017-10-10|Version 1,2017-10-06","politics
demographics",CSV,1 MB,Other,"1,311 views",124 downloads,3 kernels,0 topics,https://www.kaggle.com/anupamakhan/daca-recipients-as-of-sept-4-2017,"Context
Aggregate data from US Citizenship and Immigration Services on Deferred Action for Childhood Arrivals program as of Sept 4 2017 https://www.uscis.gov/tools/reports-studies/immigration-forms-data/data-set-form-i-821d-deferred-action-childhood-arrivals
Content
Country of birth: approximate active DACA recipients
Notes:
This table refers to individuals who were granted Deferred Action for Childhood Arrivals (DACA) as of September 4, 2017. The number of individuals who were ever granted DACA as of September 4, 2017 was approximately 800,000. This total excludes persons who applied for an initial grant of DACA and were not approved, as well as initial DACA requestors that were approved at first, but later had their initial request denied or terminated. Nearly 40,000 DACA recipients have adjusted to lawful permanent resident (LPR) status, leaving about 760,000 who are not LPRs. About 70,000 individuals who were granted DACA either failed to renew at the end of their 2-year validity period or were denied on renewal, leaving approximately 690,000 active DACA recipients as of September 4, 2017.
Totals do not add due to rounding.
Countries with fewer than 10 active DACA recipients are included in other.
Not available: data are not available in electronic systems. Source: U.S. Citizenship and Immigration Services,""
State of Residence:
Notes:
This table refers to individuals who were granted Deferred Action for Childhood Arrivals (DACA) as of September 4, 2017. The number of individuals who were ever granted DACA as of September 4, 2017 was approximately 800,000. This total excludes persons who applied for an initial grant of DACA and were not approved, as well as initial DACA requestors that were approved at first, but later had their initial request denied or terminated. Nearly 40,000 DACA recipients have adjusted to lawful permanent resident (LPR) status, leaving about 760,000 who are not LPRs. About 70,000 individuals who were granted for DACA either failed to renew at the end of their 2-year validity period or were denied on renewal, leaving approximately 690,000 active DACA recipients as of September 4, 2017.
State of residence at the time of most recent application.
Totals do not add due to rounding.
Territories with less than 10 residents are included in other.
Not available: data are not available in electronic systems. Source: U.S. Citizenship and Immigration Services, CLAIMS3 and ELIS Systems.""
Core-based Statistical Areas
Notes:
This table refers to individuals who were granted Deferred Action for Childhood Arrivals (DACA) as of September 4, 2017. The number of individuals who were ever granted DACA as of September 4, 2017 was approximately 800,000. This total excludes persons who applied for an initial grant of DACA and were not approved, as well as initial DACA requestors that were approved at first, but later had their initial request denied or terminated. Nearly 40,000 DACA recipients have adjusted to lawful permanent resident (LPR) status, leaving about 760,000 who are not LPRs. About 70,000 individuals who were granted DACA either failed to renew at the end of their 2-year validity period or were denied on renewal, leaving approximately 690,000 active DACA recipients as of September 4, 2017.
Core Based Statistical Areas (CBSA) at the time of most recent application. CBSAs are defined by the Office of Management and Budget.
Totals may not add due to rounding.
CBSAs with fewer than 1,000 residents are included in other.
Not available: data are not available in electronic systems. Source: U.S. Citizenship and Immigration Services, CLAIMS3 and ELIS Systems.""
Age and Sex
Notes:
These tables refer to individuals who were granted Deferred Action for Childhood Arrivals (DACA) as of September 4, 2017. The number of individuals who were ever granted DACA as of September 4, 2017 was approximately 800,000. This total excludes persons who applied for an initial grant of DACA and were not approved, as well as initial DACA requestors that were approved at first, but later had their initial request denied or terminated. Nearly 40,000 DACA recipients have adjusted to lawful permanent resident (LPR) status, leaving about 760,000 who are not LPRs. About 70,000 individuals who were granted DACA either failed to renew at the end of their 2-year validity period or were denied on renewal, leaving approximately 690,000 active DACA recipients as of September 4, 2017.
Age as of September 4, 2017 and marital status as of the time of most recent application.
Totals do not add due to rounding.
Interquartile range is the range between the 25th percentile and the 75th percentile. About half of the active DACA recipients are 20 to 27 years old.
Not available: data are not available in electronic systems. Source: U.S. Citizenship and Immigration Services, CLAIMS3 and ELIS Systems.""
Marital status
Notes:
This table refers to individuals who were granted Deferred Action for Childhood Arrivals (DACA) as of September 4, 2017. The number of individuals who were ever granted DACA as of September 4, 2017 was approximately 800,000. This total excludes persons who applied for an initial grant of DACA and were not approved, as well as initial DACA requestors that were approved at first, but later had their initial request denied or terminated. Nearly 40,000 DACA recipients have adjusted to lawful permanent resident (LPR) status, leaving about 760,000 who are not LPRs. About 70,000 individuals who were granted DACA either failed to renew at the end of their 2-year validity period or were denied on renewal, leaving approximately 690,000 active DACA recipients as of September 4, 2017.
Marital status at time of most recent application.
Not available: data are not available in electronic systems.
Inspiration
I converted this data set from the published PDF so I could practice making choropleth maps.
Acknowledgements
I got the CBSA topojson from here: https://discourse.looker.com/t/custom-topojson-for-2014-core-based-statistical-areas-cbsa/2028"
17 Years of Resident Advisor Reviews,16972 Rows of Underground Electronic Music Reviews,MarcSchroeder,5,"Version 1,2018-01-03","journalism
music",CSV,15 MB,Other,918 views,74 downloads,,0 topics,https://www.kaggle.com/marcschroeder/17-years-of-resident-advisor-reviews,"Context
Started in 2001, Resident Advisor (RA) has become the web's largest resource for information about underground electronic music around the world. The site maintains a huge database of music and tech reviews, artists, labels, news, podcasts, and events.
Content
Gathered using a web-scraping script, the dataset below is of the site's entire collection of music reviews from the start of the site through the end of 2017. It contains the following fields:
Release Type (album or single)
Artist Release
Title
Label
Release Month
Release Year
Style (genres of release listed by RA)
Rating (score out of 5 given by RA)
Date of Review
Review Author
Body of Review
Release Tracklist
Acknowledgements
Thanks to the RA team for the journalism over the years, and for (hopefully) being cool with this dataset being published here."
SA & Victorian pet ownership data,Love animals? Have a crack at providing geographic insights on animal ownership!,Team PuppyGoGo,5,"Version 4,2017-11-24|Version 3,2017-11-17|Version 2,2017-11-14|Version 1,2017-10-23","australia
animals
pets
demographics",CSV,3 MB,CC0,844 views,89 downloads,2 kernels,0 topics,https://www.kaggle.com/puppygogo/sa-dog-ownership-sample,"Context...
Ever wondered the what and where of dog ownership? So have we!
Content...
Have a look at a sample set of South Australian and Victorian animal registration data. Data is publicly available from the data.gov.au website under a creative commons licence. Information includes: breed, location, desexed and colour. Datasets are for the 2015, 2016 & 2017 periods (depending on availability). SA information has been consolidated in ~82,500 lines of data!
Acknowledgements...
A big thank you to the SA and Victorian shires for having such great datasets!
Inspiration...
We love dogs and really want to understand the distribution of pets across SA and Victoria. We will leave it up to you the insights you want to create!"
Gymnastics World Championships 2017,Men's All-Around Final Results,Charlie,5,"Version 1,2017-10-25","gymnastics
sports",CSV,7 KB,CC0,669 views,43 downloads,7 kernels,0 topics,https://www.kaggle.com/cjdaffern/gymnastic-champs-mens-all-round,"Results from the Men's All-Around Final of the Artistic Gymnastics World Championships which took place in Montreal, Canada in October 2017.
Includes: athlete name, athlete nationality, overall rank, apparatus scores separated into difficulty and execution scores, and apparatus ranks.
Data source: https://mtl2017gymcan.com/en/results/"
A-Z Handwritten Alphabets in .csv format,3700000+ English Alphabets Image Data-set,Sachin Patel,5,"Version 5,2018-02-16|Version 4,2018-02-16|Version 3,2018-01-19|Version 2,2018-01-18|Version 1,2018-01-15","writing
linguistics
machine learning",Other,82 MB,CC0,519 views,103 downloads,2 kernels,0 topics,https://www.kaggle.com/sachinpatel21/az-handwritten-alphabets-in-csv-format,"Context
For recognising handwritten forms, the very first step was to gather data in a considerable amount for training. Which I struggled to collect for weeks.
Content
The dataset contains 26 folders (A-Z) containing handwritten images in size 28*28 pixels, each alphabet in the image is center fitted to 20*20 pixel box.
Each image is stored as Gray-level
Note: Might contain some noisy image as well
Acknowledgements
The images are taken from NIST(https://www.nist.gov/srd/nist-special-database-19) and NMIST large dataset and few other sources which were then formatted as mentioned above.
Inspiration
The dataset would serve beginners in machine learning for there created predictive model to recognise handwritten characters."
Economic calendar (EC) Forex (2011-2018),"Archive of important events, economic news, volatility in a convenient format",Sergey,5,"Version 6,2018-02-23|Version 5,2017-12-31|Version 4,2017-12-24|Version 3,2017-12-16|Version 2,2017-12-07|Version 1,2017-11-30","learning
finance
money
+ 2 more...",Other,3 MB,ODbL,412 views,65 downloads,,0 topics,https://www.kaggle.com/devorvant/economic-calendar,"Introduction
Explore the archive of relevant economic information: relevant news on all indicators with explanations, data on past publications on the economy of the United States, Britain, Japan and other developed countries, volatility assessments and much more. For the construction of their forecast models, the use of in-depth training is optimal, with a learning model built on the basis of EU and Forex data. The economic calendar is an indispensable assistant for the trader.
Data set
The data set is created in the form of an Excel spreadsheet (two files 2011-2013, 2014-2018), which can be found at boot time. You can see the source of the data on the site https://www.investing.com/economic-calendar/
column - Event date
column - Event time (time New York)
column - Country of the event
column - The degree of volatility (possible fluctuations in currency, indices, etc.) caused by this event
column - Description of the event
column - Evaluation of the event according to the actual data, which came out better than the forecast, worse or correspond to it
column - Data format (%, K x103, M x106, T x109)
column - Actual event data
column - Event forecast data
column - Previous data on this event (with comments if there were any interim changes).
Inspiration
Use the historical EU in conjunction with the Forex data (exchange rates, indices, metals, oil, stocks) to forecast subsequent Forex data in order to minimize investment risks (combine fundamental market analysis and technical).
Historical events of the EU used as a forecast of the subsequent (for example, the calculation of the probability of an increase in the rate of the Fed).
Investigate the impact of combinations of EC events on the degree of market volatility at different time periods.
To trace the main trends in the economies of the leading countries (for example, a decrease in the demand for unemployment benefits).
Use the EU calendar together with the news background archive for this time interval for a more accurate forecast."
England Obesity Stats 2017,"This stat covers gender, regions, age groups, years 2005/6 till 2015/16",fayomi,5,"Version 1,2017-11-26","food and drink
health
politics
demographics",Other,259 KB,CC0,323 views,85 downloads,,0 topics,https://www.kaggle.com/fayomi/england%20obesity%20stats%202017,This dataset does not have a description yet.
LabelMe - Let's Eat! Labeled images of meals,A curated subset of the LabelMe project with labeled images of table settings,Jack Cosgrove,5,"Version 3,2017-11-29|Version 2,2017-11-28|Version 1,2017-11-22","food and drink
image data
multiclass classification",CSV,2 MB,CC4,869 views,44 downloads,,2 topics,https://www.kaggle.com/jackcosgrove/labelme-lets-eat,"Context
The LabelMe project has been run out of MIT for many years, and allows users to upload and annotate images. Since the labels are crowdsourced, they can be of poor quality. I have been proofreading these labels for several months, correcting spelling mistakes and coalescing similar labels into a single label when possible. I have also rejected many labels that did not seem to make sense.
Content
The images in the LabelMe project as well as the raw metadata were downloaded from MIT servers. All data is in the public domain. Images within LabelMe may have been taken as far back as the early 2000s, and run up to the present day.
I have worked through 5% of the LabelMe dataset thus far. I decided to create a dataset pertaining to meals (labels such as plate, glass, napkins, fork, etc.) since there were a fair number of those in the 5% I have curated thus far. Most of the images in this dataset are of table settings.
This dataset contains: 596 unique images 2734 labeled shapes outlining objects in these images 1782 labeled image grids, with a single number representing which portion of a grid cell is filled with a labeled object
Acknowledgements
Many thanks to the people of the LabelMe project!
Inspiration
I want to see how valuable my curation efforts have been for the LabelMe dataset. I would like to see others build object recognition models using this dataset."
KLCC Parking,Data collected from parking sensor in KLCC,mypapit,5,"Version 1,2017-11-25",timelines,Other,196 KB,CC4,596 views,25 downloads,,,https://www.kaggle.com/mypapit/klccparking,"Context
KLCC car park sensor datasets - data about car park occupancy based on date and time.
Content
The dataset contains:
KLCC label
Parking Spot availability (number from 1-5500 - parking availability, FULL means no parking available, OPEN means problem reading data - aka 'missing value')
Date and time when the parking spot availability queried from the sensors.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
When exactly is the busiest hour?
Detect when there is rush hour
Compare parking occupancy rate with weather data (Does the rain impact parking rate occupancy?)
Whether there are seasonal changes or anomaly in certain date and time?
Compare parking occupancy rate before, during and after festive week
Compare parking occupancy rate between weekday and weekend or just before or after pay day."
ATM Transaction Data of City Union Bank,Data set of atm transaction,Nitin Bisht,5,"Version 1,2018-01-08",banking,CSV,827 KB,Other,348 views,56 downloads,,0 topics,https://www.kaggle.com/nitsbat/atm-transaction-data-of-city-bank,This dataset does not have a description yet.
fastText English Word Vectors,"Word vectors trained on Wikipedia 2017, UMBC webbase corpus, and statmt.org",Facebook,5,"Version 1,2018-01-11","machine learning
pre-trained model",Other,658 MB,CC3,620 views,29 downloads,,0 topics,https://www.kaggle.com/facebook/fasttext-wikinews,"English Word Vectors
About fastText
fastText is a library for efficient learning of word representations and sentence classification. One of the key features of fastText word representation is its ability to produce vectors for any words, even made-up ones. Indeed, fastText word vectors are built from vectors of substrings of characters contained in it. This allows you to build vectors even for misspelled words or concatenation of words.
About the vectors
These pre-trained vectors contain 1 million word vectors that were learned using Wikipedia 2017, the UMBC webbase corpus and the statmt.org news dataset. In total, it contains 16B tokens.
The first line of the file contains the number of words in the vocabulary and the size of the vectors. Each line contains a word followed by its vectors, like in the default fastText text format. Each value is space separated. Words are ordered by descending frequency.
Acknowledgements
These word vectors are distributed under the Creative Commons Attribution-Share-Alike License 3.0.
P. Bojanowski*, E. Grave*, A. Joulin, T. Mikolov, Enriching Word Vectors with Subword Information
A. Joulin, E. Grave, P. Bojanowski, T. Mikolov, Bag of Tricks for Efficient Text Classification
A. Joulin, E. Grave, P. Bojanowski, M. Douze, H. Jégou, T. Mikolov, FastText.zip: Compressing text classification models

(* These authors contributed equally.)"
Melbourne Housing Snapshot,Snapshot of Tony Pino's Melbourne Housing Dataset,DanB,5,"Version 4,2017-09-28|Version 3,2017-09-28|Version 2,2017-09-28|Version 1,2017-09-27","australia
housing
real estate
demographics",CSV,3 MB,CC4,"1,105 views","1,252 downloads",65 kernels,0 topics,https://www.kaggle.com/dansbecker/melbourne-housing-snapshot,"Context
Melbourne real estate is BOOMING. Can you find the insight or predict the next big trend to become a real estate mogul... or even harder, to snap up a reasonably priced 2-bedroom unit?
Content
This is a snapshot of a dataset created by Tony Pino.
It was scraped from publicly available results posted every week from Domain.com.au. He cleaned it well, and now it's up to you to make data analysis magic. The dataset includes Address, Type of Real estate, Suburb, Method of Selling, Rooms, Price, Real Estate Agent, Date of Sale and distance from C.B.D.
Notes on Specific Variables
Rooms: Number of rooms
Price: Price in dollars
Method: S - property sold; SP - property sold prior; PI - property passed in; PN - sold prior not disclosed; SN - sold not disclosed; NB - no bid; VB - vendor bid; W - withdrawn prior to auction; SA - sold after auction; SS - sold after auction price not disclosed. N/A - price or highest bid not available.
Type: br - bedroom(s); h - house,cottage,villa, semi,terrace; u - unit, duplex; t - townhouse; dev site - development site; o res - other residential.
SellerG: Real Estate Agent
Date: Date sold
Distance: Distance from CBD
Regionname: General Region (West, North West, North, North east ...etc)
Propertycount: Number of properties that exist in the suburb.
Bedroom2 : Scraped # of Bedrooms (from different source)
Bathroom: Number of Bathrooms
Car: Number of carspots
Landsize: Land Size
BuildingArea: Building Size
CouncilArea: Governing council for the area
Acknowledgements
This is intended as a static (unchanging) snapshot of https://www.kaggle.com/anthonypino/melbourne-housing-market. It was created in September 2017. Additionally, homes with no Price have been removed."
Sales Conversion Optimization,How to Cluster Customer data for campaign marketing,Gokagglers,5,"Version 1,2017-09-26","business
internet",CSV,59 KB,Other,"1,174 views",183 downloads,,,https://www.kaggle.com/loveall/clicks-conversion-tracking,"Context
Cluster Analysis for Ad Conversions Data
Content
The data used in this project is from an anonymous organisation’s social media ad campaign. The data file can be downloaded from here. The file conversion_data.csv contains 1143 observations in 11 variables. Below are the descriptions of the variables.
1.) ad_id: an unique ID for each ad.
2.) xyz_campaign_id: an ID associated with each ad campaign of XYZ company.
3.) fb_campaign_id: an ID associated with how Facebook tracks each campaign.
4.) age: age of the person to whom the ad is shown.
5.) gender: gender of the person to whim the add is shown
6.) interest: a code specifying the category to which the person’s interest belongs (interests are as mentioned in the person’s Facebook public profile).
7.) Impressions: the number of times the ad was shown.
8.) Clicks: number of clicks on for that ad.
9.) Spent: Amount paid by company xyz to Facebook, to show that ad.
10.) Total conversion: Total number of people who enquired about the product after seeing the ad.
11.) Approved conversion: Total number of people who bought the product after seeing the ad.
Acknowledgements
Thanks to the Anonymous data depositor
Inspiration
Social Media Ad Campaign marketing is a leading source of Sales Conversion and i have made this data available for the benefit of Businesses using Google Adwords to track Conversions"
Leads Dataset,Who will pay for the course ?,RockBottom,5,"Version 1,2016-08-19",,CSV,6 MB,Other,"2,388 views",170 downloads,,,https://www.kaggle.com/rockbottom73/leads-dataset,"The included data file contains details about “leads”, where a lead is a person who wants to take up a course with “SomeSchool”, a college. Every row contains information about one lead. Those leads with “Closed” in the Lead Stage column are the people who have actually paid for the course.
The purpose of the exercise is to determine which leads are more likely to close. A score should be assigned to each lead, with the possible values of “High”, “Medium” and “Low”, where “High” means that the lead is most likely to pay, and “Low” means least likely."
US Traffic Violations - Montgomery County Police,Factors contributing to traffic violations over the last few years,jana,5,"Version 1,2016-12-26","crime
road transport",CSV,351 MB,CC0,"2,925 views",329 downloads,9 kernels,0 topics,https://www.kaggle.com/jana36/us-traffic-violations-montgomery-county-polict,"Context
This data set contains traffic violation with their different categories published in Montgomery county website. The data set contains different categories of the traffic violations which should be useful for analyzing the data category wise.
Content
The name of the fields are self explanatory. The data set contains geographical location details (latitude, longitude) which might be useful for analyzing the data on the geographical maps.
Acknowledgements
I captured the data from the US govt website link: https://data.montgomerycountymd.gov/Public-Safety/Traffic-Violations/4mse-ku6q
Also I was looking for the full data set of the country - please let me know if anybody has the access of the USA all state traffic violation data.
Inspiration
Wanted to share the violations of the traffic rules - this might help people to avoid traffic violations as avoid road fatalities."
Sentiment Analysis Dataset,Data to train model for sentiment analysis,SonamSrivastava,5,"Version 2,2016-11-26|Version 1,2016-11-26",,Other,4 MB,ODbL,"4,492 views",603 downloads,4 kernels,0 topics,https://www.kaggle.com/sonaam1234/sentimentdata,Data for sentiment analysis
Kanye West Rap Verses,364 Rap Verses Compiled From 243 Songs,Vicc Alexander,5,"Version 1,2016-09-12",,Other,255 KB,CC0,"3,244 views",218 downloads,3 kernels,,https://www.kaggle.com/viccalexander/kanyewestverses,"Context: Kanye West Rap Verses (243 Songs, 364 Verses)
Content: All verses are separated by empty lines. The data has been cleaned to remove any unnecessary words or characters not part of the actual verses.
Acknowledgements: The lyrics are owned by Kanye West and his label, but the dataset was compiled by myself using Rap Genius.
Past Research: Ran the data through a RNN to try to generate new verses that sounded similar to Kanye's existing verses.
Inspiration: It'll be interesting to see what analysis people can do on this dataset. Although it's pretty small, it definitely seems like a fun dataset to mess around with.
Note: Below is a list of all the songs used for verse extraction. Songs labeled with (N) were excluded due to either not containing rap verses (only choruses), or me not being able to locate the actual lyrics.
Mercy
Niggas in Paris
Clique
Bound 2
No Church in the Wild
Father Stretch My Hand Pt. 1
New Slaves
Blood on the Leaves
Black Skinhead
Don't Like
Monster
All Day
Father Stretch My Hand Pt. 2
I Am a God
Famous
No More Parties in LA
I'm In It
Hold My Liquor
Facts
Power
Cold
New God Flow
Gotta Have It
Blame Game
Wolves
FML
Runaway
Can't Tell Me Nothing
Waves
Dark Fantasy
Gorgeous
Gold Digger
Devil in a New Dress
Otis
So Appalled
All Falls Down
Highlights
All of the Lights
On Sight
Who Gon Stop Me
Guilt Trip
Murder to Excellence
30 Hours
Send It Up
Through the Wire
Stronger
Illest Motherfucker Alive
Flashing Lights
Last Call
Homecoming
H·A·M
The Morning
Lost In The World
Saint Pablo
Freestyle 4
Feedback
Jesus Walks
Good Morning
The One
Good Life
Touch the Sky
Diamonds from Sierra Leone
Never Let Me Down
Big Brother
New Day
Hell of a Life
To the World
Hey Mama
Heard 'Em Say
White Dress
Heartless
Champion
That's My Bitch
Everything I Am
Gone
Made in America
I Wonder
Spaceship
Get Em High
Christian Dior Denim Flow
We Don't Care
Family Business
See Me Now
The Glory
Welcome to the Jungle
Looking For Trouble
Drive Slow
The Joy
The New Workout Plan
Champions
Love Lockdown
Primetime
We Major
Roses
School Spirit
Addiction
Lift Off
Barry Bonds
Bittersweet Poetry
Welcome to Heartbreak
Drunk and Hot Girls
Two Words Slow Jamz
Paranoid
Crack Music
Classic (Nike Air Force Remix)
RoboCop
Breathe In Breathe Out
Late
Bring Me Down
Christmas in Harlem
Celebration
Good Night
Lord Lord Lord
Chain Heavy
Eyes Closed
Don't Look Down
Take One for the Team
Mama's Boyfriend
Apologize
We Can Make It Better
When I See It
Because of You (Remix)
Home
Throw Some D's (Remix)
Livin' in a Movie
Another You
Impossible
Back Niggaz
Birthday Song
Back to Basics
Line for Line
What You Do To Me
In Common (Remix)
Pussy Print
Guard Down
Piss On Your Grave
Jukebox Joints
SMUCKERS
All Your Fault
Can't Stop
Drunk in Love (Remix)
Welcome to the World
Blazing
Glenwood
Ayyy Girl
We Fight We Love (Remix)
Anyone But Him
Erase Me
Diamonds (Remix)
Hate
Ego (Remix)
Alright
I'm the Shit (Remix)
Flight School
Teriya-King
Punch Drunk Love (The Eye)
Therapy
Digital Girl
Promise Land
It's Over
Go Hard
Beat Goes On
Everyone Nose
Down
In the Mood
Southside
My Drink n My 2 Step (Remix)
Still Dreaming
Tell Me When to Go (Remix)
Fly Away
They Say
Paid the Price
Call Some Hoes
The Way That You Do
Welcome Back (Remix)
Confessions Pt. 2 (Remix)
My Baby
Gettin' It In
I Changed My Mind
Selfish
Higher
Talk About Our Love
I See Now
Getting Out the Game
03 'til Infinity
So Soulful
Oh Oh
U Know
Candy
The Good, the Bad and the Ugly
Changing Lanes
The Bounce
Let's Get Married (Remix)
Pretty Girl Rock (Remix)
That Part
U Mad
Blessings
I Won
I Wish You Would
Marvin & Chardonnay
E.T.
Forever
The Big Screen
Supernova
Make Her Say
Run This Town
Gifted
Walkin' on the Moon
Knock You Down
Stay Up! (Viagra)
Put On
American Boy
Pro Nails
I Still Love H.E.R.
Wouldn't Get Far
Number One (With Pharrell)
Grammy Family
Extravaganza
Brand New
Wouldn't You Like 2 Ryde
This Way
Us Placers
Don't Stop!
Sanctified
Hurricane 2.0
Start It Up
In for the Kill (Remix)
Deuces (Remix)
Alors on Danse (Remix)
Live Fast Die Young
Maybach Music 2
Swagga Like Us (Remix)
Lollipop (Remix)
Plastic
Finer Things
Anything
Buy U a Drank (Remix)
This Ain't a Scene, It's an Arms Race (Remix)
Pusha Man
Selfish
Real Love
Hold On (Remix)
(N) Coldest Winter
(N) Ultralight Beams
(N) Only One
(N) I Love Kanye
(N) Why I Love You
(N) Fade
(N) Welcome to the Jungle
(N) Amazing
(N) Say You Will
(N) Street Lights
(N) See You in my Nightmares
(N) Awesome (Freestyle)
(N) Rosalind Ballroom
(N) Pinocchio Story
(N) God Level
(N) Bad News
(N) I Feel Like That
(N) My Way Home
(N) I'll Fly Away
(N) All We Got
(N) M.P.A.
(N) Mula
(N) The Summer League
(N) Nobody
(N) Rollin'
(N) Touch It
(N) We Alright
(N) Punch Drunk Love (The Eye)
(N) More
(N) Take It as a Loss
(N) Figure It Out
(N) One Man Can Change the World
(N) Thank You
(N) Pride N Joy
(N) Everybody
(N) The Corner
(N) Down and Out
(N) The Food (N) Welcome 2 Chicago"
Effects of Population on Crimes,A 2012 Crime dataset submitted to UCR by various US County Police Departments.,M Ganiyu,5,"Version 1,2017-01-13",,CSV,18 KB,Other,"1,211 views",104 downloads,11 kernels,,https://www.kaggle.com/mascotinme/population-against-crime,"Context
The data set was extracted from FBI-UCR website for the year 2012 on population less than 250,000.
Content
Here is the list of its 12 variables; Population, Violent_crime_total, Murder_and_Manslaughter, Forcible_rape, Robbery, Aggravated_assault, Property_crime_total, Burglary, Larceny_theft, Motor_vehicle_theft, lat, long.
Acknowledgements
I really appreciate the FBI-UCR for their generosity.
Inspiration
What impact does Population have on crimes?"
Arabic - Egyptian comparable Wikipedia corpus,Arabic (Modern Standard) and Egyptian Arabic dialect comparable documents,Motaz Saad,5,"Version 2,2017-09-29|Version 1,2017-09-29","languages
linguistics",{}JSON,264 MB,CC4,844 views,72 downloads,3 kernels,,https://www.kaggle.com/mksaad/arb-egy-cmp-corpus,"Context
Egyptian is an Arabic dialect, and it is the only Arabic dialect that has articles on Wikipedia. That is why I decided to extract Arabic-Egyptian comparable corpus from Wikipedia to make these resources available for linguists and computational linguists.
Content
The dataset is composed of a set of text documents in both Arabic (Modern Standard) and Egyptian dialect aligned at document level. comparable documents share the same document ID.
Acknowledgements
Thanks to Wikipedia and Wikipedia contributors who make these resource available. This corpus was collected by: M. Saad and B. O. Alijla, ""WikiDocsAligner: An Off-the-Shelf Wikipedia Documents Alignment Tool,"" 2017 Palestinian International Conference on Information and Communication Technology (PICICT), Gaza, Palestine, 2017, pp. 34-39. doi: 10.1109/PICICT.2017.27
Inspiration
What are the most common words in Egyptian and Arabic? What are the most frequent words in Egyptian and Arabic? What are the least frequent (rare) words in Egyptian and Arabic?"
Pima Indians Diabetes Data Set,medical records for Pima Indians,sariya,5,,,CSV,23 KB,Other,,,,,https://www.kaggle.com/dssariya/pima-indians-diabetes-data-set,
Protocol Gifts,"Data from the ""Protocol Gift Unit"" in the US Department of State",Izzie Toren,5,"Version 1,2017-01-11","politics
international relations",CSV,848 KB,CC4,"1,803 views",86 downloads,7 kernels,,https://www.kaggle.com/ytoren/protocol-gifts,"Context
This data-set contains information from the ""Protocol Gift Util"" in the US department of state, which documents all of the official gifts accepted by the president and white house staff. Quoting from the U.S. Department of State website:
The Protocol Gift Unit within the Office of the Chief of Protocol serves as the central processing point for all tangible gifts received from foreign sources by employees of the Executive Branch of the Federal government. The Unit is responsible for the creation and maintenance of the official record of all gifts presented by the Department of State to officials of foreign governments. Working closely with the Chief of Protocol and the staffs of the President, the Vice President, and the Secretary of State, the Gift Unit selects the gifts presented to foreign dignitaries. Gifts received by the President, Vice President, and the Secretary of State and their spouses from foreign governments are also handled by the Gift Unit in the Office of Protocol.
Content
The file contains data scraped from the the Protocol Gift Unit website (the R script and more information about exclusions and possible issues can be found here.
Number of recorded gifts: 1913 (after some exclusions)
Years: 2002 to 2015
Encoding: UTF8 (with many special characters)
Inspiration
Looking forward to see how people can use creative text mining techniques to extract more information about the different columns (for example classify givers / receivers, tag geographies, extract the gift object from the description text, etc.). You can find my future humble attempts here."
Arxiv Astrophysics Collaboration Network,Co-authoring collaboration network of e-print server Arxiv Astro Physics,CharlesYang,5,"Version 1,2017-10-30","social sciences
networks",CSV,10 MB,CC0,565 views,15 downloads,,0 topics,https://www.kaggle.com/charlesxjyang/arxiv-astrophysics-collab,"Context
This Dataset represents essentially the entirety of the Astro-physics section for Arxiv as the dataset spans from January 1993 to April 2003, beginning a few months after the inception of Arxiv. It represents the co-authorship and collaboration network of this community in Arxiv as an undirected graph where an edge between two nodes means they co-authored a paper before.
Content
This dataset includes two files, one txt and the other csv. The two are identical in terms of content, they merely represent two different file types of the same dataset. Each file has a description header at the top, followed by two columns which contain all of the edges in the network. Each row represents an undirected edge between the two author id's i.e. these two authors co-authored a paper together.
Acknowledgements
This dataset was first published in the following paper
J. Leskovec, J. Kleinberg and C. Faloutsos. Graph Evolution: Densification and Shrinking Diameters. ACM Transactions on Knowledge Discovery from Data (ACM TKDD), 1(1), 2007.
More information on the dataset can be found at More information on this particular dataset
I posses no ownership over this data, please cite Jure Leskovec and Andrej Krevl if you use this dataset. Similar large network datasets can be found at Stanford Large Network Dataset Collection
Inspiration
How do co-author networks evolve? Can you confirm Zipf's Law? What are some different ways to calculate centrality or weight a particular author holds in the community?"
Indonesian Stoplist,Most common words (stop words) in Bahasa Indonesia,Oswin Rahadiyan Hartono,5,"Version 1,2016-10-24",,CSV,6 KB,Other,"2,252 views",147 downloads,,0 topics,https://www.kaggle.com/oswinrh/indonesian-stoplist,"In text mining and natural language processing, stop words are words which are being eliminated on the pre-processing step to aim the greater accuracy. In many cases, stop words removal is to reduce potential noises, in some cases isn't.
This dataset contains a list of stop words in Bahasa Indonesia
Acknowledgements : Fadillah Z Tala
Past Research : Stemming Algorithm Comparison : Porter vs Nazief Adriani, Stemming Effects on IR in Bahasa Indonesia"
Titanic,compare Above 18 years of male and female between the differnt class passengers,priyanka Kukunuru,5,"Version 1,2017-01-04",,Other,442 KB,Other,"7,862 views","1,673 downloads",74 kernels,0 topics,https://www.kaggle.com/prkukunoor/TitanicDataset,"Context
I am planning to compare Above 18 years of male and female between the different class passengers in Titanic Data set
Content
A century has sailed by since the luxury steamship RMS Titanic met its catastrophic end in the North Atlantic, plunging two miles to the ocean floors after sideswiping an iceberg during its maiden voyage.Rather than the intended Port of New York, a deep-sea grave became the pride of the White Star Line’s final destination in the early hours of April 15, 1912.More than 1,500 people lost their lives in the disaster In this project I will be performing an exploratory analysis on the data
Acknowledgements
I noticed that more women survived in raw number and percentage than men and opposite are true of 3rd class passengers. The bars are a good choice to show the difference between categories, but you may want to look into a grouped bar chart1 for an easier comparison of how many survived or didn't in each group. While there were far more men on the boat, less survived than the women. The class seemed to have a direct effect on a passenger's chance of survival. While it is good to see the difference in the numbers of those who survived to those who didn't.
Inspiration"
movie_rating_data,"20 million ratings and 465,000 tag applications",ASHUTOSH KUMAR,5,"Version 2,2017-11-10|Version 1,2017-11-05",film,CSV,526 MB,Other,257 views,37 downloads,,0 topics,https://www.kaggle.com/ashukr/movie-rating-data,"Context
Stable benchmark dataset. 20 million ratings and 465,000 tag applications applied to 27,000 movies by 138,000 users.
Content
this dataset has got three files named as ratings.csv, movies.csv and tags.csv
movies.csv In the 3 columns stored are the values of movieId, title and genre. The title has got the release year of movie in parenthesis. The movie list range from Dickson Greeting (1891) to movies of 2015. With the total of 27278 movies.
ratings.csv the movies have been rated by 138493 users on the scale of 1 to 5, this file contains the information divided in the column 'userId', 'movieId', 'rating' and 'timestamp'.
tags.csv this file has the data divided under category 'userId','movieId' and 'tag'
Acknowledgements
I got this data from MovieLens, for a mini project. http://grouplens.org/datasets/movielens/20m/""> This is the link to original data set
Inspiration
You have got a ton data. You can use this to make fun decisions like which is the best movie series of all time or create a completely new story out of the data that you have."
National Institute of the Korean Language Corpus,Korean frequency lists for NLP,Rachael Tatman,5,"Version 1,2017-10-07","languages
asia
linguistics",CSV,2 MB,Other,906 views,62 downloads,,0 topics,https://www.kaggle.com/rtatman/national-institute-of-the-korean-language-corpus,"Context:
How frequently a word occurs in a language is an important piece of information for natural language processing and linguists. In natural language processing, very frequent words tend to be less informative than less frequent one and are often removed during preprocessing.
This dataset contains frequency information on Korean, which is spoken by 80 million people. For each item, both the frequency (number of times it occurs in the corpus) and its relative rank to other lemmas is provided.
Content:
This dataset contains six sub-files with frequency information. The files have been renamed in English, but no changes have been made to the file contents. The files and their headers are listed below. The text in this dataset is UTF-8.
Frequency by Jamo (letter)
순위: Rank
빈도: Frequency
위치: Location
자모: Jamo (Hangul letter)
Frequency
순위: Rank
빈도: Frequency
항목: Location
범주: Category
Frequency by Syllable
순위: Rank
빈도: Frequency
음절: Syllable
Borrowings
순위: Rank
빈도: Frequency
항목: Item
풀이: Root
Non Standard Words
순위: Rank
빈도: Frequency
어휘: Vocabulary
풀이: Notes
품사: Part of Speech
Frequency (Longer version)
순위: Rank
빈도: Frequency
항목: Location
범주: Category
Acknowledgements:
This dataset was collected and made available by the National Institute of Korean Language. The dataset and additional documentation (in Korean) can be found here.
This dataset is distributed under a Korean Open Government Liscence, type 4. It may be redistributed with attribution, without derivatives and not for commercial purposes.
Inspiration:
What are the most frequent jamo (Hangul characters) in Korean? Least frequent?
What qualities do borrowed words have?
Is there a relationship between word length and frequency?
You may also like:
English word frequency
Japanese lemma frequency
List of simplified Chinese characters ordered by frequency rank
Stopword lists for African languages"
Halloween Candy Analysis,Trends of the spookiest (and sweetest) festival there is.,Ady1,5,"Version 1,2017-11-05",food and drink,Other,277 KB,CC0,502 views,36 downloads,,0 topics,https://www.kaggle.com/ady123/halloween-candy-analysis,"Halloween isn't just a festival about spooky pumpkins, spooky ghosts, and eye-catching costumes. Halloween is one of the largest datasets out there. In the trick-or-treat baskets little kids carry down the streets is a bank of knowledge and data that can unleash the trends found in communities around America. When I want trick-or-treating this halloween, I was intrigued to find what kind of candy I got, which one I got the most of, and most importantly, how did the price affect consumer choices in my community.
When I got back from trick-or treating this October 31st, 2017 (data is fresh like a cucumber), I made sure to empty out what I collected in my middle-class community and sort it out by brand, type/category, price, flavor, etc. After deciding which factors were most relevant, I created three graphs - one bar graph showing the amount of candies collected for each brand, another bar graph looking at the quantity for each type/category of confectionery, and finally, a line graph that analyzed the relationship between unit price per piece and amount of candies collected. All of this is for the sole purpose of displaying how different factors such as brand and price affect consumer choices, appeal to the general population, and what kind of candies to be expecting next year! This is an analysis of middle class economics, brand appeal, and choice of consumers.
I would like to thank my sister, who helped contribute to half of my dataset by trick-or-treating in different parts of the neighborhood, making sure the sample collected would be diverse! I also appreciate my dad's interest in this project and his own expertise that was involved in filtering everything out and making the dataset as accurate as possible!
From publishing this dataset, I hope to inspire many more young data scientists such as myself to take on new projects that intrigue them or analyze their own halloween candy from their community/neighborhood! I wanted to see how simple festivals like Halloween are affected by variables like consumer choice, price range, and brand appeal. Keep in mind, this is an analysis of my middle-class community. There are many more datasets and demographics waiting to be analyzed!"
Random Acts of Pizza,Predicting Altruism Through Free Pizza,Kaggle,5,"Version 1,2017-01-19",,{}JSON,15 MB,Other,"1,883 views",53 downloads,3 kernels,0 topics,https://www.kaggle.com/kaggle/random-acts-of-pizza,"Context
This dataset includes 5671 requests collected from the Reddit community Random Acts of Pizza between December 8, 2010 and September 29, 2013 (retrieved on September 30, 2013). All requests ask for the same thing: a free pizza. The outcome of each request -- whether its author received a pizza or not -- is known. Meta-data includes information such as: time of the request, activity of the requester, community-age of the requester, etc.
This dataset was featured in our completed playground competition entitled Random Acts of Pizza. The objective of the competition was to create an algorithm capable of predicting which requests will garner a cheesy (but sincere!) act of kindness.
Content
The data are stored in JSON format. Each JSON entry corresponds to one request (the first and only request by the requester on Random Acts of Pizza). We have removed fields from the test set which would not be available at the time of posting. The datasets include the following fields:
""giver_username_if_known"": Reddit username of giver if known, i.e. the person satisfying the request (""N/A"" otherwise).
""number_of_downvotes_of_request_at_retrieval"": Number of downvotes at the time the request was collected.
""number_of_upvotes_of_request_at_retrieval"": Number of upvotes at the time the request was collected.
""post_was_edited"": Boolean indicating whether this post was edited (from Reddit).
""request_id"": Identifier of the post on Reddit, e.g. ""t3_w5491"".
""request_number_of_comments_at_retrieval"": Number of comments for the request at time of retrieval.
""request_text"": Full text of the request.
""request_text_edit_aware"": Edit aware version of ""request_text"". We use a set of rules to strip edited comments indicating the success of the request such as ""EDIT: Thanks /u/foo, the pizza was delicous"".
""request_title"": Title of the request.
""requester_account_age_in_days_at_request"": Account age of requester in days at time of request.
""requester_account_age_in_days_at_retrieval"": Account age of requester in days at time of retrieval.
""requester_days_since_first_post_on_raop_at_request"": Number of days between requesters first post on RAOP and this request (zero if requester has never posted before on RAOP).
""requester_days_since_first_post_on_raop_at_retrieval"": Number of days between requesters first post on RAOP and time of retrieval.
""requester_number_of_comments_at_request"": Total number of comments on Reddit by requester at time of request.
""requester_number_of_comments_at_retrieval"": Total number of comments on Reddit by requester at time of retrieval.
""requester_number_of_comments_in_raop_at_request"": Total number of comments in RAOP by requester at time of request.
""requester_number_of_comments_in_raop_at_retrieval"": Total number of comments in RAOP by requester at time of retrieval.
""requester_number_of_posts_at_request"": Total number of posts on Reddit by requester at time of request.
""requester_number_of_posts_at_retrieval"": Total number of posts on Reddit by requester at time of retrieval.
""requester_number_of_posts_on_raop_at_request"": Total number of posts in RAOP by requester at time of request.
""requester_number_of_posts_on_raop_at_retrieval"": Total number of posts in RAOP by requester at time of retrieval.
""requester_number_of_subreddits_at_request"": The number of subreddits in which the author had already posted in at the time of request.
""requester_received_pizza"": Boolean indicating the success of the request, i.e., whether the requester received pizza.
""requester_subreddits_at_request"": The list of subreddits in which the author had already posted in at the time of request.
""requester_upvotes_minus_downvotes_at_request"": Difference of total upvotes and total downvotes of requester at time of request.
""requester_upvotes_minus_downvotes_at_retrieval"": Difference of total upvotes and total downvotes of requester at time of retrieval.
""requester_upvotes_plus_downvotes_at_request"": Sum of total upvotes and total downvotes of requester at time of request.
""requester_upvotes_plus_downvotes_at_retrieval"": Sum of total upvotes and total downvotes of requester at time of retrieval.
""requester_user_flair"": Users on RAOP receive badges (Reddit calls them flairs) which is a small picture next to their username. In our data set the user flair is either None (neither given nor received pizza, N=4282), ""shroom"" (received pizza, but not given, N=1306), or ""PIF"" (pizza given after having received, N=83).
""requester_username"": Reddit username of requester.
""unix_timestamp_of_request"": Unix timestamp of request (supposedly in timezone of user, but in most cases it is equal to the UTC timestamp -- which is incorrect since most RAOP users are from the USA).
""unix_timestamp_of_request_utc"": Unit timestamp of request in UTC.
Acknowledgements
Visit the competition page if you are interested in checking out past discussions, competition leaderboard, or more details regarding the competition. If you are curious to see how your results rank compared to others', you can still make a submission at the competition submission page!
This data was collected and graciously shared by Althoff et al. (Buy them a pizza -- data collection is a thankless and tedious job!) We encourage participants to explore their accompanying paper and ask that you cite the following reference in any publications that result from your work:
Tim Althoff, Cristian Danescu-Niculescu-Mizil, Dan Jurafsky. How to Ask for a Favor: A Case Study on the Success of Altruistic Requests, Proceedings of ICWSM, 2014."
Employment (All),"All employees, in the thousands, of the total U.S. private industry",US Bureau of Labor Statistics,5,"Version 1,2016-11-07",,CSV,6 KB,Other,"1,162 views",61 downloads,,0 topics,https://www.kaggle.com/bls/employment-all,"Context
This dataset lists the number of employees (in the thousands) of the total private industry in the United States from March 31, 1939 through August 30, 2016. Averages are taken quarterly and are seasonally adjusted.
Content
Data includes the date of the quarterly collection and the number of employees (in the thousands).
Inspiration
Is there a general trend of employment throughout the year?
Have certain historical events correlated with drastic changes in employment? What other datasets could you use to prove causation?
Several other BLS datasets break down employment numbers by industry, gender, state, etc. What other employment trends can you find?
Acknowledgement
This dataset is part of the US Department of Labor Bureau of Labor Statistics Datasets (the Employment and Unemployment database), and the original source can be found here."
Water Conservation Supplier Compliance,Reporting compliance by water suppliers in drought-stricken California,California Environmental Protection Agency,5,"Version 1,2016-11-17","ecology
agriculture",CSV,41 KB,CC0,"2,315 views",212 downloads,8 kernels,0 topics,https://www.kaggle.com/calepa/water-conservation-supplier-compliance,"Context
California has been dealing with the effects of an unprecedented drought. September 2016 marks the 16th month since the state’s 400-plus urban water suppliers were directed to be in compliance with the emergency conservation standards that followed the Governor’s April 1, 2015, Executive Order. The State Water Board has been requiring water delivery information from urban water suppliers for 28 consecutive months, following the historic July 2014 board action to adopt emergency water conservation regulations.
On May 18, following the Governor’s May 9 Executive Order, the Board adopted a statewide water conservation approach that replaces the prior percentage reduction-based water conservation standard with a localized “stress test” approach that mandates urban water suppliers act now to ensure at least a three-year supply of water to their customers under drought conditions.
Content
This fact sheet contains more details on how water conservation is monitored by the California EPA. This dataset describes the cumulative savings and compliance of water suppliers from June 2015 - August 2016.
Inspiration
What percentage of suppliers are actually meeting water conservation compliance standards?
Which hydrologic regions of California are in most danger of not meeting their residents' water needs?
Which city has the most residential gallons per capita per day (R-GPCD)? The least?
Are any cities outliers within their hydrological regions? Why might they be more or less successful in their water conservation efforts?
Acknowledgement
This dataset is part of the CalEPA Water Boards Water Conservation Reporting, and the original source can be found here."
IMDB Horror Movie Dataset [2012 Onwards],Dataset of 3300+ movies based on popularity,PromptCloud,5,"Version 1,2017-10-31",internet,CSV,2 MB,CC4,827 views,129 downloads,,0 topics,https://www.kaggle.com/PromptCloudHQ/imdb-horror-movie-dataset,"Context
On the occasion of Halloween, we thought of sharing a spooky dataset for the community to crunch on the data! This is a subset of larger dataset that goes back to 1911 containing more than 9500 movies. The complete dataset can be downloaded here.
Remember - ""This Halloween could get a lot more spookier, but treats are guaranteed"".
Content
The dataset goes back to 2012 and contains the following data fields:
Title
Genres
Release Date
Release Country
Movie Rating
Review Rating
Movie Run Time
Plot
Cast
Language
Filming Locations
Budget
Acknowledgements
The data was extracted by PromptCloud's in-house data extraction solution.
Inspiration
Some of the things that can be explored are the following:
Number of horror movies released over the years
Number of movies released in terms of country
Rating and run time distribution
Spooky regions by considering the shooting location
Text mining on the description text"
2017 March ML Mania Predictions,Forecasting the 2017 NCAA Basketball Tournament,William Cukierski,5,"Version 1,2017-03-17","basketball
artificial intelligence",CSV,26 MB,CC4,"2,708 views",393 downloads,5 kernels,,https://www.kaggle.com/wcukierski/2017-march-ml-mania-predictions,"Kaggle’s March Machine Learning Mania competition challenged data scientists to predict winners and losers of the men's 2017 NCAA basketball tournament. This dataset contains the selected predictions of all Kaggle participants. These predictions were collected and locked in prior to the start of the tournament.
The NCAA tournament is a single-elimination tournament that begins with 68 teams. There are four games, usually called the “play-in round,” before the traditional bracket action starts. Due to competition timing, these games are included in the prediction files but should not be used in analysis, as it’s possible that the prediction was submitted after the play-in round games were over.
Data Description
Each Kaggle team could submit up to two prediction files. The prediction files in the dataset are in the 'predictions' folder. You can map the files to the teams by team_submission_key.csv.
The submission format contains a probability prediction for every possible game between the 68 teams. Refer to the competition documentation for data details. For convenience, we have included the data files from the competition dataset in the dataset (you may find TourneySlots.csv and TourneySeeds.csv useful for determining matchups). However, the focus of this dataset is on Kagglers' predictions."
Wars by death tolls,Around 200 major wars in human history with their death tolls,Soumitra Agarwal,5,"Version 1,2017-03-08","death
war",CSV,9 KB,ODbL,797 views,47 downloads,,0 topics,https://www.kaggle.com/artimous/wars-by-death-tolls,"Context
Wars are one of the few things the human species isn't very proud of. In recent times, major political leaders have taken up steps that both increase and decrease the tension between the allies and the rivals. As being part of the data science community, I believe that this field isn't explored as much as it affects us. This era of comfort seems delusional when we visualise and try to predict how close the next great war might be.
Content
Version 1 : The data-set is very small for the initial version with the death tolls and the timelines of major wars. We can infer the participating countries (as much as possible from the names of the wars) and analyse how death toll has been on the rise/fall in the recent years.
Files : War.csv. Columns
Name - War name
Time - The time period for the war (including start and end years for longer wars)
Casualties - Number of deaths during that war
Subsequent version (on a good response) would include weapons used, participating countries and more entries.
Acknowledgements
The data is scrapped from Wikipedia with manual cleaning here and there.
Inspiration
I hope that this data-set can start a butterfly effect which leads to an uprising against wars in general."
Finishers Boston Marathon 2017,"This data has the names, times and general data of the finishers",rojour,5,"Version 1,2017-04-29",,CSV,4 MB,Other,815 views,104 downloads,,,https://www.kaggle.com/rojour/finishers-boston-marathon-2017,"Context
This is a list of the finishers of the Boston Marathon of 2017.
Content
It contains the name, age, gender, country, city and state (where available), times at 9 different stages of the race, expected time, finish time and pace, overall place, gender place and division place.
Acknowledgements
Data was scrapped from the official marathon website. There are many other people that have done this type of scrapping and some of those ideas were use to get the data.
Inspiration
I was a participant in the marathon, as well as a data science student, so it was a natural curiosity."
Pisa Scores,Program for International Student Assessment mean scores (Pisa) from 2015,JorgeZazueta,5,"Version 1,2017-05-03",education,CSV,112 KB,Other,"1,813 views",209 downloads,8 kernels,0 topics,https://www.kaggle.com/zazueta/pisa-scores-2015,"Context
PISA stands for ""Program for International Student Assessment"" and it is applied to 15 year-old students across the world to assess their performance in Math, Reading and Science. These are the 2015 scores.
Content
The dataset contains mean (Pisa) attainment scores in math, reading and science by country and gender.
Acknowledgements
Queried from the World Bank Learning outcomes database http://datatopics.worldbank.org/education/wDataQuery/QLearning.aspx
Inspiration
How attainment compares by country? Why some perform better than others? Can Pisa scores predict social environments such as freedom of press?"
#Inauguration and #WomensMarch Tweets,A look into how social media reacted to Trump's Inauguration,AdhokshajaPradeep,5,"Version 1,2017-02-08","gender
politics
internet",CSV,8 MB,CC0,"2,175 views",195 downloads,5 kernels,0 topics,https://www.kaggle.com/adhok93/inauguration-and-womensmarch-tweets,"The Presidency of Donald Trump
On Jan 20th, 2017, Donald J. Trump was elected as the 45th President of the United States. This marked the end of a brutal and contentious campaign. He goes in as one of the most unpopular presidents in modern history(based on the popular vote).
The Inauguration and the Women's March
Trump's election to the presidency led to the organization of the Women's March , where millions of men and women took to the streets to protest the new government's stance on women's rights and healthcare. Social media blew up with searchable terms like ""#WomensMarch"" prompting major news organizations to cover the mass protests.
Data Acquisition
The data was acquired using the twitteR package's searchTwitter() function. This function makes a call to the Twitter API. A total of 30000 tweets containing #Inauguration and #WomensMarch were obtained (15000 for each).
Data Set Attributes
1 ""X"" : Serial Number
2 ""text"" : Tweet Text
3 ""favorited"" : TRUE/FALSE
4 ""favoriteCount"" : Number of Likes
5 ""replyToSN"" : Screen Handle name of the receiver
6 ""created"" : YYYY-MM-DD H:M:S
7 ""truncated"" : If the Tweet is Truncated (TRUE/FALSE)
8 ""replyToSID"": ID of the receiver
9 ""id"" : ID
10 ""replyToUID"": User ID of the receiver
11 ""statusSource"": Device Information (Web Client,IPhone,Android etc)
12 ""screenName"" : Screen name of the Tweeter
13 ""retweetCount"": Number of Retweets
14 ""isRetweet"" : TRUE/FALSE
15 ""retweeted"" : Has this tweet been retweeted(TRUE/FALSE)
16 ""longitude"" : longitude
17 ""latitude"" : latitude
Some Questions
How do the polarity/number of tweets change by time? Which locations had negative sentiments about the Inauguration? What about the Women's March? How to the retweet and mention networks look like for each case? Number of Tweets per Day? Which day has the most activity? What are the other hashtags used?"
Raspberry Turk Project,Large collection of overhead images of chess pieces used to train Raspberry Pi,joeymeyer,5,"Version 1,2017-03-15",artificial intelligence,Other,17 MB,CC4,"1,864 views",80 downloads,,0 topics,https://www.kaggle.com/joeymeyer/raspberryturk,"Context
This dataset was created as part of the Raspberry Turk project. The Raspberry Turk is a robot that can play chess—it's entirely open source, based on Raspberry Pi, and inspired by the 18th century chess playing machine, the Mechanical Turk. The dataset was used to train models for the vision portion of the project.
Content
In the raw form the dataset contains 312 480x480 images of chessboards with their associated board FENs. Each chessboard contains 30 empty squares, 8 orange pawns, 2 orange knights, 2 orange bishops, 2 orange rooks, 2 orange queens, 1 orange king, 8 green pawns, 2 green knights, 2 green bishops, 2 green rooks, 2 green queens, and 1 green king arranged in different random positions.
Scripts for Data Processing
The Raspberry Turk source code includes several scripts for converting this raw data to a more usable form.
To get started download the raw.zip file below and then:
$ git clone git@github.com:joeymeyer/raspberryturk.git
$ cd raspberryturk
$ unzip ~/Downloads/raw.zip -d data
$ conda env create -f data/environment.yml
$ source activate raspberryturk
From this point there are two scripts you will need to run. First, convert the raw data to an interim form (individual 60x60 rgb/grayscale images) using process_raw.py like this:
$ python -m raspberryturk.core.data.process_raw data/raw/ data/interim/
This will split the raw images into individual squares and put them in labeled folders inside the interim folder. The final step is to convert the images into a dataset that can be loaded into a numpy array for training/validation. The create_dataset.py utility accomplishes this. The tool takes a number of parameters that can be used to customize the dataset (ex. choose the labels, rgb/grayscale, zca whiten images first, include rotated images, etc). Below is the documentation for create_dataset.py.
$ python -m raspberryturk.core.data.create_dataset --help
usage: raspberryturk/core/data/create_dataset.py [-h] [-g] [-r] [-s SAMPLE]
                                                 [-o] [-t TEST_SIZE] [-e] [-z]
                                                 base_path
                                                 {empty_or_not,white_or_black,color_piece,color_piece_noempty,piece,piece_noempty}
                                                 filename

Utility used to create a dataset from processed images.

positional arguments:
  base_path             Base path for data processing.
  {empty_or_not,white_or_black,color_piece,color_piece_noempty,piece,piece_noempty}
                        Encoding function to use for piece classification. See
                        class_encoding.py for possible values.
  filename              Output filename for dataset. Should be .npz

optional arguments:
  -h, --help            show this help message and exit
  -g, --grayscale       Dataset should use grayscale images.
  -r, --rotation        Dataset should use rotated images.
  -s SAMPLE, --sample SAMPLE
                        Dataset should be created by only a sample of images.
                        Must be value between 0 and 1.
  -o, --one_hot         Dataset should use one hot encoding for labels.
  -t TEST_SIZE, --test_size TEST_SIZE
                        Test set partition size. Must be value between 0 and
                        1.
  -e, --equalize_classes
                        Equalize class distributions.
  -z, --zca             ZCA whiten dataset.
Example of how it can be used:
$ python -m raspberryturk.core.data.create_dataset data/interim/ promotable_piece data/processed/example_dataset.npz --rotation --grayscale --one_hot --sample=0.3 --zca
Finally, the dataset is created and can be easily loaded into Python either using raspberryturk.core.data.dataset.Dataset or simply np.load.
In [1]: from raspberryturk.core.data.dataset import Dataset
In [2]: d = Dataset.load_file('data/processed/example_dataset.npz')
or
In [1]: with open('data/processed/example_dataset.npz', 'r') as f:
      :     data = np.load(f)
Visit the data collection page of the Raspberry Turk website for more details.
Creator
Joey Meyer"
Daily returns for Apple and Microsoft stock,Clean dataset of daily return for learning purposes,GuillaumeTouzin,5,"Version 1,2017-05-12",,CSV,169 KB,CC0,996 views,94 downloads,,,https://www.kaggle.com/gtouzin/samplestocksreturn,"Content
Set of the daily returns for Apple and Microsoft stock from May 2000 to May 2017. This is a clean dataset used for learning purposes, so I deleted some outlier that should be included any model used in any real world application.
Acknowledgements
The raw data come from Yahoo finance."
How do Brazilian politicians use their quota?,Where and how do politicians use their money?,JoniHoppen,5,"Version 1,2017-05-02",,CSV,394 MB,Other,585 views,38 downloads,,0 topics,https://www.kaggle.com/joniarroba/how-brazilian-politicians-use-their-quota,"Context - # Quota for Exercising Parliamentary Activity (CEAP)
The Serenata de Amor operation is a projet created by Brazilian Data Scientists looking to data of the publich administration - all infomation you might need - Information and more datasets - you can find here at the official site https://github.com/datasciencebr - By the way, everything was made available in English to make sure you understand up-front!
Data Dictionary
The Quota for Exercising Parliamentary Activity (aka CEAP) is a montly quota available exclusively for covering costs of deputies with the exercise of parliamentary activity. The Bureau Act 43 of 2009 🇧🇷 describe the guidelines for its use.
1. Congressperson Name (congressperson_name)
| 🇧🇷 | 🇬🇧 | |:------:|:------:| | Nome Parlamentar | Congressperson Name | | txNomeParlamentar | congressperson_name | | Nome adotado pelo Parlamentar ao tomar posse do seu mandato. Compõe-se de dois elementos: um prenome e o nome; dois nomes; ou dois prenomes, salvo, a juízo do Presidente da Casa legislativa, que poderá alterar essa regra para que não ocorram confusões. | Name used by the congressperson during his term in office. Usually it is composed by two elements: a given name and a family name; two given names; or two forename, except if the head of the Chamber of Deputies explicitly alter this rule in order to avoid confusion. |
2. Unique Identifier of Congressperson (congressperson_id)
| 🇧🇷 | 🇬🇧 | |:------:|:------:| | Identificador Único do Parlamentar | Unique Identifier of Congressperson | | ideCadastro | congressperson_id | | Número que identifica unicamente um deputado federal na CD. | Unique identifier number of a congressperson at the Chamber of Deputies. |
3. Congressperson Document Number (congressperson_document)
| 🇧🇷 | 🇬🇧 | |:------:|:------:| | Número da Carteira Parlamentar | Congressperson Document Number | | nuCarteiraParlamentar | congressperson_document | | Documento usado para identificar um deputado federal na CD. Pode alterar a cada Legislatura nova. | Document used to identify the congressperson at the Chamber of Deputies. May change from one term to another. |
4. Legislative Period Number (term)
| 🇧🇷 | 🇬🇧 | |:------:|:------:| | Número da Legislatura | Legislative Period Number | | nuLegislatura | term | | Legislatura: Período de quatro anos coincidente com o mandato parlamentar dos Deputados Federais. No contexto da cota CEAP, representa o ano base de início da legislatura e é utilizado para compor a Carteira Parlamentar, pois esta poderá ser alterada à medida que se muda de Legislatura. | Legislative period: 4 years period, the same period of the term of congresspeople. In the context of this allowance, it represents the initial year of the legislature. It is also used as part of the Congressperson Document Number since it changes in between legislatures. |
5. State (state)
| 🇧🇷 | 🇬🇧 | |:------:|:------:| | Sigla da UF | State | | sgUF | state | | No contexto da cota CEAP, representa a unidade da federação pela qual o deputado foi eleito e é utilizada para definir o valor da cota a que o deputado tem. | In the context of this allowance it represents the state or federative unit that elected the congressperson; it is also used to define the value of the allowance to the congressperson. |
6. Party (party)
| 🇧🇷 | 🇬🇧 | |:------:|:------:| | Sigla do Partido | Party | | sgPartido | party | | O seu conteúdo representa a sigla de um partido. Definição de partido: é uma organização formada por pessoas com interesse ou ideologia comuns, que se associam com o fim de assumir o poder para implantar um programa de governo. Tem personalidade jurídica de direito privado e goza de autonomia e liberdade no que diz respeito à criação, organização e funcionamento, observados os princípios e preceitos constitucionais. | It represents the abbreviation of a party. Definition of party: it is an organization built by people with interests or ideologies in common. They form an association with the purpose of achieving power to implement a government program. They are legal entities, free and autonomous when it comes to their creation and self-organization, since they respect the constitutional commandments. |
7. Legislative Period Code (term_id)
| 🇧🇷 | 🇬🇧 | |:------:|:------:| | Código da Legislatura | Legislative Period Code | | codLegislatura | term_id | | Legislatura: Período de quatro anos coincidente com o mandato parlamentar dos Deputados Federais. No contexto da cota CEAP, o seu conteúdo representa o código identificador da Legislatura, que um número ordinal sequencial, alterado de um em um, a cada início de uma nova Legislatura (por exemplo, a Legislatura que iniciou em 2011 é a 54ª Legislatura). | Legislative period: 4 years period, the same period of the term of congresspeople. In the context of this allowance it represents the identifying code of the legislature, an ordinal number incremented by one each new legislature (e.g. the 2011 legislature is the 54th legislature). |
8. Subquota Number (subquota_number)
| 🇧🇷 | 🇬🇧 | |:------:|:------:| | Número da Subcota | Subquota Number | | numSubCota | subquota_number | | No contexto da Cota CEAP, o conteúdo deste dado representa o código do Tipo de Despesa referente à despesa realizada pelo deputado e comprovada por meio da emissão de um documento fiscal, a qual é debitada na cota do deputado. | In the context of this allowance this is the code of the category group referring to the nature of the expense claimed by the congressperson's receipt, the receipt of what was debited from the congressperson's account. |
9. Subquota Description (subquota_description)
| 🇧🇷 | 🇬🇧 | |:------:|:------:| | Descrição da Subcota | Subquota Description | | txtDescricao | subquota_description | | O seu conteúdo é a descrição do Tipo de Despesa relativo à despesa em questão. | The description of the category group referring to the nature of the expense. |
10. Subquota Specification Number (subquota_group_id)
| 🇧🇷 | 🇬🇧 | |:------:|:------:| | Número da Especificação da Subcota | Subquota Specification Number | | numEspecificacaoSubCota | subquota_group_id | | No contexto da Cota CEAP, há despesas cujo Tipo de Despesa necessita ter uma especificação mais detalhada (por exemplo, “Combustível”). O conteúdo deste dado representa o código desta especificação mais detalhada. | In the context of this allowance there are expenses under certain category groups that require further specifications (e.g. fuel). This variable represents the code of these detailed specification. |
11. Subquota Specification Description (subquota_group_description)
| 🇧🇷 | 🇬🇧 | |:------:|:------:| | Descrição da Especificação da Subcota | Subquota Specification Description | | txtDescricaoEspecificacao | subquota_group_description | | Representa a descrição especificação mais detalhada de um referido Tipo de Despesa. | Description of the detailed specification required by certain category groups. |
12. Supplier (supplier)
| 🇧🇷 | 🇬🇧 | |:------:|:------:| | Fornecedor | Supplier | | txtFornecedor | supplier | | O conteúdo deste dado representa o nome do fornecedor do produto ou serviço presente no documento fiscal | Name of the supplier of the product or service specified by the receipt. |
13. CNPJ/CPF (cnpj_cpf)
| 🇧🇷 | 🇬🇧 | |:------:|:------:| | CNPJ/CPF | CNPJ/CPF | | txtCNPJCPF | cnpj_cpf | | O conteúdo deste dado representa o CNPJ ou o CPF do emitente do documento fiscal, quando se tratar do uso da cota em razão do reembolso despesas comprovadas pela emissão de documentos fiscais. | CNPJ or CPF are identification numbers issued for, respectively, companies and people by Federal Revenue of Brazil. CNPJ are 14 digits long and CPF are 11 digits long. This field is the identification number (CNPJ or CPF) of the legal entity issuing the receipt. The receipt is a proof of the expense and is a valid document used to claim for a reimbursement. |
14. Document Number (document_number)
| 🇧🇷 | 🇬🇧 | |:------:|:------:| | Número do Documento | Document Number | | txtNumero | document_number | | O conteúdo deste dado representa o número de face do documento fiscal emitido ou o número do documento que deu causa à despesa debitada na cota do deputado. | This field is the identifying number issued in the receipt, in the proof of expense declared by the congressperson in this allowance. |
15. Fiscal Document Type (document_type)
| 🇧🇷 | 🇬🇧 | |:------:|:------:| | Indicativo de Tipo de Documento Fiscal | Fiscal Document Type | | indTipoDocumento | document_type | | Este dado representa o tipo de documento do fiscal – 0 (Zero), para Nota Fiscal; 1 (um), para Recibo; e 2, para Despesa no Exterior. | Type of receipt — 0 (zero) for bill of sale; 1 (one) for simple receipt; and 2 (two) to expense made abroad. |
16. Issue Date (issue_date)
| 🇧🇷 | 🇬🇧 | |:------:|:------:| | Data de Emissão | Issue Date | | datEmissao | issue_date | | O conteúdo deste dado é a data de emissão do documento fiscal ou a data do documento que tenha dado causa à despesa. | Issuing date of the receipt. |
17. Document Value (document_value)
| 🇧🇷 | 🇬🇧 | |:------:|:------:| | Valor do Documento | Document Value | | vlrDocumento | document_value | | O seu conteúdo é o valor de face do documento fiscal ou o valor do documento que deu causa à despesa. Quando se tratar de bilhete aéreo, esse valor poderá ser negativo, significando que o referido bilhete é um bilhete de compensação, pois compensa um outro bilhete emitido e não utilizado pelo deputado (idem para o dado vlrLiquido abaixo). | Value of the expense in the receipt. If it refers to fly tickets this value can be negative, meaning that it is a credit related to another fly tickets issued but not used by the congressperson (the same is valid for net_value). |
18. Remark Value (remark_value)
| 🇧🇷 | 🇬🇧 | |:------:|:------:| | Valor da Glosa | Remark Value | | vlrGlosa | remark_value | | O seu conteúdo representa o valor da glosa do documento fiscal que incidirá sobre o Valor do Documento, ou o valor da glosa do documento que deu causa à despesa. | Remarked value of the expense concerning the value of the receipt, or remarked value of the expense. |
19. Net Value (net_value)
| 🇧🇷 | 🇬🇧 | |:------:|:------:| | Valor Líquido | Net Value | | vlrLiquido | net_value | | O seu conteúdo representa o valor líquido do documento fiscal ou do documento que deu causa à despesa e será calculado pela diferença entre o Valor do Documento e o Valor da Glosa. É este valor que será debitado da cota do deputado. Caso o débito seja do Tipo Telefonia e o valor seja igual a zero, significa que a despesa foi franqueada. | Net value of the receipt calculated from the value of the receipt and the remarked value. This is the value that is going to be debited from the congressperson's account. If the category group is Telephone and the value is zero, it means the expense was franchised out. |
20. Month (month)
| 🇧🇷 | 🇬🇧 | |:------:|:------:| | Mês | Month | | numMes | month | | O seu conteúdo representa o Mês da competência financeira do documento fiscal ou do documento que deu causa à despesa. É utilizado, junto com o ano, para determinar em que período o débito gerará efeito financeiro sobre a cota. | Month of the receipt. It is used together with the year to determine in which month the debt will be considered in the context of this allowance. |
21. Year (year)
| 🇧🇷 | 🇬🇧 | |:------:|:------:| | Ano | Year | | numAno | year | | O seu conteúdo representa o Ano da competência financeira do documento fiscal ou do documento que deu causa à despesa. É utilizado, junto com o mês, para determinar em que período o débito gerará efeito financeiro sobre a cota. | Year of the receipt. It is used together with the month to determine in which month the debt will be considered in the context of this allowance. |
22. Installment Number (installment)
| 🇧🇷 | 🇬🇧 | |:------:|:------:| | Número da Parcela | Installment Number | | numParcela | installment | | O seu conteúdo representa o número da parcela do documento fiscal. Ocorre quando o documento tem de ser reembolsado de forma parcelada. | The number of the installment of the receipt. Used when the receipt has to be reimbursed in installments. |
23. Passenger (passenger)
| 🇧🇷 | 🇬🇧 | |:------:|:------:| | Passageiro | Passenger | | txtPassageiro | passenger | | O conteúdo deste dado representa o nome do passageiro, quando o documento que deu causa à despesa se tratar de emissão de bilhete aéreo. | Name of the passenger when the receipt refers to a fly ticket. |
24. Leg of the Trip (leg_of_the_trip)
| 🇧🇷 | 🇬🇧 | |:------:|:------:| | Trecho | Leg of the Trip | | txtTrecho | leg_of_the_trip | | O conteúdo deste dado representa o trecho da viagem, quando o documento que deu causa à despesa se tratar de emissão de bilhete aéreo. | Leg of the trip when the receipt refers to a fly ticket. |
25. Batch Number (batch_number)
| 🇧🇷 | 🇬🇧 | |:------:|:------:| | Número do Lote | Batch Number | | numLote | batch_number | | No contexto da Cota CEAP, o Número do Lote representa uma capa de lote que agrupa os documentos que serão entregues à Câmara para serem ressarcidos. Este dado, juntamente com o Número do Ressarcimento, auxilia a localização do documento no Arquivo da Casa. | In the context of this allowance the batch number refers to the cover number of a batch grouping receipts handed in to the Chamber of Deputies to be reimbursed. This data together with the reimbursement number helps in finding the receipt in the Lower House Archive. |
26. Reimbursement Number (reimbursement_number)
| 🇧🇷 | 🇬🇧 | |:------:|:------:| | Número do Ressarcimento | Reimbursement Number | | numRessarcimento | reimbursement_number | | No contexto da Cota CEAP, o Número do Ressarcimento indica o ressarcimento do qual o documento fez parte por ocasião do processamento do seu reembolso. Este dado, juntamente com o Número do Ressarcimento, auxilia a localização do documento no Arquivo da Casa. | In the context of this allowance the reimbursement number points to document issued in the reimbursement process. This data together with the reimbursement number helps in finding the receipt in the Chamber of Deputies Archive. |
27. Reimbursement Value (reimbursement_value)
| 🇧🇷 | 🇬🇧 | |:------:|:------:| | Valor da Restituição | Reimbursement Value | | vlrRestituicao | reimbursement_value | | O seu conteúdo representa o valor restituído do documento fiscal que incidirá sobre o Valor do Documento. | Reimbursement value referring to the document value. |
28. Applicant Identifier (applicant_id)
| 🇧🇷 | 🇬🇧 | |:------:|:------:| | Identificador do Solicitante | Applicant Identifier | | nuDeputadoId | applicant_id | | Número que identifica um Parlamentar ou Liderança na Transparência da Cota para Exercício da Atividade Parlamentar. | Identifying number of a congressperson or the Chamber of Deputies leadership for the sake of transparency and accountability within this allowance. |
Acknowledgements
All the activists involved in this great initiative.
Inspiration
Here are just some questions!
In which period of the year do congressmen spend more? Is there a relation between spending and session attendance patterns? Which party spends more, in average, in each of the 27 states? In average, which are the top spenders? Do congressmen spend more on their birthdays? Which professions are associated with most spending? Does education level relates with spending patterns? What is the relation between spending patterns and congressmen profile characteristics?"
Bag of Words Meets Bags of Popcorn,Use Google's Word2Vec for movie reviews,rocha,5,"Version 1,2017-05-19",,Other,32 MB,CC0,"1,249 views",143 downloads,15 kernels,0 topics,https://www.kaggle.com/rochachan/bag-of-words-meets-bags-of-popcorn,"Context
The competition is over 2 yrs ago. I just wanna play around the dataset.
Content
The labeled data set consists of 50,000 IMDB movie reviews, specially selected for sentiment analysis. The sentiment of reviews is binary, meaning the IMDB rating < 5 results in a sentiment score of 0, and rating >=7 have a sentiment score of 1. No individual movie has more than 30 reviews. The 25,000 review labeled training set does not include any of the same movies as the 25,000 review test set. In addition, there are another 50,000 IMDB reviews provided without any rating labels.
id - Unique ID of each review
sentiment - Sentiment of the review; 1 for positive reviews and 0 for negative reviews
review - Text of the review
Acknowledgements
The origin place is here. Awesome tutorial is here, we can play with it.
Inspiration
Just for study and learning"
RollerCoaster Tycoon Data,Every roller coaster I have built in RCT2 for iPad,Nolan Conaway,5,"Version 5,2017-08-03|Version 4,2017-06-26|Version 3,2017-06-05|Version 2,2017-06-05|Version 1,2017-05-24",video games,CSV,16 KB,Other,"2,538 views",149 downloads,11 kernels,,https://www.kaggle.com/nolanbconaway/rollercoaster-tycoon-rides,"RollerCoaster Tycoon Data
When I was a kid i loved RollerCoaster Tycoon. I recently found out that Atari had ported the classic game for iOS, and I immediately purchased it. Now I play whenever I get a free moment. Usually I manage to play a few games a week while I watch TV or listen to podcasts.
If you do not know, a main aspect of the game involves designing custom roller coasters. When you build something new, you can go to a special window to view the ride statistics:
The most important metrics are at the top: You want to maximize excitement without making the ride too intense or nauseating.
When I was a kid I had no patience for numbers (I created rides for maximum awesomeness), but these days I'm fond of that sort of thing. After I completed a few park scenarios, I began to wonder If i can use my own data to build better roller coasters. So I started saving my ride stats in a spreadsheet: After completing each scenario, I just logged the stats from each ride into the spreadsheet. This repository contains that data, as well as any analyses I might conduct on it.
The Data
I still play a few games a week (duh), so occasionally I will update the database with my future creations (feel free to send me your own!). Each ride is cataloged based on the info from the window above:
park_id: Integer. Unique park identifier, zero-indexed.
theme: String. Title of the park scenario.
rollercoaster_type: String. Category of roller coaster.
custom_design: Boolean. True/False on whether I designed the ride, or if it was pre-designed.
excitement: Float. Ride excitement from 0 (very low) with no specified maximum, but it is very rarely above 10.0. Larger numbers are always better.
intensity: Float. Ride intensity from 0 (very dull) with no specified maximum, though most (well-designed) rides are under 10.0. Each customer has their own intensity preference.
nausea: Float. Ride nausea from 0 (very low) with no specified maximum, but lower is better and rides rarely have values above 10.0.
excitement_rating, intensity_rating, `nausea_rating: String. Descriptors of the excitement, intensity, and nausea ratings.
max_speed: Integer. Maximum speed (mph).
avg_speed: Integer. Average speed (mph).
ride_time: Integer. Total duration of the ride in seconds.
ride_length: Integer. Length of the ride in feet.
max_pos_gs, max_neg_gs, max_lateral_gs: Float. Values describing the maximum observed positive, negative, and lateral G-Forces.
total_air_time: Float. Number of seconds in which riders experience weightlessness.
drops: Integer. Number of downhill segments.
highest_drop_height: Integer. Highest height (with 0 being sea-level?) from which a drop takes place. Note: rides can drop to -6 in the game.
inversions: Integer. Number of times riders are upside-down during the ride. This accounts for loops, corkscrews, etc. values of -1 indicate missing information (see caveat #2).
Caveats
So far I've only been keeping track of roller coasters, so the data does not include customizable thrill rides or water rides (excepting the Dinghy Slide, which is classified as a roller coaster in the game).
It is unfortunate that the first handful of rides I built did not have any inversions, and it took me several weeks to realize that the game does not show this info unless there are inversions. During that time, I simply ignored the inversions count, so we just do not have that info for many rides. Some rides cannot have inversions, and I filled in that information after the fact. So, a value for inversions = -1 indicates that the ride could have had inversions."
ESA's Mars Express Operations Dataset,Support space operations with Machine Learning for the Mars Express Orbiter,European Space Agency,5,"Version 1,2017-05-27",,Other,357 MB,Other,869 views,132 downloads,,0 topics,https://www.kaggle.com/europeanspaceagency/mars-express-power-hackathon,"This dataset is part of the Mars Express Power Challenge which spans over several competitions and hackathons. The data is licensed as Creative Commons and to know how you can share it, follow the link below:
License: ESA CC BY-SA 3.0 IGO
Description
It has now been more than 12 years that the Mars Express Orbiter (MEX) provides science data from Mars about its ionosphere and ground subsurface composition. The 3D imagery of Mars has provided the community with unprecedented information about the planet. Today, thanks to the work of careful and expert operators, Mars Express Orbiter still provides information that supports ground exploration missions on Mars (Curiosity, Opportunity, ...) and a lot of other research.
The Mars Express Orbiter is operated by the European Space Agency from its operations centre (Darmstadt, Germany) where all the telemetry is analysed. The health status of the spacecraft is carefully monitored to plan future science observations and to avoid power shortages.
Operators of Mars Express keep track of the thermal power consumption thanks to the telemetry data. The spacecraft uses electric power coming from the solar arrays (or batteries, during eclipses) not only to supply power to the platform units, but also to the thermal subsystem, which keeps the entire spacecraft within its operating temperature range. The remaining available power can be used by the payloads to do science operations:
SciencePower = ProducedPower − PlatformPower − ThermalPower
The Mars Express Power Challenge focuses on the difficult problem of predicting the thermal power consumption. Three full Martian years of Mars Express telemetry are made available and you are challenged to predict the thermal subsystem power consumption on the following Martian year. If successful, the winning method will be a new tool helping the Mars Express Orbiter deliver science data for a longer period of time.
more details on the data could be found there: https://kelvins.esa.int/mars-express-power-challenge/
License of the data: ESA CC BY-SA 3.0 IGO"
The Bachelor contestants,"Includes name, age, occupation, hometown, and week eliminated",Brian Gonzalez,5,"Version 2,2017-03-07|Version 1,2017-03-07",popular culture,CSV,28 KB,CC0,"1,785 views",129 downloads,5 kernels,,https://www.kaggle.com/brianbgonz/the-bachelor-contestants,"Context
I thought it might be neat to do some simple analytics on The Bachelor contestant data.
Content
Contestant info (incomplete) from seasons 1, 2, 5, 9-21 of ABC's The Bachelor.
Acknowledgements
Pulled from Wikipedia and Reddit user u/nicolee314.
Inspiration
Are there any predictors for success on reality dating shows? Has the typical contestant changed over the years? Are certain qualities under/over-represented in these contestants?"
Woodbine Horse Racing Results,"Woodbine Race Track in Toronto, ON",Benjamin Visser,5,"Version 2,2017-05-13|Version 1,2017-05-02",horse racing,CSV,1 MB,Other,"1,315 views",101 downloads,3 kernels,,https://www.kaggle.com/noqcks/woodbine-races,"Context
This represents race data for Woodbine Track in Toronto, ON from the period of 06/2015 - 04/2017"
Powerball Numbers,Winning numbers for powerball,ScottHendrickson,5,"Version 2,2017-03-18|Version 1,2017-03-18",,CSV,60 KB,CC0,"1,317 views",81 downloads,2 kernels,,https://www.kaggle.com/scotth64/powerball-numbers,"Context
In the powerball game, the numbers that the lottery selects are random, but the numbers that players choose to play are not. This data could be used to build models that predict prize amounts as a function of the numbers drawn or other interesting correlations
Content
This dataset is space delimited and represents winning powerball numbers dating back to 1997
Acknowledgements
These numbers were acquired from http://www.powerball.com/powerball/pb_numbers.asp
Inspiration
With machine learning advances accelerating rapidly, could someone create a random number generator that could predict powerball winning numbers ?"
Active Volcanoes in the Philippines,23 active volcanoes >,Gabriel Joshua Miguel,5,"Version 1,2017-05-09",,CSV,2 KB,Other,976 views,33 downloads,,,https://www.kaggle.com/gabbygab/active-volcanoes-in-the-philippines,"Context
Active volcanoes in the Philippines, as categorized by the Philippine Institute of Volcanology and Seismology (PHIVOLCS), include volcanoes in the country having erupted within historical times (within the last 600 years), with accounts of these eruptions documented by humans; or having erupted within the last 10,000 years (holocene) based on analyses of datable materials. However, there is no consensus among volcanologists on how to define an ""active"" volcano. As of 2012, PHIVOLCS lists 23 volcanoes as active in the Philippines, 21 of which have historical eruptions; one, Cabalian, which is strongly fumarolic volcano[further explanation needed]; and one, Leonard Kniaseff, which was active 1,800 years ago (C14).
There are 50 Philippines volcanoes listed by the royal Smithsonian Institution's Global Volcanism Program (GVP) at present,of which 20 are categorized as ""historical"" and 59 as ""Holocene"".The GVP lists volcanoes with historical, Holocene eruptions, or possibly older if strong signs of volcanism are still evident through thermal features like fumaroles, hot springs, mud pots, etc.
Content
Name
m
ft
Coordinates
Province
Eruptions
Acknowledgements
Philippine Institute of Volcanology and Seismology (PHIVOLCS)"
"Visa Free Travel by Citizenship, 2016",Inequality in world citizenship,sdorius,5,"Version 1,2017-04-22",,CSV,5 KB,Other,521 views,28 downloads,,0 topics,https://www.kaggle.com/sdorius/visafree2016,"Not all world citizens are treated equally. One way we might think about measuring inequality in the value of a person’s citizenship is by the number of countries in which they can freely travel. This data set contains information on the global reach of national passports for 199 countries. Data identify the number of countries in which each passport is granted visa free travel, a visa on arrival, and the number of countries in which each passport is welcomed by destination countries. The data were downloaded from https://www.passportindex.org/byIndividualRank.php on April 11, 2016 using the following R scraper package (https://github.com/sdorius/passportr).
INFORMATION ABOUT VARIABLES: visarank: Passport Index rank (was originally labeled 'Global Rank'). visafree: Number of countries in which the passport allows for visa-free travel (originally labeled 'Individual Rank'). visaoa: Number of countries in which the passport allows for visa-on-arrival (originally labeled 'Individual Rank'). visawelc Number of passports accepted for travel by the destination country (originally labeled 'Welcoming Rank')."
The Global Competitiveness Index dataset,100s of indicators shedding light on the development prospects of 140+ economies,World Economic Forum,5,"Version 1,2017-01-26",,Other,6 MB,CC4,"3,339 views",267 downloads,,0 topics,https://www.kaggle.com/weforum/global-competitiveness,"Context
Launched in 1979, the World Economic Forum's Global Competitiveness Report is the longest-running, most comprehensive assessment of the drivers of economic development. The 2016-2017 edition covers 138 economies. Please contact gcp@weforum.org for any question.
Go here for more information about the Global Competitiveness Report and Index.
Content
The data contains all the components of the Global Competitiveness Index, that's 170 time series, including 113 individual indicators. The file contains the following tabs:
About this Dataset: all the legal stuff and important disclaimers
Data: contains the data organised as follows: Entities (economies and regions) are listed across. For each data point, the value, rank, period, source, source date, and note (if any) are reported. The dataset includes the GCI editions 2007-2008 to 2016-2017. Editions are ""stacked"" vertically, starting with the most recent. Earlier editions are not included due to change in the methodology.
Entities: list of all entities, i.e. economies, with ISO codes and groups to which they belong (i.e. World Bank's income group, IMF's regional classification, and Forum's own regional classification)
Meta data: list all series with reference ID, descriptions, units, placement in the index, etc. Detailed methodology"
Farmers Markets in New York City,How do population demographics impact the location of farmers markets?,NYC Open Data,5,"Version 1,2017-01-25","food and drink
demographics
agriculture",CSV,11 KB,CC0,"2,594 views",279 downloads,,0 topics,https://www.kaggle.com/nycopendata/farmers-markets,"Context
The non-profit organization GrowNYC operates a network of 50+ Greenmarkets and 15 Youthmarkets throughout the city, including the flagship Union Square Greenmarket, to ensure that all New Yorkers have access to the freshest, healthiest local food.
Acknowledgements
The farmers market directory was published by the NYC Department of Health and Mental Hygiene, and the population statistics were provided by the US Census Bureau's 2015 American Community Survey."
A Tribuna,Journalistic documents published on the Internet,TheScientistBR,5,"Version 2,2017-03-09|Version 1,2017-03-02","news agencies
internet",Other,134 MB,Other,"1,537 views",86 downloads,2 kernels,,https://www.kaggle.com/TheScientistBR/atribuna,"Context
The newspaper publications on the Internet increases every day. There are many news agencies, newspapers and magazines with digital publications on the big network. Published documents made available to users who, in turn, use search engines to find them. To deliver the closest searched documents, these documents must be previously indexed and classified. With the huge volume of documents published every day, many researches have been carried out in order to find a way of dealing with the automatic document classification.
Content
The ""Tribuna"" database is of journalistic origin with its digital publication, a factor that may be important for professionals of the area, also serving to understand other similar datasets. In order to carry out the experiment, we adopted the ""A Tribuna"" database, whose main characteristics presented previously, show that the collection is a good source of research, since it is already classified by specialists and has 21 classes that can be Displayed in the table below.
Acknowledgements
My thanks to the company ""A Tribuna"" that gave all these text files for experiment at the Federal University of Espírito Santo. To the High Desermpenho Computation Laboratory (LCAD) for all the help in the experiments. Thanks also to Prof. PhD Oliveira, Elias for all the knowledge shared.
Inspiration
There are two issues involving this dataset:
What is the best algorithm for sorting these documents?
What are the elements that describe each of the 21 classes in the collection?"
2015 Global Open Data Index,What is the state of open data around the world?,Open Knowledge International,5,"Version 1,2017-03-04",research,CSV,257 KB,CC0,"2,200 views",264 downloads,8 kernels,0 topics,https://www.kaggle.com/okfn/open-data,"Context
The Global Open Data Index is an annual effort to measure the state of open government data around the world. The crowdsourced survey is designed to assess the openness of specific government datasets according to the Open Definition.
The Global Open Data Index is not an official government representation of the open data offering in each country, but an independent assessment from a citizen’s perspective. It is a civil society audit of open data, and it enables government progress on open data by giving them a measurement tool and a baseline for discussion and analysis of the open data ecosystem in their country and internationally from a key user’s perspective.
The Global Open Data Index plays a powerful role in sustaining momentum for open data around the world and in convening civil society networks to use and collaborate around this data. Governments and open data practitioners can review the index results to see how accessible the open data they publish actually appears to their citizens, see where improvements are necessary to make open data truly open and useful, and track their progress year to year.
According to the common open data assessment framework there are four different ways to evaluate data openess — context, data, use and impact. The Global Open Data Index is intentionally narrowly focused on the data aspect, hence, limiting its inquiry only to the datasets publication by national governments. It does not look at the broader societal context, seek to assess use or impact in a systematic way, or evaluate the quality of the data. This narrow focus of data publication enables it to provide a standardized, robust, comparable assessment of the state of the publication of key data by governments around the world.
Acknowledgements
The Global Open Data Index is compiled by Open Knowledge International with the assistance of volunteers from the Open Knowledge Network around the world.
Inspiration
What is the state of open data around the world? Which countries or regions score the highest in all the data categories? Did any countries receive lower open data scores than in previous years?"
Slums and informal settlements detection,Use satellite imagery and machine learning to help fight against poverty.,Federico Baylé,5,"Version 1,2017-07-13",,Other,323 MB,ODbL,470 views,26 downloads,,,https://www.kaggle.com/fedebayle/slums-argentina,"Context
This datasets comes from a personal project that begun with my MSc thesis in Data Mining at Buenos Aires University. There I detect slums and informal settlements for La Matanza (Buenos Aires) district. The algorithm developed there helps to reduce to 15% of the total territory to analyze.
After successfully finish the thesis, I created a map of slums for whole Argentina. Map and thesis content are available at fedebayle.github.io/potencialesvya.
As far as I know, this is the first research of its kind in Argentina, which I think would help my country to contribute to UN Millennium Development Goal 7, Target 11, ""Improving the lives of 100 million slum dwellers"".
Content
This datasets contains georeferenced images about urban slums and informal settlements for two districts in Argentina: Buenos Aires and Córdoba (~ 15M habitants).
The image of Cordoba was taken on 2017-06-09 and the images of Buenos Aires on 2017-05-04.
Each image comes from Sentinel-2 sensor, with 32x32px and 4 bands (bands 2, 3, 4, 8A, 10 meter resolution). Those who prefix is ""vya_"" contains slums (positive class). Sentinel-2 is an Earth observation mission developed by ESA as part of the Copernicus Programme to perform terrestrial observations in support of services such as forest monitoring, land cover changes detection, and natural disaster management.
Images are in .tif format.
Image names consist of:
(vya_)[tile id]_[raster row start]_[raster row end]_[raster column start]_[raster column end].tif
This is a highly imbalanced class problem:
Córdoba: 37 of 13,004 images corresponds to slums.
Buenos Aires: 1,008 of 46,047 images corresponds to slums.
Acknowledgements
I would not have been able to create this dataset if the Sentinel program did not exist. Thanks to European Space Agency!
Inspiration
The cost of conducting a survey of informal settlements and slums is high and requires copious logistical resources. In Argentina, these surveys have been conducted only each 10 years at census.
Algorithms developed with this data could be used in different countries and help to fight poverty around the world."
NYC Hourly Temperature,Temperature and weather condition.,mavez DABAS,5,"Version 3,2017-07-28|Version 2,2017-07-28|Version 1,2017-07-28","united states
weather",CSV,216 KB,CC0,532 views,55 downloads,,0 topics,https://www.kaggle.com/mavezdabas/nychourlytemperature,"Context
Hourly weather data for New York City. Extracted from online web sources. The following data set is cleaned for the purpose for NYC Taxi ETA calculation.
Content
We have features such as Date, Time, temperature (F), Dew Point (F), Humidity, Wind Speed (MPH), Condition.
Acknowledgements
The cleaned version is user owned. Used in past research for weather data analysis in Boston. Performed the similar calculation to extract the dataset.
Inspiration
The hourly dataset is cleaned with no missing values. Along with temperature the dataset also consists of features like Humidity and Condition such as snow, rain etc."
Keras Models,Let's see if we can upload Keras Model files to Kaggle,PhillipChin,5,"Version 4,2017-06-21|Version 3,2017-06-21|Version 2,2017-06-21|Version 1,2017-06-20",,Other,1 GB,CC0,"1,498 views",58 downloads,4 kernels,,https://www.kaggle.com/ekkus93/keras-models,"Context
So I was trying to use a VGG19 pretrained model with Keras but the Docker instance couldn't download the model file. There's an open ticket for this issue here: https://github.com/Kaggle/docker-python/issues/73
Content
Just starting off with VGG16 and VGG19 for now. If this works, I'll upload some more. The weights for the full vgg16 and vgg19 files were too large to upload as a single files. I tried uploading them in parts but there wasn't enough room to extract them in the working directory.
Here's an example on how to use the model files:
keras_models_dir = ""../input/keras-models""
model = applications.VGG16(include_top=False, weights=None) model.load_weights('%s/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5' % keras_models_dir)
Here's some more examples on how to use it: https://www.kaggle.com/ekkus93/keras-models-as-datasets-test
Acknowledgements
I downloaded the files from here: https://github.com/fchollet/deep-learning-models
Inspiration
I just wanted try out something with the Dogs vs Cats dataset and VGG19."
Boston Housing,Concerns housing values in suburbs of Boston,Chad Schirmer,5,"Version 1,2017-06-11",,CSV,12 KB,CC4,"3,086 views",335 downloads,8 kernels,0 topics,https://www.kaggle.com/schirmerchad/bostonhoustingmlnd,"Context
The dataset for this project originates from the UCI Machine Learning Repository. The Boston housing data was collected in 1978 and each of the 506 entries represent aggregated data about 14 features for homes from various suburbs in Boston, Massachusetts.
Acknowledgements
https://github.com/udacity/machine-learning
https://archive.ics.uci.edu/ml/datasets/Housing"
Mobile location history of 10/2014,Google Maps routes in October 2014,Julian Simon de Castro,5,"Version 1,2017-06-08","geography
demographics",{}JSON,6 MB,CC0,"1,582 views",131 downloads,,,https://www.kaggle.com/juliansimon/location-history,"History of Locations of an android mobile in the month of October 2014.
The history of locations of each of the mobile can be downloaded from this link: Source: [https: //takeout.google.com/settings/takeout/custom/location_history][1]
Content
These data are interesting to perform geolocation studies associated with time. Format file: JSON
Acknowledgements
Thanks to Google for allowing us to download our history of our data"
Forbes Top 2000 Companies,Who is leading the market today?,"I,Coder",5,"Version 3,2017-07-06|Version 2,2017-07-05|Version 1,2017-07-04",,CSV,502 KB,Other,"1,290 views",208 downloads,,,https://www.kaggle.com/ash316/forbes-top-2000-companies,"Every Year Forbes.com releases a list of Top 2000 companies worldwide. These companies are ranked by various metrics like the company size, its market value, sales, profit,etc.
So this is the Forbes 2000 companies for 3 years viz 2013-2015-2017. The data was scraped from Forbes.com. The files contain the list of the Top 2000 companies for the 3 years mentioned. The attributes in the dataset are:
Rank- Rank of the company for that year
Company- Name of the company
Country- Country it belongs to
Sales- Sales that corresponding year
Market Values- Market value in billions
Profit- Profit earned that year
Assests- Total revenue the company has
Sector- Expertise of the company like Finance or Technology,etc."
"Hotel Reviews from Chennai, India","Reviews of over 500 hotels across the city of Chennai, India",RanjithaKorrapati,5,"Version 2,2017-06-24|Version 1,2017-06-24","cities
india
hotels
linguistics",CSV,1 MB,Other,"1,967 views",223 downloads,2 kernels,,https://www.kaggle.com/ranjitha1/hotel-reviews-city-chennai,"Context
The volume of text data is increasing at a humongous rate everyday which has made it almost impossible to evaluate the data manually. In order to make the process of analyzing this text automatic there are various machine learning techniques that could be applied. This data set is for those enthusiasts who are willing to play with text data and perform sentiment analysis / text classification.
Content
The data has been scraped from trivago,India. A python script was run to examine the get requests and make those requests explicitly in order to obtain required data in JSON. This data was further parsed and written into a csv file.
The data is in the form of a csv file with over 4000 reviews. There are 5 columns:
Column 1: Name of the hotel Column 2: Title of the review Column 3: Text of the review Column 4: Sentiment of the review*( 1: Negative 2:Average 3:Positive) Column 5: Rating percentage
*There are three values for sentiment as mentioned above. A value of 1 represents a negative reviews whereas a value of 3 represents a positive one.
Acknowledgements
I would like to thank my friend iniquitouspsyche for helping me out in scraping the data from trivago.
Inspiration
This data set consists of actual reviews from real people. So this data set will give a real time experience as to how to deal with textual data ."
Describing New York City Roads,A Collection of Road Variables in New York for the Taxi Playground Challenge,Curtis Chong,5,"Version 6,2017-08-06|Version 5,2017-08-02|Version 4,2017-08-02|Version 3,2017-08-01|Version 2,2017-08-01|Version 1,2017-08-01","road transport
taxi services",CSV,22 MB,CC0,"1,631 views",83 downloads,,,https://www.kaggle.com/splacorn/speed-limits-in-nyc-taxi-playground-challenge,"Author's Note:
This dataset was originally coined: ""Speed Limits in New York City"". Since then, I have changed the name of the dataset to ""Describing New York City Roads"" to better reflect the contents of the dataset.
- Curtis
Context
New York City Speed Limits
The New York Department of Transportation Regulates the speed limits for its roads (Afterall, we can't be hitting 88 MPH on a regular day). This dataset describes the speed limits for particular road segments of New York City streets.
The New York City Centerline
Which streets are inherently faster? How will speed limits come into play? How will nearby bike lanes slow down vehicles (and ultimately taxis)? These are the kinds of questions that can only be answered with contextual data of the streets themselves.
Fortunately, most major cities provide a public Centerline file that describes the path of all railroads, ferry routes, and streets in the city. I've taken the New York City Centerline and packaged a dataset that tries to extract meaning out of all the road connections within the city.
Content
New York City Speed Limits
Every speed limit region is a straight line. (Which represents a segment of road). These lines are expressed by two pairs of coordinates.
lat1 - The first latitude coord
lon1 - The first longitude coord
lat2 - The second latitude coord
lat2 - The second longitude coord
street - The name of the street the speed limit is imposed on
speed - The speed limit of that road section
signed - Denotes if there is a physical sign on the street that displays the speed limit to cars.
region - The city region that the road resides in. There are 5 regions: (Bronx, Brooklyn, Manhattan, Queens, and Staten Island)
distance - The length of the speed limit road section (in Miles).
The New York City Centerline
street - The name of the street
post_type* - The extension for the street name.
st_width - The width of the street (in feet). There are varying widths for the size of a street so it was hard to derive a lane count/ street using this feature. As a rule of thumb, the average lane is around 12 feet wide.
bike_lane - Defines which segments are part of the bicycle network as defined by the NYC Department of Transportation. There are 11 classes:
1 = Class I
2 = Class II
3 = Class III
4 = Links
5 = Class I, II
6 = Class II, III
7 = Stairs
8 = Class I, III
9 = Class II, I
10 = Class III, I
11 = Class III, II
Bike class information: https://en.wikipedia.org/wiki/Cycling_in_New_York_City#Bikeway_types
bike_traf_dir** - Describes the direction of traffic: (FT = With, TF = Against, TW = Two-Way)
traf_dir** - Describes the direction of traffic: (FT = With, TF = Against, TW = Two-Way)
rw_type - The type of road. There are 6 types of roads: (1 = Street, 2 = Highway, 3 = Bridge, 4 = Tunnel, 9 = Ramp, 13 = U-Turn). Note: I parsed awkward path types such as ""Ferry route"" and ""trail"".
start_contour*** - Numeric value indicating the vertical position of the feature's ""from"" node relative to grade level.
end_contour*** - Numeric value indicating the vertical position of the feature's ""to"" node relative to grade level.
snow_pri - The Department of Sanitation (DSNY) snow removal priority designation.
V = Non-DSNY
C = Critical (These streets have top priority)
S = Sector (These streets are second priority)
H = Haulster (Small spreaders with plows attached for treating areas with limited accessibility - can hold two tons of salt)
region - The city region that the road resides in. There are 5 regions: (Bronx, Brooklyn, Manhattan, Queens, and Staten Island)
length - The length of the road (in Miles).
points - The coordinates that define the road. Each coordinate is separated by '|' and the lat and lon values per coordinate are separated by ';'. (Side note: Round road sections are plotted by points along the curve).
*For those who may not be aware, road names are based on a convention. ""Avenue""s, ""Boulevard""s, and ""Road""s are different for distinct reasons. I left these fields in the dataset in case you wish to find any patterns that are pertinent to those types of roads. To learn more about road conventions, visit this link: http://calgaryherald.com/news/local-news/in-naming-streets-strict-rules-dictate-roads-rises-trails-and-more
**To explain how direction works I'll provide you with an image: http://imgur.com/a/UflwX. Think of every road on the centerline as a vector. It points from one location to another. It always points from the very first coordinate to the very last coordinate. Now pay attention to the direction of the road (circled). Note how it points in the same direction as the vector denoted by the centerline data. The ""traf_dir"" attribute of the street is ""FT"" because the vector is headed in the same direction as traffic is (it is a one-way street). For ""traf_dir"" with a value of ""TW"", the direction of the vector doesn't matter as the road is a two-way street.
***I've had little luck finding what the ""grade levels"" represent. The original aliases are ""TO_LVL_CO"" and ""FRM_LVL_CO"". I'll keep searching tonight and will try to dig up what elevation these grades represent. I highly suspect the grades are contour lines because I know they have some relevance to elevation. In the meantime here are the ""grades"" that each value represents:
1 = Below Grade 1
2 = Below Grade 2
3 = Below Grade 3
4 = Below Grade 4
5 = Below Grade 5
6 = Below Grade 6
7 = Below Grade 7
8 = Below Grade 8
9 = Below Grade 9
10 = Below Grade 10
11 = Below Grade 11
12 = Below Grade 12
13 = At Grade
14 = Above Grade 1
15 = Above Grade 2
16 = Above Grade 3
17 = Above Grade 4
18 = Above Grade 5
19 = Above Grade 6
20 = Above Grade 7
21 = Above Grade 8
22 = Above Grade 9
23 = Above Grade 10
24 = Above Grade 11
25 = Above Grade 12
26 = Above Grade 13
99 = Not Applicable
All in all, their documentation could be better and here is a reference to it if you want to look at the source: (https://data.cityofnewyork.us/api/views/exjm-f27b/files/cba8af99-6cd5-49fd-9019-b4a6c2d9dff7?download=true&filename=Centerline.pdf)
Acknowledgements
I want to thank the New York City Department of Transportation (NYCDOT) and the city of New York for aggregating the original data sets.
New York City Speed Limits http://www.nyc.gov/html/dot/html/about/vz_datafeeds.shtml 28‐11 Queens Plaza, 8th FL Long Island City, New York 11101
The New York City Centerline https://catalog.data.gov/dataset/nyc-street-centerline-cscl data.cityofnewyork.us New York, NY 10007"
Titanic Dataset Analysis,Analyzing the survivability rate of passengers based on sibling relationships,CITIES,5,"Version 1,2017-02-08",,CSV,60 KB,Other,"1,130 views",162 downloads,32 kernels,0 topics,https://www.kaggle.com/cities/titanic123,"Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
GSMArena Phone Dataset,8000+ phones specifications scraped from GSMArena Website,Arwin Neil Baichoo,5,"Version 1,2017-06-29",,CSV,5 MB,ODbL,"2,178 views",202 downloads,,,https://www.kaggle.com/arwinneil/gsmarena-phone-dataset,"Context
GSMArena Phones Dataset is a labeled dataset extracted from GSMArena , one of the most popular online provider of phone information, and holds a large collection of phone specification. The original purpose of this dataset was data exploration and potentially some machine learning.
The Dataset
There are 108 unique phone brands with 39 variables: network_technology , 2G_bands , 3G_bands , 4G_bands, network_speed, GPRS, EDGE ,announced ,status ,dimentions ,weight_g ,weight_oz ,SIM ,display_type ,display_resolution ,display_size ,OS ,CPU ,Chipset , GPU ,memory_card ,internal_memory ,RAM ,primary_camera ,secondary_camera ,loud_speaker ,audio_jack ,WLAN ,bluetooth ,GPS ,NFC ,radio ,USB ,sensors ,battery ,colors ,approx_price_EUR ,img_url
Notes
Multivalued columns use ""|"" or ""/"" as delimiters.
The time period when the data was scraped will be mentioned in the dataset description below. The number of devices, prices and other variable are bound to change with time.
More details available about variables in column description below.
The Scraper
The dataset was scraped using a little CLI I wrote in C#, check it out on my Github https://github.com/arwinneil/phone-dataset"
Test Driven Data,Test patterns for gaining intuition of hyperparmeters in machine learning,SuperDave,5,"Version 2,2017-06-20|Version 1,2017-06-19","artificial intelligence
programming",CSV,2 MB,CC0,"1,591 views",164 downloads,,,https://www.kaggle.com/superdave/test-driven-data,"Context
In machine learning there is a long path from understanding to intuition. I have created many data files of traditional electronics test pattern to see the response of different activation, loss, optimizers, and metrics in Keras. These files should give some ability to test drive your chosen type of machine learning with a very deliberate data set.
I wanted something that was infinitely predicable to see how all the different settings effected the algorithms to set a base line for me to gain intuition as to how they should behave once I make more complex models.
Content
These files most contain single line 10,000 example patterns in sine, cosine, triangle, and others. Frequency and amplitude in some change through out the set. One has 2,500 example with 4 features of a sine wave 90 degrees out of phase from each other. The values are all between zero and one so no scaling should be necessary.
CosineDecAmpFreqInc, CosineDecreasingAmp, CosineIncAmpFreqInc, CosineIncAmpFreqSlowing, ExponentialDecayTenWaves, ExponentialRiseTenWaves, FourSineWaves, LinearFall, LinearRise, Lorentz, Multitone, Pulse10Waves, Pulse10WavesInverted, RandomSamples, SinFiveWaves, SinFourtyWaves, SinTenWaves, SinTwentyWaves, 30,000 SquareFiveWaves, SquareTenWaves, SweepOneToFive, SweepOneToTwo, SweepOneToTwoPointFive, SyncPattern, TriangleFiveWaves, TriangleTenWaves.csv
Acknowledgements
Some were generated using Tektronics ArbExpress and modified in Excel for scale. Some I generated in c#.
Inspiration
How about a good Toy example of a LSTM in Keras with multivariate data and a single prediction of one of the columns. I did the 4 sine wave .csv to try this. So far the examples I have found just average all of them."
"Rocket alerts in Israel made by ""Tzeva Adom""",List of rocket alerts in Israel published by the Israeli Home Front Command,sab30226,5,"Version 3,2017-06-25|Version 2,2017-06-23|Version 1,2017-06-21","war
international relations",Other,1017 KB,CC4,"1,340 views",87 downloads,,2 topics,https://www.kaggle.com/sab30226/rocket-alerts-in-israel-made-by-tzeva-adom,"Context
Since 2001, Palestinian militants have launched thousands of rocket and mortar attacks on Israel from the Gaza Strip as part of the continuing Arab–Israeli conflict. From 2004 to 2014, these attacks have killed 27 Israeli civilians, 5 foreign nationals, 5 IDF soldiers, and at least 11 Palestinians and injured more than 1900 people, but their main effect is their creation of widespread psychological trauma and disruption of daily life among the Israeli population.
source:
Wikipedia[https://en.wikipedia.org/wiki/Palestinian_rocket_attacks_on_Israel]
Content
This dataset contains the latest rocket alerts released by the Israeli ""Home Front Security"". The data was aggregated in the http://tzeva-adom.com site.
The column contains date in dd/mm/yy format, time mm:hh format and the name of the area in Hebrew (And sometimes messages like:
הופעלה בישוב בעוטף עזה- ככה''נ איכון שווא which means that it's a false alarm. Those messages can be easily distinguished by the length of them.
Area sizes may differ and does not report exact coordinates.
A list of all the possible areas and messages is in a separate file.
Data is reported from 2013-2014.
Acknowledgements
Context was taken from the Wikipedia page: Palestinian rocket attacks on Israel. Data generated by the Israeli Home Front Command and was made easily accessible to developers(Many apps were created based on it. Reporting the alarm during the last conflict).
The data was aggregated at the site http://tzeva-adom.com.
Inspiration
Israel has a system called ""Iron Dome"" which intercepts rockets(But only from certain distance). The challenge for Israel is where those systems should be deployed and at what times. Furthermore, it will be interesting to find patterns in the times rockets were being launched, trying to see if different places were targeted in different times of day. Also, areas that were targeted at the same time find if it's possible to cluster the places into different groups of areas.
Operation ""Protective Edge"" took place from 8 July until 26 August 2014. After it ended, The rocket attack ended (more or less) until today(June 2017). It will be interesting to check out how the operation effects the alerts, the launching patterns, targeted areas, etc.
Though the names in this dataset are in Hebrew, no Hebrew knowledge is needed for working with this dataset. I tried to find an appropriate automatic transliteration service to English, but non of them proved useful. If anyone knows how to get them in English, even using some list from the internet of the cities names in English and their corresponding Hebrew names, I'll appreciate your contribution to the dataset's GitHub repository:
https://github.com/tomersa/tzeva_adom_dataset.git
Also, you may contact me and I'll add the changes."
Korea Horse Racing,"Horse racing dataset from Korea, 2005 to 2014",park thirty-two,5,"Version 7,2017-06-04|Version 6,2017-05-29|Version 5,2017-05-29|Version 4,2017-05-29|Version 3,2017-05-29|Version 2,2017-05-29|Version 1,2017-05-29",,CSV,37 MB,Other,"2,038 views",196 downloads,,,https://www.kaggle.com/parksami/koreahorseracedata,"Context
This represents race data for Seoul, Je-Ju, Pu-Kyoung Track in Korea, ON from the period of 2005 - 2014
This dataset is made for korean kaggle user.
Content
2005 - 2014 horse racing dataset from korea
P file has
ID/RCTRCK/RACE_DE/RACE_NO/PARTCPT_NO/RANK/RCHOSE_NM HRSMN/RCORD/ARVL_DFFNC/EACH_SCTN_PAS GE_RANK/A_WIN_SUTM_EXPECT_ALOT/WIN_STA_EXPECT_ALOT
S file has
ID/RACE_DE/PRDCTN_NATION_NM/SEX/AGE/BND_WT/TRNER/RCHOSE_OWNR_NM/RCHOSE_BDWGH"
4chan.org/pol forum posts with keyword Trump,Scraped 4chan forums looking for keywords trump **NSFW**,bobbob,5,"Version 1,2017-07-02",,SQLite,115 MB,CC0,871 views,24 downloads,,,https://www.kaggle.com/bobconbob/4chan4trump,"Context
a taste of http://reddit.com/r/4chan4trump as seen at http://4chan.org/pol
sqlite databases of http://4chan.org/pol threads. sporadic but multiple day scrapping. Only finds the keyword Trump then scrapes. /r/4chan4Trump has screen caps for verification, also, helpful reddit bots which give you some quick wiki and other link facts, although the posting scrapper malfunctioned, all the posts are in the DB. If anyone wants too find the thread, the post's ID is in the reddit thread. Everythings parsable by the 4chan ID so just make sure you look through any thread with posts before posting something.
If anyone wants to contribute to the threads, they should be easy enough to parse & post. Alot of the wiki/youtube bots are supper helpful in referencing the links. There's a story behind every dataset and here's your opportunity to share yours.
Content
{ 'utc': unix timestamp, 'link': 'http://boards.4chan.org/pol/thread/129419703/demand-to-know-the-truth-about-seth-rich', 'reported': whether it was posted on /r/4chan4trump, 'id': serial 4chan id (basically a parallel processing, ie, the order of posts continually goes up) 'text': the entire scrape of the text, contains a unique ID per original post, so you can identify the same party 'country': location told to users on 4chan, likely just a IP location database 'link_ids': not used, but could be used to parse the text, 'body': just the message without the header found in text 'identity': how 4chan identifies users on a per thread basis 'thread': id of thread, the same as an id but has subthreads. if id === thread, then it's the original post, but also as the table name for good measure } For those unfamilair, 4chan is an image forum, so some posts just are image responses, so there's some holes in the dataset, but I'm sure the NSA has a bot to fill in the gaps.
Acknowledgements
Crazy is crazy, conspiracy is conspiracy, meme is meme and propaganda is one, all, and part.
Inspiration
was bored, and knew that there's some Zeitgeist going on when it came to the 2016 presidential escapades and aftermath. I would recommend thinks like levenstein distances in comparison with places like reddit.com/r/the_donald to detect some unhealthy cross overs and the viral spread."
Ubudehe Livestock 1,Ubudehe Livestock 1 from Rwanda NISR,Jean Pierre Rukundo,5,"Version 2,2017-08-04|Version 1,2017-08-03",economics,CSV,10 MB,CC0,685 views,24 downloads,2 kernels,,https://www.kaggle.com/jprukundo/ubudehelivestock1,"Overview Identification COUNTRY Rwanda TITLE Integrated Household Living Conditions Survey 2010-2011
TRANSLATED TITLE Enquête Intégrale sur les conditions de vie des ménages 2010-2011
STUDY TYPE Income/Expenditure/Household Survey SERIES INFORMATION This is the third in a series of periodic standardized income and expenditure surveys. The Rwanda EICV is conducted with a periodicity of 5 years. The surveys in the series are as follows:
EICV1 2000-2001
EICV2 2005-2006
EICV3 2010-2011
ID NUMBER RWA-NISR-EICV3-02 Version VERSION DESCRIPTION Version 2.0: Final public-use dataset
PRODUCTION DATE 2012-10-19 NOTES Version 2.0
The date of this version corresponds to the date of NISR approval of the final public-use datasets.
Overview ABSTRACT The 2010/11 Integrated Household Living Conditions Survey or EICV3 (Enquête Intégrale sur les Conditions de Vie des Ménages) is the third in the series of surveys which started in 2000/01 and is designed to monitor poverty and living conditions in Rwanda. The survey methodology has changed little over its 10 years, making it ideal for monitoring changes in the country. In 2010/11, for the first time the achieved sample size of 14,308 households in the EICV3 was sufficient to provide estimates which are reliable at the level of the district.
KIND OF DATA Sample survey data [ssd]
UNITS OF ANALYSIS For the purposes of this study, the following units of analysis are considered:
-communities
-households
-persons
Scope NOTES The scope of survey is defined by the need to evaluate poverty determinants and effects of poverty in various domains. This includes gathering data in specific sectors and examning summary statistics and computed indicators by consumption indicator, gender etc. The survey primarily seeks to compute household consumption aggregates and correlate consumption to the following areas are within the scope and integrated into the survey:
Education (education expenditures): general education, curriculum, vocational training and, higher learning, school-leaving, literacy and apprenticeship.
Health (health expenditures): disability and health problems, general health and preventative vaccination over the past 12 months.
Migration (travel expenditures): rural-urban migration, internal and external migration.
Housing (expenditures on utilities, rent etc.): status of the housing occupancy, services and installations, physical characteristics of the dwelling, access and satisfaction towards basic services.
Economic activity (revenue): unemployment, underemployment and job search, occupation, wage or salaried employment characteristics, VUP Activities, all other activities, domestic work.
Non-agricultural activities (revenue): activity status, formal and informal sector activity.
Agriculture (income and expenditure) : livestock, land and agricultural equipment, details of holding parcels/blocs and agricultural policy changes, crop harvests and use on a large and small scale crop production, harvests and use, transformation (processing) of agricultural products.
In addition to the specific sector information, consumption and/or wealth holding information was collected:
Consumption: Expenditure on non food items, food expenditure, subsistence farming (own consumption) with different recall periods.
Other cash flows : transfers out by household, transfers received by the household, income support programs & other revenues (excluding all incomes accrued from saving), VUP, UBUDEHE & RSSP schemes, other expenditure (excluding expenditures related to any form of saving).
Stock items: credit, durable assets and savings (household assets and liabilities)
TOPICS Topic Vocabulary URI consumption/consumer behaviour [1.1] CESSDA http://www.nesstar.org/rdf/common economic conditions and indicators [1.2] CESSDA http://www.nesstar.org/rdf/common EDUCATION [6] CESSDA http://www.nesstar.org/rdf/common general health [8.4] CESSDA http://www.nesstar.org/rdf/common employment [3.1] CESSDA http://www.nesstar.org/rdf/common unemployment [3.5] CESSDA http://www.nesstar.org/rdf/common housing [10.1] CESSDA http://www.nesstar.org/rdf/common time use [13.9] CESSDA http://www.nesstar.org/rdf/common migration [14.3] CESSDA http://www.nesstar.org/rdf/common information technology [16.2] CESSDA http://www.nesstar.org/rdf/common Coverage GEOGRAPHIC COVERAGE This is a national survey with representivity at the (5) provicial and (30) district level and includes urban and rural households.
GEOGRAPHIC UNIT The cluster
UNIVERSE All household members.
Producers and Sponsors PRIMARY INVESTIGATOR(S) Name Affiliation National Institute of Statistics of Rwanda (NISR) Ministry of finance and economics planning (MINECOFIN) OTHER PRODUCER(S) Name Affiliation Role Oxford Policy Management DFID Permanante assistance Geoffrey Greenwell UNDP Designer of data system David Megill UNDP Statistician Metadata Production METADATA PRODUCED BY Name Abbreviation Affiliation Role Juste NITIEMA Oxford Policy Management (OPM) Developed the document Geoffrey Greenwell UNDP Reviewed and edited document Ruben Muhayiteto NISR Revision of metadata DATE OF METADATA PRODUCTION 2011-06-02 DDI DOCUMENT VERSION Version 1.0 (Oct. 19,2012)
This version of the document represents the first draft of the public-use dataset of the EICV 3 study.
Version 1.1 (June 28th ,2016): Changed the title from French into English
DDI DOCUMENT ID RWA-NISR-DDI-EICV3-02"
Aristo MINI Corpus,"1,197,377 science-relevant sentences drawn from public data",Allen Institute for Artificial Intelligence,5,"Version 1,2017-07-14","science and culture
linguistics
artificial intelligence",Other,99 MB,CC4,875 views,54 downloads,,0 topics,https://www.kaggle.com/allenai/aristo-mini-corpus,"Overview
The Aristo Mini corpus contains 1,197,377 (very loosely) science relevant sentences drawn from public data. It provides simple science-relevant text that may be useful to help answer elementary science questions. It was used in the Aristo Mini system (http://allenai.org/software/) and is also available as a resource in its own right.
Reference
Please refer to this corpus as ""The Aristo Mini Corpus (Dec 2016 Release)"" if you mention it in a publication, with a pointer to this dataset so others can obtain it.
Contents
The Aristo Mini corpus is primarily a ""science"" subset of Simple Wikipedia. Sentences were taken from the Simple Wikipedia pages either within the ""Science"" category or from pages whose titles also occurs in a 4th Grade Study guide (by Barron’s). Also included are approximately 32 thousand definitions from Simple Wiktionary, and around 50 thousand 4th grade science-like sentences drawn from the web.
Inspiration
This dataset was originally collected with the purpose of a question answering system that could answer 4th grade science questions. The simple structure and single (broad) subject makes it a useful resource for other natural language processing tasks as well. Some interesting projects might include:
domain-specific word embeddings
testing sentence-level parsers
entity recognition across contexts -training a Markov chain to generate new sciency-sounding sentences"
"Vehicle and Tire Recalls, 1967-Present",What manufacturer has recalled the most vehicles in the past fifty years?,NHTSA,5,"Version 1,2017-02-07",automobiles,CSV,19 MB,CC0,"2,424 views",250 downloads,5 kernels,0 topics,https://www.kaggle.com/nhtsa/safety-recalls,"Context
The National Traffic and Motor Vehicle Safety Act (1966) gives the Department of Transportation’s National Highway Traffic Safety Administration (NHTSA) the authority to issue vehicle safety standards and to require manufacturers to recall vehicles that have safety-related defects or do not meet federal safety standards. More than 390 million cars, trucks, buses, recreational vehicles, motorcycles, and mopeds, 46 million tires, 66 million pieces of motor vehicle equipment, and 42 million child safety seats have been recalled to correct safety defects since 1967.
Manufacturers voluntarily initiate many of these recalls, while others are influenced by NHTSA investigations or ordered by NHTSA via the courts. If a safety defect is discovered, the manufacturer must notify NHTSA, vehicle or equipment owners, dealers, and distributors. The manufacturer is then required to remedy the problem at no charge to the owner. NHTSA is responsible for monitoring the manufacturer’s corrective action to ensure successful completion of the recall campaign.
Acknowledgements
This dataset was compiled and published by the NHTSA's Office of Defects Investigation (ODI)."
California Wire Tapping,Investigate ~$30M dollars of wiretapping in the Electronic Interceptions Reports,Electronic Frontier Foundation,5,"Version 1,2017-06-13",crime,Other,16 MB,Other,"1,269 views",94 downloads,5 kernels,0 topics,https://www.kaggle.com/eff/california-wire-tapping,"Context
In 2016, California investigators used state wiretapping laws 563 times to capture 7.8 million communications from 181,000 people, and only 19% of these communications were incriminating. The year's wiretaps cost nearly $30 million.
We know this, and much more, now that the California Department of Justice (CADOJ) for the first time has released to EFF the dataset underlying its annual wiretap report to the state legislature.
Content
The yearly “Electronic Interceptions Report” includes county-by-county granular data on wiretaps on landlines, cell phones, computers, pagers1, and other devices. Each interception is accompanied by information on the number of communications captured and the number of people those communications involved, as well as what percentage of the messages were incriminating. The report also discloses the criminal justice outcomes of the wiretaps (e.g. drugs seized, arrests made) and the costs to the public for running each surveillance operation.
Under California’s sunshine law, government agencies must provide public records to requesters in whatever electronic format they may exist. And yet, for the last three years, CADOJ officials resisted releasing the data in a machine-readable format. In fact, in 2015, CADOJ initially attempted to only release the “locked” version of a PDF of the report until EFF publicly called out the agency for ignoring these provisions of the California Public Records Act.
EFF sought the dataset because the formatting of the paper version of the report was extremely difficult to scrape or export in a way that would result in reliable and accurate data. Tables in the reports have sometimes spanned more than 70 pages.
This year, EFF has scored a major victory for open data: in response to our latest request, CADOJ has released not only an unlocked PDF, but a spreadsheet containing all the data.
What’s especially interesting about the data is that it includes data not previously disclosed in the formal report, including information on when wiretaps targeted multiple locations, devices, and websites, such as Facebook. At the same time, the data does not include some information included in the official report, such as the narrative summary of the outcome of each wiretap.
Inspiration
Some of the highlights contained in the data.
Wiretap application in Riverside County dropped from 620 wiretap applications in 2015 to 106 in 2016. This is likely due to reforms in the Riverside County District Attorney’s office following a series of investigative reports from USA Today that showed many wiretaps were likely illegal.
As in previous years, many of the wiretaps captured voluminous amounts of communications from large groups of people. The largest in terms of communications was a wiretap in a Los Angeles narcotics case in which 559,000 communications were captured from cell phones over 30 days. The largest in terms of number of people were caught up in a wiretap was a Riverside narcotics case in which 91,000 people each had a single piece of communication captured over 120 days.
The most expensive wiretap cost $1 million, mostly in personnel costs, to target a single person’s text message in a Los Angeles murder case. The most expensive wiretap in terms of non-personnel resources (i.e. equipment) cost $193,000. Two arrests were made in the associated narcotics case.
Explore the 2016 data (reproduced here as a CSV file) and the full report. Previous years’ reports are also made available here in a ZIP archive (PDFs). Let EFF know if you discover something interesting in the data by emailing dm@eff.org.
Acknowledgments
This description is reproduced with slight changes from the original blog post introducing the dataset published by Dave Maass on June 9, 2017. The dataset and contents from EFF are released under a CC-BY license and redistributed here in accordance with EFF's Copyright Policy.
Start an Analysis"
Stanford Open Policing Project - North Carolina,Data on Traffic and Pedestrian Stops by Police in North Carolina,Stanford Open Policing Project,5,"Version 1,2017-07-11","crime
law",Other,1 GB,Other,735 views,67 downloads,,,https://www.kaggle.com/stanford-open-policing/stanford-open-policing-project-north-carolina,"Context:
On a typical day in the United States, police officers make more than 50,000 traffic stops. The Stanford Open Policing Project team is gathering, analyzing, and releasing records from millions of traffic stops by law enforcement agencies across the country. Their goal is to help researchers, journalists, and policymakers investigate and improve interactions between police and the public.
If you'd like to see data regarding other states, please go to https://www.kaggle.com/stanford-open-policing.
Content:
This dataset includes 1.6 gb of stop data from North Carolina, covering all of 2010 onwards. Please see the data readme for the full details of the available fields.
Acknowledgements:
This dataset was kindly made available by the Stanford Open Policing Project. If you use it for a research publication, please cite their working paper: E. Pierson, C. Simoiu, J. Overgoor, S. Corbett-Davies, V. Ramachandran, C. Phillips, S. Goel. (2017) “A large-scale analysis of racial disparities in police stops across the United States”.
Inspiration:
How predictable are the stop rates? Are there times and places that reliably generate stops?
Concerns have been raised about jurisdictions using civil forfeiture as a funding mechanism rather than to properly fight drug trafficking. Can you identify any jurisdictions that may be exhibiting this behavior?"
RxNorm Drug Name Conventions,A normalized naming system for clinical drugs,National Library of Medicine,5,"Version 1,2017-07-06","linguistics
medicine",CSV,1016 MB,Other,"1,146 views",91 downloads,,0 topics,https://www.kaggle.com/nlm-nih/rxnorm-drug-name-conventions,"Context
RxNorm was created by the U.S. National Library of Medicine (NLM) to provide a normalized naming system for clinical drugs, defined as the combination of {ingredient + strength + dose form}. In addition to the naming system, the RxNorm dataset also provides structured information such as brand names, ingredients, drug classes, and so on, for each clinical drug. Typical uses of RxNorm include navigating between names and codes among different drug vocabularies and using information in RxNorm to assist with health information exchange/medication reconciliation, e-prescribing, drug analytics, formulary development, and other functions.
Content
The full technical documentation is available here.
Please note that the NLM updates RxNorm on a regular basis; you should assume that this version is out of date.
Acknowledgements
This dataset uses publicly available data from the U.S. National Library of Medicine (NLM), National Institutes of Health, Department of Health and Human Services. Please cite this dataset as: RxNorm META2016AB Full Update 2017_03_06 Bethesda, MD National Library of Medicine
Use this dataset with BigQuery
You can use Kernels to analyze, share, and discuss this data on Kaggle, but if you’re looking for real-time updates and bigger data, check out the data on BigQuery, too: https://cloud.google.com/bigquery/public-data/rxnorm."
International Air Traffic from and to India,"Country, City pair & Airline wise from 2015Q1 to 2017Q1.",Rajanand Ilangovan / இராஜ்ஆனந்த் இளங்கோவன்,5,"Version 1,2017-07-27","india
aviation",CSV,281 KB,CC4,"1,861 views",341 downloads,2 kernels,0 topics,https://www.kaggle.com/rajanand/international-air-traffic-from-and-to-india,"Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks.
Context
This dataset contains detail about the international air traffic from and to Indian territories from Jan-2015 to Mar-2017 in the below level. a) Country wise b) City pair wise c) Airline wise
Content
Time Period: 2015Q1 - 2017Q1 Granularity: Quarterly and Monthly
Acknowledgements
Directorate General of Civil Aviation, India has published this dataset at their website."
Predict Outcome of Pregnancy,Unit level survey data collected from 9 🇮🇳 states.,Rajanand Ilangovan / இராஜ்ஆனந்த் இளங்கோவன்,5,"Version 2,2017-09-07|Version 3,2017-09-07|Version 1,2017-09-07","india
public health
health",Other,3 GB,CC4,"1,475 views",154 downloads,,,https://www.kaggle.com/rajanand/ahs-woman-1,"Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks.
Context:
*Annual Health Survey : Woman Schedule *
This dataset contains data on Annual Health Survey : Woman Schedule. Woman Schedule comprised two sections. Section-I (this dataset) contains information relating to the outcome of pregnancy(s) (live birth/still birth/abortion); birth history; type of medical attention at delivery; details of maternal health care(ante-natal/natal/post-natal); immunization of children; breast feeding practices including supplements; occurrence of child diseases (Pneumonia, Diarrhoea and fever); registration of births, etc. use, sources and practices of family planning methods; details relating to future use of contraceptives and unmet need;awareness about RTI/STI, HIV/AIDS, administration of HAF/ORT/ORS during diarrhoea and danger signs of ARI/Pneumonia. It also includes more information relating the Ever Married Women (EMW) like conception details, usage of NPT kit, registration of pregnancy, health problems and subsequent treatments during ante-natal/natal/post-natal period, cost incurred by the woman during delivery etc.
There are total of 197 variables/columns in this dataset.
Survey:
Base line survey - 2010-11 (4.14 million households in the sample) 1st update - 2011-12 (4.28 million households in the sample) 2nd update - 2012-13 (4.32 million households in the sample)
Questionnaire: Refer page no 131 to 151 of this fact sheet for the survey questions.
The survey was conducted in the below 9 states.
A. Empowered Action Group (EAG) States
Uttarakhand (05)
Rajasthan (08)
Uttar Pradesh (09)
Bihar (10)
Jharkhand (20)
Odisha (21)
Chhattisgarh (22)
Madhya Pradesh (23)
B. Assam. (18)
These nine states, which account for about 48 percent of the total population, 59 percent of Births, 70 percent of Infant Deaths, 75 percent of Under 5 Deaths and 62 percent of Maternal Deaths in the country, are the high focus States in view of their relatively higher fertility and mortality.
Acknowledgements
Department of Health and Family Welfare, Govt. of India has published this dataset in Open Govt Data Platform India portal under Govt. Open Data License - India.
Thanks Marco for sharing questionnaire details.
Inspirations
Is it possible to predict the pregnancy outcome(live birth/still birth/abortion)?"
Hypernymy,Pairs of terms annotated to whether one is a hypernym of the other,Vered Shwartz,5,"Version 1,2017-08-13",,CSV,2 MB,GPL,497 views,58 downloads,,0 topics,https://www.kaggle.com/vered1986/hypernymy,"Context
Hypernymy is an important lexical-semantic relation for NLP tasks. For instance, knowing that Tom Cruise is an actor can help a question answering system answer the question ""which actors are involved in Scientology?"". While semantic taxonomies, like WordNet define hypernymy relations between word types, they are limited in scope and domain. Therefore, automated methods have been developed to determine, for a given term-pair (x, y), whether y is an hypernym of x, based on their occurrences in a large corpus. To facilitate training neural methods for hypernymy detection, which typically require a large amount of training data, we followed the common methodology of creating a dataset using distant supervision from knowledge resources, and extracted hypernymy relations from WordNet, DBPedia, Wikidata, and Yago.
Content
All instances in our dataset, both positive and negative, are pairs of terms that are directly related in at least one of the resources. These resources contain thousands of relations, some of which indicate hypernymy with varying degrees of certainty. To avoid including questionable relation types, we consider as denoting positive examples only indisputable hypernymy relations (Table 1), which we manually selected from the set of hypernymy indicating relations in Shwartz et al. (2015).
Term-pairs related by other relations (including hyponymy), are considered as negative instances, and there is a ratio of 1:4 positive to negative pairs in the dataset.
resourcerelations WordNetinstance hypernym, hypernym DBPediatype Wikidatasubclass of, instance of Yagosubclass of
Table 1: hypernymy relations in each resource.
We provide two dataset splits: random - 70/25/5 ratio for train/test/validation, and lexical - where each set has a distinct vocabulary, preventing models from overfitting to the most common class of x/y (this split maintains a similar ratio).
Files:
train_rnd.csv, test_rnd.csv, val_rnd.csv - the training, test, and validation set of the random split.
train_lex.csv, test_lex.csv, val_lex.csv - the training, test, and validation set of the lexical split.
Each file is a comma-separated file with the following fields:
x - the first term
y - the second term
label - TRUE if y is a hypernym of x, else FALSE
Acknowledgements
If you use this dataset for research purposes, please cite the following publication:
Improving Hypernymy Detection with an Integrated Path-based and Distributional Method.
Vered Shwartz, Yoav Goldberg and Ido Dagan. ACL 2016."
Proper-names Categories,Is-A relation between a name (e.g. Lady Gaga) and a common noun (e.g. singer),Vered Shwartz,5,"Version 1,2017-08-13",,CSV,78 KB,GPL,528 views,59 downloads,,0 topics,https://www.kaggle.com/vered1986/propernames-categories,"Context
Recognizing lexical inference is an important component in semantic tasks. Various lexical semantic relations, such as synonomy, class membership, part-of, and causality may be used to infer the meaning of one word from another, in order to address lexical variability.
As many of the existing lexical inference datasets are constructed from WordNet, important linguistic components that are missing from them are proper-names (Lady Gaga) and recent terminology (social networks). This dataset contains both components.
To construct the dataset, we sampled articles from different topics in online magazines. As candidate (x, y) pairs, we extracted pairs of noun phrases x and y that belonged to the same paragraph in the original text, selecting those in which x is a proper-name. These pairs were manually annotated. To balance the ratio of positive and negative pairs in the dataset, we sampled negative examples according to the frequency of y in positive pairs, creating ""harder"" negative examples, such as (Sherlock, lady) and (Kylie Minogue, vice president).
Content
This dataset contains pairs of (x, y) terms in which x is a proper-name and y is a common noun, annotated to whether x is a y. For instance, (Lady Gaga, singer) is true, but (Lady Gaga, film) is false.
Files:
full_dataset.csv: the full dataset
train.csv: the training set
test.csv: the test set
validation.csv: the validation set
Each file is a comma-separated file with the following format:
x: the x term (proper-name)
y: the y term (common noun)
label: TRUE if x is a y, else FALSE
Acknowledgements
If you use the dataset for any published research, please include the following citation:
""Learning to Exploit Structured Resources for Lexical Inference"".
Vered Shwartz, Omer Levy, Ido Dagan and Jacob Goldberger. CoNLL 2015."
INDIA and it^s numbers,Explore india and it^s people with their data,SAURAV SUMAN,5,"Version 3,2017-08-06|Version 2,2017-08-06|Version 1,2017-08-06","india
healthcare
politics",CSV,3 KB,CC0,750 views,69 downloads,,0 topics,https://www.kaggle.com/tastelesswine/india,"Context
We want to explore india with the help of data and numbers.
Content
In this Dataset We will know about india and it^s people, and what problem they are facing.(This dataset is small right now and it will get upadted as time flow. )
Acknowledgements
It wouldn't be Possible without the help of Niti Aayog and other govt sources. http://niti.gov.in
Inspiration
India is 2nd most populous country in the world, and facing a billion problem but the good thing is it have billion minds to solve it."
India Crime List (2014 and 2015),List of different crime and arrest done in 2014 and 2015,SAURAV SUMAN,5,"Version 1,2017-08-13","humans
crime
politics
+ 2 more...",CSV,4 KB,CC0,332 views,54 downloads,,0 topics,https://www.kaggle.com/tastelesswine/india-crime-list-2014-and-2015,"Context
India-crime-list-2014-and-2015
Content
Persons Arrested under IPC Crimes and Percentage Variation in 2015 Over 2014 (All India) Note: $: Custodial rapes in 2015 include all types of custodial rapes including rape in police custody whereas prior to 2014, only rapes in police custody were collected; *: Sections of Crime Head have been modified therefore no comparison can be made prior to 2014.
Acknowledgements
We wouldn't be here without the help of data.gov.in.
Inspiration
We want to study different types of crime and how things are related and number of arrest"
100K Coursera's Course Reviews Dataset,100K+ Scraped Course Reviews from the Coursera Website (As of May 2017),Jan Charles Maghirang Adona,5,"Version 1,2017-08-07",,{}JSON,39 MB,ODbL,"1,462 views",82 downloads,,0 topics,https://www.kaggle.com/septa97/100k-courseras-course-reviews-dataset,"Context
This dataset was used for my undergraduate research. This is my first dataset in Kaggle so do not be surprised if you find any errors and mistakes.
Content
I scraped the website of Coursera and pre-labelled the dataset depending on their rating. For a 5-star rating, the review was labelled as Very Positive, Positive for 4-star, Neutral for 3-star, Negative for 2-star, and Very Negative for 1-star. There are 2 files, reviews.json and reviews_by_course.json. The reviews.json file has no grouping, just an array of course reviews and its corresponding label. For the reviews_by_course.json, each JSON object corresponds to the course reviews and labels in a specific course.
reviews.json
data
The actual course review.
id
The unique identifier for a review.
label
The rating of the course review.
reviews_by_course.json
data
An array of reviews from a specific course.
id
The course tag. This is in the URL of the course. For example, in this URL, machine-learning would be the course tag.
label
An array of labels from a specific course. Each label corresponds to a review from the data array depending on their array index."
Catalonia GDP by demand components (2000-2016),Macroeconomic GDP indicators,XavierMartinezBartra,5,"Version 1,2017-08-09","business
social sciences
economics",CSV,2 KB,CC0,919 views,56 downloads,,0 topics,https://www.kaggle.com/xavier14/catalonia-gdp-by-demand-components-20002016,"Context
In this DataSet we have a compilation of demand components of the GDP - Gross Domestic Product -of CATALONIA -one of the 17 autonomous comunities of Spain-, and the spanish region with the highest GDP output.
Content
Columns :
GDP
Domestic demand
Consumer expenditure household
Consumer public adm Gross capital
Equip. Goods others
Const.
Ext. Balance Foreign balance
Total exports goods and services
Exports goods and services
Foreign consump. Territory Total imports goods and services
Imports goods and services
National residents consump. Abroad
Acknowledgements
All units of the DataFrame are presented in Millions of euros (Base 2010). The data has been extracted from the Idescat, economic annual Accounts of Catalonia."
KNYC Metars 2016,The hourly weather info - KNYC - 2016,CarlesBalsach,5,"Version 1,2017-07-22",weather,CSV,697 KB,CC0,"1,156 views",181 downloads,14 kernels,,https://www.kaggle.com/cabaki/knycmetars2016,"Context
METAR is a format for reporting weather information. A METAR weather report is predominantly used by pilots in fulfillment of a part of a pre-flight weather briefing, and by meteorologists, who use aggregated METAR information to assist in weather forecasting.
Content
This is the METARs aggregated information for 2016 in KNYC.
Acknowledgements
Thanks to wunderground for providing the data
Inspiration
This dataset is ment to be used as a extra information for those willing to extract conclusions from their own dataset where hourly the weather information could be useful for their predictions / analysis. You can contact me if you have any doubt or suggestion."
Spelling Variation on Urban Dictionary,A gr8t dataset of wrods used on Urban Dictionary,Rachael Tatman,5,"Version 1,2017-07-27","languages
popular culture
literature
linguistics",CSV,9 KB,CC4,"1,159 views",33 downloads,,0 topics,https://www.kaggle.com/rtatman/spelling-variation-on-urban-dictionary,"Context:
Urban Dictionary is an online dictionary of informal language that anyone can add to. As a result, a lot of the user-provided dictionary entries contain interesting variant spellings. While some are (presumably) typos, others are new linguistic innovations.
Content:
This dataset contains 716 variant spellings found in text scraped from Urban Dictionary, as well as the standard spellings of those words (in UK English).
Acknowledgements:
If you use this dataset in your work, please cite the following paper:
Saphra, N., & Lopez, A. (2016). Evaluating Informal-Domain Word Representations With UrbanDictionary. ACL 2016, 94. URL: http://www.anthology.aclweb.org/W/W16/W16-2517.pdf
Inspiration:
What’s the average edit distances between a variant spelling and the standard spelling? Does this differ by part of speech? (You can find part of speech using the Natural Language Toolkit pos_tag function, which is available in Python kernels.)
Can you automatically classify whether a variant spelling is a typo or an intentional innovation (like gr8t)?
Can you come up with an edit distance metric that takes into account how close letters are to each other on a standard keyboard? Does that help you identify typos?
Can you build an Standard English to Urban Dictionary “translator”?"
Unimorph,Morphological annotation for 352 languages,Rachael Tatman,5,"Version 1,2017-08-12",linguistics,Other,1 GB,CC4,769 views,53 downloads,,0 topics,https://www.kaggle.com/rtatman/unimorph,"Context:
The fact that some languages extensively use suffixes and prefixes to convey grammatical meaning(e.g. subject-verb agreement) poses a challenge to most current human language technology (HLT). Suffixes and prefixes in such languages can more generally be called morphemes, which are defined as the meaningful subparts of words. The rules that languages use to combine morphemes, together with the actual morphemes that they use (i.e. suffixes and prefixes themselves), are both referred to as a language's morphology. Languages which make extensive use of morphemes to build words are said to be morphologically-rich. These include languages such as Turkish and can be contrasted with so-called analytic languages such as Mandarin Chinese, which does not use suffixes or prefixes all.
The goal of the Universal Morphological Feature Schema is to allow an inflected word from any language to be dened by its lexical meaning (typically carried in the root or stem) and by a rendering of its inflectional morphemes in terms of features from the schema (i.e. a vector of universal morphological features). When an inflected word is defined in this way, it can then be translated into any other language since all other inflected words from all other languages can also be defined in terms of the Universal Morphological Feature Schema. Although building an interlingual representation for the semantic content of human language as a whole is typically seen as prohibitively difficult, the comparatively small extent of grammatical meanings that are conveyed by overt, affixal inflectional morphology places a natural bound on the range of meaning that must be expressed by an interlingua for inflectional morphology.
Content:
This dataset contains Unimorph morphological annotations for 352 languages. Each language’s annotations are in a separate file, and each file has a different number of words.
Many cells in each file are empty. This is because not every feature that is annotated applies to every part of speech. Nouns, for example, do not have a tense. In addition, not every language makes use of every possible morphological marking. For instance, English does not have an evidentiality inflection, while other languages, like Mongolian and Eastern Pomo, do.
Acknowledgments:
The Unimorph framework was developed by John Sylak-Glassman. If you use this framework in your work, please cite the following paper:
Sylak-Glassman, J. (2016). The composition and use of the universal morphological feature schema (unimorph schema). Technical report, Department of Computer Science, Johns Hopkins University.
You may also like:
Extinct Languages: Number of endangered languages in the world, and their likelihood of extinc
Stopword Lists for 19 Languages: Lists of high-frequency words usually removed during NLP analysis
Atlas of Pidgin and Creole Language Structures: Information on 76 Creole and Pidgin Languages"
Penn World Table,Compare Economic Growth Across Countries,Jacob Boysen,5,"Version 1,2017-09-06","finance
money
economics",Other,7 MB,CC4,"1,174 views",137 downloads,3 kernels,0 topics,https://www.kaggle.com/jboysen/penn-world-table,"Context:
The Penn World Table has long been a standard data source for those interested in comparing living standards across countries and explaining differences in cross-country growth. The article describing version 5.6 (Summers and Heston 1991), is among the most widely cited papers in economics with well over 1000 citations. This version (9.0) attempts to mitigate many concerns raised since. See this article for additional discussion.
Content:
Database with information on relative levels of income, output, input and productivity, covering 182 countries between 1950 and 2014. See legend, user guide and source for additional information.
Acknowledgements:
This file contains the data of PWT 9.0, as available on www.ggdc.net/pwt. Please refer to www.ggdc.net/pwt for extensive documentation of the different concepts and how these data were constructed.
When using these data, please refer to the following paper available for download at www.ggdc.net/pwt:
Feenstra, Robert C., Robert Inklaar and Marcel P. Timmer (2015), ""The Next Generation of the Penn World Table"""" American Economic Review, 105(10), 3150-3182."
Internal Navigation Dataset,Figure out a way to navigate accurately with your wireless network data,leigh,5,"Version 1,2017-08-11",navigation,CSV,3 KB,Other,871 views,51 downloads,,0 topics,https://www.kaggle.com/ljewell/internal-navigation-dataset,"Context
Global Positioning Systems (GPS) available in many consumer products such as mobile phones has mostly solved the problem of navigation but remains a challenge for indoor locations. A possible solution exists with 802.11 wireless networks and Location Based Services (LBS) that are able to compute location of a Wireless Station (WS) using triangulation of telemetry such as Receiver Signal Strength Indicators (RSSI) from nearby Wireless Access Points (WAP). The WS coordinates have inaccuracies due to it being “a function of distance, geometry, and materials” (Mengual, Marbán & Eibe, 2010) making distance travelled calculation inaccurate. In an experiment plotting a moving workstation the estimated distanced travelled was 483 metres compared to the actual distance of 149 metres (322% difference).
Content
The LBS system receives data from the wireless network, computes location information for each workstation and stores the data for later retrieval. The data can be sourced from the LBS using an REST API that returns JSON formatted data. To enable comparison to the estimated calculation, a controlled experiment with a wireless station moving to 20 known locations and turning on the wireless interface for 90 sec periods at a time was conducted.
The continual stream of coordinates from the LBS can change not only due to the WS physically moving but also due to the errors in the location calculation itself. These errors can be significant and render any distance calculation meaningless. The experiment captured the calculated position from the wireless network and the actual measured x and y coordinates of a workstation in 20 locations in an office building. The challenge is to figure out using the wireless location ways to improve the accuracy of the prediction.
Field Name Description time - Conversion of Singapore time to to seconds, from 00:00:00 x - x axis coordinates of floor map in feet y - y axis coordinates of floor map, origin is top left of floor map cf - 95% confidence in feet of radius away from x and y client likely to be. realx - x axis measured coordinates of the real location of the test subject realy - x axis measured coordinates of the real location of the test subject
Inspiration
The distance travelled by the workstation was 149 mtrs. How close can you get to this calculation used the predicted locations ?"
Congressional Election Disbursements,House & Senate campaign expenditures for 2010-2016,Federal Election Commission,5,"Version 2,2017-09-19|Version 1,2017-09-06",politics,CSV,1011 MB,CC0,"1,061 views",135 downloads,3 kernels,,https://www.kaggle.com/fec/congressional-election-expenditures,"Modern American congressional campaigns usually spend millions of dollars. This dataset provides a detailed breakdown of where that money goes. However, the descriptions are provided as unstructured text. Can you provide a useful clustering of the expenses?
This data comes from the US Federal Election Commission. You can find the original dataset here."
CDC 500 Cities,Dozens of Public Health Datapoints Reported by Residents of 500 US Cities,Centers for Disease Control and Prevention,5,"Version 1,2017-08-10",,CSV,572 KB,ODbL,"1,606 views",148 downloads,,0 topics,https://www.kaggle.com/cdc/500-cities,"Context:
Public health is a large and expensive problem for policymakers to understand in order to provide health services and prevent future epidemics. Self-reported data can be tricky due to many sampling issues, but it can paint an interesting picture of how healthy a given area’s population might be.
Content:
Data includes small area samples of residents from 500 US cities. Recorded is the percent of residents who answered a public health-related question affirmatively (see here). In addition to crude data, additional data is provided with age adjustment applied. 95% Confidence Intervals also provided for both datapoints.
Acknowledgements:
This data was collected by Centers for Disease Control and Prevention, National Center for Chronic Disease Prevention and Health Promotion, Division of Population Health. 500 Cities Project Data [online]. 2016 [accessed Aug 10, 2017]. URL: https://www.cdc.gov/500cities.
Inspiration:
Are there any regional health trends?
Any unusual hotspots of declining health? Higher levels of wellness?
Can you split the data by geography and predict neighboring cities health?
Who's healthier, larger or smaller cities?"
Global Population Estimates,The World Bank's global population estimates,World Bank,5,"Version 1,2017-08-15","countries
populated places
demographics",CSV,42 MB,Other,"1,201 views",164 downloads,,0 topics,https://www.kaggle.com/theworldbank/global-population-estimates,"This database presents population and other demographic estimates and projections from 1960 to 2050. They are disaggregated by age-group and gender and cover approximately 200 economies.
This dataset was kindly made available by the World Bank."
Author Disambiguation,Practice disambiguation with this batch of research paper metadata,AMiner,5,"Version 1,2017-08-11",,{}JSON,23 MB,Other,716 views,41 downloads,,0 topics,https://www.kaggle.com/aminer/author-disambiguation,"Name ambiguity has long been viewed as a challenging problem in many applications, such as scientific literature management, people search, and social network analysis. When we search a person name in these systems, many documents (e.g., papers, webpages) containing that person’s name may be returned. Which documents are about the person we care about? Although much research has been conducted, the problem remains largely unsolved, especially with the rapid growth of the people information available on the Web.
Content
This data set contains 110 author names and their disambiguation results (ground truth). For each author, there are 3 json entries. The most important files are xxx_xml, xxx(classify)_txt, and xxx_txt. The xxx(classify)_txt contains the ground truth and the other two files (xxx_xml and xxx_txt) provide features to perform the disambiguation. At the high-level, the xxx_xml file includes title, venue, coauthor, affiliation, and the xxx.txt further contains citation, co-affiliation-occur and homepage.
Let us use ""Ajay Gupta"" as the example to explain what information contained in each file.
Ajay Gupta.xml. The raw file. is formatted as a XML file. In the XML file, the author name is associated with a number of publications. An example of a publication is as follow: "" Explanation-based Failure Recovery 1987 Ajay Gupta AAAI 13048 0 null "" where denotes the title of the publication; denotes the publication year; denotes the publication venue; denotes the publication id; denotes the labeled person, e.g., all publications with ""0"" can be considered as published by the same person; denotes the affiliation of the author(s).
Ajay Gupta(classify).txt: the answer file is the ground truth. It is actually extracted from the raw-file by viewing publications with the same ""0"" as a person. The format is in plain text. The following is an example: "" Ajay Gupta -1:13048 388794 596099 1265282 1179332 675629 39153 258611 -2:988870 1490190 -3:1393934 -4:1398544 -5:1739014 -6:1671104 515636 1678096 -7:1126381 1205032 275987 277587 276300 1549674 1034401 -8:600181 846439 149270 175996 264268 264291 299548 1384744 300057 302056 545651 1212517 -9:1316053 "" where the first line denotes the author name and each of the following line indicates a disambiguated person. For example the first line indicates that an author published 8 papers. The corresponding IDs of those papers are respectively 13048, 388794, 596099, 1265282, 1179332, 675629, 39153, 258611.
Ajay Gupta.txt: the intermediate feature files. It contains 8 matrices, which respectively represents 8 features: co-affiliation, coauthor, citation, co-venue, google (ignored), co-affiliation-occur, titleSim, homepage. Each matrix records the correlation between any two papers published by Ajay Gupta. Each element, e.g., m^0_{ij}, the i-th row and the j-column in the 0-th matrix, denotes whether the two papers (i and j) contain the same affiliation. In this sense, the problem of name disambiguation can be basically considered as a pairwise clustering problem. The second matrix records the number of same coauthors, except Ajay Gupta. The third matrix records whether the a paper cites another paper. The fourth matrix records whether a paper is published at the same venue with another paper. The fifth matrix records whether the two papers (titles) can be found at a same web page (e.g., conference page). (This matrix is not complete). The sixth matrix records whether the affiliation of author ""a"" of a paper appears in the content of another paper, or vice versa. The seventh matrix records the cosine similarity between titles of any two papers. The eighth matrix records whether two papers appear on the same homepage. Please note that the 5th-8th matrixes cannot be extracted from the raw-data file (xxx.xml) and they are generated using other program.
This dataset is a lightly edited from the version provided by AMiner. The three core files for each author have been bundled into a single json for convenience.
Acknowledgements
This dataset was kindly made available by AMiner. use the data for publication, please kindly cite the following papers:
@article{Tang:12TKDE, author = {Jie Tang and Alvis C.M. Fong and Bo Wang and Jing Zhang}, title = {A Unified Probabilistic Framework for Name Disambiguation in Digital Library}, journal ={IEEE Transactions on Knowledge and Data Engineering}, volume = {24}, number = {6}, year = {2012}, }
@INPROCEEDINGS{ wang:adana:, AUTHOR = ""Xuezhi Wang and Jie Tang and Hong Cheng and Philip S. Yu"", TITLE = ""ADANA: Active Name Disambiguation"", BOOKTITLE = ""ICDM'11"", PAGES = {794-803}, YEAR = {2011}"
Library of Southern Literature,The full text of 115 influential works of Southern literature,Documenting the American South (DocSouth),5,"Version 1,2017-08-15","literature
united states
history
linguistics",CSV,46 MB,Other,669 views,69 downloads,,0 topics,https://www.kaggle.com/docsouth-data/library-of-southern-literature,"The goal of the ""Library of Southern Literature"" is to make one hundred of the most important works of Southern literature published before 1920 available world-wide for teaching and research. Currently, this collection includes over eighty titles that were digitized with special funding from the Chancellor and the University Library of the University of North Carolina at Chapel Hill. The Southern United States has been a distinct region since the colonial period, and its literature has developed in connection with, but also divergently from American literature as a whole. The South claims prominent and world-renowned authors, including Edgar Allan Poe and Mark Twain, but also lesser known authors who created works that reflected Southern attitudes and experiences. Teachers of literature as well as historians and other scholars of Southern culture need access to literary texts that illustrate these differences, and many of these important Southern works are no longer in print and are not widely held in libraries. Noting this lack of available titles, the late Dr. Robert Bain volunteered to help Documenting the American South by developing the bibliography for it. Dr. Bain was a faculty member at the University of North Carolina at Chapel Hill and taught American and Southern literature from 1964 until 1995. He also co-edited five scholarly works on Southern writers. To prepare this bibliography of Southern Literature, Dr. Bain wrote to some fifty scholars throughout the United States who specialize in Southern and American literature requesting that they nominate what they considered to be the ten most important works of Southern literature published before 1920. From their responses, Dr. Bain compiled the bibliography on which this collection is based. He completed the list three months before his death in July 1996. With additional funding from the University Library, this collection continues to grow. Dr. Joe Flora, professor of English at UNC-Chapel Hill, guides the expansion of this collection beyond Dr. Bain's original bibliography. The original texts for the ""Library of Southern Literature"" come from the University Library of the University of North Carolina at Chapel Hill, which includes the Southern Historical Collection, one of the largest collections of Southern manuscripts in the country and the North Carolina Collection, the most complete printed documentation of a single state anywhere. The DocSouth Editorial Board, composed of faculty and librarians at UNC and staff from the UNC Press, oversees this collection and all other collections on Documenting the American South.
Context
The North American Slave Narratives collection at the University of North Carolina contains 344 items and is the most extensive collection of such documents in the world.
The physical collection was digitized and transcribed by students and library employees. This means that the text is far more reliable than uncorrected OCR output which is common in digitized archives.
More information about the collection and access to individual page images can be be found here: http://docsouth.unc.edu/neh
The plain text files have been optimized for use in Voyant and can also be used in text mining projects such as topic modeling, sentiment analysis and natural language processing. Please note that the full text contains paratextual elements such as title pages and appendices which will be included in any word counts you perform. You may wish to delete these in order to focus your analysis on just the narratives.
The .csv file acts as a table of contents for the collection and includes Title, Author, Publication Date a url pointing to the digitized version of the text and a unique url pointing to a version of the text in plain text (this is particularly useful for use with Voyant: http://voyant-tools.org/).
Copyright Statement and Acknowledgements
With the exception of ""Fields's Observation: The Slave Narrative of a Nineteenth-Century Virginian,"" which has no known rights, the texts, encoding, and metadata available in Open DocSouth are made available for use under the terms of a Creative Commons Attribution License (CC BY 4.0:http://creativecommons.org/licenses/by/4.0/). Users are free to copy, share, adapt, and re-publish any of the content in Open DocSouth as long as they credit the University Library at the University of North Carolina at Chapel Hill for making this material available.
If you make use of this data, considering letting the holder of the original collection know how you are using the data and if you have any suggestions for making it even more useful. Send any feedback to wilsonlibrary@unc.edu.
About the DocSouth Data Project
Doc South Data provides access to some of the Documenting The American South collections in formats that work well with common text mining and data analysis tools.
Documenting the American South is one of the longest running digital publishing initiatives at the University of North Carolina. It was designed to give researchers digital access to some of the library’s unique collections in the form of high quality page scans as well as structured, corrected and machine readable text.
Doc South Data is an extension of this original goal and has been designed for researchers who want to use emerging technology to look for patterns across entire texts or compare patterns found in multiple texts. We have made it easy to use tools such as Voyant (http://voyant-tools.org/) to conduct simple word counts and frequency visualizations (such as word clouds) or to use other tools to perform more complex processes such as topic modeling, named-entity recognition or sentiment analysis."
NYC Baby Names,Baby names popular in New York City.,City of New York,5,"Version 1,2017-09-09",children,CSV,870 KB,CC0,"1,456 views",265 downloads,,0 topics,https://www.kaggle.com/new-york-city/nyc-baby-names,"Context
Baby names for children recently born in New York City. This dataset is notable because it includes a breakdown by the ethnicity of the mother of the baby: a source of ethnic information that is missing from many other similar datasets published on state and national levels.
Content
This dataset includes columns for the name, year of birth, sex, and mother's ethnicity of the baby. It also includes a rank column (that name's popularity relative to the rest of the names on the list).
Acknowledgements
This data is published as-is by the City of New York.
Inspiration
How do baby names in New York City differ from national trends?
What names are most, more, or less popular amongst different ethnicities?"
FDA Enforcement Actions,"Food, drug, and medical device enforcements",Food and Drug Administration,5,"Version 1,2017-09-12","health law
public health",{}JSON,1 GB,CC0,"1,258 views",103 downloads,,0 topics,https://www.kaggle.com/fda/fda-enforcement-actions,"The objective of FDA regulatory programs is to assure compliance with the Federal Food, Drug, and Cosmetic Act (the Act). Specific enforcement activities include actions to correct and prevent violations, remove violative products or goods from the market, and punish offenders. The type of enforcement activity FDA uses will depend on the nature of the violation. The range of enforcement activities include issuing a letter notifying the individual or firm of a violation and requesting correction, to criminal prosecution of the individual or firm. Adulteration or misbranding is usually the result of an individual failing to take steps to assure compliance with the law. Such an individual may be liable for a violation of the Act and, if found guilty, be subject to the penalties specified by the law.
Acknowledgements
This dataset was kindly made available by the United States Food and Drug Administration. You can find the most current version of the dataset here.
Inspiration
All but two out of every thousand drug enforcement actions were voluntary recalls. Does this hold for food and medical devices as well? Was there anything special about the non-voluntary enforcement actions that leads the industry to largely self-police?"
Federal Air Marshal Misconduct,Misconduct and punishment of US Flight Marshals,Dan Ofer,5,"Version 1,2017-09-19","united states
crime
transport",CSV,365 KB,Other,601 views,25 downloads,,0 topics,https://www.kaggle.com/danofer/air-marshal-misconduct,"Context
Federal air marshals fly undercover on passenger planes and are trained to intervene in the event of a hijacking. This database contains information on 5,214 cases of misconduct committed by federal air marshals by date and field office and what discipline was meted out in response.
Content
Data covers November 2002 to February 2012. I cleaned the data to remove some extraneous columns and to add an easier ""target"" column (disciplinary results, with duplicates merged and the number of days in a suspension removed). The original ""Final Disposition"" columns remains unchanged. Columns: Field Office , Allegation, Date Case Opened , Final Disposition.
Acknowledgements
Data gathered and distributed from the Transportation Security Administration by ProPublica: https://www.propublica.org/datastore/dataset/federal-air-marshal-misconduct-database
ProPublica is an independent, non-profit newsroom that produces investigative journalism in the public interest. Falls under ProPublica Data Terms of use.
Inspiration
What types of misconduct can result in suspension, or a ""slap on the wrist""?
What types of misconduct are ignored and occurred?"
Orders data,UNDERSTANDING GROCERY SHOPPING BEHAVIOR,karthickveerakumar,5,"Version 1,2017-08-20",,CSV,1 MB,Other,611 views,112 downloads,,2 topics,https://www.kaggle.com/karthickveerakumar/orders-data,"Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
Indian Liver Patient Dataset,Data about Liver Patients in India,JeevanNagaraj,5,"Version 1,2017-09-26","healthcare
health",CSV,23 KB,ODbL,"2,547 views",290 downloads,32 kernels,0 topics,https://www.kaggle.com/jeevannagaraj/indian-liver-patient-dataset,"Data Set Information
This data set contains 416 liver patient records and 167 non liver patient records.The data set was collected from test samples in North East of Andhra Pradesh, India. 'is_patient' is a class label used to divide into groups(liver patient or not). This data set contains 441 male patient records and 142 female patient records. Any patient whose age exceeded 89 is listed as being of age ""90"".
Attribute Information
age Age of the patient
gender Gender of the patient
*tot_bilirubin* Total Bilirubin
*direct_bilirubin* Direct Bilirubin
alkphos Alkaline Phosphotase
sgpt Alamine Aminotransferase
sgot Aspartate Aminotransferase
*tot_proteins* Total Protiens
albumin Albumin
*ag_ratio* Albumin and Globulin Ratio
is_patient Selector field used to split the data into two sets (labeled by the experts)
Acknowledgements
The data set has been elicit from UCI Machine Learning Repository. My sincere thanks to them."
Reddit r/Place History,All 16 million moves in the r/place collaborative art project,Aleksey Bilogur,5,"Version 1,2017-09-20","popular culture
internet",CSV,494 MB,Other,997 views,47 downloads,,0 topics,https://www.kaggle.com/residentmario/reddit-rplace-history,"Context
r/Place was a wildly successful April Fool's joke perpetrated by Reddit over the course of 72 hours April 1-3, 2017. The rules of Place, quoting u/Drunken_Economist were:
There is an empty canvas.
You may place a tile upon it, but you must wait to place another.
Individually you can create something.
Together you can create something more.
1.2 million redditors used these premises to build the largest collaborative art project in history, painting (and often re-painting) a million-pixel canvas with 16.5 million tiles in 16 colors.
The canvas started out completely blank, and ended looking like this:
How did that happen?
Content
This dataset is a full time placement history for r/place over time. Each record is a single move: one user changing one pixel to one of 15 different colors.
Acknowledgements
This data was published as-is by Reddit.
Inspiration
Users were heavily rate-limited in their ability to place pixels, so this dataset shows what happens when users of similar stripes ""band together"" to build something greater than themselves. With a pixel-by-pixel history, what can you tell about the relative popularity of different regions in the figure? Can you use image analysis techniques to segment the image into different regions, and measure what happens to them over time?"
Publicly Supported Symbols of the Confederacy,"Dataset containing over 1,500 publicly supported symbols of the confederacy.",DaveRosenman,5,"Version 2,2017-08-26|Version 1,2017-08-19",,CSV,171 KB,Other,535 views,31 downloads,,0 topics,https://www.kaggle.com/daverosenman/publicly-supported-symbols-of-the-confederacy,"Context
The Southern Poverty Law Center maintains a list of publicly supported symbols of the confederacy. It is available here. There data table can be downloaded in multiple formats here
Content
I cleaned up the data set a bit.
Removed the following columns: cartodb_id, the_geom,field_1,uid,secondary_class_for_internal_use
Changed 'Unknown' dates to NA
Arranged records by states and by feature_name
Removed Holidays
Changed year_dedicated for the following feature names: - 'City of Confederate Corners' from '1860's (late)' to '1865' (source:http://www.amap1.org/images/2008%20Folder/AMAP%20Newsletter%2012-08.pdf ...says '1865-ish', so just an estimate)
- 'Confederate Monument, La Plaza Del Constitucion' from '~1880' to 1879. Source: http://www.drbronsontours.com/bronsonconfederatememorial.html

- Changed 'Confederate Women Fountain (Women of the Sixties)' from 1911-1920 to 1916. Source: http://www.arkansaspreservation.com/National-Register-Listings/PDF/PU4770S.nr.pdf

- Changed 'Confederate Monument', 'Carline County Courthouse' from 'Unknown (perhaps 1906: see last link in sources)' to 1906. 
source:  'A History of Caroline County, Virginia', By Marshall Wingfield, page 243 (https://books.google.com/books?id=xxVhymOH3usC&pg=PA276&lpg=PA276&dq=%22caroline+county%22+confederate+memorial+1906&source=bl&ots=qyRGTV13Js&sig=ii-kA__3BhZg9WzTaD5VCKHb3b8&hl=en&sa=X&ved=0ahUKEwjq-eqj-uHVAhUKJCYKHU-aAs4Q6AEIUzAM#v=snippet&q=monument&f=false)

- Changed ""To Our Soldiers of the Confederacy"", ""King William Courthouse"" from '1901-1903' to 1903. Source: http://docsouth.unc.edu/commland/monument/15/ 
Added rededicated column. Removed rededicated values from year_dedicated column. Rededicated column also includes: -remodeled, replace,reopened, and relocated monuments.
-readopted flags
Acknowledgements
Would like to thank the SPLC for making the dataset that this dataset is based on available and for their interesting and important report on the history of confederate public symbols."
UFC PPV Sales,Contains PPV Sales for UFC PPV's Dating Back to UFC 32,DaveRosenman,5,"Version 3,2018-01-15|Version 2,2018-01-15|Version 1,2017-08-30",,CSV,6 KB,Other,939 views,105 downloads,,0 topics,https://www.kaggle.com/daverosenman/ufc-ppv-sales,"Context
Dataset contains list of every UFC PPV event + the estimated number of PPV sales (source: https://www.tapology.com/search/mma-event-figures/ppv-pay-per-view-buys-buyrate.
Content
UFC's 171 and 156 are missing because those events were cancelled.
Acknowledgements
Thanks to tapology.com for providing the raw data.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
Top Ranked English Movies Of This Decade.,Understanding the success of movies with high ratings.,Sai Pranav,5,"Version 2,2017-11-30|Version 1,2017-08-27","film
entertainment",CSV,86 KB,CC4,"1,825 views",289 downloads,,0 topics,https://www.kaggle.com/saipranava/top-ranked-enlglish-movies-of-this-decade,"About this project
This is my first pet project ever. In this project I'm going to do exploratory analysis on the top rated IMDb movies of all time. I wanted to analyse the the box office success of a highly rated movie and its relationship with various variables like, the age, gender of user/voter. For this purpose I looked at the top 100 Movies of all time on IMDb. I used Octoparse for scraping.
I realized that it would be too complicated to analyse movies over various generations, because the tastes of every generation is different, so I narrowed down onto movies of this decade, i.e. movies released between 2010-2016 ) both years included, I know 2010 doesn't really fall in to this decade, but included it in order to have a decent sized data set. Then I narrowed down to movies targeting a particular culture, because Indian movies and American movies are quite different, and trying to predict what people like, with both kinds of movies mixed up didn't seem like a good idea.
I excluded movies released in 2017 as it takes time for ratings to stabilize and box office reports to be finalized.
So from this list, I further shortened the list to movies made in English, and finally ended up with a dataset of 118 movies and 55 variables. One of the most important variable, the box office collections of each movie could not be scraped due to technical difficulties, this is an area I really need help with, so if someone can contribute here, it would be great!
Objectives
What are the goals or questions you're investigating with this project?
Visualize and Analyse the ratings given by different age groups for a genre.
Visualize and Analyse the ratings given by male and female groups for a genre.
Visualize and Analyse the number of votes given by different age groups for a genre.
Visualize and Analyse the number of votes given by male and female groups for a genre.
Visualize and Analyse the Box office success of a movie in U.S. with various variables.
Visualize and Analyse the Box office success of a movie outside the U.S. with various variables.
Visualize and Analyse the overall Box office success of a movie with various variables.
Revisit the project after a couple of years and see if any of the models we built have made any accurate predictions.
Get involved
How can others contribute to this project? Are there tasks that need to be done, or skills you're looking for?
Scrape data to get the U.S. box office data and non-U.S. box office data of each movie. COMPLETED (Check IMDB2.csv)
Integrate that data into our dataset. COMPLETED (Check IMDB2.csv)
Clean the data.
Scrape data to understand how much each of these movies was pirated.
Work on the objectives.
External resources
https://github.com/saipranava/IMDB
Photo by Jakob Owens on Unsplash"
Real Location Retrieval from Text,Location Metonymy Resolution Dataset,Rachael Tatman,5,"Version 1,2017-08-19","geography
linguistics",Other,349 KB,GPL,"1,136 views",84 downloads,,0 topics,https://www.kaggle.com/rtatman/real-location-retrieval-from-text,"Overview
This dataset is a Wikipedia-based MR dataset called RelocaR, which is tailored towards locations as well as improving previous defi- ciencies in annotation guidelines. The corpus is designed to evaluate the capability of a classifier to distinguish literal, metonymic and mixed location mentions. In terms of dataset size, ReLocaR contains 1,026 training and 1,000 test instances. The data was sampled using Wikipedia’s Random Article API. We kept the sentences, which contained at least one of the places from a manually compiled list of countries and capitals of the world. The natural distribution of literal versus metonymic examples is approximately 80/20 so we had to discard the excess literal examples during sampling to balance the classes.
Annotation Guidelines
ReLocaR has three classes, literal, metonymic and mixed. Literal reading comprises territorial interpretations (the geographical territory, the land, soil and physical location) i.e. inanimate places that serve to point to a set of coordinates (where something might be located and/or happening) such as “The treaty was signed in Italy.”, “Peter comes from Russia.”, “Britain’s Andy Murray won the Grand Slam today.”, “US companies increased exports by 50%.”, “China’s artists are among the best in the world.” or “The reach of the transmission is as far as Brazil.”.
A metonymic reading is any location occurrence that expresses animacy (Coulson and Oakley, 2003) such as “Jamaica’s indifference will not improve the negotiations.”, “Sweden’s budget deficit may rise next year.”. The following are other metonymic scenarios: a location name, which stands for any persons or organisations associated with it such as “We will give aid to Afghanistan.”, a location as a product such as “I really enjoyed that delicious Bordeaux.”, a location posing as a sports team “India beat Pakistan in the playoffs.”, a governmental or other legal entity posing as a location “Zambia passed a new justice law today.”, events acting as locations “Vietnam was a bad experience for me”.
The mixed reading is assigned in two cases: either both readings are invoked at the same time such as in “The Central European country of Slovakia recently joined the EU.” or there is not enough context to ascertain the reading i.e. both are plausible such as in “We marvelled at the art of ancient Mexico.”. In difficult cases such as these, the mixed class is assigned.
Acknowledgements
This dataset was collected by Milan Gritta, Mohammad Taher Pilehvar, Nut Limsopatham and Nigel Collier. It is redistributed here under a GNU general public license. If you use this data in your work, please cite the following paper:
Gritta, M., Pilehvar, M. T., Limsopatham, N., & Collier, N. 2017. “Vancouver Welcomes You! Minimalist Location Metonymy Resolution”. Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) URL: http://aclweb.org/anthology/P/P17/P17-1115.pdf
In addition, code to replicate the paper’s results can be found here."
Six Degrees of Francis Bacon,An early modern social network,Rachael Tatman,5,"Version 2,2017-08-30|Version 1,2017-08-30","europe
north america
history
+ 2 more...",CSV,12 MB,CC4,"1,458 views",357 downloads,4 kernels,0 topics,https://www.kaggle.com/rtatman/six-degrees-of-francis-bacon,"Overview
Six Degrees of Francis Bacon is a digital reconstruction of the early modern social network (EMSN). Historians and literary critics have long studied the way that early modern people associated with each other and participated in various kinds of formal and informal groups. By data-mining existing scholarship that describes relationships between early modern persons, documents, and institutions, we have created a unified, systematized representation of the way people in early modern England were connected.
Contents
This dataset contains information on 171419 relationships between 15824 early modern figures (including, of course, the titular Francis Bacon). The individuals have been separated into 109 distinct labelled groups and the relationships fall under one of 64 labelled categories.
This dataset contains the following files:
SDFB_groups.csv: a list of the groups of individuals in the dataset
SDFB_people.csv: a list of all individuals in the dataset
SDFB_relationships_100000000_100020000.csv: This and the following “relationships” files contain information on relationships between specific individuals
SDFB_relationships_100020001_100040000.csv
SDFB_relationships_100040001_100060000.csv
SDFB_relationships_100060001_100080000.csv
SDFB_relationships_100080001_100100000.csv
SDFB_relationships_100100001_100120000.csv
SDFB_relationships_100120001_100140000.csv
SDFB_relationships_100140001_100160000.csv
SDFB_relationships_100160001_100180000.csv
SDFB_relationships_greater_than_100180000.csv
SDFB_RelationshipTypes.csv: the types of relationships found in the database
table-of-contents.csv: a table of contents for the files
Acknowledgements
Please cite Six Degrees of Francis Bacon as follows:
SDFB Team, Six Degrees of Francis Bacon: Reassembling the Early Modern Social Network. www.sixdegreesoffrancisbacon.com (August 29, 2017).
Inspiration
This dataset is an excellent place to explore network analysis and visualization. Each individual is a node, and each relationship an edge.
Can you visualize this social network?
Who is the most central figure in this social network?
Do different groups have different degrees of connectivity? Plexity?"
United States Trademark Applications,Information on individual trademark applications,Rachael Tatman,5,"Version 1,2017-09-21","government agencies
united states
product",CSV,210 MB,CC0,841 views,49 downloads,,,https://www.kaggle.com/rtatman/trademark-application,"Context:
This dataset contains pending and registered trademark text data (no drawings/images) to include word mark, serial number, registration number, filing date, registration date, goods and services, classification number(s), status code(s), design search code(s), pseudo mark(s) from the April 7, 1884 - Present. The file format .json, converted from eXtensible Markup Language (XML) in accordance with the U.S. Trademark Applications Version 2.0 Document Type Definition (DTD).
Content:
This dataset is made up of one .json file with the following fields. (Note that the documentation was written for an XML file, which was subsequently converted into .json). The following is a description of some of the fields; full documentation can be found in the applications-documentation.pdf file.
The trademark-applications-daily element is mandatory and will occur one time identifying the beginning of a daily application process. The trademark applications daily element will contain one occurrence of the version and data processed indicator elements, and zero or more occurrences of the file segments element between the trademark-applications-daily start and trademark-applications-daily end tags.
The version element is mandatory and will occur one time between the version start and version end tags identifying the version number and version date of the Trademark Applications DTD. The version element will contain one occurrence of the version number and version date elements.
The version-no element is mandatory and will occur one time between the version-no start and version-no end tags containing the version number of the Trademark Applications DTD. Example: the first production version will be 1.0
The version-date element is mandatory and will occur one time between the version-date start and version-date end tags containing the date of the Trademark Applications DTD, an 8-position numeric in the format YYYYMMDD.
Application Information Section
The application-information element is optional and will occur zero or one times between the application-information start and application-information end tags containing the daily trademark applications data.
The data-available-code element is optional and will occur zero or one times when trademark data is NOT present. The data-available-code start tag and the data-available-code end tag will be present and contain a one-position “N” indicating that trademark data is Not present. This element will not be present when data is available.
The file-segments element is optional and will occur zero or more times between the file-segments start and file- segments end tags identifying the beginning of the Trademark Applications file. The file-segments element will contain zero or more occurrences of the file segment and action keys elements.
The file-segment element is optional and will occur zero or more times between the file-segment start and file-segment end tags containing the File Segment text data, a four-position alphabetic field identifying the type of data in the Trademark Daily XML Process. The Trademark Applications file-segment will always have a constant of “TRMK”. The”TRMK” file segment contains new trademark data and modifications made to existing Trademark data.
The action-keys element is optional and will occur zero or more times between the action-keys start and action-keys end tags. The action keys element will contain zero or more occurrences of the action key and case file elements.
The action-key element is optional and will occur zero or more times between the action-key start and action-key end tags containing the KEY ACTION, a two-position alphanumeric field. The contents of the action key will be one of the following values for marks appearing in the Trademark Official Gazette (OG).
Case file section
The Case file section and case-file end tags containing the following elements in the sequence as follows:
The serial-number element is mandatory and will occur one time between the serial-number start and serial-number end tags containing the SERIAL NUMBER, an eight-position numeric field consisting of the following:
Serial number
Position 1-2 will contain a series code:
SERIES CODE, FILING DATE
70, 1881 – 03/31/05
71, 04/01/05 – 12/31/55
72, 01/01/56 – 08/31/73
73, 09/01/73 – 11/15/89
74, 11/16/89 – 09/30/95
75, 10/01/95 – 03/19/2000
76, 03/20/2000 – Present - (76 - Will be used for all paper filed applications.)
78, 03/20/2000 – Present - (78 - Will be used for electronically filed (e-TEAS) applications.)
Position 3-8 will be a six-position serial number right justified with leading zeros.
NOTE: When the serial-number begins with 80, 81 or 82, the actual serial number is unknown and the serial number is created by placing an “8” before the seven-digit registration number. If the registration number is less than seven digits in length, it is preceded by zeros.
When the serial-number begins with 89, non-registration data consists of information entered in the database because of treaty obligations, U.S. Statutes, or other requirements.
The registration-number element is optional and will occur zero or one times between the registration-number start and registration-number end tags containing the REGISTRATION NUMBER, a seven-position numeric field right justified with leading zeros. NOTE: If a mark does not contain a registration number, the registration number element will contain zeros.
The transaction-date element is optional and will occur zero or one times between the transaction-date start and transaction-date end tags containing the TRANSACTION DATE, an eight position date in the format YYYYMMDD. The transaction date is the date of the Trademark Daily XML Process for Action Key entries.
Case File Header Section
The case-file-header element is optional and occurs zero or one times between the case-file-header start and case-file-header end tags identifying the first record sequence of a trademark application document, which contains the equivalent of a TWTF/GENX record. Each case file header element will contain the following optional elements. NOTE: Any of the following elements that do not have the required date will contain zeros.
The filing-date element is optional and occurs zero or one times between the filing-date start and filing-date end tags containing the FILING DATE, an eight position date in the format YYYYMMDD, which is the date on which a statutorily complete trademark application is filed at the USPTO.
The registration date element is optional and occurs zero or one times between the registration-date start and registration-date end tags containing the REGISTRATION DATE, an eight-position date in the format YYYYMMDD that identifies the date the mark was registered.
The status-code element is optional and occurs zero or one times between the status-code start and status-code end tags containing the STATUS CODE, a three position numeric field which identifies the status of the mark.
The status codes can be found in the trademark-status-codes file.
The status-date element is optional and occurs zero or one times between the status-date start and status-date end tags containing the STATUS DATE, an eight-position date in the format YYYYMMDD, which is the date on which the current status was reported to the system.
The mark-identification element is optional and occurs zero or one times between the mark-identification start and mark-identification end tags containing the MARK-1-LIN, a variable length alphanumeric field containing the characters of the actual mark.
The mark-drawing-code element is optional and occurs zero or one times between the mark-drawing-code start and mark-drawing-code end tags containing the MARK DRAWING CODE, a four-position alphanumeric field. The first position identifies the physical characteristics of the mark.
The published-for-opposition-date element is optional and occurs zero or one times between the published-for-opposition-date start and published-for-opposition-date end tags containing the DATE PUBLISHED FOR OPPOSITION, an eight-position date in the format YYYYMMDD, which is the date that the mark published for opposition in the Official Gazette.
The amend-to-register-date element is optional and occurs zero or one times between the amend-to-register-date start and amend-to-register end tags containing the AMENDED TO REGISTER DATE, an eight-position date in the format of YYYYMMDD. This element contains the date on which a case is entered as an amendment to a register.
The abandonment-date element is optional and occurs zero or one times between the abandonment-date start and abandonment-date end tags containing the DATE ABANDONED, an eight-position date in the format YYYYMMDD, which is the date that the mark is abandoned.
The cancellation-code element is a optional and occurs zero or one times between the cancellation-code start and cancellation-code end tags containing the CANCELLATION CODE, a one-position alphanumeric which identifies the section of the statute under which an entire registration is being cancelled or under which some classes in a multiple class registration are being cancelled.
CC, Definition
0, No entry
1, Section 7(d) Entire Registration
2, Section 8 Entire Registration
3, Section 18 Entire Registration
4, Section 24 Entire Registration
5, Section 37 Entire Registration
6, Entire Registration inadvertently Issued
7, Inadvertently issued-entire Registration restored to pendency
A, Section 7 (d ) - Class(es) in multiple class Registration
B, Section 8 - Class(es) in multiple class Registration
C, Section 18 - Class(es) in multiple class Registration
D, Section 24 - Class(es) in multiple class Registration
E, Section 37 - Class(es) in multiple class Registration
The cancellation-date element is optional and occurs zero or one times between the cancellation-date start and cancellation-date end tags containing the CANCELLATION DATE, an eight-position date in the format YYYYMMDD, which is the date that the cancellation of the entire registration was recorded.
The republished 12c-date element is optional and occurs zero or one times between the republished-12c-date start and republished-12c-date end tags containing the DATE PUBLISHED UNDER SECTION 12(C), an eight-position date in the format YYYYMMDD, which is the date of publication under Section 12(c).
The domestic-representative-name element is optional and occurs zero or one times between the domestic-representative-name start and domestic-representative-name end tags and contains the DOMESTIC REPRESENTATIVE information for the application.
The attorney-docket-number element is optional and occurs zero or one times between the attorney-docket-number start and attorney-docket-number end tags containing the ATTORNEY DOCKET NUMBER, a twelve-position number containing the reference or identification number of a case as assigned and used in the office of the attorney filing the application.
The attorney-name element is optional and occurs zero or one times between the attorney-name start and attorney-name end tags and contains the ATTORNEY information from the OWNX record.
The principal-register-amended-indicator element is optional and occurs zero or one times between the principal-register-amended-in start and principal-register-amended-in end tags containing the FLAG AMENDED TO THE PRINCIPAL REGISTER, a one-position alphabetic indicating the register has been amended for an application on the Supplemental Register. A “T” in this field indicates an amendment to the Principal Register. An “F” in this field indicates no amendment to the Principal Register.
The supplemental-register-amended-indicator element is optional and occurs zero or one times between the supplemental-register-amended-in start and supplemental-register-amended-in end tags containing the FLAG AMENDED TO THE SUPPLEMENTAL REGISTER, a one-position field indicating the register has been amended for an application on the Principal Register. A “T” in this field indicates an amendment to the Supplemental Register. An “F” in this field indicates no amendment to the supplemental Register.
Prior Registration Applications Section
The prior-registration-applications element is optional and occurs zero or one times between the prior-registration-applications start and prior-registration-applications end tags, which contains the equivalent of all TWTF/PRUS records. Each prior registration applications element will contain the optional other related in and prior registration application elements.
The other-related-indicator element is optional and occurs zero or more times between the other-related-in start and other-related-in end tags containing the AND OTHERS, a one-position alphabetic field. A “T” in this field would indicate that the words “and others” appears in conjunction with a list of prior registrations that are claimed as being related to this mark. An “F” would indicate that the statement is not present.
Acknowledgements:
This dataset is provided by the United States Government and is in the public domain. Daily uploads of this dataset are available online here."
Austin 311 Calls,"463k Public Complaints, 2013-17",Jacob Boysen,5,"Version 1,2017-08-19",government agencies,CSV,163 MB,CC0,957 views,79 downloads,3 kernels,0 topics,https://www.kaggle.com/jboysen/austin-calls,"Context:
311 calls are a good snapshot of public complaints, and provide interesting analytical data to predict future resource allocation by policymakers.
Content:
Date, time, location, description, handling office, and status are included.
Acknowledgements:
This dataset was compiled by the City of Austin and published on Google Cloud Public Data.
Inspiration:
Any notable trends in location or volume of certain calls?
Can you predict future 311 calls?
Dataset Description
Use this dataset with BigQuery You can use Kernels to analyze, share, and discuss this data on Kaggle, but if you’re looking for real-time updates and bigger data, check out the data on BigQuery, too."
SCOTUS Opinions Corpus,"Lots of Big, Important Words",Jacob Boysen,5,"Version 1,2017-08-26",law,Other,558 MB,CC0,"1,122 views",66 downloads,,,https://www.kaggle.com/jboysen/scotus-corpus,"Context:
Free Law Project seeks to provide free access to primary legal materials, develop legal research tools, and support academic research on legal corpora. We work diligently with volunteers to expand our efforts at building an open source, open access, legal research ecosystem. Currently Free Law Project sponsors the development of CourtListener, Juriscraper, and RECAP. CourtListener is a free legal research website containing millions of legal opinions from federal and state courts. With CourtListener, lawyers, journalists, academics, and the public can research an important case, stay up to date with new opinions as they are filed, or do deep analysis using our raw data.
Content:
This dataset contains the corpus of all Supreme Court opinions and some additional supporting information. Corpus was initially collated as individual JSON objects here; these JSON objects were joined into a single csv. Note that the actual opinions column is rendered in HTML. Citations are links to additional Courtlistener API calls that are not included in this corpus.
Acknowledgements:
CourtListener scraped and assembled this and other similar legal datasets for public use.
Inspiration:
Can you join this data with other Supreme Court data like here and here?
Sentiment analysis would be particularly illuminating."
Churn in Telecom's dataset,,david_becks,5,"Version 1,2017-09-25",,CSV,303 KB,Other,"1,689 views",327 downloads,,0 topics,https://www.kaggle.com/becksddf/churn-in-telecoms-dataset,This dataset does not have a description yet.
YouTube Comedy Slam,Votes for the funniest videos,UCI Machine Learning,5,"Version 1,2017-09-20",humor,CSV,32 MB,CC0,"1,372 views",86 downloads,,0 topics,https://www.kaggle.com/uciml/youtube-comedy-slam,"Context
This dataset provides user vote data on which video from a pair of videos was funnier. YouTube Comedy Slam was a discovery experiment running on YouTube 2011 and 2012. In the experiment, pairs of videos were shown to users and the users voted for the video that they found funniest.
Content
The datasets includes roughly 1.7 million votes recorded chronologically. The first 80% are provided here as the training dataset and the remaining 20% as the testing dataset.
Each row in this text file represents one anonymous user vote and there are three comma-separated fields.
The first two fields are YouTube video IDs.
The third field is either 'left' or 'right'.
Left indicates the first video from the pair was voted to be funnier than the second. Right indicates the opposite preference.
Acknowledgements
Sanketh Shetty, 'Quantifying comedy on YouTube: why the number of o's in your LOL matter,' Google Research Blog, https://research.googleblog.com/2012/02/quantifying-comedy-on-youtube-why.html.
Dataset was downloaded from UCI ML repository: https://archive.ics.uci.edu/ml/datasets/YouTube+Comedy+Slam+Preference+Data
Inspiration
Predict which videos are going to be funny!"
Indian Hotels on Cleartrip,This dataset contains Indian hotel (5000) present on Cleartrip.com,PromptCloud,5,"Version 1,2017-09-15","hotels
internet",CSV,15 MB,CC0,"1,013 views",137 downloads,,0 topics,https://www.kaggle.com/PromptCloudHQ/indian-hotels-on-cleartrip,"Context
This is a pre-crawled dataset, taken as subset of a bigger dataset (more than 42,000 hotels) that was created by extracting data from Cleartrip.com, a leading travel portal in India.
Content
Analyses can be performed on the hotel description, facilities and various ratings to name a few.
Acknowledgements
This dataset was created by PromptCloud's in-house web-crawling service."
Doctor and lawyer profiles on Avvo.com,"20,000 doctor and lawyer profiles",PromptCloud,5,"Version 1,2017-09-16","healthcare
relationships
law
internet",CSV,6 MB,CC4,753 views,51 downloads,,0 topics,https://www.kaggle.com/PromptCloudHQ/doctor-and-lawyer-profiles-on-avvocom,"Context
This is a pre-crawled dataset, taken as subset of a bigger dataset (more than 2.4 million profiles) that was created by extracting data from Avvo.com.
Content
This dataset has following fields:
address
categories
description
image_count
name
payment_option
phone
profile_id
profile_url
video_count
website
Acknowledgements
This dataset was created by PromptCloud's in-house web-crawling service.
Inspiration
Analysis can be performed on the profile category."
Commercial Paper,Rates & Volumes for 1998-2017,Federal Reserve,5,"Version 1,2017-09-19","finance
banking",CSV,1 MB,CC0,"1,207 views",86 downloads,,0 topics,https://www.kaggle.com/federalreserve/commercial-paper-rates,"Commercial paper, in the global financial market, is an unsecured promissory note with a fixed maturity of not more than 270 days.
Commercial paper is a money-market security issued (sold) by large corporations to obtain funds to meet short-term debt obligations (for example, payroll), and is backed only by an issuing bank or company promise to pay the face amount on the maturity date specified on the note. Since it is not backed by collateral, only firms with excellent credit ratings from a recognized credit rating agency will be able to sell their commercial paper at a reasonable price. Commercial paper is usually sold at a discount from face value, and generally carries lower interest repayment rates than bonds due to the shorter maturities of commercial paper. Typically, the longer the maturity on a note, the higher the interest rate the issuing institution pays. Interest rates fluctuate with market conditions, but are typically lower than banks' rates.
Commercial paper – though a short-term obligation – is issued as part of a continuous rolling program, which is either a number of years long (as in Europe), or open-ended (as in the U.S.)
Acknowledgements
This dataset was made available by the Federal Reserve. You can find the original dataset, updated daily, here.
Inspiration
Based solely on this dataset, when would you say the Great Recession financial crisis started? How does that compare with media reports?"
Averaged Perceptron Tagger,The model that nltk.pos_tag loads,NLTK Data,5,"Version 1,2017-08-18",,Other,6 MB,Other,935 views,28 downloads,,0 topics,https://www.kaggle.com/nltkdata/averaged-perceptron-tagger,"Context
The averaged_perceptron_tagger.zip contains the pre-trained English [Part-of-Speech (POS]](https://en.wikipedia.org/wiki/Part_of_speech) tagger in NLTK.
The nltk.tag.AveragedPerceptronTagger is the default tagger as of NLTK version 3.1. The model was trained on on Sections 00-18 of the Wall Street Journal sections of OntoNotes 5
The original implementation comes from Matthew Honnibal, it outperforms the predecessor maximum entropy POS model in NLTK.
The version from Textblob was ported over to NLTK in pull-request #122.
Acknowledgements
Credit goes to Matthew Honnibal.
The reimplementation in Textblob by Steven Loria
Re-reimplementation in NLTK by Long Duong"
UK Land Registry Transactions,"Applications for first registrations, leases, dealings, searches, etc",HM Land Registry,5,"Version 1,2017-08-18","housing
finance
government",CSV,32 MB,Other,"1,123 views",101 downloads,,0 topics,https://www.kaggle.com/hm-land-registry/uk-land-registry-transactions,"Transaction data gives numbers of applications for first registrations, leases, transfers of part, dealings, official copies and searches lodged with HM Land Registry by account holders in the preceding month. The information is divided into data showing all applications lodged, transactions for value, by region and local authority district. Transactions for value include freehold and leasehold sales.
The data published on this page gives you information about the number and types of applications. The data reflects the volume of applications lodged by customers using an HM Land Registry account number on their application form. The data does not include applications that are not yet completed, or were withdrawn.
Content
This dataset has been altered from its original format. Specifically, the monthly files have been aggregated and columns whose names changed over time have been merged to use the current title. Some acronyms that will be helpful to know while reading the column names, per the documentation:
Acronym Title Description
DFL Dispositionary first lease An application for the registration of a new lease granted by the proprietor of registered land
DLG Dealing An application in respect of registered land. This includes transfers of title, charges and notices
FR First registration An application for a first registration of land both freehold and leasehold. For leasehold this applies when the landlord’s title is not registered
TP Transfer of part An application to register the transfer of part of a registered title
OS(W) Search of whole An application to protect a transaction for value, such as purchase, lease or charge for the whole of a title
OS(P) Search of part An application to protect a transaction for value, such as purchase, lease or charge for part of a title
OS(NPW) Non-priority search of whole An application to search the whole of the register without getting priority
OS(NPP) Non-priority search of part An application to search a part of the register without getting priority
OC1 Official copy An application to obtain an official copy of a register or title plan represents a true record of entries in the register and extent of the registered title at a specific date and time. The data includes historical editions of the register and title plan where they are kept by the registrar in electronic form
OC2 Official copy of a deed or document An application to obtain a copy of a document referred to in the register or relates to an application. This includes correspondence, surveys, application forms and emails relating to applications that are pending, cancelled or completed
SIM Search of the index map An application to find out whether or not land is registered and, if so, to obtain the title number
Acknowledgements
This data was kindly released by HM Land Registry under the Open Government License 3.0. You can find their current release here.
Inspiration
-What does this dataset tell us about the HM Land Registry's records of housing Prices Paid? Are searches a leading indicator of price changes?"
Code of Federal Regulations,XML annotated US regulations as of mid 2017,U.S. Government Publishing Office,5,"Version 1,2017-08-29","government
law",Other,336 MB,CC0,486 views,51 downloads,,0 topics,https://www.kaggle.com/us-gpo/code-of-federal-regulations,"The Code of Federal Regulations (CFR) is the codification of the general and permanent rules and regulations (sometimes called administrative law) published in the Federal Register by the executive departments and agencies of the federal government of the United States.
The 50 subject matter titles contain one or more individual volumes, which are updated once each calendar year, on a staggered basis. The annual update cycle is as follows: titles 1-16 are revised as of January 1; titles 17-27 are revised as of April 1; titles 28-41 are revised as of July 1; and titles 42-50 are revised as of October 1. Each title is divided into chapters, which usually bear the name of the issuing agency. Each chapter is further subdivided into parts that cover specific regulatory areas. Large parts may be subdivided into subparts. All parts are organized in sections, and most citations to the CFR refer to material at the section level.
The CFR is published in multiple formats by the US Government Publishing Office. You can find the latest version of the XML format here: http://www.gpo.gov/fdsys/bulkdata/CFR."
ICLR 2017 Reviews,"ICLR 2017 paper titles, authors, abstracts, reviews, and 4-way decisions.",Abhinav Maurya,5,"Version 1,2017-08-22","research
linguistics
artificial intelligence
computer science",CSV,10 MB,GPL,932 views,41 downloads,,0 topics,https://www.kaggle.com/ahmaurya/iclr2017reviews,"Context
ICLR (International Conference on Learning Representations) is a premier machine learning conference. Unlike the other two flagship machine learning conferences ICML and NIPS, ICLR chooses a single-blind public review process in which the reviews and their rebuttals are both carried out transparently and in the open. This dataset was created by crawling the public ICLR 2017 paper review site. It seems ICLR is going double-blind from 2018, so my guess is that authors will remain anonymous during the review process. So, this dataset is unique because it captures a public academic review process with academic affiliations and all paper decisions including rejections.
Content
The dataset consists of two CSV files:
iclr2017_papers.csv: This file has a row per submission. It includes the paper title, authors, author conflicts, abstracts, tl;dr (a simplified abstract), and final decision (Accept/Oral, Accept/Poster, Accept/InviteToWorkshop, Reject). Each row has a unique identifier key called the ""paper_id.""
iclr2017_conversations.csv: This file has a row per textual review, rebuttal, or comment. It is related to the previous papers dataset using the secondary key ""paper_id."" All rows talking about a single paper share the same ""paper_id."" The conversations associated with each paper can be thought of as a forest. Each tree in the forest begins with a review followed by rebuttals and further comments/conversation. Each such textual entry composed by an individual is listed in its own row. The nodes of the tree are connected using the fields ""child_id"" and ""parent_id"" which can be used to construct the entire conversation hierarchy.
Acknowledgements
All rights for abstracts rest with the paper authors. Reproduction of abstracts here is solely for purposes of research. Thanks to the authors of Beautiful Soup 4 Python package which considerably simplified the process of curating this dataset.
Inspiration
This dataset was created to understand gender disparities in paper submissions and acceptances. Annotating each author with a binary gender is a pending task. The dataset can also be used to model communication processes employed in negotiation, persuasion, and decision-making. Another use of this dataset could be in modeling and understanding textual time-series data."
Agricultural Estimates,Main crops production and yield in ARG,Pablo Lebed,5,"Version 1,2018-02-14","agriculture
data cleaning",CSV,1 MB,ODbL,922 views,114 downloads,,0 topics,https://www.kaggle.com/pablolebed/agricultural-estimates-arg,"Context
This dataset contains crops production and yield over the years in Argentina. Data is provided by province and district for each seed or harvest campaign from 1969 to 2017.
Content
Sup. Sembrada (Ha) --- Hectares sown
Sup. Cosechada (Ha) --- Hectares harvested
Produccion (Tn) --- Tonnes produced
Rendimiento (Kg/Ha) --- Harvest performance
Acknowledgements
This dataset was kindly made publicly available by Datos Argentina under the ODbL license.
Photo by Benjamin Davies on Unsplash.
Inspiration
My Data Science adventure start here. My objective is to use this dataset together with Kaggle Kernels as a starting point in my learning process."
Game of Thrones Subtitles,Subtitles for each episode across 7 seasons,GunnvantSaini,5,"Version 1,2018-02-20","writing
popular culture
film",{}JSON,2 MB,CC0,500 views,50 downloads,,0 topics,https://www.kaggle.com/gunnvant/game-of-thrones-srt,"Context
This dataset contains every line from every season of the HBO TV show Game of Thrones.
Content
Each season has one JSON file. In eachJSON file there is a key for each episode and each episode is further mapped at a dialogue level.
Inspiration
The idea is to use this data set to see if one can create a summary of what transpired in each episode or season."
Crop Residue Cover Measurement,On the ground or in the air?,Frédéric Kosmowski,5,"Version 2,2018-02-14|Version 1,2018-02-12","africa
agriculture
survey analysis",Other,230 KB,CC4,380 views,39 downloads,,,https://www.kaggle.com/fkosmowski/crop-residue-cover-measurement,"Context
In commercial agriculture it is common practice to retain so-called crop residue (agricultural product left on the field after a harvest) on planting fields. This is a commonly recommended practice in conservation agriculture.
This dataset is a measurement of the adoption and impact of the methodology for a selection of agricultural fields. Six different crop residue coverage measurement methods are included: i) interviewee (respondent) estimation; ii) enumerator estimation visiting the field; iii) interviewee with visual-aid without visiting the field; iv) enumerator with visual-aid visiting the field; v) field picture collected with a drone and analyzed with image-processing methods and vi) satellite picture of the field analyzed with remote sensing methods.
Content
This dataset is a CSV file with a selection of characteristics about fields included in the sample.
Acknowledgements
This dataset was created as part of the following study: ""On the Ground or in the Air? A Methodological Experiment on Crop Residue Cover Measurement in Ethiopia"".
Inspiration
What are the characteristics of the Ethiopian farmers sampled in this dataset? How well do they follow conservation practices?"
Paddy-Grass Distinguisher,this data-set contains over 1000 images of paddy plants and weeds.,Aruna Jayasena,5,"Version 1,2018-02-15",,Other,52 MB,Other,41 views,4 downloads,,0 topics,https://www.kaggle.com/archfx/paddygrass-distinguisher,"Context
There are no any data-sets for distinguishing paddy plants from other weeds. This data-set was taken from different fields from Sri Lanka.
Content
This data-set contains 1200 images of paddy and other weeds.
Acknowledgements
This data can be be used for any sort of machine learning projects. Use this for learning purposes only.
Inspiration
this data-set was developed for Semester project of the university."
One week of Betfair data: horses,A detailed history of prices (odds) traded for each horse race,Foxtrot,4,"Version 1,2017-08-31","horse racing
sports
time series",CSV,144 MB,Other,750 views,46 downloads,,0 topics,https://www.kaggle.com/zygmunt/betfair-horses,"A sample of Betfair data, normally available to those who spend a lot of money wagering. Horse races only. Other sports are available at https://www.kaggle.com/zygmunt/betfair-sports.
See http://data.betfair.com/ for a description.
The file has 688184 data rows. It is 143 MB uncompressed.
Columns:
EVENT_ID
COUNTRY
FULL_DESCRIPTION
COURSE
SCHEDULED_OFF
EVENT
ACTUAL_OFF
SELECTION
SETTLED_DATE
ODDS
LATEST_TAKEN (when these odds were last matched on the selection)
FIRST_TAKEN (when these odds were first matched on the selection)
IN_PLAY (IP - In-Play, PE - Pre-Event, NI - Event did not go in-play)
NUMBER_BETS (number of individual bets placed)
VOLUME_MATCHED (sums the stakes of both back and lay bets)
SPORTS_ID
SELECTION_ID
WIN_FLAG (1 if the selection was paid out as a full or partial winner, 0 otherwise)"
"Brooklyn Home Sales, 2003 to 2017",Brooklyn New York housing and GIS data,Tommy Wu,4,"Version 2,2018-02-16|Version 1,2018-02-16",housing,Other,77 MB,CC0,146 views,15 downloads,,0 topics,https://www.kaggle.com/tianhwu/brooklynhomes2003to2017,"Context
I'm trying to make a Choropleth map over time of home sale prices by block in Brooklyn for the last 15 years to visualize gentrification. I have the entire dataset for all 5 boroughs of New York, but am starting with Brooklyn.
Content and Acknowledgements
Primary dataset is the NYC Housing Sales Data Found in this Link: http://www1.nyc.gov/site/finance/taxes/property-rolling-sales-data.page
The data in all the separate excel spreadsheets for 2003-2017 was merged via VBA scripting in Excel and further cleaned & de-duped in R
Additionally, in my hunt for shapefiles I discovered these wonderful shapefiles from NYCPluto: https://www1.nyc.gov/site/planning/data-maps/open-data/dwn-pluto-mappluto.page
I left joined it by ""Block"" & ""Lot"" onto the primary data frame, but 25% of the block/lot combo's ended up not having a corresponding entry in the Pluto shapefile and are NAs.
Note that as in other uploaded datasets of NYC housing on Kaggle, many of these transactions have a sale_price of $0 or only a nominal amount far less than market value. These are likely property transfers to relatives and should be excluded from any analysis of market prices.
Inspiration
Can you model Brooklyn home prices accurately?"
Diabetes by Demographies,"Percentage of Diabetes by Age, Ethnicity and Year",Gokagglers,4,"Version 1,2017-08-23",,CSV,3 KB,Other,505 views,72 downloads,,0 topics,https://www.kaggle.com/loveall/diabetes-by-demographies,"Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
Quotables,A corpus of quotes .,Liling Tan,4,"Version 1,2017-08-16","people
linguistics",Other,5 MB,CC0,328 views,26 downloads,,0 topics,https://www.kaggle.com/alvations/quotables,"Quotables
A corpus of quotes.
Quotable Quote
""I fear not the man who has practiced 10,000 kicks once, but I fear the man who has practiced one kick 10,000 times."" - Bruce Lee
Content
Two-columns file: authors are in the first column, quotes in the second, separated by a tab
Statistics:
36,165 quotes with
878,450 words from
2,297 people
Acknowledgements
Originally from https://github.com/alvations/Quotables
Dataset image comes from Mona Eendra"
Google Product Taxonomy,Product categories for Google Shopping,Liling Tan,4,"Version 1,2017-08-18",,Other,24 MB,Other,"1,800 views",162 downloads,,0 topics,https://www.kaggle.com/alvations/google-product-taxonomy,"Google Taxonomy
This product taxonomy lists seemed to be used by merchants in tagging their products on Google Shopping, see https://support.google.com/merchants/answer/6324436?hl=en
These lists were also used in the SemEval Taxonomy Evaluation tasks:
http://alt.qcri.org/semeval2015/task17/
http://alt.qcri.org/semeval2016/task13/
Content
Contains the product category lists for:
Czech
Danish
German (Swiss / Germany)
English (Australian / British / American)
Spanish (Spanish)
French (Swiss / France)
Italian (Swiss / Italy)
Japanese
Dutch
Norwegian
Polish
Portuguese (Brazillian)
Russian
Swedish
Turkish
Chinese
This post might be helpful for others too: https://www.en.advertisercommunity.com/t5/Google-Shopping-and-Merchant/Taxonomy-List-Countries-Some-missing/td-p/599656
Acknowledgements
The individual Google product taxonomy files came from
http://www.google.com/basepages/producttype/taxonomy-with-ids.<language_code>-<country_code>.txt
Dataset image comes from Edho Pratama
Disclaimer
I am not affiliated to Google or own these product categories lists. If this offends any copyrights/licenses, please request for me to remove it."
SeedLing,A Seed Corpus for the Human Language Project,Liling Tan,4,"Version 2,2017-08-24|Version 1,2017-08-19","culture and humanities
languages
linguistics",Other,6 MB,Other,538 views,33 downloads,,0 topics,https://www.kaggle.com/alvations/seedling,"Context
A broad-coverage corpus such as the Human Language Project envisioned by Abney and Bird (2010) would be a powerful resource for the study of endangered languages. SeedLing was created as a seed corpus for the Human Language Project to cover a broad range of languages (Guy et al. 2014).
TAUS (Translation Automation User Society) also see the importance of the Human Language Project in the context of keeping up with the demand for capacity and speed for translation. TAUS' definition of the Human Language Project can be found on https://www.taus.net/knowledgebase/index.php/Human_Language_Project
A detailed explanation of how to use the corpus can be found on https://github.com/alvations/SeedLing
Content
The SeedLing corpus on this repository includes the data from:
ODIN: Online Database of Interlinear Text
Omniglot: Useful foreign phrases from www.omniglot.com
UDHR: Universal Declaration of Human Rights
Acknowledgements
Citation:
Guy Emerson, Liling Tan, Susanne Fertmann, Alexis Palmer and Michaela Regneri . 2014. SeedLing: Building and using a seed corpus for the Human Language Project. In Proceedings of The use of Computational methods in the study of Endangered Languages (ComputEL) Workshop. Baltimore, USA.
@InProceedings{seedling2014,
  author    = {Guy Emerson, Liling Tan, Susanne Fertmann, Alexis Palmer and Michaela Regneri},
  title     = {SeedLing: Building and using a seed corpus for the Human Language Project},
  booktitle = {Proceedings of The use of Computational methods in the study of Endangered Languages (ComputEL) Workshop},
  month     = {June},
  year      = {2014},
  address   = {Baltimore, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {},
  url       = {}
}
References:
Steven Abney and Steven Bird. 2010. The Human Language 
Project: Building a universal corpus of the world’s languages. 
In Proceedings of the 48th Annual Meeting of the Association 
for Computational Linguistics, pages 88–97.

Sime Ager. Omniglot - writing systems and languages 
of the world. Retrieved from www.omniglot.com.

William D Lewis and Fei Xia. 2010. Developing ODIN: A multilingual 
repository of annotated language data for hundreds of the world’s 
languages. Literary and Linguistic Computing, 25(3):303–319.

UN General Assembly, Universal Declaration of Human Rights, 
10 December 1948, 217 A (III), available at: 
http://www.refworld.org/docid/3ae6b3712c.html 
[accessed 26 April 2014]
Inspiration
This corpus was created in a span a semester in Saarland University by a linguist, a mathematician, a data geek and two amazing mentors from the COLI department. It wouldn't have been possible without the cross-disciplinary synergy and the common goal we had.
Expand/Explore the Human Language Project.
Go to the field and record/document their language. Make them computationally readable.
Grow the Seedling!"
Microdados Censo Escolar 2015,Parcial: Escolas + Turmas,Keik@,4,"Version 1,2017-09-14",education,CSV,92 MB,ODbL,569 views,49 downloads,,0 topics,https://www.kaggle.com/lucianakeiko/microdados-censo-escolar-2015,"O Censo Escolar é um levantamento de dados estatístico-educacionais de âmbito nacional realizado todos os anos e coordenado pelo Inep. Ele é feito com a colaboração das secretarias estaduais e municipais de Educação e com a participação de todas as escolas públicas e privadas do país.
Trata-se do principal instrumento de coleta de informações da educação básica, que abrange as suas diferentes etapas e modalidades: ensino regular (educação Infantil e ensinos fundamental e médio), educação especial e educação de jovens e adultos (EJA). O Censo Escolar coleta dados sobre estabelecimentos, matrículas, funções docentes, movimento e rendimento escolar.
Essas informações são utilizadas para traçar um panorama nacional da educação básica e servem de referência para a formulação de políticas públicas e execução de programas na área da educação, incluindo os de transferência de recursos públicos como merenda e transporte escolar, distribuição de livros e uniformes, implantação de bibliotecas, instalação de energia elétrica, Dinheiro Direto na Escola e Fundo de Manutenção e Desenvolvimento da Educação Básica e de Valorização dos Profissionais da Educação (Fundeb).
Além disso, os resultados obtidos no Censo Escolar sobre o rendimento (aprovação e reprovação) e movimento (abandono) escolar dos alunos do ensino Fundamental e Médio, juntamente com outras avaliações do Inep (Saeb e Prova Brasil), são utilizados para o cálculo do Índice de Desenvolvimento da Educação Básica (IDEB), indicador que serve de referência para as metas do Plano de Desenvolvimento da Educação (PDE), do Ministério da Educação.
Para saber mais sobre o Censo Escolar: http://portal.inep.gov.br/basica-censo
Apresentados em formato ASCII, os microdados são acompanhados de inputs, ou seja, canais de entrada para leitura dos arquivos por meio da utilização dos softwares SAS e SPSS.
Os Microdados passaram a ser estruturados em formato CSV (Comma-Separated Values), e seus dados estão delimitados por Pipe ( | ), de modo a garantir que praticamente qualquer software estatístico, inclusive open source, consiga importar e carregar as bases de dados.
Devido à amplitude de nossas bases, os arquivos foram divididos por região geográfica (Norte, Nordeste, Sudeste, Sul e Centro-Oeste), tanto para as variáveis de Matrículas, quanto para as de Docentes."
EEG-Alcohol,This data contains EEG correlates of genetic predisposition to alcoholism,Leonidas,4,"Version 1,2017-08-19","alcohol
neurology
neuroscience
+ 2 more...",Other,885 MB,Other,"2,383 views",170 downloads,2 kernels,0 topics,https://www.kaggle.com/nnair25/Alcoholics,"Data Set Information: This data arises from a large study to examine EEG correlates of genetic predisposition to alcoholism. It contains measurements from 64 electrodes placed on subject's scalps which were sampled at 256 Hz (3.9-msec epoch) for 1 second.
There were two groups of subjects: alcoholic and control. Each subject was exposed to either a single stimulus (S1) or to two stimuli (S1 and S2) which were pictures of objects chosen from the 1980 Snodgrass and Vanderwart picture set. When two stimuli were shown, they were presented in either a matched condition where S1 was identical to S2 or in a non-matched condition where S1 differed from S2.
Attribute Information
Each trial is stored in its own file and will appear in the following format.
trial number sensor position sample num sensor value subject identifier matching condition channel name time
0 FP1 0 -8.921 a S1 obj 0 co2a0000364 0
0 AF8 87 4.14 a S1 obj 33 co2a0000364 0.33
The columns of data are:
the trial number,
sensor position,
sample number (0-255),
sensor value (in micro volts),
subject identifier(Alcoholic(a) or Control (c)),
matching condition(a single object shown (S1 obj), object 2 shown in a matching condition (S2 match), and object 2 shown in non matching condition (S2 nomatch)),
channel number(0-63),
name(a serial code assigned to each subject),
time(inverse of sample num measured in seconds))
Acknowledgements
There are no usage restrictions on this data.
Acknowledgments for this data should made to Henri Begleiter at the Neurodynamics Laboratory at the State University of New York Health Center at Brooklyn.
You can check out more info about it on: https://archive.ics.uci.edu/ml/datasets/eeg+database"
Natural Stories Corpus,A corpus of stories with human reading times (by word),Rachael Tatman,4,"Version 1,2017-08-29","reading
languages
literature
linguistics",CSV,31 MB,CC4,803 views,67 downloads,,0 topics,https://www.kaggle.com/rtatman/natural-stories-corpus,"Context:
It is now a common practice to compare models of human language processing by predicting participant reactions (such as reading times) to corpora consisting of rich naturalistic linguistic materials. However, many of the corpora used in these studies are based on naturalistic text and thus do not contain many of the low-frequency syntactic constructions that are often required to distinguish processing theories. The corpus includes self-paced reading time data for ten naturalistic stories.
Content:
This is a corpus of naturalistic stories meant to contain varied, low-frequency syntactic constructions. There are a variety of annotations and psycholinguistic measures available for the stories. The stories in with their various annotations are coordinated around the file words.tsv, which specifies a unique code for each token in the story under a variety of different tokenization schemes. For example, the following lines in words.tsv cover the phrase the long-bearded mill owners.:
1.54.whole the
1.54.word the
1.54.1 the
1.55.whole long - bearded
1.55.word long - bearded
1.55.1 long
1.55.2 -
1.55.3 bearded
1.56.whole mill
1.56.word mill
1.56.1 mill
1.57.whole owners .
1.57.word owners
1.57.1 owners
1.57.2 .
The first column is the token code; the second is the token itself. For example, 1.57.whole represents the token owners.and 1.57.word represents the token owners. The token code consists of three fields:
The id of the story the token is found in,
The number of the token in the story,
An additional field whose value is whole for the entire token including punctuation, word for the token stripped of punctuation to the left and right, and then 1 through n for each sub-token in whole as segmented by NLTK's TreebankWordTokenizer.
The various annotations (frequencies, parses, RTs, etc.) should reference these codes so that we can track tokens uniformly.
This dataset contains reading time data collected for 10 naturalistic stories. Participants typically read 5 stories each. The data is contained in batch1_pro.csv and batch2_pro.csv
all_stories.tok contains the 10 stories, with one word per row. Item is the story number, zone is the region where the word falls within the story. Note that some wordforms in all_stories.tok differ from those in words.tsv, reflecting typos in the SPR experiment as run.
Acknowledgements:
If you use this dataset in your work, please cite the following paper:
Futrell, R., Gibson, E., Tily, H., Blank, I., Vishnevetsky, A., Piantadosi, S. T., & Fedorenko, E. (2017). The Natural Stories Corpus. arXiv preprint arXiv:1708.05763.
A more complete version of this dataset, with additional supporting files, can be found in this GitHub repository maintained by by Richard Futrell at Massachusetts Institute of Technology, Titus von der Malsburg at the University of Potsdam and Cory Shain at The Ohio State University.
Inspiration:
What words do participants tend to read more slowly or quickly?
Are certain parts of speech read more quickly or slowly?
How much variation in reading speed is there between individuals?
What’s the relationship between word length & reading speed?"
Austin Waste and Diversion,"Garbage In, Garbage Out",Jacob Boysen,4,"Version 1,2017-08-26",government agencies,CSV,61 MB,CC0,"1,038 views",89 downloads,2 kernels,0 topics,https://www.kaggle.com/jboysen/austin-waste,"Context:
This dataset is trash. Who in Austin makes it, who takes it, and where does it go?
Content:
Data ranges 2008-2016 and includes dropoff site, load id, time of load, type of load, weight of load, date, route number, and route type (recycling, street cleaning, garbage etc).
Acknowledgements:
This dataset was created by Austin city government and hosted on Google Cloud Platform. You can use Kernels to analyze, share, and discuss this data on Kaggle, but if you’re looking for real-time updates and bigger data, check out the data on BigQuery, too
Inspiration:
How much trash is Austin generating?
Which are the trashiest routes? Who recycles the best?
Any seasonal changes?
Try to predict trash route usage from historical trash data"
Virtual Reality Driving Simulator Dataset,Dataset from drivers driving a virtual reality driving simulator.,Sasan Jafarnejad,4,"Version 1,2017-08-16",,CSV,28 MB,Other,"1,267 views",73 downloads,,0 topics,https://www.kaggle.com/sasanj/virtual-reality-driving-simulator-dataset,"Context
We developed this platform to be able to study driver behavior in a controlled environment and perform various experiments economically and with greater flexibility.
VehicularLab - University of Luxembourg
Content
More information will become available with the publication of the corresponding article that is currently submitted to IEEE VNC 2017.
Acknowledgements
To be acknowledged!
Inspiration
This dataset can be used to discover differences in individuals driving behavior."
Life Expectancy (WHO),Statistical Analysis on factors influencing Life Expectancy,KumarRajarshi,4,"Version 1,2018-02-10","countries
data cleaning
regression analysis
multiple regression",CSV,326 KB,Other,280 views,70 downloads,,0 topics,https://www.kaggle.com/kumarajarshi/life-expectancy-who,"Context
Although there have been lot of studies undertaken in the past on factors affecting life expectancy considering demographic variables, income composition and mortality rates. It was found that affect of immunization and human development index was not taken into account in the past. Also, some of the past research was done considering multiple linear regression based on data set of one year for all the countries. Hence, this gives motivation to resolve both the factors stated previously by formulating a regression model based on mixed effects model and multiple linear regression while considering data from a period of 2000 to 2015 for all the countries. Important immunization like Hepatitis B, Polio and Diphtheria will also be considered. In a nutshell, this study will focus on immunization factors, mortality factors, economic factors, social factors and other health related factors as well. Since the observations this dataset are based on different countries, it will be easier for a country to determine the predicting factor which is contributing to lower value of life expectancy. This will help in suggesting a country which area should be given importance in order to efficiently improve the life expectancy of its population.
Content
The project relies on accuracy of data. The Global Health Observatory (GHO) data repository under World Health Organization (WHO) keeps track of the health status as well as many other related factors for all countries The data-sets are made available to public for the purpose of health data analysis. The data-set related to life expectancy, health factors for 193 countries has been collected from the same WHO data repository website and its corresponding economic data was collected from United Nation website. Among all categories of health-related factors only those critical factors were chosen which are more representative. It has been observed that in the past 15 years , there has been a huge development in health sector resulting in improvement of human mortality rates especially in the developing nations in comparison to the past 30 years. Therefore, in this project we have considered data from year 2000-2015 for 193 countries for further analysis. The individual data files have been merged together into a single data-set. On initial visual inspection of the data showed some missing values. As the data-sets were from WHO, we found no evident errors. Missing data was handled in R software by using Missmap command. The result indicated that most of the missing data was for population, Hepatitis B and GDP. The missing data were from less known countries like Vanuatu, Tonga, Togo, Cabo Verde etc. Finding all data for these countries was difficult and hence, it was decided that we exclude these countries from the final model data-set. The final merged file(final dataset) consists of 22 Columns and 2938 rows which meant 20 predicting variables. All predicting variables was then divided into several broad categories:Immunization related factors, Mortality factors, Economical factors and Social factors.
Acknowledgements
The data was collected from WHO and United Nations website with the help of Deeksha Russell and Duan Wang.
Inspiration
The data-set aims to answer the following key questions: 1. Does various predicting factors which has been chosen initially really affect the Life expectancy? What are the predicting variables actually affecting the life expectancy? 2. Should a country having a lower life expectancy value(<65) increase its healthcare expenditure in order to improve its average lifespan? 3. How does Infant and Adult mortality rates affect life expectancy? 4. Does Life Expectancy has positive or negative correlation with eating habits, lifestyle, exercise, smoking, drinking alcohol etc. 5. What is the impact of schooling on the lifespan of humans? 6. Does Life Expectancy have positive or negative relationship with drinking alcohol? 7. Do densely populated countries tend to have lower life expectancy? 8. What is the impact of Immunization coverage on life Expectancy?"
GrammyAwardsInNumbers,Dataset containing Historical Grammy Award Data,Christopher Lambert,4,"Version 1,2018-01-29","music
united states
mass media
+ 2 more...",{}JSON,694 KB,CC0,484 views,48 downloads,,0 topics,https://www.kaggle.com/theriley106/grammyawardsinnumbers,"This is a dataset containing all grammy winners categorized by award title. I saw this data for sale yesterday for $3xx.xx, so I decided to scrape it and host it on Kaggle for free. It's stored as JSON, with the following keys:
""name""
Name of the individual receiving the award
""awardType""
This indicates if the award was for an artist or for an album. If the ""awardType"" == ""Work"" it's for something they published. If ""awardType"" == ""Individual"" it was given to the artist specifically (ie Artist Of Year)
""category""
This describes the award category (ie ""Artist Of Year"", ""Album Of Year"", etc)
""annualGrammy""
This is the # Grammy it was. So last nights grammy was #60, the first was #1, etc.
""awardFor""
This is whatever is being recognized with the award. This can be an artist, a song, an album, etc. (ie ""Alesia Cara"" or ""Transcendental"")
Here is the script used to scrape this information."
Air pressure system failures in Scania trucks,Predict failures and minimize costs based on sensor readings,UCI Machine Learning,4,"Version 3,2018-02-19|Version 2,2018-02-16|Version 1,2018-02-16",mechanical engineering,CSV,36 MB,GPL,358 views,28 downloads,,,https://www.kaggle.com/uciml/aps-failure-at-scania-trucks-data-set,"Context
The dataset consists of data collected from heavy Scania trucks in everyday usage. The system in focus is the Air Pressure system (APS) which generates pressurized air that is utilized in various functions in a truck, such as braking and gear changes. The datasets' positive class consists of component failures for a specific component of the APS system. The negative class consists of trucks with failures for components not related to the APS. The data consists of a subset of all available data, selected by experts.
Content
The training set contains 60000 examples in total in which 59000 belong to the negative class and 1000 positive class. The test set contains 16000 examples. There are 171 attributes per record.
The attribute names of the data have been anonymized for proprietary reasons. It consists of both single numerical counters and histograms consisting of bins with different conditions. Typically the histograms have open-ended conditions at each end. For example, if we measuring the ambient temperature ""T"" then the histogram could be defined with 4 bins where:
The attributes are as follows: class, then anonymized operational data. The operational data have an identifier and a bin id, like ""Identifier_Bin"". In total there are 171 attributes, of which 7 are histogram variables. Missing values are denoted by ""na"".
Acknowledgements
This file is part of APS Failure and Operational Data for Scania Trucks. It was imported from the UCI ML Repository.
Inspiration
The total cost of a prediction model the sum of Cost_1 multiplied by the number of Instances with type 1 failure and Cost_2 with the number of instances with type 2 failure, resulting in a Total_cost. In this case Cost_1 refers to the cost that an unnecessary check needs to be done by an mechanic at an workshop, while Cost_2 refer to the cost of missing a faulty truck, which may cause a breakdown. Cost_1 = 10 and Cost_2 = 500, and Total_cost = Cost_1*No_Instances + Cost_2*No_Instances.
Can you create a model which accurately predicts and minimizes [the cost of] failures?"
TIMIT-corpus,TIMIT Corpus Sample (LDC93S1),NLTK Data,4,"Version 2,2017-11-16|Version 1,2017-08-22",,Other,21 MB,CC4,"1,365 views",70 downloads,,0 topics,https://www.kaggle.com/nltkdata/timitcorpus,"Context
The canonical metadata on NLTK:
<package id=""timit"" name=""TIMIT Corpus Sample""
     sample=""True""
     license=""This corpus sample is Copyright 1993 Linguistic Data Consortium, and is distributed under the terms of the Creative Commons Attribution, Non-Commercial, ShareAlike license.  http://creativecommons.org/""
     webpage=""http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC93S1""
     unzip=""1""
     />"
Lung Nodule Malignancy,From suspicious nodules to diagnosis,Kevin Mader,4,"Version 1,2017-09-21",healthcare,Other,167 MB,Other,"1,011 views",114 downloads,3 kernels,,https://www.kaggle.com/kmader/lungnodemalignancy,"Context
The DataScienceBowl covered the whole process of diagnosing lung cancer and I am to make the individual steps more clear. After segmenting lungs and identifying suspicious nodes, it is important to classify them as malignant or benign.
Content
This dataset consists of several thousand examples formatted in multipage TIFF (for use with tools like ImageJ and KNIME) and HDF5 (for Python and R).
Acknowledgements
The data were preprocessed and extracted partially from the LUNA16 competition (https://luna16.grand-challenge.org/description/) and should be used with the same policy that data has.
Inspiration
The dataset is more for practice with medical images and CNN's but it would be interesting to see how the best manually created features (HoG, SIFT, ...) perform against various Deep Learning approaches. It would also be quite interesting to try and visualize exactly which parts of an image made the algorithm guess malignant or benign."
FAA Laser Incident Reports,A report of laser incidents from 2010 to 2014,Chris Crawford,4,"Version 1,2017-08-09",aviation,CSV,1 MB,CC0,738 views,64 downloads,2 kernels,0 topics,https://www.kaggle.com/crawford/laser-incident-report,"Context
On February 14, 2012, the President signed Public Law 112-95 , the ""FAA Modernization and Reform Act of 2012."" Section 311 amended Title 18 of the United States Code (U.S.C.) Chapter 2, § 39, by adding § 39A, which makes it a federal crime to aim a laser pointer at an aircraft. As a result of this law, the FAA has compiled a report of laser incidents
Content
There is a datafile for each year and the column headers changed a little from year to year so keep that in mind when you're loading the data.
DATE - Date of report
TIME (UTC) - Time of laser incident
ACID - Aircraft ID (AC/ID, Aircraft ID, etc)
No. A/C - Number of aircraft
TYPE A/C - Type of aircraft
ALT - Altitude
MAJOR CITY - Nearest major city abbreviation
COLOR - Color of laser
Injury Reported - Were there injuries?
CITY - Nearest city
STATE - State
Acknowledments
Original file was converted into separate CSV files for each year. Original dataset can be found here: https://www.faa.gov/about/initiatives/lasers/laws/"
LCS 2017 Summer Split Fantasy Player & Team Stats,LCS Summer 2017 Fantasy Stats,danielwatabe,4,"Version 2,2017-10-11|Version 1,2017-08-11",video games,CSV,118 KB,CC0,745 views,48 downloads,3 kernels,0 topics,https://www.kaggle.com/danielwatabe/lcs-2017-summer-split-fantasy-player-team-stats,"As the Summer Split Regular Season comes to a close. We can look back on which players and teams performed well in terms of fantasy points.
Next to the Column Names. In the parenthesis is how many points are awarded. For example in team stats Dragon Kills(+1) gives 1 point per dragon killed. Baron Kills gives 2 points. In player stats ""10+K/A(+2)"" means +2 points when Kill+Assists is greater than 10 in a match. In player stats ""3K(+2),4K(+5),5k(+10)"" means +2 points per triple kill, +5 points per quadra, and +10 points per penta
These are the point breakdowns according to the Fantasy LCS website
""LCS Players are scored accordingly:
2 points per kill
-0.5 points per death
1.5 points per assist
0.01 points per creep kill
2 points for a triple kill
5 points for a quadra kill (doesn't also count as a triple kill)
10 points for a penta kill (doesn't also count as a quadra kill)
2 points if a player attains 10 or more assists or kills in a game (this bonus only applies once)
LCS Teams are scored accordingly:
2 points per win
2 points per Baron Nashor killed
1 point per Dragon killed
2 points per First Blood earned
1 point per Tower destroyed
2 points if the team wins in less than 30 minutes""
Source: http://fantasy.na.lolesports.com/en-US/stats"
Fantasy Premier League - 2017/18,Data for the 2017/18 season of the Fantasy Premier League,Thomas,4,"Version 2,2017-08-14|Version 1,2017-08-13",association football,CSV,389 KB,CC4,"2,313 views",266 downloads,,,https://www.kaggle.com/thomasd9/fantasy-premier-league-201718,"Context
The Fantasy Premier League has become more popular every year. In the FPL, people pick fantasy teams of real-life players, and every week, receive points based on their picks' real-life performance.
Within this dataset, we have some historical data for the player performance in previous seasons, as well as future match fixtures.
Content
The three main components currently in this dataset are:
The individual players' current performance stats.
The individual players' past performance stats (how much historical data depends on the player).
A list of future match fixtures.
All the data was taken from the Official Fantasy Premier League website.
N.B. A lot of the data was cobbled together from the output of publicly accessible JSON endpoints, therefore there are a lot of duplications (as fixture data was initially from the perspective of the individual players). Also, since a lot of this data is used to drive the UI of a Web Application, there are a lot of redundancies, all of which could do with being cleaned up.
Inspiration
A lot of my friends are massively into all aspects of the Premier League (fantasy or otherwise), so my main motivation in putting this dataset together was to see was it possible to gain a competitive advantage over my very domain knowledgeable friends, with little to no domain knowledge myself.
The obvious questions that could be answered with this data correspond to predicting the future performance of players based on historical metrics."
Factorial Digit Frequencies,Frequencies of each digit from 0! to 8000!,Ian Chu Te,4,"Version 1,2017-09-09",numbers,CSV,361 KB,CC0,715 views,44 downloads,,0 topics,https://www.kaggle.com/ianchute/factorial-number-frequencies,"Context
Digit frequencies of the values of well-known mathematical series are a curiousity within the field of number theory. However, some are quite costly to compute and may cause stack overflow and out-of-memory issues. I am publishing this factorial digit frequency dataset for the convenience of fellow data enthusiasts who are interested in the field of number theory.
Content
This dataset contains decimal digit (0-9) frequencies of the number 0! to 8000! (total of 8001 rows) There are 10 columns - one for each digit. NOTE: The CSV file contains header (0-9).
Acknowledgements
This dataset was generated by using the Clojure language and the trampoline function which avoids annoying stack overflow issues when doing very deep recursion. I would like to thank Rich Hickey and his colleagues in creating the Clojure language.
Replication
The script to generate the numbers can be found here: https://github.com/ianchute/Factorial-Number-Frequencies/blob/master/generate.clj (You may need to convert the resulting JSON file to CSV.)"
European Soccer Dataset : La Liga,Detailed information of games played in the last 5 decades,Sakti Prasad,4,"Version 1,2017-09-11",,CSV,52 KB,GPL,537 views,78 downloads,,0 topics,https://www.kaggle.com/spn007/la-liga-dataset,This data set contains all the match information of the last 47 years of La Liga. Extract as much insights possible from this data set and give some statistics of this high standard football league.
The Buildings of South East England,A large shapefile building outlines,Sohier Dane,4,"Version 1,2017-09-22","cities
geography",Other,934 MB,CC0,605 views,25 downloads,,0 topics,https://www.kaggle.com/sohier/buildings-of-south-east-england,"This dataset is a single large shapefile of the buildings in southeast England. You can use it to make gorgeous maps or join it with other datasets for some really nice visualizations.
Acknowledgements
This dataset was kindly made available by Alasdair Rae, with the underlying raw data from the British Ordnance Survey. You can find the original shapefiles here, plus shapefiles for the rest of the UK."
Fortune 500 Companies of 2017 in US [Latest],"Dataset of Fortune 500 and their ranks, revenues, profits and market value",Shahebaz,4,"Version 2,2017-09-10|Version 1,2017-09-10",business,CSV,40 KB,Other,465 views,86 downloads,,0 topics,https://www.kaggle.com/shaz13/fotune500-2017,"Description
Company Name - Name of the Fortune 500 company
Number of Employees - Total number of employees in the company
Previous Rank - Rank for the year 2016
Revenues - Revenue of the company for the year 2016-17 in $ millions.
Revenue Change - Percentage of Revenue change from last year
Profits - Profits of the company in $ million
Profit Change - Change in the percentage of profit from previous year.
Assets - Value of assets in $ millions
Market Value - Market Value of the company as of 3/31/17
Acknowledgements
This dataset is compiled by Fortune Magazine (www.fortune.com/fortune500). This is the limited version of the original data."
Oreo Flavors Taste-Test Ratings,Ratings of 12 Oreo Varieties from 5 tasters,Rachael Tatman,4,"Version 1,2017-09-07","food and drink
product",CSV,1 KB,CC4,620 views,45 downloads,,0 topics,https://www.kaggle.com/rtatman/oreo-flavors-tastetest-ratings,"Context:
Oreos are traditionally chocolate sandwich cookies with a (vegan) creme filling. However, there are literally dozens of Oreo flavors and varieites (over 50, by this count). With such a variety of choice, how do you know which flavors are tasty enough to make it worth grabbing a whole pack?
Content:
This dataset contains of twelve different Oreo varieties from five tasters, collected at an Oreo tasting party. Not every taster tasted every type of cookie, due in part to food allergies.
The varieties included in this dataset are:
PB&J
Mega Stuf
Lemon
Chocolate
Birthday Cake
Double Stuf
Red Velvet
Dunkin' Donuts Mocha
Mini
Coconut (Thins)
Mint
Cinnamon Bun
Each rater rated flavors on a five point scale, with 1 being the lowest rating and 5 the highest. A hyphen (-) indicates no rating for that flavor from that rater. The notes and discussion were contributed by all tasters.
Acknowledgements:
This dataset was collected by Rachael Tatman and a group of willing volunteers. Oreo is a trademark of Nabisco.
Inspiration:
Which Oreo flavor was there the most disagreement on? The most agreement?
How does the strength of a flavor relate to the ratings it's given?"
SF Beaches Water Quality,"Contaminant Sampling Across 15 Beaches, Summer 2017",Jacob Boysen,4,"Version 1,2017-09-06","bodies of water
safety",CSV,33 KB,CC0,"1,165 views",92 downloads,3 kernels,0 topics,https://www.kaggle.com/jboysen/sf-beaches-water,"Context:
Shoreline bacteria are routinely monitored at fifteen stations around the perimeter of San Francisco where water contact recreation may occur. These include three stations within the Candlestick Point State Recreation Area, one station at Islais Creek, two stations at Aquatic Park, two stations along Crissy Field Beach, three stations at Baker Beach, one station at China Beach, and three stations along Ocean Beach.
Content:
Dataset represents 552 samples taken across 15 locations over summer of 2017. Additional monitoring is conducted whenever a treated discharge from the City’s Combined sewer system occurs that affects a monitored beach.
Acknowledgements:
The beach monitoring program is a cooperative effort between the San Francisco Public Utilities Commission and the San Francisco Department of Public Health. Samples are collected weekly year round. READ MORE about the combined sewer system and a detailed explanation of the Beach Monitoring Program.
Inspiration:
Are there any patterns in beach water quality?"
Deadly traffic accidents in the UK (2015),,Karolina Wullum,4,"Version 1,2017-09-18",road transport,CSV,18 MB,Other,"1,266 views",119 downloads,2 kernels,0 topics,https://www.kaggle.com/kwullum/deadly-traffic-accidents-in-the-uk-2015,"The dataset is taken from data.gov.uk and contains all traffic-related deaths in the UK in 2015.
Source: https://data.gov.uk/dataset/road-accidents-safety-data/resource/ceb00cff-443d-4d43-b17a-ee13437e9564"
Barcelona Unemployment,Barcelona registered unemployment percentages by hood and month,Marc Velmer,4,"Version 1,2017-09-12","business
finance",CSV,41 KB,CC4,975 views,79 downloads,,0 topics,https://www.kaggle.com/marcvelmer/barcelona-unemployment,"Context
This dataset represents the % of registered unemployment in the city of Barcelona (Spain) from year 2012 till 2016.
Registered unemployment corresponds to the job demands pending cover by the last day of each month, excluding employees who want to change jobs, the ones that do not have readily available or incompatible situation, the ones that are asking for a specific occupation and the temporary agricultural beneficiaries special unemployment benefit.
Content
All files in this dataset have the same format. Every row represents a hood from the city.
District number
Hood name
Number of citizens from this hood with ages between 16 and 64 (legal ages for having a job)
12 columns (one per month), % of unemployment
In Barcelona we have hoods and districts. Every hood belongs to a district. A district is formed by several hoods.
Acknowledgements
This data can be found in ""Open Data BCN - Barcelona's City Hall Open Data Service"", which is the owner of the CSV files.
Inspiration
A few weeks ago I needed this datasets for testing purposes.
I have uploaded this information here, because, in my honest opinion, ""data"" and ""research"" should be shared with everybody. Enjoy!"
Istanbul Stock Exchange,Exchange data from 2009 to 2011,UCI Machine Learning,4,"Version 1,2017-09-21",finance,CSV,62 KB,CC0,"1,718 views",108 downloads,2 kernels,0 topics,https://www.kaggle.com/uciml/istanbul-stock-exchange,"Context
This dataset was used in the paper ""A novel Hybrid RBF Neural Networks model as a forecaster, Statistics and Computing"" (Akbilgic et al) to show off a new forecasting algorithm. The paper showed good results when using a HRBF-NN model for predicting daily stock movements.
Content
The data was collected (by the owners) from imkb.gov.tr and finance.yahoo.com and is organized by working days in the Istanbul Stock Exchange.
Columns:
Istanbul stock exchange national 100 index
Standard & poor's 500 return index
Stock market return index of Germany
Stock market return index of UK
Stock market return index of Japan
Stock market return index of Brazil
MSCI European index
MSCI emerging markets index
Acknowledgements
Akbilgic, O., Bozdogan, H., Balaban, M.E., (2013) A novel Hybrid RBF Neural Networks model as a forecaster, Statistics and Computing. DOI 10.1007/s11222-013-9375-7 PhD Thesis: Oguz Akbilgic, (2011) Hibrit Radyal TabanlÄ± Fonksiyon AÄŸlarÄ± ile DeÄŸiÅŸken SeÃ§imi ve Tahminleme: Menkul KÄ±ymet YatÄ±rÄ±m KararlarÄ±na Ä°liÅŸkin Bir Uygulama, Istanbul University
This dataset was downloaded from the UCI ML Repository: https://archive.ics.uci.edu/ml/datasets/ISTANBUL+STOCK+EXCHANGE
Inspiration
Use this dataset to create predictive algorithms. Then get rich!"
Hospital Payment and Value of Care,Government data about payment and quality of care,Centers for Medicare & Medicaid Services,4,"Version 1,2017-08-10","healthcare
hospitals
public health
finance",CSV,8 MB,CC0,705 views,64 downloads,,0 topics,https://www.kaggle.com/cms/paymentandvalue2017,"Data from: https://data.medicare.gov/Hospital-Compare/Payment-and-value-of-care-Hospital/c7us-v4mf More information coming soon!
Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
USDA PLANTS Checklist,Names of plants that grow in North America.,United States Department of Agriculture,4,"Version 1,2017-09-23","plants
agriculture",CSV,6 MB,CC0,986 views,57 downloads,,0 topics,https://www.kaggle.com/usdeptofag/usda-plants-checklist,"Context
This dataset is a checklist of plants known to grow in North America. The list is maintained by the United States Department of Agriculture.
Content
This data includes scientific names and common names of all plants in the United States.
Acknowledgements
This dataset is published as-is by the United States Department of Agriculture.
Inspiration
What words are commonly used in plant names? Can you detect any trends in, say, adjectives commonly used in plant names that are less commonly used in the English language?"
Properties on StayZilla,"6,000 Properties on StayZilla",PromptCloud,4,"Version 1,2017-09-16","hotels
internet",CSV,2 MB,CC4,"1,107 views",103 downloads,,0 topics,https://www.kaggle.com/PromptCloudHQ/properties-on-stayzilla,"Context
This is a pre-crawled dataset, taken as subset of a bigger dataset (more than 61,000 properties) that was created by extracting data from StayZilla.com, an Indian AirBnB-like startup founded in 2005 that closed its operations in 2017.
Content
This dataset has following fields:
additional_info - Special considerations regarding this property.
amenities - Pipe (|) delimited list of amenities offered at the property.
check_in_date
check_out_date
city
country
crawl_date
description - Textual description of the property, as entered into the site by the lister.
highlight_value - Property highlights, as entered into the site by the lister.
hotel_star_rating - In case the property is a hotel, its out-of-five star rating. Not all hotels have ratings.
image_count - Number of images posted to the site by the lister.
image_urls
internet - Does this property have Internet access yes/no.
landmark
latitude
longitude
occupancy - How many adults and children may book the listing.
pageurl
property_address
property_id
property_name
property_type - Home? Hotel? Resort? Etc.
qts - Crawler timestamp.
query_time_stamp - Copy of qts.
room_price
room_types - Number of beds and baths for the room.
search_term
service_value - Whether or not the property is verified with StayZilla (plus some junk entries).
similar_hotel - Some similar listings by name.
sitename
things_to_do - Nearby activities as entered by the lister.
things_to_note - Special notes entered by the lister.
Acknowledgements
This dataset was created by PromptCloud's in-house web-crawling service.
Inspiration
What is the shape of the Indian property-sharing market, and how does it differ from that of, say, the United States? (try comparing this dataset to, say, the Boston AirBnB dataset).
What are the contents of textual descriptions for properties?
Where are StayZilla properties located geographically?"
WWI Bombing Operations,Details on 1441 Allied Runs,United States Air Force,4,"Version 1,2017-09-14","military
war",CSV,1 MB,CC0,973 views,97 downloads,,0 topics,https://www.kaggle.com/usaf/wwi-bombing-operations,"Context:
THOR is a painstakingly cultivated database of historic aerial bombings from World War I through Vietnam. THOR has already proven useful in finding unexploded ordinance in Southeast Asia and improving Air Force combat tactics. Our goal is to see where public discourse and innovation takes this data. Each theater of warfare has a separate data file, in addition to a THOR Overview.
Content:
The THOR data contains data for Allied aircraft carrying a combined bomb load of more than a million pounds over 1,437 recorded missions. This Theater History of Operations (THOR) dataset combines digitized paper mission reports from WWI. It can be searched by date, conflict, geographic location and more than 30 additional data attributes forming a live-action sequence of events. See the data dictionary here and additonal background here.
Acknowledgements:
THOR is a dataset project initiated by Lt Col Jenns Robertson and continued in partnership with Data.mil, an experimental project, created by the Defense Digital Service in collaboration with the Deputy Chief Management Officer and data owners throughout the U.S. military.
Inspiration:
Can you create animated maps of certain campaigns? See, for example, this map of the Argonne-Meuse offensive.
Can you match weather data with campaigns?
Where were the most campaigns?"
Stanford Open Policing Project - Illinois,Data on Traffic and Pedestrian Stops by Police in Illinois,Stanford Open Policing Project,4,"Version 1,2017-07-21","crime
law",Other,1017 MB,Other,612 views,56 downloads,3 kernels,0 topics,https://www.kaggle.com/stanford-open-policing/stanford-open-policing-project-illinois,"Context:
On a typical day in the United States, police officers make more than 50,000 traffic stops. The Stanford Open Policing Project team is gathering, analyzing, and releasing records from millions of traffic stops by law enforcement agencies across the country. Their goal is to help researchers, journalists, and policymakers investigate and improve interactions between police and the public.
If you'd like to see data regarding other states, please go to https://www.kaggle.com/stanford-open-policing.
Content:
This dataset includes over 1 gb of stop data from Illinois, covering all of 2010 onwards. Please see the data readme for the full details of the available fields.
Acknowledgements:
This dataset was kindly made available by the Stanford Open Policing Project. If you use it for a research publication, please cite their working paper: E. Pierson, C. Simoiu, J. Overgoor, S. Corbett-Davies, V. Ramachandran, C. Phillips, S. Goel. (2017) “A large-scale analysis of racial disparities in police stops across the United States”.
Inspiration:
How predictable are the stop rates? Are there times and places that reliably generate stops?
Concerns have been raised about jurisdictions using civil forfeiture as a funding mechanism rather than to properly fight drug trafficking. Can you identify any jurisdictions that may be exhibiting this behavior?"
New York City - 2013 Campaign Contributions,"Donors, recipients, and dollar amounts from the 2013 NYC election cycle",City of New York,4,"Version 2,2017-09-09|Version 1,2017-09-08","money
politics",CSV,5 MB,CC0,580 views,35 downloads,,,https://www.kaggle.com/new-york-city/nyc-2013-campaign-contributions,"Context
A record of every campaign donation made during the 2013 New York City election cycle. This dataset includes the donor, recipient, and dollar amount of contributions to campaigns both for the mayor of New York City (for the 2013-2017 term, which was won by Bill de Blasio) and for the variety of other publicly elected positions in the City of New York.
Content
This dataset includes identifying information about the name of the person and/or corporation which made the donation, the dollar amount of the donation, whether or not it was later refunded (if so, a date of refund is provided), the archetype the donation falls under, the name of the candidate being donated to, and identifying information about the donor (including self-identified work area or profession).
This dataset includes a field for campaign finance codes, read here for more information on what these are.
Acknowledgements
This data is published as-is by the City of New York.
Inspiration
What is the distribution of donations made across candidates?
What is the distribution of small-to-large donations?
Who is doing the donating? How are they spatially distributed through the city?"
Swiss Coins,Count and classify swiss coins from images,AIFirst,4,"Version 2,2017-09-23|Version 1,2017-09-23","money
image data
multiclass classification
counting",Other,33 MB,CC0,957 views,98 downloads,4 kernels,0 topics,https://www.kaggle.com/ai-first/swisscoins,"Context
It is difficult to determine how many coins are in a large pile of money, we want to make an app to automatically count them.
Content
Each folder has a different type of coin (1fr = 1 franc, 50rp = 50 rappen / cents) and since all of the franc coins have the same back we have a fr_back folder and all of the rappen have the same we have a rp_back
Acknowledgements
The data was collected by https://github.com/Zarkonnen/aimeetup_coins
Inspiration
Making a tool to classify coints"
Intermediate point data (Taxi trip duration),First 2000 rows with intermediate point data using Google Maps API,Soumitra Agarwal,4,"Version 1,2017-07-30",,CSV,2 MB,ODbL,879 views,43 downloads,,,https://www.kaggle.com/artimous/intermediate-point-data-taxi-trip-duration,"Context
Realising which routes a taxi takes while going from one location to another gives us deep insights into why some trips take longer than others. Also, most taxis rely on navigation from Google Maps, which reinforces the use case of this dataset. On a deeper look, we can begin to analyse patches of slow traffic and number of steps during the trip (explained below).
Content
The data, as we see it contains the following columns :
trip_id, pickup_latitude, pickup_longitude (and equivalents with dropoff) are picked up from the original dataset.
distance : Estimates the distance between the start and the end latitude, in miles.
start_address and end_address are directly picked up from the Google Maps API
params : Details set of parameters, flattened out into a single line. (Explained below)
Parameters
The parameters field is a long string of a flattened out JSON object. At its very basic, the field has space separated steps. The syntax is as follows :
Step1:{ ... }, Step2:{ ...
Each step denotes the presence of an intermediate point.
Inside the curly braces of each of the steps we have the distance for that step measured in ft, and the start and end location. The start and end location are surrounded by round braces and are in the following format :
Step1:{distance=X ft/mi start_location=(latitude, longitude) end_location ...}, ...
One can split the internal params over space to get all the required values.
Acknowledgements
All the credit for the data goes to the Google Maps API, though limited to 2000 queries per day. I believe that even that limited amount would help us gain great insights.
Future prospects
More data : Since the number of rows processed are just 2000, with a good response we might be able to get more. If you feel like contributing, please have a look at the script here and try and run in for the next 2000 rows.
Driver instructions : I did not include the driver instruction column in the data from the google API as it seemed to complex to use in any kind of models. If that is not the general opinion, I can add it here."
Loans data,Loans data for personal data,karthickveerakumar,4,"Version 2,2017-06-22|Version 1,2017-06-21",,CSV,734 KB,Other,954 views,109 downloads,,2 topics,https://www.kaggle.com/karthickveerakumar/loans-data,Loans data
"Water Levels in Venezia, Italia",Venezia water levels from 1983 to 2015,Luis Bronchal,4,"Version 2,2017-05-24|Version 1,2017-02-08",time series,Other,12 MB,CC0,"1,363 views",85 downloads,,,https://www.kaggle.com/lbronchal/venezia,"Context
This dataset contains hourly measurements of the water level in Venezia from 1983 to 2015.
Content
The original data come from Centro Previsioni e Segnalazioni Maree It has been cleaned and aggregated to obtain this dataset. The original data and the script used in the process can be seen in github
Acknowledgements
Centro Previsioni e Segnalazioni Maree
Inspiration
This dataset can be used to do exploratory analysis and forecasting models."
Colbert 1k,Captions from almost a thousand videos of The Late Show's YouTube channel,Caio Lente,4,"Version 1,2017-06-26","humor
linguistics
internet",CSV,4 MB,Other,"1,782 views",41 downloads,,2 topics,https://www.kaggle.com/ctlente/colbert-1k,"Context
In this dataset you'll find the captions from 990 videos of The Late Show with Stephen Colbert. Captions were cleaned in order to make text mining easier.
Content
This dataset has only three columns: id, link, and captions. The first one contains the video's ID, the second one contains its link, and the last one contains the video's captions.
The data was collected in the week of 2017-06-19 with a script I created. I also tried to collect more information about the videos (like date and title), but YouTube's API was being too stubborn.
Acknowledgements
The content of all videos belong to The Late Show. Captions were extracted with the help of ccSubs.
Inspiration
I collected this data to write the blog post ""Colbert's Fixation""."
Indian Premier League(IPL)Data(till 2016),IPL OLTP tables converted to a Set of CSV files,RaghuReddy,4,"Version 1,2017-06-16",,CSV,7 MB,ODbL,376 views,188 downloads,,,https://www.kaggle.com/raghu07/ipl-datatill-2016,"This is the ball by ball data of all the IPL cricket matches till season 9. (I will be coming up with season 10 data as soon as possible)
Source: http://cricsheet.org/ (data is available on this website in the YAML format. This is converted to CSV format by using R Script ,SQL,SSIS.
Research scope: Predicting the winner of the next season of IPL based on past data, Visualizations, Perspectives, etc."
Rare diseases - Sentiment analysis,Sentiment analysis on rare disease Facebook groups,Natalia,4,"Version 1,2017-07-06","linguistics
medicine
internet",CSV,2 MB,CC0,"1,554 views",119 downloads,,,https://www.kaggle.com/natt77/rare-diseases-sentimient-analysis,"Sentiment analysis on rare disesases Facebook groups.
The origin of this file is 'Rare Diseases on Facebook Groups', also available in Kaggle. This file contained Facebook posts from rare diseases groups, in Spanish.
Based on that file, the post_message was extracted and translated into english using Google Translate. Sentimient analysis was performed to obtain the polarity and subjectivity using Python (Textblob).
The result is this output file, containing the post_message both in spanish and english and the polarity and subjectivity of the message."
Technical Indicator Backtest,1993-2017 SPY Index daily return,Zhijin,4,"Version 1,2017-05-30","finance
technology forecasting",CSV,413 KB,CC0,"1,460 views",87 downloads,8 kernels,0 topics,https://www.kaggle.com/zhijinzhai/technical-indicator-backtest,"Context
Using TA-Lib : Technical Analysis Library. Backtest on the SPY Index data, using support and resistance indicators or any other indicator.
Content
Data contains daily SPY Index: Date Open High Low Close Adj Close Volume
Acknowledgements
Support for Resistance: Technical Analysis WIKI link: https://en.wikipedia.org/wiki/Support_and_resistance
Inspiration
Do your best for the backtest and technical indicator implementation"
Jester Collaborative Filtering Dataset,"70,000+ users' rating of 100 witty jokes",Aakaash Jois,4,"Version 3,2017-06-16|Version 2,2017-06-16|Version 1,2017-06-15",humor,CSV,25 MB,CC4,"1,674 views",144 downloads,,,https://www.kaggle.com/aakaashjois/jester-collaborative-filtering-dataset,"Context
The funniness of joke is very subjective. Having more than 70,000 users rate jokes, can an algorithm be written to identify the universally funny joke?
Content
The data file are in .csv format.
The complete dataset is 100 rows and 73422 columns.
The complete dataset is split into 3 .csv files.
JokeText.csv contains the Id of the joke and the complete joke string.
UserRatings1.csv contains the ratings provided by the first 36710 users.
UserRatings2.csv contains the ratings provided by the last 36711 users.
The dataset is arranged such that the initial users have rated higher number of jokes than the later users.
The rating is a real value between -10.0 and +10.0.
The empty values indicate that the user has not provided any rating for that particular joke.
Acknowledgements
The dataset is associated with the below research paper.
Eigentaste: A Constant Time Collaborative Filtering Algorithm. Ken Goldberg, Theresa Roeder, Dhruv Gupta, and Chris Perkins. Information Retrieval, 4(2), 133-151. July 2001.
More information and datasets can be found at http://eigentaste.berkeley.edu/dataset/
Inspiration
Since funniness is a very subjective matter, it will be very interesting to see if data science can bring out the details on what makes something funny."
Bike Share Daily Data,This is a bike sharing data by Capitol system for year 2011-2012,PradeepKumar,4,"Version 1,2017-07-24",cycling,CSV,1 MB,Other,"1,063 views",143 downloads,,2 topics,https://www.kaggle.com/contactprad/bike-share-daily-data,"==========================================
Bike Sharing Dataset
Hadi Fanaee-T
Laboratory of Artificial Intelligence and Decision Support (LIAAD), University of Porto INESC Porto, Campus da FEUP Rua Dr. Roberto Frias, 378 4200 - 465 Porto, Portugal
=========================================
Background
Bike sharing systems are new generation of traditional bike rentals where whole process from membership, rental and return back has become automatic. Through these systems, user is able to easily rent a bike from a particular position and return back at another position. Currently, there are about over 500 bike-sharing programs around the world which is composed of over 500 thousands bicycles. Today, there exists great interest in these systems due to their important role in traffic, environmental and health issues.
Apart from interesting real world applications of bike sharing systems, the characteristics of data being generated by these systems make them attractive for the research. Opposed to other transport services such as bus or subway, the duration of travel, departure and arrival position is explicitly recorded in these systems. This feature turns bike sharing system into a virtual sensor network that can be used for sensing mobility in the city. Hence, it is expected that most of important events in the city could be detected via monitoring these data.
=========================================
Data Set
Bike-sharing rental process is highly correlated to the environmental and seasonal settings. For instance, weather conditions, precipitation, day of week, season, hour of the day, etc. can affect the rental behaviors. The core data set is related to
the two-year historical log corresponding to years 2011 and 2012 from Capital Bikeshare system, Washington D.C., USA which is publicly available in http://capitalbikeshare.com/system-data. We aggregated the data on two hourly and daily basis and then extracted and added the corresponding weather and seasonal information. Weather information are extracted from http://www.freemeteo.com.
=========================================
Associated tasks
- Regression: 
    Predication of bike rental count hourly or daily based on the environmental and seasonal settings.

- Event and Anomaly Detection:  
    Count of rented bikes are also correlated to some events in the town which easily are traceable via search engines.
    For instance, query like ""2012-10-30 washington d.c."" in Google returns related results to Hurricane Sandy. Some of the important events are 
    identified in [1]. Therefore the data can be used for validation of anomaly or event detection algorithms as well.
=========================================
Files
- Readme.txt
- hour.csv : bike sharing counts aggregated on hourly basis. Records: 17379 hours
- day.csv - bike sharing counts aggregated on daily basis. Records: 731 days
=========================================
Dataset characteristics
Both hour.csv and day.csv have the following fields, except hr which is not available in day.csv
- instant: record index
- dteday : date
- season : season (1:springer, 2:summer, 3:fall, 4:winter)
- yr : year (0: 2011, 1:2012)
- mnth : month ( 1 to 12)
- hr : hour (0 to 23)
- holiday : weather day is holiday or not (extracted from http://dchr.dc.gov/page/holiday-schedule)
- weekday : day of the week
- workingday : if day is neither weekend nor holiday is 1, otherwise is 0.
+ weathersit : 
    - 1: Clear, Few clouds, Partly cloudy, Partly cloudy
    - 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist
    - 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds
    - 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog
- temp : Normalized temperature in Celsius. The values are divided to 41 (max)
- atemp: Normalized feeling temperature in Celsius. The values are divided to 50 (max)
- hum: Normalized humidity. The values are divided to 100 (max)
- windspeed: Normalized wind speed. The values are divided to 67 (max)
- casual: count of casual users
- registered: count of registered users
- cnt: count of total rental bikes including both casual and registered
=========================================
License
Use of this dataset in publications must be cited to the following publication:
[1] Fanaee-T, Hadi, and Gama, Joao, ""Event labeling combining ensemble detectors and background knowledge"", Progress in Artificial Intelligence (2013): pp. 1-15, Springer Berlin Heidelberg, doi:10.1007/s13748-013-0040-3.
@article{ year={2013}, issn={2192-6352}, journal={Progress in Artificial Intelligence}, doi={10.1007/s13748-013-0040-3}, title={Event labeling combining ensemble detectors and background knowledge}, url={http://dx.doi.org/10.1007/s13748-013-0040-3}, publisher={Springer Berlin Heidelberg}, keywords={Event labeling; Event detection; Ensemble learning; Background knowledge}, author={Fanaee-T, Hadi and Gama, Joao}, pages={1-15} }
=========================================
Contact
For further information about this dataset please contact Hadi Fanaee-T (hadi.fanaee@fe.up.pt)"
Tel-Aviv Sublets Posts on Facebook,Find yourself a sublet in Israel's most challenging housing market,sab30226,4,"Version 1,2017-06-19","home
internet",CSV,3 MB,CC0,851 views,52 downloads,,,https://www.kaggle.com/sab30226/telaviv-sublets-posts-on-facebook,"Context
In 2015 I was looking for subletting an apartment in Tel Aviv. Since, housing in Tel-Aviv is in high demand I thought I'll try to scrap the facebook groups of Tel Aviv sublets in order to find one. This dataset is in hebrew mostly, eventhough, some posts are in English. It is best suited for people who are looking for NLP challenges and to try working with an hebrew dataset (Which is a challenge in itself).
Content
The dataset is made of real posts that I've scraped in 2015 from a facebook group for subletting in Tel-Aviv named ""sublets in telaviv for short periods"" or in hebrew סאבלטים בתל אביב לתקופות קצרות.
Acknowledgements
Using the Facebook Graph API which allows to get the data easily.
Inspiration
This dataset holds several challenges. First of all, I wonder how much a sublet for a month should cost in the city and how the prices change over the years (The dataset hold a few years of posts). Also, it can be interesting to find the differences in prices between different regions of the city(הצפון הישן, מרכז העיר, פלורנטין)."
Air-Quality,https://archive.ics.uci.edu/ml/datasets/Air+Quality,Citrahsagala,4,"Version 1,2017-06-16",,CSV,733 KB,Other,"1,014 views",100 downloads,,,https://www.kaggle.com/citrahsagala/airquality,"Context
I get this dataset from UCI Machine Learning. I very interested with this dataset because one of our global warming problem is about air quality in some big city very serious. In UCI ML get this data from sensor device that located in Italy. Also you can read about the dataset in the description.
Content
I get this data from UCI Machine Learning. Here is about descripstion rows and column also another description. ""The dataset contains 9358 instances of hourly averaged responses from an array of 5 metal oxide chemical sensors embedded in an Air Quality Chemical Multisensor Device. The device was located on the field in a significantly polluted area, at road level,within an Italian city. Data were recorded from March 2004 to February 2005 (one year)representing the longest freely available recordings of on field deployed air quality chemical sensor devices responses. Ground Truth hourly averaged concentrations for CO, Non Metanic Hydrocarbons, Benzene, Total Nitrogen Oxides (NOx) and Nitrogen Dioxide (NO2) and were provided by a co-located reference certified analyzer. Evidences of cross-sensitivities as well as both concept and sensor drifts are present as described in De Vito et al., Sens. And Act. B, Vol. 129,2,2008 (citation required) eventually affecting sensors concentration estimation capabilities. Missing values are tagged with -200 value. ""
Acknowledgements
Thank to UCI https://archive.ics.uci.edu/ml/index.php
Inspiration
I would like to see another method to classify or cluster this dataset with timeseries purpose."
Top 1000 Golf Players Historical,The Top 1000 ranked players in Golf from 2000 to 2015,Kelvin Wellington,4,"Version 1,2017-07-18","golf
sports",CSV,24 MB,Other,"1,308 views",124 downloads,,,https://www.kaggle.com/odartey/top-1000-golf-players-historical,"Context
Rankings are a constant phenomenon in society, with a persistent interest in the stratification of items in a set across various disciplines. In sports, rankings are a direct representation of the performance of a team or player over a certain period. Given the straightforward nature of rankings in sports (points based system) there is the opportunity to statistically explore rankings of sports disciplines.
Content
The dataset comprises weekly rankings data of the Top 1000 golf players between Sep 2000 and June April 2015. The data is housed in a single csv file.
Acknowledgements
Data was sourced from the Official Golf World Rankings (OGWR) website: ogwr.com
Inspiration
This dataset could be of use to anyone interested in the distribution of rankings in competitive events"
Pokemon (Gen 7),"Includes stats,abilities,moves among other features.",Ajinkya Jumbad,4,"Version 3,2017-07-13|Version 2,2017-07-13|Version 1,2017-07-12",,Other,119 KB,Other,628 views,23 downloads,,0 topics,https://www.kaggle.com/ajinkyablaze/pokemon-gen-7,"This is my first ever dataset. Any suggestions would be welcome.
There are two main files in this dataset. Pokemon.csv gives a general info on all pokemons, including mega evolutions, but excluding alternate forms of pokemons. The Move.csv gives info about the moves learned by the pokemon BUT only through level ups*.
I would like to thank pokemon.com bulbapedia
The data is yours to play with :) In the future, i might add many more files. I have the data, just need to process it."
Rdatasets,An archive of datasets distributed with different R packages,Rachael Tatman,4,"Version 1,2017-07-12","statistics
programming languages
programming",CSV,241 KB,Other,"1,548 views",101 downloads,,0 topics,https://www.kaggle.com/rtatman/rdatasets,"Context:
Packages for the R programming language often include datasets. This dataset collects information on those datasets to make them easier to find.
Content:
Rdatasets is a collection of 1072 datasets that were originally distributed alongside the statistical software environment R and some of its add-on packages. The goal is to make these data more broadly accessible for teaching and statistical software development.
Acknowledgements:
This data was collected by Vincent Arel-Bundock, @vincentarelbundock on Github. The version here was taken from Github on July 11, 2017 and is not actively maintained.
Inspiration:
In addition to helping find a specific dataset, this dataset can help answer questions about what data is included in R packages. Are specific topics very popular or unpopular? How big are datasets included in R packages? What the naming conventions/trends for packages that include data? What are the naming conventions/trends for datasets included in packages?
License:
This dataset is licensed under the GNU General Public License ."
Clap Emoji in Tweets,Where do clap emojis show up in tweets?,Rachael Tatman,4,"Version 1,2017-07-14","linguistics
human-computer interaction",CSV,713 KB,CC0,"1,035 views",47 downloads,2 kernels,0 topics,https://www.kaggle.com/rtatman/clap-emoji-in-tweets,"Context:
This dataset was collected to answer questions about where in tweets users put clap emoji, especially when clap emoji are used 👏 between 👏 every 👏 word. 👏
Content:
This dataset is made of up information on 27035 unique tweets continaing at least on clap emoji collected on July 7th, 2017. Tweets were collected through Fireant using the Twitter streaming API. For each tweet, every word in the tweet is marked as either the clap emoji or a different word.
Acknowledgements:
This dataset was collected by Rachael Tatman during the process of linguistic research on the clap emoji. A blog post on an analysis of this data can be found here. The dataset here is released to the public domain.
Inspiration:
While this dataset was originally collected to look specific at the clap-word-clap-word pattern, it can also be used to investigate other problems. -Do most tweets which contain the tweet emoji contain more than one? -When multiple claps are used together (👏👏👏) , are they more likely to show up at the beginning or the end of the tweet? -Can you predict the distribution of claps over the tweet? (Perhaps by using a Bernoulli distribution?) -How can you visualize where tweet emoji are used?"
Atlas of Pidgin and Creole Language Structures,Information on 76 Creole and Pidgin Languages,Rachael Tatman,4,"Version 1,2017-07-28","languages
linguistics",CSV,1 MB,Other,704 views,41 downloads,,0 topics,https://www.kaggle.com/rtatman/atlas-of-pidgin-and-creole-language-structures,"Context:
When groups of people who don’t share a spoken language come together, they will often create a new language which combines elements of their first languages. These languages are known as “pidgins”. If they are then learned by children as their first language they become fully-fledged languages known as “creoles”. This dataset contains information on both creoles and pidgins spoken around the world.
Content:
This dataset includes information on the grammatical and lexical structures of 76 pidgin and creole languages. The language set contains not only the most widely studied Atlantic and Indian Ocean creoles, but also less well known pidgins and creoles from Africa, South Asia, Southeast Asia, Melanesia and Australia, including some extinct varieties, and several mixed languages.
This dataset is made up of several tables, each of which contains different pieces of information:
language: A table of language names & the unique id’s associated with them.
language_data: A table of data on the different languages, including the name speakers’ call their language (Autoglossonym), other names the language is called, how many speakers it has, the language which contributed the most words to the language (Major lexifier), other languages which contribute to that language, where it is spoken, and where it is an official language fro. The column language_id has the id linked to the language table.
language_source: The sources referenced on each language (referencing the language and source tables).
langauge_table: Information on the geographic location of each language.
source: Information on the scholarly sources referenced for information on language.
Acknowledgements:
This dataset contains information from the online portion of the Atlas of Pidgin and Creole Language Structures (APiCS). It is distributed under a Creative Commons Attribution 3.0 Unported License . If you use this dataset in your work, please use this citation:
Salikoko S. Mufwene. 2013. Kikongo-Kituba structure dataset. In: Michaelis, Susanne Maria & Maurer, Philippe & Haspelmath, Martin & Huber, Magnus (eds.) Atlas of Pidgin and Creole Language Structures Online. Leipzig: Max Planck Institute for Evolutionary Anthropology. (Available online at http://apics-online.info/contributions/58, Accessed on 2017-07-28.)
Inspiration:
Which areas of the world have the most creoles/pidgins?
Which language has contributed to the most creoles/pidgins? Why might this be?
Can you map the areas of influence of the various lexicalized Major Lexifier languages?
You may also be interested in:
World Language Family Map
The Sign Language Analyses (SLAY) Database
World Atlas of Language Structures: Information on the linguistic structures in 2,679 languages"
New York City Taxi Trip - Hourly Weather Data,Improve Machine Learning with more detailed weather data,Meinertsen,4,"Version 2,2017-08-01|Version 1,2017-07-28",,CSV,1 MB,CC0,"1,030 views",140 downloads,4 kernels,,https://www.kaggle.com/meinertsen/new-york-city-taxi-trip-hourly-weather-data,"Hourly weather data for the New York City Taxi Trip Duration Challange
Here is some detailed weather data for the New York City Taxi Trip Duration Challange. I noticed that many contenders use daily weather data and thought that the ML could be improved with hourly data for NYC (default KNYC station) since pickup_datetime is given. python code on github can return same data for any city
Content
Wundergrounds API provides hourly weather data in JSON format, but I assume most people just want the complete data set in csv. i stands for imperial, m for metric so the difference stands in the relative unit for the returned value (ex. Fahrenheit vs. Celsius).
Note that values will = -9999 or -999 for Null or Non applicable (NA) variables. (replaced with NaN in Version 2) Wundergrounds full Phrase Glossary
datetime: Date and time of day (EST)
tempm: Temperature in Celcius
tempi: Temperature in Fahrenheit
dewptm: Dewpoint in Celcius
dewpti: Dewpoint in Fahrenheit
hum: Humidity %
wspdm: Wind speed in kph
wspdi: Wind speed in mph
wgustm: Wind gust in kph
wgusti: Wind gust in mph
wdird: Wind direction in degrees
wdire: Wind direction description
vism: Vivibility in Km
visi: Visibility in miles
pressurem: Pressure in mBar
pressurei: Pressure in inHg
windchillm: Wind chill in Celcius
windchilli: Wind chill in Fahrenheit
heatindexm: Heat index Celcius
heatindexi: Heat index Fahrenheit
precipm: Precipitation in mm
precipi: Precipitation in inches
conds: Conditions: See full list of conditions
icon
fog: Boolean
rain: Boolean
snow: Boolean
hail: Boolean
thunder: Boolean
tornado: Boolean
Thanks to Wunderground"
Corporate Prosecution Registry,Details of federal cases in the United States against corporations since 2001,University of Virginia,4,"Version 1,2017-06-27","business
crime",CSV,924 KB,Other,852 views,72 downloads,,0 topics,https://www.kaggle.com/university-of-virginia/corporate-prosecution-registry,"Context
The goal of this Corporate Prosecutions Registry is to provide comprehensive and up-to-date information on federal organizational prosecutions in the United States, so that we can better understand how corporate prosecutions are brought and resolved. It includes detailed information about every federal organizational prosecution since 2001, as well as deferred and non-prosecution agreements with organizations since 1990.
Dataset Description
These data on deferred prosecution and non-prosecution agreements were collected by identifying agreements through news searches, press releases by the Department of Justice and U.S. Attorney’s Office, and also when practitioners brought agreements to our attention. The Government Accountability Office conducted a study of federal deferred prosecution and non-prosecution agreements with organizations, and in August 2010, the GAO provided a list of those agreements in response to an information request. Finally, searches of the Bloomberg dockets database located additional prosecution agreements with companies that had not previously been located. Jon Ashley has contacted U.S. Attorney’s Offices to request agreements. An effort by the First Amendment Clinic at the University of Virginia School of Law to litigate Freedom of Information Act requests resulted in locating a group of missing agreements which are now available on the Registry.
This Registry only includes information about federal organizational prosecutions, and not cases brought solely in state courts. Nor does this Registry include leniency agreements entered through the Antitrust Division’s leniency program, which are kept confidential. The Registry also does not include convictions overturned on appeal, or cases in which the indictment was dismissed or the company was acquitted at a trial.
The U.S. Sentencing Commission reports sentencing data concerning organizational prosecutions each year. That data does not include cases resolved without a formal sentencing, such as deferred and non-prosecution agreements.
Acknowledgements
The Corporate Prosecutions Registry is a project of the University of Virginia School of Law. It was created by Professor Brandon Garrett (bgarrett@virginia.edu) and Jon Ashley (jaa6c@virginia.edu). Please cite this dataset as: “Brandon L. Garrett and Jon Ashley, Corporate Prosecutions Registry, University of Virginia School of Law, at http://lib.law.virginia.edu/Garrett/corporate-prosecution-registry/index.html”
Inspiration
Which industries face the most prosecutions?
Which government organizations have been the most successful at pursuing cases against corporations?
Not a single case in the dataset led to a trial conviction. Can you link these corporate cases to criminal cases against the individuals involved? How many of them were convicted instead?"
Stanford Open Policing Project - Ohio,Data on Traffic and Pedestrian Stops by Police in Ohio,Stanford Open Policing Project,4,"Version 1,2017-07-25","government agencies
crime
law",Other,988 MB,Other,749 views,71 downloads,,0 topics,https://www.kaggle.com/stanford-open-policing/stanford-open-policing-project-ohio,"Context:
On a typical day in the United States, police officers make more than 50,000 traffic stops. The Stanford Open Policing Project team is gathering, analyzing, and releasing records from millions of traffic stops by law enforcement agencies across the country. Their goal is to help researchers, journalists, and policymakers investigate and improve interactions between police and the public.
If you'd like to see data regarding other states, please go to https://www.kaggle.com/stanford-open-policing.
Content:
This dataset includes over 1 gb of stop data from Ohio. Please see the data readme for the full details of the available fields.
Acknowledgements:
This dataset was kindly made available by the Stanford Open Policing Project. If you use it for a research publication, please cite their working paper: E. Pierson, C. Simoiu, J. Overgoor, S. Corbett-Davies, V. Ramachandran, C. Phillips, S. Goel. (2017) “A large-scale analysis of racial disparities in police stops across the United States”.
Inspiration:
How predictable are the stop rates? Are there times and places that reliably generate stops?
Concerns have been raised about jurisdictions using civil forfeiture as a funding mechanism rather than to properly fight drug trafficking. Can you identify any jurisdictions that may be exhibiting this behavior?"
"Cook County Asset Forfeiture (Chicago, IL)","A FOIA request for asset forfeiture in Cook County, IL",Reason Foundation,4,"Version 1,2017-07-21",,CSV,3 MB,CC0,446 views,19 downloads,,0 topics,https://www.kaggle.com/reason-foundation/cook-county-asset-forfeiture-chicago-il,"Context
""Law enforcement in Cook County, which includes Chicago, seized items from residents ranging from a cashier's check for 34 cents to a 2010 Rolls Royce Ghost with an estimated value of more than $200,000. They also seized Xbox controllers, televisions, nunchucks, 12 cans of peas, a pair of rhinestone cufflinks, and a bayonet.""
-Reason.com
Content
There wasn't much documentation on the dataset, but the data fields are somewhat explanatory.
InventNumb: Inventory number
PolRptNumb: Police report number
INumber: ?
SeizeDate: Date siezed
SeizeAddress: Address where it was siezed
SeizeCity: City where it was siezed
SeizeState: State where it was siezed
SeizeZip: Zip code for where it was siezed
InvItemNumb: Invenotry Item number?
Descr: Description of item
EstValue: estimated value of item
Vin: VIN number? (vehicles)
Findings: ?
ForfDate: Forfieture date?
ForfValue: Value of forfieture?
CaseNumb: Case number
AD: ?
ADDate: Date?
Acknowledgements
This dataset is the result of FOIA request by Lucy Parsons Labs, a Chicago-based transparency non-profit. It contains information about to the seizures of assets in Cook County (Chicago, IL).
These data were presented in a Reason.com article showing the disparity of Police seizures between poor and wealthier parts of Cook County, IL. http://reason.com/blog/2017/06/13/poor-neighborhoods-hit-hardest-by-asset
The original excel file was converted to CSV
Inspiration
This dataset comes from a Freedom of Information Act request. I love datasets like these because of the potential for finding new socioeconomic insights and improving government accountability."
QBI Image Segmentation,Segmentation and Feature Extraction from Binary Images on Fossil Data,Kevin Mader,4,"Version 2,2017-03-16|Version 1,2017-03-16",,Other,41 MB,CC0,"1,359 views",79 downloads,5 kernels,,https://www.kaggle.com/kmader/qbi-image-segmentation,"Image Segmentation
The data and kernels here are connected to the lecture material on image segmentation from the Quantitative Big Imaging Course at ETH Zurich (kmader.github.io/Quantitative-Big-Imaging-2017). The specific description of the exercise tasks can be found (https://github.com/kmader/Quantitative-Big-Imaging-2016/blob/master/Exercises/03-Description.md)"
Swiss Rail Plan,"The locations, timetables, and fare information for the SBB/CFF/FFS Rail Network",Kevin Mader,4,"Version 3,2017-04-18|Version 2,2017-04-18|Version 1,2017-04-14",rail transport,CSV,315 MB,Other,"1,529 views",85 downloads,3 kernels,,https://www.kaggle.com/kmader/swiss-rail-plan,"Introduction
The basic inspiration was the inability to search through lots of alternative routes while traveling over Easter weekend and having to manually to point-to-point searches. The hope is by using a bit of R/Python the search for the best routes can be made a lot easier.
Data Structure
The data is organized in a format called GTFS which is explained in detail here but only available in German. Kernels should make it clear how to work with most of the data
Source / Attribution
The data all comes from opentransportdata.swiss and can be downloaded in the original format by following this link"
CoMNIST,Cyrillic-oriented MNIST: A dataset of Latin and Cyrillic letter images,gregv,4,"Version 1,2017-04-10","writing
russia
linguistics
+ 2 more...",Other,105 MB,CC4,"2,246 views",165 downloads,,,https://www.kaggle.com/gregvial/comnist,"Cyrillic-oriented MNIST
CoMNIST services
A repository of images of hand-written Cyrillic and Latin alphabet letters for machine learning applications.
The repository currently consists of 20,000+ 278x278 png images representing all 33 letters of the Russian alphabet and the 26 letters of the English alphabet. Find original source on my github These images have been hand-written on touch screen through crowd-sourcing.
The dataset will be regularly extended with more data as the collection progresses
An API that reads words in images
CoMNIST also makes available a web service that reads drawing and identifies the word/letter you have drawn. On top of an image you can submit an expected word and get back the original image with mismtaches highlighted (for educational purposes)
The API is available at this address: http://35.187.34.5:5002/api/word It is accessible via a POST request with following input expected: { 'img': Mandatory b64 encoded image, with letters in black on a white background 'word': Optional string, the expected word to be read 'lang': Mandatory string, either 'en' or 'ru', respectively for Latin or Cyrillic (russian) alphabets 'nb_output': Mandatory integer, the ""tolerance"" of the engine }
The return information is the following: { 'img': b64 encoded image, if a word was supplied as an input, then modified version of that image highlighting mismatches 'word': string, the word that was read by the API }
Participate
The objective is to gather at least 1000 images of each class, therefore your contribution is more that welcome! One minute of your time is enough, and don't hesitate to ask your friends and family to participate as well.
English version - Draw Latin only + common to cyrillic and latin
French version - Draw Latin only + common to cyrillic and latin
Russian version - Draw Cyrillic only
Find out more about CoMNIST on my blog
Credits and license
A big thanks to all the contributors!
These images have been crowd-sourced thanks to the great web-design by Anna Migushina available on her github.
CoMNIST logo by Sophie Valenina"
Ultimate 25k+ Matches Football Database -European,"25k+ matches, players & teams attributes for European Professional Football",Prajit Datta,4,"Version 1,2016-12-23",,SQLite,299 MB,ODbL,"3,662 views",205 downloads,2 kernels,2 topics,https://www.kaggle.com/prajitdatta/ultimate-25k-matches-football-database-european,"**The ultimate Soccer database for data analysis and machine learning
What you get:**
+25,000 matches
+10,000 players
11 European Countries with their lead championship
Seasons 2008 to 2016
Players and Teams' attributes* sourced from EA Sports' FIFA video game series, including the weekly updates - - Team line up with squad formation (X, Y coordinates)
Betting odds from up to 10 providers
Detailed match events (goal types, possession, corner, cross, fouls, cards etc...) for +10,000 matches
*16th Oct 2016: New table containing teams' attributes from FIFA !
Original Data Source:
You can easily find data about soccer matches but they are usually scattered across different websites. A thorough data collection and processing has been done to make your life easier. I must insist that you do not make any commercial use of the data. The data was sourced from:
http://football-data.mx-api.enetscores.com/ : scores, lineup, team formation and events
http://www.football-data.co.uk/ : betting odds. Click here to understand the column naming system for betting odds:
http://sofifa.com/ : players and teams attributes from EA Sports FIFA games. FIFA series and all FIFA assets property of EA Sports.
When you have a look at the database, you will notice foreign keys for players and matches are the same as the original data sources. I have called those foreign keys ""api_id"".
Improving the dataset:
You will notice that some players are missing from the lineup (NULL values). This is because I have not been able to source their attributes from FIFA. This will be fixed overtime as the crawling algorithm is being improved. The dataset will also be expanded to include international games, national cups, Champion's League and Europa League. Please ask me if you're after a specific tournament.
Please get in touch with Hugo Mathien if you want to help improve this dataset.
CLICK HERE TO ACCESS THE PROJECT GITHUB
Important note for people interested in using the crawlers: since I first wrote the crawling scripts (in python), it appears sofifa.com has changed its design and with it comes new requirements for the scripts. The existing script to crawl players ('Player Spider') will not work until i've updated it.
Exploring the data:
Now that's the fun part, there is a lot you can do with this dataset. I will be adding visuals and insights to this overview page but please have a look at the kernels and give it a try yourself ! Here are some ideas for you:
The Holy Grail... ... is obviously to predict the outcome of the game. The bookies use 3 classes (Home Win, Draw, Away Win). They get it right about 53% of the time. This is also what I've achieved so far using my own SVM. Though it may sound high for such a random sport game, you've got to know that the home team wins about 46% of the time. So the base case (constantly predicting Home Win) has indeed 46% precision.
Probabilities vs Odds
When running a multi-class classifier like SVM you could also output a probability estimate and compare it to the betting odds. Have a look at your variance vs odds and see for what games you had very different predictions.
Explore and visualize features
With access to players and teams attributes, team formations and in-game events you should be able to produce some interesting insights into The Beautiful Game . Who knows, Guardiola himself may hire one of you some day! Database released under Open Database License, individual papers copyright their original authors"
Red Wine Dataset,This is a subset of wine quality dataset which contains only red wine samples,piyushgoyal443,4,"Version 1,2017-03-05",,Other,97 KB,ODbL,"10,619 views",740 downloads,11 kernels,,https://www.kaggle.com/piyushgoyal443/red-wine-dataset,"Citation Request: This dataset is public available for research. The details are described in [Cortez et al., 2009]. Please include this citation if you plan to use this database:
P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553. ISSN: 0167-9236.
Available at: [@Elsevier] http://dx.doi.org/10.1016/j.dss.2009.05.016 [Pre-press (pdf)] http://www3.dsi.uminho.pt/pcortez/winequality09.pdf [bib] http://www3.dsi.uminho.pt/pcortez/dss09.bib
Title: Wine Quality
Sources Created by: Paulo Cortez (Univ. Minho), Antonio Cerdeira, Fernando Almeida, Telmo Matos and Jose Reis (CVRVV) @ 2009
Past Usage:
P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553. ISSN: 0167-9236.
In the above reference, two datasets were created, using red and white wine samples. The inputs include objective tests (e.g. PH values) and the output is based on sensory data (median of at least 3 evaluations made by wine experts). Each expert graded the wine quality between 0 (very bad) and 10 (very excellent). Several data mining methods were applied to model these datasets under a regression approach. The support vector machine model achieved the best results. Several metrics were computed: MAD, confusion matrix for a fixed error tolerance (T), etc. Also, we plot the relative importances of the input variables (as measured by a sensitivity analysis procedure).
Relevant Information:
The two datasets are related to red and white variants of the Portuguese ""Vinho Verde"" wine. For more details, consult: http://www.vinhoverde.pt/en/ or the reference [Cortez et al., 2009]. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).
These datasets can be viewed as classification or regression tasks. The classes are ordered and not balanced (e.g. there are munch more normal wines than excellent or poor ones). Outlier detection algorithms could be used to detect the few excellent or poor wines. Also, we are not sure if all input variables are relevant. So it could be interesting to test feature selection methods.
Number of Instances: red wine - 1599; white wine - 4898.
Number of Attributes: 11 + output attribute
Note: several of the attributes may be correlated, thus it makes sense to apply some sort of feature selection.
Attribute information:
For more information, read [Cortez et al., 2009].
Input variables (based on physicochemical tests): 1 - fixed acidity (tartaric acid - g / dm^3) 2 - volatile acidity (acetic acid - g / dm^3) 3 - citric acid (g / dm^3) 4 - residual sugar (g / dm^3) 5 - chlorides (sodium chloride - g / dm^3 6 - free sulfur dioxide (mg / dm^3) 7 - total sulfur dioxide (mg / dm^3) 8 - density (g / cm^3) 9 - pH 10 - sulphates (potassium sulphate - g / dm3) 11 - alcohol (% by volume) Output variable (based on sensory data): 12 - quality (score between 0 and 10)
Missing Attribute Values: None
Description of attributes:
1 - fixed acidity: most acids involved with wine or fixed or nonvolatile (do not evaporate readily)
2 - volatile acidity: the amount of acetic acid in wine, which at too high of levels can lead to an unpleasant, vinegar taste
3 - citric acid: found in small quantities, citric acid can add 'freshness' and flavor to wines
4 - residual sugar: the amount of sugar remaining after fermentation stops, it's rare to find wines with less than 1 gram/liter and wines with greater than 45 grams/liter are considered sweet
5 - chlorides: the amount of salt in the wine
6 - free sulfur dioxide: the free form of SO2 exists in equilibrium between molecular SO2 (as a dissolved gas) and bisulfite ion; it prevents microbial growth and the oxidation of wine
7 - total sulfur dioxide: amount of free and bound forms of S02; in low concentrations, SO2 is mostly undetectable in wine, but at free SO2 concentrations over 50 ppm, SO2 becomes evident in the nose and taste of wine
8 - density: the density of water is close to that of water depending on the percent alcohol and sugar content
9 - pH: describes how acidic or basic a wine is on a scale from 0 (very acidic) to 14 (very basic); most wines are between 3-4 on the pH scale
10 - sulphates: a wine additive which can contribute to sulfur dioxide gas (S02) levels, wich acts as an antimicrobial and antioxidant
11 - alcohol: the percent alcohol content of the wine
Output variable (based on sensory data): 12 - quality (score between 0 and 10)"
NYC Uber Pickups with Weather and Holidays,"Subset of Uber pickup data with weather, borough, and holidays",Yannis Pappas,4,"Version 1,2017-02-14",,CSV,2 MB,CC0,"1,947 views",189 downloads,,0 topics,https://www.kaggle.com/yannisp/uber-pickups-enriched,"Context
This is a forked subset of the Uber Pickups in New York City, enriched with weather, borough, and holidays information.
Content
I created this dataset for a personal project of exploring and predicting pickups in the area by merging data that intuitively seem possible factors for the analysis. These are:
Uber Pickups in New York City, from 01/01/2015 to 30/06/2015 (uber-raw-data-janjune-15.csv). (by FiveThirtyEight via kaggle.com)
Weather data from National Centers for Environmental Information.
LocationID to Borough mapping. (by FiveThirtyEight)
NYC public holidays.
The main dataset contained over 10 million observations of 4 variables which aggregated per hour and borough, and then joined with the rest of the datasets producing 29,101 observations across 13 variables. These are:
pickup_dt: Time period of the observations.
borough: NYC's borough.
pickups: Number of pickups for the period.
spd: Wind speed in miles/hour.
vsb: Visibility in Miles to nearest tenth.
temp: temperature in Fahrenheit.
dewp: Dew point in Fahrenheit.
slp: Sea level pressure.
pcp01: 1-hour liquid precipitation.
pcp06: 6-hour liquid precipitation.
pcp24: 24-hour liquid precipitation.
sd: Snow depth in inches.
hday: Being a holiday (Y) or not (N).
Acknowledgements / Original datasets:
Uber Pickups in New York City, from 01/01/2015 to 30/06/2015 (uber-raw-data-janjune-15.csv). (by FiveThirtyEight via kaggle.com)
Weather data from National Centers for Environmental Information.
LocationID to Borough mapping. (by FiveThirtyEight)
(Picture credits: Buck Ennis)"
Welfare Error Rates,Unemployment Error and Fraud / SNAP Payment Errors,BrandtCowan,4,"Version 1,2017-02-04","finance
politics",CSV,4 KB,Other,"1,666 views",88 downloads,,,https://www.kaggle.com/brandtcowan/govtassistpayerrors,"Context
Data related to estimated fraud or payment errors for unemployment and food assistance, used in discussion about alleged widespread fraud and abuse of welfare.
Content
Data pulled from Gov't Websites on February 3rd, 2017.
SNAP rates come from :https://www.fns.usda.gov/sites/default/files/snap/2014-rates.pdf UI Error data comes from : https://www.dol.gov/general/maps/data > > > 2016 IPIA 1-year data [07/01/2015 - 06/30/2016] >> tab: Integrity Rates with CI
Inspiration
Want to plot this against election results or possibly find polling data about alleged abuse of welfare system to match against actual findings"
"100,000 Random Internet Domain Names",Sample selection from registred domain names lists.,Eugene,4,"Version 1,2017-01-09",,CSV,2 MB,CC0,595 views,37 downloads,,0 topics,https://www.kaggle.com/domainsindex/100000-random-internet-domain-names,"Content
In case you want to test a crawler, verify marketing ideas, we prepared a random dataset which includes 100,000 of registered domain names."
Spanish Region and Election Results,Characteristics of 8119 regions of Spain,BTH Project,4,"Version 1,2017-01-14",,CSV,2 MB,Other,"1,345 views",41 downloads,,,https://www.kaggle.com/mlprojectbth/spanish-region-and-election-results,"Context
There's a story behind every dataset and here's your opportunity to share yours.
This dataset collects characteristics of the population in each region (age distribution, unemployment rate, immigration percent and primary economic sector) and cross it with the votes per each political part.
It has 52 fields:
1) Code [String]: Region code of the different Spanish areas. There are 8126 different regions, but the dataset only contains 8119, because some sources were incomplete.
2) RegionName [String]: Name of the region.
3) Population [Int]: Amount of people living in that area (1st January 2015)
4) TotalCensus [Int]: Number of people over 18 years old, which means that can vote.
5) TotalVotes [Int]: Number of total votes.
6) AbstentionPtge [Float]: Percent of the people that have not votes in the election. (TotalCensus-TotalVotes)/TotalCensus*100 %
7) BlankVotesPtge [Float]: Percent of votes that were blank. Calculated as follows: BlankVotes/TotalVotes*100 %
8) NullVotesPtge [Float]: Percent of votes that were null. Calculated as follows: NullVotes/TotalVotes*100 %
9) PP_Ptge [Float]: Percent of the votes given to the political party called “Partido Popular”. (PP_Votes)/TotalVotes*100 %
10) PSOE_Ptge [Float]: Percent of the votes given to the political party called “Partido Socialista Obrero Español” (PSOE_Votes)/TotalVotes*100 %
11) Podemos_Ptge [Float]: Percent of the votes given to the political party called “Podemos” (Podemos_Votes)/TotalVotes*100 %
12) Ciudadanos_Ptge [Float]: Percent of the votes given to the political party called “Ciudadanos” (Ciudadanos_Votes)/TotalVotes*100 %
13) Others_Ptge [Float]: Percent of the votes given to the others political parties (∑▒MinoritaryVotes)/TotalVotes*100 %
14) Age_0-4_Ptge [Float]: Percent of the populations which age is between 0 and 4 years old. It is calculated as follows: (Number of people in (0-4))/TotalPopulation*100 %
15) Age_5-9_Ptge [Float]: Percent of the populations which age is between 5 and 9 year old.
16) Age_10-14_Ptge [Float]: Percent of the populations which age is between 10 and 14 years old
17) Age_15-19_Ptge [Float]: Percent of the populations which age is between 15 and 19 years old
18) Age_20-24_Ptge [Float]: Percent of the populations which age is between 20 and 24 years old
19) Age_25-29_Ptge [Float]: Percent of the populations which age is between 25 and 29 years old
20) Age_30-34_Ptge [Float]: Percent of the populations which age is between 30 and 34 years old
21) Age_35-39_Ptge [Float]: Percent of the populations which age is between 35 and 39 years old
22) Age_40-44_Ptge [Float]: Percent of the populations which age is between 40 and 44 years old
23) Age_45-49_Ptge [Float]: Percent of the populations which age is between 45 and 49 years old
24) Age_50-54_Ptge [Float]: Percent of the populations which age is between 50 and 54 years old
25) Age_55-59_Ptge [Float]: Percent of the populations which age is between 55 and 59 years old
26) Age_60-64_Ptge [Float]: Percent of the populations which age is between 60 and 64 years old
27) Age_65-69_Ptge [Float]: Percent of the populations which age is between 65 and 69 years old
28) Age_70-74_Ptge [Float]: Percent of the populations which age is between 70 and 74 years old
29) Age_75-79_Ptge [Float]: Percent of the populations which age is between 75 and 79 year old
30) Age_80-84_Ptge [Float]: Percent of the populations which age is between 80 and 84 years old
31) Age_85-89_Ptge [Float]: Percent of the populations which age is between 85 and 89 year old
32) Age_90-94_Ptge [Float]: Percent of the populations which age is between 90 and 94 years old
33) Age_95-99_Ptge [Float]: Percent of the populations which age is between 95 and 99 years old
34) Age_100+_Ptge [Float]: Percent of the populations which is older than 100 years old.
35) ManPopulationPtge [Float]: Percentage of masculine population in a region. Calculated as follows: ManPopulation/TotalPopulation*100
36) WomanPopulationPtge [Float]: Percentage of masculine population in a region. Calculated as follows: WomanPopulation/TotalPopulation*100
37) SpanishPtge [Float]: Percentage of people with spanish nationality in a region. Calculated as follows: NativeSpanishPopulation/TotalPopulation*100
38) ForeignersPtge [Float]: Percentage of foreign people in a region. Calculated as follows: ForeignPopulation/TotalPopulation*100
39) SameComAutonPtge [Float]: Percentage of people who live in the same autonomic community (same province) that was born. Calculated as follows: SameComAutonPopulation/TotalPopulation*100
40) SameComAutonDiffProvPtge [Float]: Percentage of people who live in the same autonomic community (different province) that was born. Calculated as follows: SameComAutonDiffProvPopulation/TotalPopulation*100
41) DifComAutonPtge [Float]: Percentage of people who live in different autonomic community that was born. Calculated as follows: SameComAutonDiffProvPopulation/TotalPopulation*100
42) UnemployLess25_Ptge [Float]: Percent of unemployed people that are under 25 years and older than 18. It is calculated over the total amount of unemployment. (UnemploymentLess25_Man+ UnemploymentLess25_Woman)/TotalUnemployment*100
43) Unemploy25_40_Ptge [Float]: Percent of unemployed people that are 25-40 years over the total amount of unemployment. (Unemployment(25-40)_Man+ Unemployment(25-40)_Woman )/TotalUnemployment*100
44) UnemployMore40_Ptge [Float]: Percent of unemployed people that are older that 40 and younger than 69 years over the total amount of unemployment. (Unemployment(40-69)_Man+Unemployment(40-69)_Woman)/TotalUnemployment*100
45) UnemployLess25_population_Ptge [Float]: Percent of unemployed people younger than 25 and older than 18, over the total population of the region. Note that the percent is calculated over the total population and not over the total active population. (UnemploymentLess25_Man+ UnemploymentLess25_Woman)/TotalPopulation*100
46) Unemploy25_40_population_Ptge [Float]: Percent of unemployed people (25-40) years old, over the total population of the region. Note that the percent is calculated over the total population and not over the total active population. (Unemployment(25-40)_Man+ Unemployment(25-40)_Woman )/TotalPopulation*100
47) UnemployMore40_population_Ptge [Float]: Percent of unemployed people (40-69) years old, over the total population of the region. Note that the percent is calculated over the total population and not over the total active population. (UnemploymentLess25_Man+ UnemploymentLess25_Woman)/TotalPopulation*100
48) AgricultureUnemploymentPtge [Float]: Percent of unemployment in the agriculture sector relative to the total amount of unemployment. PeopleUnemployedInAgriculture/TotalUnemployment*100
49) IndustryUnemploymentPtge [Float]: Percent of unemployment in the industry sector relative to the total amount of unemployment. PeopleUnemployedInIndustry/TotalUnemployment*100
50) ConstructionUnemploymentPtge [Float]: Percent of unemployment in the construction sector relative to the total amount of unemployment. PeopleUnemployedInConstruction/TotalUnemployment*100
51) ServicesUnemploymentPtge [Float]: Percent of unemployment in the services sector relative to the total amount of unemployment. PeopleUnemployedInServices/TotalUnemployment*100
52) NotJobBeforeUnemploymentPtge [Float]: Percent of unemployment of people that didn’t have an employ before, over the total amount of unemployment. PeopleUnemployedWithoutEmployBefore/TotalUnemployment*100
References:
[1] Unemployment: www.datos.gob.es/es/catalogo/e00142804-paro-registrado-por-municipios
[2] Age distribution per region Relation between Spanish and foreigners Relation between woman and man Relation between people born in the same area or different areas of Spain http://www.ine.es/dynt3/inebase/index.htm?type=pcaxis&file=pcaxis&path=%2Ft20%2Fe245%2Fp05%2F%2Fa2015
[3] Congress elections result of Spanish election (June 2016) http://www.infoelectoral.interior.es/min/areaDescarga.html?method=inicio"
A Recruiter Year in Review!,Weekly recruiting activity,JBD,4,"Version 3,2017-01-20|Version 2,2017-01-17|Version 1,2017-01-17",employment,CSV,289 KB,CC4,"2,038 views",140 downloads,,,https://www.kaggle.com/jdirmeitis/how-or-what-is-my-team-doing,"Context
I am the Director of a technical recruiting team and I would love to get a better look at their peaks, valleys, hurdles, and hot spots to see how i can better manager and show them the value in process improvements. The data is dirty at best they didnt always track or enter in the correct pieces. I have been here for 6 months and it has been a steady improvement. I welcome any recommendations or data you can suggest.
You will see our client numbers from open projects, to the activity and team production
Content
I am attaching what i call the MBO form ""Management by Objective"". The form is completed weekly by my team to help track how they are doing from an activity perspective. Its a name i'm living with not loving just yet. This data is a year long look into how many people they screened how many people they submitted to projects and how many people they put on projects. The other issue is some of the data changed when i joined the team so please look at in in 2 pieces. Jan - June 2016 July - Dec 2016... I just started in June and added more granularity. There has also been some turn over so you will see partial data from some of my team
Acknowledgements
My team is responsible for data gathering, im responsible for review and strategy adjustments
Inspiration
I would like to get a better view into what their energy cycle looks like. (they work in waves). I would like to know what the sweet spot for motivation might be. (early in the month-later in the month) I would like to identify activity trends on a week to week basis Help me understand if this is a constant - consistent level of activity or if i can get more juice from this orange I also dont know what i dont know... any insight is greatly appreciated"
Interactive Fiction Competition Entrants,Ranking and background data for IFComp games,Brian Rushton,4,"Version 4,2017-04-18|Version 3,2017-04-15|Version 2,2017-02-03|Version 1,2017-02-03","literature
video games",CSV,99 KB,CC4,"1,834 views",30 downloads,,,https://www.kaggle.com/brirush/ifcomp,"Context
The Interactive Fiction Competition is the internet's oldest game programming competition, starting in 1995 and continuing to this day. Interactive Fiction is an unusual genre because it has been centralized in the Interactive Fiction Database for the last decade, and essentially all games of interest are recorded there. It is also a niche genre, and the amount of public interest has varied much less than other genres since 1995.
Content
This database contains the placement (i.e. 1st place, 2nd place, etc.) of all games ever entered into the IFComp, together with their genre, number of ifdb ratings and average ifdb rating. It also contains the system; a few programming languages have tended to dominate the competition. It also contains the forgiveness rating, which indicates how difficult the game can be expected to be. Much of the information is null.
Note that the IFDB was created in 2006, 11 years after the competition began.
Inspiration
IFDB ratings represent lasting interest, as these have been gathered over many years, while IFComp rankings represent a short, 6-week period.
It would be interesting to see the relationship between the two, how that relationship has changed over time, etc.
Also, Inform and TADS have always dominated the competition, but it's recently been changing. The change in systems over the years should be interesting.
Finally, it would be interesting to see time-adjusted averages of reviews for comp games over time, to see if new games are receiving less interest than old games.
Acknowledgements
Mike Roberts has been the curator of IFDB since its inception. The IFComp is currently run by Jason Macintosh under the aegis of the Interactive Fiction Technology Foundation."
Breast Cancer Wisconsin (Prognostic) Data Set,From: UCI Machine Learning Repository,Sarah VCH,4,"Version 1,2017-04-01",,CSV,122 KB,ODbL,"2,861 views",284 downloads,3 kernels,,https://www.kaggle.com/sarahvch/breast-cancer-wisconsin-prognostic-data-set,"Context
Data From: UCI Machine Learning Repository http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wpbc.names
Content
""Each record represents follow-up data for one breast cancer case. These are consecutive patients seen by Dr. Wolberg since 1984, and include only those cases exhibiting invasive breast cancer and no evidence of distant metastases at the time of diagnosis.
The first 30 features are computed from a digitized image of a
fine needle aspirate (FNA) of a breast mass.  They describe
characteristics of the cell nuclei present in the image.
A few of the images can be found at
http://www.cs.wisc.edu/~street/images/

The separation described above was obtained using
Multisurface Method-Tree (MSM-T) [K. P. Bennett, ""Decision Tree
Construction Via Linear Programming."" Proceedings of the 4th
Midwest Artificial Intelligence and Cognitive Science Society,
pp. 97-101, 1992], a classification method which uses linear
programming to construct a decision tree.  Relevant features
were selected using an exhaustive search in the space of 1-4
features and 1-3 separating planes.

The actual linear program used to obtain the separating plane
in the 3-dimensional space is that described in:
[K. P. Bennett and O. L. Mangasarian: ""Robust Linear
Programming Discrimination of Two Linearly Inseparable Sets"",
Optimization Methods and Software 1, 1992, 23-34].

The Recurrence Surface Approximation (RSA) method is a linear
programming model which predicts Time To Recur using both
recurrent and nonrecurrent cases.  See references (i) and (ii)
above for details of the RSA method. 

This database is also available through the UW CS ftp server:

ftp ftp.cs.wisc.edu
cd math-prog/cpo-dataset/machine-learn/WPBC/
1) ID number 2) Outcome (R = recur, N = nonrecur) 3) Time (recurrence time if field 2 = R, disease-free time if field 2 = N) 4-33) Ten real-valued features are computed for each cell nucleus:
a) radius (mean of distances from center to points on the perimeter)
b) texture (standard deviation of gray-scale values)
c) perimeter
d) area
e) smoothness (local variation in radius lengths)
f) compactness (perimeter^2 / area - 1.0)
g) concavity (severity of concave portions of the contour)
h) concave points (number of concave portions of the contour)
i) symmetry 
j) fractal dimension (""coastline approximation"" - 1)""
Acknowledgements
Creators:
Dr. William H. Wolberg, General Surgery Dept., University of
Wisconsin,  Clinical Sciences Center, Madison, WI 53792
wolberg@eagle.surgery.wisc.edu

W. Nick Street, Computer Sciences Dept., University of
Wisconsin, 1210 West Dayton St., Madison, WI 53706
street@cs.wisc.edu  608-262-6619

Olvi L. Mangasarian, Computer Sciences Dept., University of
Wisconsin, 1210 West Dayton St., Madison, WI 53706
olvi@cs.wisc.edu 
Inspiration
I'm really interested in trying out various machine learning algorithms on some real life science data."
Annotated Corpus for Named Entity Recognition,Corpus annotated with BIO and POS tags,Anton Dmitriev,4,"Version 1,2017-04-18",,Other,2 MB,Other,"1,386 views",177 downloads,,,https://www.kaggle.com/velavok/nercorpus,"Annotated (BIO) Corpus for Named Entity Recognition
This corpus is made up of texts of news sites and built specifically to train the classifier to predict named entities such as PER, LOC, etc.
Annotation scheme: Sentence Parts of speech BIO tags
Number of tagged entities:
'O': 167112 'PER': 11692 'ORG': 9736 'LOC': 8431 'MISC': 4195 Count of sentences: 14000
Essential info about entities:
O = Other PER = Person ORG = Organization LOC = Geographical Entity MISC = Miscellaneous"
INDUSTRIAL INTERNET OF THINGS DATA,DEMAND V/S RESPONSE DATA FOR IoT ANALYTICS,MACHINE LEARNING DATASETS,4,"Version 1,2017-05-18",,CSV,656 KB,CC0,"2,526 views",185 downloads,,,https://www.kaggle.com/pitasr/industrialiot,"Industrial demand/ response IoT data for IoT analytics. Can be used for academic purpose only.
Highlights
•
Facility energy management systems require interconnection and interoperation.
•
An IoT-based communication framework with a common information model is proposed.
•
A practical example of an IoT-based energy-management platform.
•
Validation of the ability of the platform to enhance system interoperability.
•
The platform with improved energy management achieves energy saving targets.
Don't forget to cite:
Wei, Min, Seung Ho Hong, and Musharraf Alam. ""An IoT-based energy-management platform for industrial facilities."" Applied Energy 164 (2016): 607-619."
NASCAR Champion History (1949-Present),67 Years of NASCAR Champion History Data,David Rubal,4,"Version 2,2017-08-17|Version 1,2017-04-09",,CSV,4 KB,CC0,"1,118 views",55 downloads,,,https://www.kaggle.com/drubal/nascar-champion-history-1949present,"Context
I have been a NASCAR fan since the 1970s and share the amazing history of this sport by creating and posting a simple dataset focused on 67 years of NASCAR Champion history (1949-Present).
Content
There are five columns (Year, Driver, Car Number, Manufacturer and Wins) and 116 rows in this dataset. I will update the dataset every every year going forward after the champion has been determined. Please suggest other data to be included in the dataset!
Acknowledgements
The NASCAR information was compiled from information found on https://en.wikipedia.org/wiki/List_of_Monster_Energy_NASCAR_Cup_Series_champions"
Global Social Survey Programs: 1948-2014,Analyze the worldwide expansion of the social survey from 1948-2014,sdorius,4,"Version 1,2017-04-18","research
history
international relations",CSV,3 MB,Other,"1,562 views",114 downloads,3 kernels,0 topics,https://www.kaggle.com/sdorius/gssi2017,"BACKGROUND:
The survey stands out as one of the most significant methodological innovations in the history of the social sciences. The instrument has made possible the efficient and reliable collection of measurements on individual attitudes, beliefs, values, behaviors, traits, and states (Alwin 2009), and coupled with modern sampling techniques, surveys have allowed researchers to generalize findings to much larger populations with remarkable precision. Survey data have helped governments to obtain essential descriptive information on, for example, their citizen’s living standards, demographic attributes, health status, purchasing intentions. Such data have, in turn, been used to craft evidence-based social and economic policies, more accurately forecast labor market participation and project population distributions, and monitor poverty rates. Commercial firms, especially those in the fields of market research and consumer behavior, have also been influential in the development of methods and the production of designed data (Groves 2011). Social scientists use survey data to study a wide range of social, economic, political, demographic, and cultural topics and to collect measurements of voter preferences, personality traits, social networks, and public opinions, to name a few. Survey data are an essential component of evidence-based social policy-making and program monitoring and evaluation and vital tool for understanding the worldviews and aspirations of publics around the world.
The progression of the social survey from its origins as a community and, later, national measuring device was attended by greater consensus and less variation in survey design and administration. Over the course of more than 60 years of cross-cultural survey research, the standardized survey (Gobo and Mauceri 2014; Harkness 2010) became the de facto one, with hallmark features including an almost exclusive reliance on input harmonized questionnaires and closed-ended questions, English language parent survey back translated into local-language-administered surveys, and best efforts at nationally representative sampling techniques that allow for generalization to, in most cases, the non-institutionalized adult population. With few exceptions, survivor programs originated in rich, western countries, and among survey programs originating in non-western countries, survey design and content has often been patterned after surveys designed to measure attitudes, beliefs, behaviors, states, and institutions endemic to western, industrialized societies. This is not to say that cross-national survey programs are monolithic or lacking in variety of design, content, international membership, and purposes.
THE DATA:
To get a clearer understanding of how the social survey has spread around the world over the last 60 years, we collected information from just under 40 international survey programs and compiled it into a time-series data set that allows researchers to empirically evaluate the global social survey data infrastructure from a life-history perspective. Collectively, these 40 survey programs have fielded over 7000 national social surveys comprising nearly 8 million completed interviews on a broad range of social, economic, and political topics. Units of analysis in the data set are survey programs and national social surveys. Additional data about programs and surveys include samples sizes, data collection methods, fielding dates, and countries surveyed, to name a few.
PARTICIPATING SOCIAL SURVEY PROGRAMS:
How Nations See Each Other (HNSEO) Pattern of Human Concerns Data (PHCD) Civic Culture Study (CCS) Attitudes Toward Europe Study (ATE) Political Participation and Equality (PPE) Eurobarometer (EUROB) European Values Study (EVS) World Values Survey (WVS) International Social Survey Programme (ISSP) The Political Culture of Southern Europe (PCSE) Central and Eastern Eurobarometer (CEEB) Post-Communist Publics (PCP) New European Barometer (NEB) Comparative National Elections Project (CNEP) New Soviet Citizen Surveys (NSCS) Values and Political Change in Post-Communist Europe (VPCP) Latinobarometro (LATINB) Coping with Government in the Former Soviet Union (CGFE) Afrobarometer (AFROB) Asia Europe Survey (ASES) Voice of the People Surveys (VOTP) Asian Barometer (ASIANB) Candidate Countries Eurobarometer (CCEB) Comparative Study of Electoral Systems (CSES) European Social Survey (ESS) Pew Global Attitudes Surveys (PGAS) Worldviews 2002 (WORLD)* Asia Barometer (ASIAB) Transatlantic Trends Survey (TTS) AmericasBarometer (AMERAB) Arab Barometer (ARABB) East Asia Social Survey (EASS) The Globalization of Personal Data Project (GPD) A Quest for Citizenship in an Ever Closer Europe (INTUNE) Caucasus Barometer (CAUCAB) Transatlantic Trends: Immigration (TTI) EU Neighbourhood Barometer (EUNB)"
US campsites,Data on campsites run by US Federal Government,Caroline Cypranowska,4,"Version 1,2017-04-30",sports,CSV,8 MB,CC0,"1,420 views",159 downloads,14 kernels,0 topics,https://www.kaggle.com/cypranowska/us-campsites,"New: See this dataset visualized in D3
Context
This data, acquired from the Recreation Information Database Catalog, contains campsite info for all campground facilities run by the United States National Park Service, the United States Forest Service, the Bureau of Land Management and other Federal Government agencies. Read the API documentation.
Content
Fields include facility ID, campsite ID, campsite type, facility name, facility location (latitude, longitude, state), and managing agency.
Acknowledgements
A humble thank you to the federal agencies that collect these data and maintain public access, including USFS, NPS, BLM, USACE, FWS, and BOR. Thank you to American tax payers for keeping these facilities afloat. Thank you to Levi Gadye for granting permission for the use of the cover photo.
Inspiration
I was looking for a campsite for a group of friends in between Salt Lake City and Grand Teton NP, and was struggling to find a tent-only campsite along the highway corridors leading to Grand Teton NP. In the past I had always felt that finding tent only campsites in California was quite easy compared to other places I’ve camped in the American West. I tidied this data set from RIDB to determine whether or not my ease in booking tent-only campsites in CA was due to a larger number of those sites, or if the state of CA had a relative enrichment for tent-only campsites."
Depth Generation - Lightfield Imaging,A collection of RGB+D images from the Lytro Illum Lightfield Camera,4Quant,4,"Version 1,2017-03-29",,Other,193 MB,CC4,800 views,48 downloads,2 kernels,0 topics,https://www.kaggle.com/4quant/depth-generation-lightfield-imaging,"About
The data is based on images I have taken with my Lytro Illum camera (https://pictures.lytro.com/ksmader) they have been exported as image data and depth maps. The idea is to make and build tools for looking at Lytro Image data and improving the results
Data
The data are from the Lytro Illum and captured as 40MP images which are then converted to 5MP RGB+D images. All of the required data for several test images is provided
Questions/Challenges
Build a neural network which automatically generates depth information from 2D RGB images
Build a tool to find gaps or holes in the depth images and fixes them automatically
Build a neural network which can reconstruct 3D pixel data from RGBD images"
"Disputed Territories and Wars, 1816-2001",What territory claimed by two or more countries has caused the most violence?,University of North Texas,4,"Version 1,2017-02-02","oceans
international relations",CSV,1 MB,Other,"1,109 views",103 downloads,,0 topics,https://www.kaggle.com/unt/disputed-territories,"Content
This dataset contains territorial claims across the entire interstate system between 1816-2001 and includes information on participants, dates, the significance of the claimed territories, and militarization of these claims. A territorial claim is defined as explicit contention between two or more nation-states claiming sovereignty over a specific piece of territory. Official government representatives (i.e., individuals who are authorized to make or state foreign policy positions for their governments) must make explicit statements claiming sovereignty over the same territory.
Our goal is to identify cases where nation-states have disagreed over specific issues in the modern era, as well as measuring what made those issues valuable to them and studying how they chose to manage or settle those issues. The Issue Correlates of War (ICOW) project does not endorse official positions on any territorial claim. Inclusion/exclusion of specific cases, and coding of details related to those cases, follows strict guidelines presented in the project's coding manuals.
Acknowledgements
This data was collected by Professor Paul Hensel of the University of North Texas and his research assistants."
LA Vacant Building Complaints,Complaints filed with the Los Angeles Department of Building and Safety,LA Times Data Desk,4,"Version 1,2017-04-06","cities
architecture
civil engineering",CSV,20 MB,Other,887 views,60 downloads,3 kernels,0 topics,https://www.kaggle.com/la-times/la-vacant-building-complaints,"Los Angeles residents have made roughly 4,000 complaints since 2011 about abandoned and vacant buildings in the city according to an analysis by the LA Times. This dataset was originally collected and analyzed for ""Fire officials were concerned about Westlake building where 5 died in a blaze"", a June 15, 2016, story by the Los Angeles Times.
Lists of open and closed complaints filed with the Los Angeles Department of Building and Safety were downloaded from the city's data portal. The two files were combined into a single spreadsheet. A new column called ""Year Received"" was generated from the existing ""Date Received"" field using LibreOffice's YEAR() function. The new file was named combined_complaints.csv.
Acknowledgements
Data and analysis originally published on the LA Times Data Desk GitHub."
US College Sailing Results,This dataset includes US College sailing results between 2008 and 2016,Anthony Goldbloom,4,"Version 1,2016-10-15",,Other,83 MB,CC0,"1,148 views",111 downloads,9 kernels,0 topics,https://www.kaggle.com/antgoldbloom/us-college-sailing-results,"This is a database dump from the website that is used to report on US college sailing results.
This dataset is being upload for the purposes of creating a new ranking system for US College Sailing."
A Million Pseudo-Random Digits,A modern tribute to RAND's classic reference work,DataCanary,4,"Version 1,2016-09-20",,CSV,2 MB,CC0,668 views,24 downloads,,0 topics,https://www.kaggle.com/datacanary/a-million-pseudorandom-digits,"In the days of slide rules and punch cards, RAND Corp.'s A Million Random Digits was a treasured reference work for statisticians and engineers. You can easily spend an afternoon browsing through the hundreds of loving tributes to the book on its Amazon page.
This dataset, used for testing our uploader, is a tribute to that classic tome."
Evan's Fruit Dataset,This is a collective dataset developed By Evan Li for fruit detection with phone,Eagles2F,4,"Version 1,2016-08-24",,Other,913 KB,CC0,"5,141 views",702 downloads,,,https://www.kaggle.com/eagles2f/evans-fruit-dataset,"Context: What is the subject matter of the dataset?
Content: What fields does it include?
Acknowledgements: Who owns the dataset and how was it compiled?
Past Research: In brief, what other analysis has been done on this data?
Inspiration: Why is this dataset worthy of further analysis? What questions would you like answered by the community? What feedback would be helpful on the data itself?"
Paintings,Students rating famous paintings from different art movements.,Miroslav Sabo,4,"Version 2,2016-12-06|Version 1,2016-08-26","visual arts
painting",CSV,6 KB,CC0,"1,971 views",168 downloads,2 kernels,0 topics,https://www.kaggle.com/miroslavsabo/paintings,"In 2013, students of the Statistics class at FSEV UK were asked to rate how much they like each one of 39 paintings (on a scale from 1 to 5). These comprise of 13 different art movements (exactly 3 paintings for each art movement).
S1-S48: students' ratings, where one means ""don't like at all"" (integer)
art movement: the art movement the painting belongs to (categorical)
artist: the author of the painting (categorical)
painting: the name of the painting (categorical)"
Wind Farms,Onshore Industrial Wind Turbine Locations for the United States,Jason McNeill,4,"Version 1,2016-11-06",,CSV,12 MB,Other,"1,958 views",106 downloads,4 kernels,0 topics,https://www.kaggle.com/txtrouble/wind-farms,"This is a USGS dataset which includes the position, type, and size of every wind turbine in the United States.
The data was compiled by USGS in part using data from the Federal Aviation Administration Digital Obstacle File to help identify wind turbines. See the link for a full description of the data and how it was collected."
Social Network Fake Account Dataset,dataset contains fake accounts' content,Yao Lu,4,"Version 2,2016-11-04|Version 1,2016-10-25",,Other,349 MB,CC4,"5,356 views",391 downloads,,,https://www.kaggle.com/bitandatom/social-network-fake-account-dataset,"The dataset focus on find those zombie followers(fake account created by automated registration bot). All the fake accounts are human-like with both profile image and some personal information. They also have a lot of followers and posts. All the data are collected from Weibo, a Chinese twitter like platform. The dataset contains fake account's profile page and contents.
Please cite the following paper if you want to use it:
Linqing Liu, Yao Lu, Ye Luo, Renxian Zhang, Laurent Itti and Jianwei Lu. ""Detecting ""Smart"" Spammers on Social Network: A Topic Model Approach."" Proceedings of NAACL-HLT. 2016."
311 service requests NYC,Complaints to the police in NYC from 2010,PabloMonleon,4,"Version 1,2016-12-08",,CSV,225 MB,ODbL,"1,582 views",108 downloads,4 kernels,0 topics,https://www.kaggle.com/pablomonleon/311-service-requests-nyc,This dataset contains information about the complaints made to the NYPD from 2010 until the present. Obtained from: https://nycopendata.socrata.com/Social-Services/311-Service-Requests-from-2010-to-Present
Near Earth Asteroids,"Data from the JPL's NEO program, tracking near earth objects.",Mark DiMarco,4,"Version 1,2016-10-06",,CSV,61 KB,Other,"1,482 views",77 downloads,,,https://www.kaggle.com/markmarkoh/near-earth-asteroids,"Near-Earth Objects (NEOs) are comets and asteroids that have been nudged by the gravitational attraction of nearby planets into orbits that allow them to enter the Earth's neighborhood. Composed mostly of water ice with embedded dust particles, comets originally formed in the cold outer planetary system while most of the rocky asteroids formed in the warmer inner solar system between the orbits of Mars and Jupiter. The scientific interest in comets and asteroids is due largely to their status as the relatively unchanged remnant debris from the solar system formation process some 4.6 billion years ago. The giant outer planets (Jupiter, Saturn, Uranus, and Neptune) formed from an agglomeration of billions of comets and the left over bits and pieces from this formation process are the comets we see today. Likewise, today's asteroids are the bits and pieces left over from the initial agglomeration of the inner planets that include Mercury, Venus, Earth, and Mars.
As the primitive, leftover building blocks of the solar system formation process, comets and asteroids offer clues to the chemical mixture from which the planets formed some 4.6 billion years ago. If we wish to know the composition of the primordial mixture from which the planets formed, then we must determine the chemical constituents of the leftover debris from this formation process - the comets and asteroids."
Turkish sentences for word2vec training,Train with gensim word2vec,AhmetAksoy,4,"Version 1,2016-10-13",,Other,54 MB,CC0,"2,464 views",83 downloads,4 kernels,0 topics,https://www.kaggle.com/ahmetax/hury-dataset,"In the dataset files there are news from a Turkish newspaper: Hurriyet. The news are separated in sentences. Stopwords, numbers and puctuation characters are all removed from the sentences. The sentences are separated from each other by a newline character. Words are separated by a space. Files are in utf-8 text format."
Wind Predictions,Can you predict the direction of the wind?,Heiko,4,"Version 4,2016-11-30|Version 3,2016-11-30|Version 2,2016-11-30|Version 1,2016-11-28",,CSV,135 KB,Other,"1,529 views",114 downloads,2 kernels,,https://www.kaggle.com/marc000/wind-predictions,This training.csv file of this dataset contains wind directions and speeds for 3 different street. The test.csv dataset contains wind directions and speeds for the first 2 streets. The goal is to predict the direction for street #3.
Mammographic Mass Data Set,Discrimination of benign and malignant mammographic masses,GauthamSenthil,4,"Version 3,2016-10-31|Version 2,2016-10-31|Version 1,2016-10-31",,CSV,11 KB,CC4,"2,898 views",172 downloads,11 kernels,0 topics,https://www.kaggle.com/overratedgman/mammographic-mass-data-set,"Mammography is the most effective method for breast cancer screening available today. However, the low positive predictive value of breast biopsy resulting from mammogram interpretation leads to approximately 70% unnecessary biopsies with benign outcomes. To reduce the high number of unnecessary breast biopsies, several computer-aided diagnosis (CAD) systems have been proposed in the last years.These systems help physicians in their decision to perform a breast biopsy on a suspicious lesion seen in a mammogram or to perform a short term follow-up examination instead. This data set can be used to predict the severity (benign or malignant) of a mammographic mass lesion from BI-RADS attributes and the patient's age. It contains a BI-RADS assessment, the patient's age and three BI-RADS attributes together with the ground truth (the severity field) for 516 benign and 445 malignant masses that have been identified on full field digital mammograms collected at the Institute of Radiology of the University Erlangen-Nuremberg between 2003 and 2006. Each instance has an associated BI-RADS assessment ranging from 1 (definitely benign) to 5 (highly suggestive of malignancy) assigned in a double-review process by physicians. Assuming that all cases with BI-RADS assessments greater or equal a given value (varying from 1 to 5), are malignant and the other cases benign, sensitivities and associated specificities can be calculated. These can be an indication of how well a CAD system performs compared to the radiologists.
Class Distribution: benign: 516; malignant: 445
Attribute Information:
6 Attributes in total (1 goal field, 1 non-predictive, 4 predictive attributes)
BI-RADS assessment: 1 to 5 (ordinal, non-predictive!)
Age: patient's age in years (integer)
Shape: mass shape: round=1 oval=2 lobular=3 irregular=4 (nominal)
Margin: mass margin: circumscribed=1 microlobulated=2 obscured=3 ill-defined=4 spiculated=5 (nominal)
Density: mass density high=1 iso=2 low=3 fat-containing=4 (ordinal)
Severity: benign=0 or malignant=1 (binominal, goal field!)
Missing Attribute Values: - BI-RADS assessment: 2 - Age: 5 - Shape: 31 - Margin: 48 - Density: 76 - Severity: 0
I acknowledge that this dataset is not mine and I have only reformatted the data and uploaded it to kaggle. Source:
Matthias Elter Fraunhofer Institute for Integrated Circuits (IIS) Image Processing and Medical Engineering Department (BMT) Am Wolfsmantel 33 91058 Erlangen, Germany matthias.elter '@' iis.fraunhofer.de (49) 9131-7767327
Prof. Dr. Rüdiger Schulz-Wendtland Institute of Radiology, Gynaecological Radiology, University Erlangen-Nuremberg Universitätsstraße 21-23 91054 Erlangen, Germany
Relevant Papers:
M. Elter, R. Schulz-Wendtland and T. Wittenberg (2007) The prediction of breast cancer biopsy outcomes using two CAD approaches that both emphasize an intelligible decision process. Medical Physics 34(11), pp. 4164-4172
Citation Request:
M. Elter, R. Schulz-Wendtland and T. Wittenberg (2007) The prediction of breast cancer biopsy outcomes using two CAD approaches that both emphasize an intelligible decision process. Medical Physics 34(11), pp. 4164-4172"
Predicting Movie Revenue,"depends on multiple factors like cast, budget, review, rating, release year",sumanth,4,"Version 1,2016-10-06",,Other,99 KB,ODbL,"3,048 views",588 downloads,3 kernels,,https://www.kaggle.com/sumanthpola/predicting-movie-revenue,"Movie revenue depends on multiple factors such as cast, budget, film critic review, MPAA rating, release year, etc. Because of these multiple factors there is no analytical formula for predicting how much revenue a movie will generate. However by analyzing revenues generated by previous movies, one can build a model which can help us predict the expected revenue for a movie. Such a prediction could be very useful for the movie studio which will be producing the movie so they can decide on expenses like artist compensations, advertising, promotions, etc. accordingly. Plus investors can predict an expected return-on-investment."
DataCampTraining(Titanic),https://campus.datacamp.com/courses/kaggle-python-tutorial-on-machine-learning,MahdiJavid,4,"Version 1,2017-01-05",,CSV,3 KB,Other,"1,340 views",102 downloads,7 kernels,0 topics,https://www.kaggle.com/mahdijavid/datacamptraining,"Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
Coursera Data Science Capstone Datasets,Extracted from the Helsinki Corpora (HC) English language database,Data to Information to Knowledge to Wisdom,4,"Version 1,2016-10-17",,CSV,471 MB,ODbL,"2,726 views",85 downloads,,,https://www.kaggle.com/d2i2k2w/coursera-data-science-capstone-datasets,"Corpus is the Latin word for ""body."" The Coursera sponsored Data Science Capstone corpus is extracted from blogs, news and Twitter text files archived in the HC Corpora English language online database, a collection of free text corpora"
Facial keypoints,FP PCMiners (Detecting keypoint means),PCMiners,4,"Version 5,2016-12-05|Version 4,2016-12-03|Version 3,2016-12-02|Version 2,2016-11-25|Version 1,2016-11-25",,CSV,783 MB,Other,"1,261 views",157 downloads,8 kernels,0 topics,https://www.kaggle.com/pcminers/fpoints,Facial keypoints detection -> improving prediction
Dictionary for Sentiment Analysis,Set of words along with their polarity,MilindParadkar,4,"Version 1,2016-12-06",,CSV,1 KB,Other,"2,561 views",228 downloads,4 kernels,0 topics,https://www.kaggle.com/milind81/dictionary-for-sentiment-analysis,"Context & content
The dataset is used to perform sentiment analysis and contains words that are specific to the Pharma sector along with their respective polarity (pos/neg)."
Betfair.com Market Analysis,How to pick up some money in horse racing?,Nils Ponomarchuk,4,"Version 1,2016-12-20",,CSV,31 KB,Other,"1,678 views",45 downloads,,0 topics,https://www.kaggle.com/nils86/betfaircom-market-analysis,"This is my portfolio for my next step in a data science career. For the last 5 years I worked in a Ukrainian bank and all this time I asked to myself: ""How can I use market risk knowledge, where I'll use Yet To Maturity calculation or expected lose methodology?"" Several months ago my old school friend showed me Betfair. And I started wondering how I can use match, web scraping techniques and data mining experience to pick up some money. In this case I give some ideas to Kaggle users and show my results.
Past Research
I start from these questions:
1. What kind of horses win more money then lose?
I found 6 mask: I load data for 8 years and create pivot table with race results and type of races by distance and horses options. After that I calculate sum of PPMIN and PPMAX for this horse. And finally, I calculate dispersion to filter more stable masks.
2. How many selected horses run daily/monthly? Is something change when the season changes?
3. What kind of bets do I need to use and how much money do I need to start? When should I start?
Content
Data storage = http://www.betfairpromo.com/betfairsp/prices/index.php
Betfair rules:
Minimal bet = 4$
Base commission for win on BF platform = 6.5%
Inspiration
I'm looking for a job and similar developers who can join me in other projects."
Second-level domains list/zone file,The list includes about 20M of second-level intenet domain names,Eugene,4,"Version 3,2017-01-08|Version 2,2017-01-08|Version 1,2017-01-04",,CSV,348 MB,CC0,"1,207 views",37 downloads,4 kernels,0 topics,https://www.kaggle.com/domainsindex/secondlevel-domains-listzone-file,"Context
The dataset of registered second-level domains fetched from domains-index lists. Used dataset for our Internet second-level domain names research.
Content
Just one column with second-level domain names, one per line. About 20M of records.
Past Research
The results of the research we've made are here 1"
Finding Bubbles in Foam,Liquid Foam X-Ray tomography measurements,4Quant,4,"Version 1,2016-11-02",,CSV,37 MB,CC4,"2,117 views",27 downloads,5 kernels,,https://www.kaggle.com/4quant/simplefoam,"Understanding liquid foam is essential for a number of applications for fighting fires to making food and even extracting oil. One of the best techniques for looking inside 3D structures of foam is X-ray tomography. These type of measurements, however, cannot see bubbles, they can only see the 'Plateau Borders' where water collects.
The challenge for this data is to find the bubbles from the outlines left by the Plateau Borders. The dataset include a segmented 3D image of the borders for one sample. The bubbles must be found and extracted and output to a list of X,Y,Z, Volume points indicating the bubble center (X,Y,Z) and size (Volume in voxels)."
Job Classification Dataset,typical job class specs feature information and paygrade,People HR Analytics Repository,4,"Version 1,2017-01-08",,CSV,4 KB,CC0,"4,150 views",416 downloads,2 kernels,0 topics,https://www.kaggle.com/HRAnalyticRepository/job-classification-dataset,"Context
This is a dataset containing some fictional job class specs information. Typically job class specs have information which characterize the job class- its features, and a label- in this case a pay grade - something to predict that the features are related to.
Content
The data is a static snapshot. The contents are ID column - a sequential number Job Family ID Job Family Description Job Class ID Job Class Description PayGrade- numeric Education Level Experience Organizational Impact Problem Solving Supervision Contact Level Financial Budget PG- Alpha label for PayGrade
Acknowledgements
This data is purely fictional
Inspiration
The intent is to use machine learning classification algorithms to predict PG from Educational level through to Financial budget information.
Typically job classification in HR is time consuming and cumbersome as a manual activity. The intent is to show how machine learning and People Analytics can be brought to bear on this task."
Hessen House Prices Dataset,Over 22000 Houses with Pricing Information from Hessen Germany,Orges Leka,4,"Version 7,2017-11-23|Version 6,2017-11-23|Version 5,2017-11-23|Version 4,2017-11-22|Version 3,2017-11-19|Version 2,2017-11-18|Version 1,2017-11-18","housing
regression analysis",{}JSON,1 MB,CC0,725 views,79 downloads,3 kernels,0 topics,https://www.kaggle.com/orgesleka/hessen-house-prices-dataset,"Challenge: Can you predict the house price?
This is a challenging task, as it requires to first parse the features and the identify the relevant features for house prediction. As is the case with many real world datasets, in this dataset not all houses have the same features or some houses even have - for the prediction - irrelevant features, so maybe you can think of some clever way to union all the features before you process them with your favourite machine learning algorithm.
In case you need more fresh data ore you need more houses for your algorithm to work, you can find the yet to be improved scrapy script here This page is a forum for data scientist I started, in hope , that you will participate and maybe even improve the scrapy script.
Kind regards
Orges Leka"
Intel Xeon Scalable Processors,,Alexander Minushkin,4,"Version 4,2017-11-02|Version 3,2017-11-01|Version 2,2017-10-31|Version 2,2017-10-31|Version 1,2017-10-31",,CSV,117 KB,CC0,576 views,28 downloads,,0 topics,https://www.kaggle.com/miniushkin/intel-xeon-scalable-processors,"Context
SPECint2006 Rate Results for Intel Xeon Scalable Processors
Content
Data collected on October 30, 2017
Acknowledgements
The Standard Performance Evaluation Corporation (SPEC)
Intel ARK
Photo by Samuel Zeller on Unsplash
Inspiration
Intel introduced new processor names: Platinum, Gold, Silver and Bronze. It would be nice to visualise difference between them."
Shanghai license plate bidding price prediction,"time series, price prediction",Bazinga,4,"Version 1,2017-11-08",time series,CSV,6 KB,Other,402 views,65 downloads,2 kernels,0 topics,https://www.kaggle.com/bazingasu/shanghai-license-plate-bidding-price-prediction,"Increased automobile ownership and use in China over the last two decades has increased energy consumption, worsened air pollution, and exacerbated congestion. The government of Shanghai has adopted an auction system to limit the number of license plates issued for every month. The dataset contains historical data of auctions from Jan 2002 to Oct 2017.
how the auction system works: A starting price is given at the beginning of the auction, bidders can only bid up to 3 times for each auction and can only mark up or down within 300 CNY ( roughly 46 USD ) for each bid. At the end of each auction, only the top n ( number of plates that will be issued for the month ) bids will get the license plates at the cost of their bids. The nth bid will be the lowest deal price for the month. Please note that the auctions are conducted online and each bidder will not be able to see other bids.
Columns:
Date: Jan 2002 to Oct 2017 ( note that Feb 2008 is missing)
*num_bidder*: number of citizens who participate the auction for the month
*num_plates*: number of plates that will be issued by the government for the month
*lowest_deal_price*: explained above, in CNY
*avg_deal_price*: average deal price, in CNY ( note that since each bid can only be marked up or down within 300, it is not drifting too far away from the lowest deal price)
The goal is to predict the lowest_deal_price for each month, the actual result will be updated at the end of every month
the dataset is scraped from http://www.51chepai.com.cn/paizhaojiage/
Contact: ran_su147@hotmail.com"
500 Cities: Local Data for Better Health,A CDC Dataset about US Cities and Health,fandang,4,"Version 1,2017-11-01",,CSV,217 MB,ODbL,264 views,25 downloads,,0 topics,https://www.kaggle.com/fandang/500-cities-local-data-for-better-health,"The 500 Cities project is a collaboration between CDC, the Robert Wood Johnson Foundation, and the CDC Foundation. The purpose of the 500 Cities Project is to provide city- and census tract-level small area estimates for chronic disease risk factors, health outcomes, and clinical preventive service use for the largest 500 cities in the United States. These small area estimates will allow cities and local health departments to better understand the burden and geographic distribution of health-related variables in their jurisdictions, and assist them in planning public health interventions"
First Voyage of Christopher Columbus,Columbus's logbook written in the 1530's by Bartolome de las Casas,Donyoe,4,"Version 3,2016-10-19|Version 2,2016-10-13|Version 1,2016-10-13","sailing
history",CSV,319 KB,CC0,"1,971 views",43 downloads,3 kernels,,https://www.kaggle.com/donyoe/columbus-first,"Christopher Columbus, or Cristóbal Colón in Spanish, is an amazingly difficult man to pin down. Born about 1451, he died on 20 May 1506. We know that he sailed the Atlantic and reached the Americas on October 12, 1492 under the sponsorship of the Spanish kingdom of Castile.
His voyage in 1492 marked the beginning of European exploration of the Americas.
More information :http://www.christopher-columbus.eu/logs.htm
Content:
Date: Complete date: year-month-day
Day: Day of month
Month: Month in character
Year: 1492 or 1493
Text: The logbook itself
nmonth: Month as decimal number (1–12)
Leagues: Distance in leagues
Course: Course of the voyage
More things that can be done: What says Christopher Columbus and what Bartolome de las Casas, locations, voyage indications, etc"
National Basketball Association(NBA) Dataset,,Aman Ajmera,4,"Version 1,2017-11-05",,CSV,87 KB,CC0,"1,212 views",142 downloads,,0 topics,https://www.kaggle.com/amanajmera1/national-basketball-associationnba-dataset,This dataset does not have a description yet.
Reviews with conditions,A dataset with labelled and unlabelled sentences from reviews with conditions.,Fernando O. Gallego,4,"Version 3,2018-01-25|Version 2,2018-01-24|Version 1,2018-01-23","nlp
text data",CSV,179 MB,GPL,749 views,51 downloads,,0 topics,https://www.kaggle.com/fogallego/reviews-with-conditions,"Context
This dataset was created during my PhD (http://www.tdg-seville.info/fogallego/Personal%20Info) at the University of Seville. We didn't found any datasets with labelled conditions so we decided to build one since our main goal for the PhD was to be able to identify conditions without relying on user-defined patterns or requiring any specific-purpose dictionaries, taxonomies, or heuristics.
Content
The reviews in English and Spanish were randomly gathered from ciao.com between April 2017 and May 2017. The sentences were classified into 15 domains according to their sources, namely: adults, baby care, beauty, books, cameras, computers, films, headsets, hotels, music, ovens, pets, phones, TV sets, and video games.
Our dataset consist of two files: sentences.csv and conditions.csv. The first one contains the whole set of sentences and the second one the manually labelled conditions.
In order to better understand the meaning of each column, I'll explain them in detail:
sentence.csv:
sentence_uuid: the unique identifier of the sentence
sentence_text: the text of the sentence
language: the language of the sentence
domain: the domain of the sentence
labelled: whether the sentence was labelled or not
conditions.csv:
sentence_uuid: the unique identifier of the corresponding labelled sentence
condition_uuid: the unique identifier of the condition
begin_connective: the character position where the connective of the condition starts
end_connective: the character position where the connective of the condition ends
begin_condition: the character position where the rest of the condition starts
end_condition: the character position where the rest of the condition ends
language: the language of the corresponding labelled sentence
domain: the domain of the corresponding labelled sentence
Acknowledgements
My PhD and this dataset were supported by Opileak.com and the Spanish R&D programme (grants TIN2013- 40848-R and TIN2013-40848-R)."
World Countries and Continents Details,World exploration for data scientist,folaraz,4,"Version 3,2017-10-05|Version 2,2017-10-05|Version 1,2017-09-29","geography
demographics",CSV,47 KB,CC4,"1,041 views",147 downloads,5 kernels,0 topics,https://www.kaggle.com/folaraz/world-countries-and-continents-details,"Context
Can you tell geographical stories about the world using data science?
Content
World countries with their corresponding continents , official english names, official french names, Dial,ITU,Languages and so on.
Acknowledgements
This data was gotten from https://old.datahub.io/
Inspiration
Exploration of the world countries: - Can we graphically visualize countries that speak a particular language? - We can also integrate this dataset into others to enhance our exploration. - The dataset has now been updated to include longitude and latitudes of countries in the world."
UCI Daily and Sports Activities,,Rajorshi Chaudhuri,4,"Version 1,2018-01-14",,Other,163 MB,CC0,154 views,2 downloads,,0 topics,https://www.kaggle.com/knight079/uci-daily-and-sports-activities,This dataset does not have a description yet.
Brazillian Stock Quotes,Daily historical data,Gustavo Bonesso,4,"Version 7,2018-01-24|Version 6,2018-01-24|Version 5,2018-01-24|Version 4,2018-01-24|Version 3,2018-01-24|Version 2,2018-01-20|Version 1,2018-01-20",brazil,Other,221 MB,CC0,630 views,47 downloads,2 kernels,,https://www.kaggle.com/gbonesso/b3-stock-quotes,"Context
My objective sharing this data is to make studies about stock quotes using real data from the Brazilian stock market.
Content
This is the daily stock quotes from B3 for the 2017 year. B3 is the unique stock exchange in Brazil, here we don't have competition in this sector as in USA.
Acknowledgements
This data is directly extracted from B3 site, without changes."
FCC Net Neutrality Comments Clustered,"FCC Proceeding #17-108 (text and dupe counts only, clustered)",Jeff Kao,4,"Version 1,2017-11-28",,CSV,194 MB,CC0,655 views,29 downloads,,0 topics,https://www.kaggle.com/jeffkao/fcc-net-neutrality-comments-clustered,"Context
See description for unvectorized full text dataset. Post clustering, the same dataset. level_0 is the manually picked out clusters; level_1 is the HDBSCAN clusters. Also left in the text hash this time. Looking back through the data, I am still amazed by what a great job HDBCSAN did at breaking everything up (down?). Check out Leland's project and try it for yourself."
Clash royale Dataset,71 clash Royale Cards Data,swapnilkale,4,"Version 1,2017-10-04","games and toys
video games",CSV,5 KB,CC0,397 views,52 downloads,,0 topics,https://www.kaggle.com/swappyk/clash-royale-dataset,"Context
This dataset contains information on all 71 cards of Clash Royale. The information contained in this dataset include Card Name, Card Level, Cost, Count, Crown Tower Damage, Damage per second etc. The information was scraped from Clash royale wikipedia page
Content
Card Level (Spawn Level): Name of the Card/Character
Cost: Elixir
Count: Number of troops in the card
Crown Tower Damage: Damage to the tower
Damage: Damage per hit
Damage per second: Hits per second * Damage
Death Damage: Damage just before death
Health (+Shield): Hit points
Hit Speed: Hit speed of the card
Level: Level of the card
Maximum Spawned: In case of spawn troops
Radius: Radius covered in the arena when deployed
Range: Range within the Arena
Spawn DPS: Damage per second
Spawn Damage: Damage
Spawn Health: Hit points
Spawn Speed: Speed of the character
Spawner Health: Character health
Troop Spawned: Number of troops spawned
Type: Damaging Spells, Spawners, Troops and Defenses
Acknowledgements
Data was scraped from Clash royale wikipedia page using Beautiful soup 4 library.
Inspiration
Clash Royale is an interesting mobile game. I have been playing this game since some months now. Wonder if this could be run by a bot using some AI library. This dataset is very basic and does not cover the detailed information about the cards transformation as per levels. With this dataset can we answer the following questions -:
Group cards having similar impact considering the range, speed, damage etc ?
Which cards can be used best against the other ?
I will update the data set with the level data soon. Thanks."
Urdu Stopwords List,517 High-Frequency Urdu Words,Rachael Tatman,4,"Version 1,2017-11-14","languages
india
linguistics",Other,11 KB,Other,187 views,26 downloads,,0 topics,https://www.kaggle.com/rtatman/urdu-stopwords-list,"Context
Urdu is an indo-Aryan language with over 100 million speakers. It is mainly used in Pakistan. While often mutually intelligible with Hindi, the two languages use different scripts and as a result NLP tools developed for Hindi are often not extendible to Urdu.
Some words, like “the” or “and” in English, are used a lot in speech and writing. For most Natural Language Processing applications, you will want to remove these very frequent words. This is usually done using a list of “stopwords” which has been complied by hand.
Content
This dataset is a list of 517 high-frequency Urdu words. Very high frequency words are not generally useful for most NLP tasks and are generally removed as part of pre-processing.
The .json and .txt files have the same words in them.
Acknowledgements:
This dataset is Copyright (c) 2016 Gene Diaz and distributed here under an MIT license. See the attached license file for more information.
Inspiration:
This is more of a utility NLP dataset than one that is interesting in its own right. Try using it with these other Urdu datasets:
Urdu-Nepali Parallel Corpus: A part of speech tagged corpus for Urdu & Nepali
The Holy Quran (in 21 languages, including Urdu)
Old Newspapers: A cleaned subset of HC Corpora newspapers"
Santa Challenge,Tis the night before Christmas year: two thousand seventeen.,Bibin Paul,4,"Version 54,2018-01-13|Version 53,2018-01-13|Version 52,2018-01-13|Version 51,2018-01-13|Version 50,2018-01-12|Version 49,2018-01-12|Version 48,2018-01-12|Version 47,2018-01-12|Version 46,2018-01-12|Version 45,2018-01-12|Version 44,2018-01-12|Version 43,2018-01-12|Version 42,2018-01-12|Version 41,2018-01-12|Version 40,2018-01-12|Version 39,2018-01-12|Version 38,2018-01-12|Version 37,2018-01-12|Version 36,2018-01-12|Version 35,2018-01-11|Version 34,2018-01-11|Version 33,2018-01-11|Version 32,2018-01-11|Version 31,2018-01-11|Version 30,2018-01-11|Version 29,2018-01-11|Version 28,2018-01-11|Version 27,2018-01-11|Version 26,2018-01-11|Version 25,2018-01-11|Version 24,2018-01-11|Version 23,2018-01-11|Version 22,2018-01-11|Version 21,2018-01-10|Version 20,2018-01-10|Version 19,2018-01-10|Version 18,2018-01-10|Version 17,2018-01-10|Version 16,2018-01-10|Version 15,2018-01-10|Version 14,2018-01-10|Version 13,2018-01-10|Version 12,2018-01-10|Version 11,2018-01-10|Version 10,2018-01-10|Version 9,2018-01-10|Version 8,2018-01-10|Version 7,2018-01-10|Version 6,2018-01-09|Version 5,2018-01-09|Version 4,2018-01-09|Version 3,2018-01-09|Version 2,2018-01-09|Version 1,2018-01-09",,CSV,4 MB,CC4,448 views,49 downloads,,0 topics,https://www.kaggle.com/bibinpaul/santa-submission,This dataset does not have a description yet.
Deaths related to the Northern Ireland conflict,Patterns of politically associated violence in Northern Ireland,Christopher Clayford,4,"Version 1,2017-10-04","crime
politics",Other,467 KB,CC0,708 views,60 downloads,,0 topics,https://www.kaggle.com/cclayford/deaths-related-to-the-northern-ireland-conflict,"Context
A dataset of information on deaths related to the Northern Ireland conflict (1969-2005).
Content
Fatalities: A list of deaths related to the Northern Ireland conflict (Fact table)
Location: A list of locations where the fatalities took place [including GPS coordinates] (Links to fact table via the ""Location"" field)
Agency: This refers to groups responsible for fatal incidents [including agency groups] (Links to fact table via the ""Agency"" field)
Status: This refers to the role of fatality victims [including status groups] (Links to fact table via the ""Status"" field)
Dimension Remarks
Name: Spellings might vary from other sources
Year: Relates to year of death rather than fatal incident
Religion: Only that of Northern Ireland residents is listed
Agency: This refers to groups responsible for fatal incidents
Status: This refers to the role of fatality victims
Location: Westminster electoral areas are employed for Northern Ireland fatalities
Rationale: This refers to the inferred purpose underlying the fatal act
Causality: This probes the degree of inferred purposiveness of a fatal event
Context: This distinguishes between incidents such as gun fire, explosions, beatings
New Incident: This offers a count of discrete fatal incidents
1st Fatality: This distinguishes multiple fatality incidents
Acknowledgements
The information was compiled by Michael McKeown and was contributed by him to the CAIN Web site. Michael McKeown has taken the decision (June 2009) to make the dataset freely available via the CAIN site. While users are free to download the dataset for research purposes, the database remains copyright © of Michael McKeown.
Inspiration
""The following study represents both the revisiting and continuation of a task which had occupied me for over twenty years. The concluded work highlights complexities and ambiguities in the patterns of the violence in Northern Ireland over the past three decades which are often obscured by the polar interpretations offered by partizan commentaries. For that reason I believe it should be inserted into the public arena for further consideration and possibly as a methodological model for further enquiry."" Michael McKeown (2001)."
breast cancer,,yuqing01,4,"Version 1,2017-10-23",,CSV,122 KB,Other,"1,902 views",268 downloads,5 kernels,0 topics,https://www.kaggle.com/yuqing01/breast-cancer,This dataset does not have a description yet.
Word Occurrences in Mr. Robot,Find out F-Society's favorite lingo,Emma,4,"Version 1,2017-11-10","entertainment
computers",CSV,315 KB,CC0,181 views,2 downloads,,0 topics,https://www.kaggle.com/emmabel/word-occurrences-in-mr-robot,"Context
Mr. Robot is all about data whether it's corrupting it, encrypting it, or deleting it. I wanted to dig up some data on my favorite show.
Content
Each episode has a corresponding .csv file with the first column being the word and the second column being how often it appears in that episode.
My Python code can be found at https://github.com/emmabel96/WordOccurrences"
Heart.csv,,Yingzhu,4,"Version 1,2017-11-30",,CSV,19 KB,Other,553 views,117 downloads,,0 topics,https://www.kaggle.com/zhaoyingzhu/heartcsv,This dataset does not have a description yet.
US-based job data set from 300+ companies,Last one month job data directly extracted from company sites,JobsPikr,4,"Version 1,2017-11-02",internet,CSV,70 MB,CC4,248 views,35 downloads,90 kernels,0 topics,https://www.kaggle.com/JobsPikrHQ/usa-based-job-data-set-from-300-companies,"Context
This is the job data for the last one month (300+ U.S.-based companies) downloaded from JobsPikr - a job data delivery platform that extracts job data from various company sites across the globe on daily basis powered by machine learning techniques.
Content
Following data fields are available in this dataset:
URL
Job title
Job text
Job posting date
Last date to apply
Inspiration
It'd be interesting to perform text mining on the job description (perhaps LDA to classify the job types)."
AAU VAP Trimodal People Segmentation Dataset,"Visible, thermal, and depth in three indoor scenes",Aalborg University,4,"Version 2,2018-01-10|Version 1,2017-11-21",computer science,Other,1 GB,CC4,361 views,22 downloads,,,https://www.kaggle.com/aalborguniversity/trimodal-people-segmentation,"Context
How do you design a computer vision algorithm that is able to detect and segment people when they are captured by a visible light camera, thermal camera, and a depth sensor? And how do you fuse the three inherently different data streams such that you can reliably transfer features from one modality to another? Feel free to download our dataset and try it out yourselves!
Content
The dataset features a total of 5724 annotated frames divided in three indoor scenes. Activity in scene 1 and 3 is using the full depth range of the Kinect for XBOX 360 sensor whereas activity in scene 2 is constrained to a depth range of plus/minus 0.250 m in order to suppress the parallax between the two physical sensors. Scene 1 and 2 are situated in a closed meeting room with little natural light to disturb the depth sensing, whereas scene 3 is situated in an area with wide windows and a substantial amount of sunlight. For each scene, a total of three persons are interacting, reading, walking, sitting, reading, etc.
Every person is annotated with a unique ID in the scene on a pixel-level in the RGB modality. For the thermal and depth modalities, annotations are transferred from the RGB images using a registration algorithm found in registrator.cpp.
We have used our AAU VAP Multimodal Pixel Annotator to create the ground-truth, pixel-based masks for all three modalities.
Acknowledgements
Palmero, C., Clapés, A., Bahnsen, C., Møgelmose, A., Moeslund, T. B., & Escalera, S. (2016). Multi-modal RGB–Depth–Thermal Human Body Segmentation. International Journal of Computer Vision, pp 1-23."
Hot Dog - Not Hot Dog,,DanB,4,"Version 4,2018-01-04|Version 3,2018-01-04|Version 2,2018-01-04|Version 1,2018-01-04",,Other,45 MB,Other,342 views,51 downloads,14 kernels,0 topics,https://www.kaggle.com/dansbecker/hot-dog-not-hot-dog,"Context
Video Context
Description
Build your own version of the SeeFood App from the TV show Silicon Valley. Whether you've seen the show or not, you should watch a refresher on how it works. This dataset has everything you need to build the SeeFood app.
This data was extracted from the Food 101 dataset. A full version of the dataset is available here.
Acknowledgements
Original data from this paper."
Swedish NER corpus,"~8000 sentences annotated for Swedish NER (PER, LOC, ORG, MISC)",Andreas Klintberg,4,"Version 1,2017-12-13",languages,Other,1 MB,CC4,372 views,13 downloads,,0 topics,https://www.kaggle.com/andreasklintberg/swedish-ner-corpus,"Context
Bootstrapped and manually annotated NER Swedish web news from 2012. NER stands for Named entity recognition, and its used to describe entities in a text such as organisations, locations and people for instance.
Its a very common operation in general NLP pipeline, and several algorithms can be used to train a model. Traditionally many NER systems were trained using some kind of CRF (conditionally random fields) approach, but nowadays many people successfully uses LSTM:s or other sequence based deep learning techniques.
A tutorial on how to use this dataset to train an NER for Stanford CoreNLP is available here https://medium.com/@klintcho/training-a-swedish-ner-model-for-stanford-corenlp-part-2-20a0cfd801dd
Content
The dataset is very simple and can easily be adapted into other formats, it is specifically adapted to CoreNLP NER. Thus the first column is a word. Second column (tab separated) is either the NER category (ORG, PER, LOC, MISC) or a 0 if it does not belong to any category (not an entity). Each word is separated by a new line, and each sentence is separated by an empty new line.
Sample structure (of two sentences, one three word sentence, and another 4 word sentence): Apple ORG is 0 nice 0 . 0
Per PER is 0 not 0 sad 0
Acknowledgements
Text is annotated from http://spraakbanken.gu.se/eng/resource/webbnyheter2012. Thanks Norah Klintberg Sakal for helping out with the annotation and reviewing all annotations as well.
Inspiration
Feel free to use this for whatever you like. As most datasets it would definitely benefit from becoming larger, feel free to create a pull request https://github.com/klintan/swedish-ner-corpus/ or update it here on Kaggle."
Malicious and Benign Websites,Classify by application layer and network characteristics,Christian Urcuqui,4,"Version 1,2017-12-28","web sites
crime
computer security
machine learning",CSV,267 KB,Other,566 views,84 downloads,,,https://www.kaggle.com/xwolf12/malicious-and-benign-websites,"Context
Malicious websites are of great concern. Unfortunately there is a lack of datasets that classify malicious vs benign web characteristics. This dataset is a research production of my bachelor students that aims to fill this gap.
Content
The project sought to evaluate classification models to predict malicious and benign websites, based on application layer and network characteristics. The data were obtained by using different verified sources of benign and malicious URL, in a low interactive client honeypot to isolate network traffic. We used additional tools to get other information, such as server country with Whois.
This is the first version and we have some initial results from applying machine learning classifiers in a bachelor thesis. Further details on the data process making and the data description can be found in the article below.
Acknowledgements
If your papers or other works use our dataset, please cite our paper:
Urcuqui, C., Navarro, A., Osorio, J., & Garcıa, M. (2017). Machine Learning Classifiers to Detect Malicious Websites. CEUR Workshop Proceedings. Vol 1950, 14-17.
If you need a review article of website cybersecurity state of the art (in English and Spanish):
Urcuqui, C., Peña, M. G., Quintero, J. L. O., & Cadavid, A. N. (2017). Antidefacement. Sistemas & Telemática, 14(39), 9-27
If you have any question or feedback, please contact me: ccurcuqui@icesi.edu.co"
intrusion detection,,Jinner,4,"Version 1,2017-12-29",,CSV,2 MB,Other,218 views,64 downloads,,0 topics,https://www.kaggle.com/what0919/intrusion-detection,This dataset does not have a description yet.
Sign Language Digits Dataset,Turkey Ankara Ayrancı Anadolu High School's Sign Language Digits Dataset,Arda Mavi,4,"Version 2,2017-12-24|Version 1,2017-12-24",languages,Other,8 MB,CC4,244 views,31 downloads,2 kernels,0 topics,https://www.kaggle.com/ardamavi/sign-language-digits-dataset,"Sign Language Digits Dataset
Dataset GitHub Page: github.com/ardamavi/Sign-Language-Digits-Dataset
By Turkey Ankara Ayrancı Anadolu High School Students
Turkey Ankara Ayrancı Anadolu High School's Sign Language Digits Dataset
This dataset prepared by our school students.
Dataset Preview:
Details of datasets:
Image size: 64x64
Color space: Grayscale
File format: npy
Number of classes: 10 (Digits: 0-9)
Number of participant students: 218
Number of samples per student: 10
Details of datasets in GitHub Repo:
Repo: github.com/ardamavi/Sign-Language-Digits-Dataset
Image size: 100x100
Color space: RGB
Project executives:
Zeynep Dikle & Arda Mavi
Turkey Ankara Ayrancı Anadolu High School Students
For Development:
Processing Dataset:
For processing the dataset, look up Arda Mavi's GitHub Gist: gist.github.com/ardamavi/get_dataset.py"
Lots of code,"41 language, 43 Gigabytes",Vladislav Zavadskyy,4,"Version 1,2017-12-20",linguistics,Other,8 GB,CC4,"1,023 views",24 downloads,,0 topics,https://www.kaggle.com/zavadskyy/lots-of-code,"Content
Plain text, pulled from GitHub, sorted and concatenated into one file per language. Among those languages are:
abap: 0.138GB
actionscript: 0.684GB
ada: 0.002GB
assembly: 2.134GB
c: 4.452GB
clojure: 0.136GB
cobol: 0.483GB
code: 7.725GB
cpp: 3.248GB
crystal: 0.069GB
csharp: 1.205GB
css: 0.881GB
cuda: 0.275GB
d: 0.990GB
dart: 0.655GB
delphi: 0.514GB
erlang: 0.343GB
fortran: 1.127GB
go: 4.471GB
haskell: 0.447GB
html: 2.158GB
java: 1.049GB
js: 4.863GB
julia: 0.144GB
lua: 0.301GB
matlab: 0.257GB
perl: 0.585GB
php: 1.300GB
prolog: 0.146GB
python: 0.911GB
r: 0.214GB
ruby: 0.625GB
rust: 0.434GB
sas: 0.272GB
scala: 0.458GB
shell: 0.175GB
tex: 0.554GB
vbnet: 0.389GB
xml: 5.160GB
coffeescript: 0.106GB
lisp: 0.699GB
Useful things
Data loader written in python and simple classifying LSTM (TensorFlow), are going to be available here, once I put them there.
Acknowledgements
I would like to thank to all contributors to any repository on GitHub as it's hard to thank only to the contributors of repositories presented in this dataset. But I'll try anyway: if you've contributed to one or more repository in this list, thank you.
I'm sorry, if I forgot to mention you, even though your code is in the dataset, it's hard to keep in memory list of this size. If you feel your repository should be there, feel free to write me.
Also, I'd like to thank those guys for providing this cheesy hi-res stock image of code."
Unicode 10.0 Character Database in JSON,Machine-readable information on Unicode 10.0 Characters,Rachael Tatman,4,"Version 1,2017-12-16","writing
languages
linguistics",{}JSON,30 MB,Other,395 views,25 downloads,,0 topics,https://www.kaggle.com/rtatman/unicode-100-character-database-in-json,"Context:
In working on Unicode implementations, it is often useful to access the full content of the Unicode Character Database (UCD). For example, in establishing mappings from characters to glyphs in fonts, it is convenient to see the character scalar value, the character name, the character East Asian width, along with the shape and metrics of the proposed glyph to map to; looking at all this data simultaneously helps in evaluating the mapping.
This is a machine-readable version of the Unicode Character Database in JSON format.
Content:
The majority of information about individual codepoints is represented using properties. Each property, except for the Special_Case_Condition and Name_Alias properties, is represented by an attribute. In an XML data file, the absence of an attribute (may be only on some code-points) means that the document does not express the value of the corresponding property. Conversely, the presence of an attribute is an expression of the corresponding property value; the implied null value is represented by the empty string.
The Name_Alias property is represented by zero or more name-alias child elements. Unlike the situation for properties represented by attributes, it is not possible to determine whether all of the aliases have been represented in a data file by inspecting that data file.
The name of an attribute is the abbreviated name of the property as given in the file PropertyAliases.txt in version 6.1.0 of the UCD. For the Unihan properties, the name is that given in the various versions of the Unihan database (some properties are no longer present in version 6.1.0).
For catalog and enumerated properties, the values are those listed in the file PropertyValueAliases.txt in version 6.1.0 of the UCD; if there is an abbreviated name, it is used, otherwise the long name is used. Note that the set of possible values for a property captured in this schema may change from one version to the next.
The following properties are associated with code points:
Age property
Name properties
Name Aliases
Block
General Category
Combining properties
Bidirectionality properties
Decomposition properties
Numeric Properties
Joining properties
Linebreak properties
East Asian Width property
Case properties
Script properties
ISO Comment properties
Hangul properties
Indic properties
Identifier and Pattern and programming language properties
Properties related to function and graphic characteristics
Properties related to boundaries
Properties related to ideographs
Miscellaneous properties
Unihan properties
Tangut data
Nushu data
For additional information, please consult the full documentation on the Unicode website.
Acknowledgements:
Copyright © 1991-2017 Unicode, Inc. All rights reserved. Distributed under the Terms of Use in http://www.unicode.org/copyright.html.
Permission is hereby granted, free of charge, to any person obtaining a copy of the Unicode data files and any associated documentation (the ""Data Files"") or Unicode software and any associated documentation (the ""Software"") to deal in the Data Files or Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, and/or sell copies of the Data Files or Software, and to permit persons to whom the Data Files or Software are furnished to do so, provided that either (a) this copyright and permission notice appear with all copies of the Data Files or Software, or (b) this copyright and permission notice appear in associated Documentation."
MRI Brain Image,,Jaish K,4,"Version 1,2018-02-02",,Other,26 KB,CC0,138 views,29 downloads,,0 topics,https://www.kaggle.com/jaishofficial/mri-brain-image,This dataset does not have a description yet.
Global suicide data,,Sathu79,4,"Version 1,2017-12-23","databases
psychometrics
health",CSV,289 KB,CC0,438 views,86 downloads,,0 topics,https://www.kaggle.com/sathutr/global-suicide-data,"As the tagline of ‘American Association of Suicidology’ says I strongly believe that suicide prevention is everyone’s business. The act of ending one’s own life stating the reasons to be depression, alcoholism or any other mental disorders for that matter is not a considerable idea keeping in mind that anything can be overcome with reliable help and lifestyle. We can choose to stand together in the face of a society which may often feel like a lonely and disconnected place, and we can choose to make a difference by making lives more livable for those who struggle to cope. Through this project, I am hoping to identify the trends of suicidal rates by country, gender, age and ethnicity. And relate the trends to the possible reasons that leads to the drastic decision, which might help us to curb the thought in the very beginning.
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too. Data on suicides is deficient for two reasons, first of all, there is a problem with the frequency and reliability of vital registration data in many countries – an issue that undermine the quality of mortality estimates in general, not just suicide. Secondly, there are problems with the accuracy of the official figures made available, since suicide registration is a complicated process involving several responsible authorities with medical and legal concerns. Moreover, the illegality of suicidal behavior in some countries contributes to under reporting and misclassification. I was lucky enough to obtain enough data from different reliable resources. I will be starting off the project with the most reliable datasets available for us on suicide.
•World Health Organization (WHO) dataset which contains entity wise suicide rates, crude suicide rates per gender and country which are age standardized which has a geographical coverage of 198 countries. The time spanning from 1950-2011.
•Samaritans statistics report 2017 including data for 2013-2015, in order to reduce the time, it takes to register deaths, the maximum time between a death and registration is eight days.
•American Association of Suicidology facts and statistics which are categorized by age, gender, region and ethnicity.
Inspiration: To visualize the trends and patterns by merging different datasets available regarding the subject matter from different organizations, deriving the major causes for the drastic stride. And also observing the changes in patterns over the years by country, sex and ethnicity
Understanding the data: It is always tricky to understand the suicide statistics as they may not be so straight forward as they appear to be. Generally, the rate is per 100,000. It is done this way to adjust the underlying population size. ‘Age-standardized’ rates have been standardized to the world population to increase the confidence while making the comparisons. On the other hand, ‘Crude rates’ have not been standardized like the prior, so they are just the basic calculation of number of deaths divided by the population (x100,000). The size of the population and specific cohort is also to be taken into account as smaller groups often produce less reliable rates per 100,000. When examining the suicide trends over a period of time it is also important to look over a relatively long period. Increases and decreases for a year at a time should not be considered in isolation."
Spoken Verbs,Classify simple audio commands,JohannesBuchner,4,"Version 3,2017-12-22|Version 2,2017-12-22|Version 1,2017-12-22","languages
acoustics
communication
human-computer interaction",Other,349 MB,CC4,439 views,18 downloads,,,https://www.kaggle.com/jbuchner/spokenverbs,"Context
I want my computer to react to simple, short, predefined commands. I do not need it to understand any meaning or do complex things, just to recognize and react. I created this dataset to explore possible audio classification algorithms that can ultimately be transferred to the real world.
Content
I have selected single-syllable English verbs, obtained their pronounciations (phonemes) via to the British English Example Pronciation dictionary, and let espeak pronounce it varying the pronounciations, stress, pitch, speed and speaker.
You can play individual words samples with any audio player that understands ogg (e.g. VLC). The samples still sound a bit mechanic, but it is a start.
For simplicity, consider the dog sub-sample, which only contains four commands: ""chase"" ""fetch"" ""sit"" ""walk"". They are fairly easy for the ear to distinguish, even in poor quality audio.
To create the classification data set (db.dog.hdf5), I added AURORA noise with varying volume and convertedd the audio data to 24x24 frequency-vs-time ""images"" (spectrograms). The file comes with class labels (word ID).
Example python script to load and train: https://github.com/JohannesBuchner/spoken-command-recognition/blob/master/traincommanddetect_svm.py
More details and generating scripts can be found at https://github.com/JohannesBuchner/spoken-command-recognition
Acknowledgements
Pronounciation dictionary: BEEP: http://svr-www.eng.cam.ac.uk/comp.speech/Section1/Lexical/beep.html Noise samples: AURORA: https://www.ee.columbia.edu/~dpwe/sounds/noise/ eSPEAK: http://espeak.sourceforge.net/ and mbrola voices http://www.tcts.fpms.ac.be/synthesis/mbrola/mbrcopybin.html
Inspiration
The open question is if these computer-generated data sets, robustified by pronounciation and noise variations, can be transferred into real applications.
At the moment I see companies trying to solve a hard problem (map arbitrary, open-ended speech to text and identify meaning), while the easier problem of detecting a predefined word and mapping it to a predefined action should be solvable with currently available tools. Machine learning audio training data is lacking, and this aims to solve that."
submission38 LB-0.1448,,Submarineering,4,"Version 4,2018-02-02|Version 3,2018-01-13|Version 2,2018-01-05|Version 1,2017-12-30",,CSV,8 MB,CC0,795 views,552 downloads,4 kernels,0 topics,https://www.kaggle.com/submarineering/submission38-lb01448,This dataset does not have a description yet.
Advertising,Practice Data Analysis and Logistic Regression Prediction,fayomi,4,"Version 1,2017-12-15","demographics
marketing",CSV,105 KB,Other,578 views,105 downloads,2 kernels,0 topics,https://www.kaggle.com/fayomi/advertising,"Content
Dataset Created by Jose Portilla and Pierian Data for his Udemy Course (Python for Data Science and Machine Learning Bootcamp) if you want to be a data scientist I cannot recommend this course enough
Inspiration
Explore the data to find predict who is more likely to click the ad!"
Expat Insider 2017,InterNations survey conducted in 2017 on expats.,Joel Jacobsen,4,"Version 1,2017-12-14","politics
demographics
international relations",CSV,4 KB,Other,109 views,15 downloads,,0 topics,https://www.kaggle.com/jej13b/expat-insider-2017,"Context
This is the dataset from the 2017 Expat Insider survey conducted through InterNations.
Content
This is the dataset from the 2017 Expat Insider survey conducted through InterNations. The data is based on surveys conducted on expats regarding different aspects of living in their respective foreign countries. All values are rankings. I was not able to find an official dataset, so all data was hand typed from the official report found here https://www.internations.org/expat-insider/. This dataset only includes countries who had values in all of the conducted surveys.
Acknowledgements
Find the official report here https://cms-internationsgmbh.netdna-ssl.com/cdn/file/2017-09/Expat_Insider_2017_The_InterNations_Survey.pdf.
Inspiration
I wanted a dataset to use to compare different aspects of life for expats in their respective countries and maybe to compare with the lives of the actual citizens of each country or perhaps other related topics."
58 years of Temperature Data,"Laverton, VIC, AUS Temperatures 1943-2017",Cameron Chandler,4,"Version 2,2017-12-13|Version 1,2017-12-12","climate
weather",CSV,4 MB,Other,178 views,17 downloads,,,https://www.kaggle.com/blazethrower/laverton-vic-aus-temperatures-19432017,"Hi! I'm new here and have zero knowledge of anything data science, but would love to one day and will study it at uni. All I've been able to do is deseasonalise this data and find an increasing linear trend. But I've just graduated high school and would love to see how it's done properly.
The data was collected by the Laverton RAAF station 087031 (37.86°S, 144.76°E) from October 1, 1943 and has been meticulously collated up to August 8, 2017. The data was sourced from the Australian Bureau of Meteorology (the first year of data can be found here). There was also a period from 1999-2003 where only integer values were take (go figure) and this can be seen within the right half of this page's banner image.
I originally collated the data to see how climate change was affecting temperatures in my local area here and now, and now with a sparked passion for data science, am keen to learn how it could be processed for long term forecasting."
Bagrut grades in Israeli high schools (2013-2016),"Over 60,000 average Bagrut grades in Israel",Eran Machlev,4,"Version 3,2018-01-03|Version 2,2018-01-03|Version 1,2018-01-03",education,CSV,596 KB,Other,603 views,47 downloads,,,https://www.kaggle.com/emachlev/bagrut-israel,"Info
Te'udat Bagrut (Hebrew: תעודת בגרות‬) is a certificate which attests that a student has successfully passed Israel's high school matriculation examination. Bagrut is a prerequisite for higher education in Israel. A Bagrut certificate is awarded to students who pass the required written (and in some cases oral) subject-matter examinations with a passing mark (56% or higher) in each exam. The Bagrut certificate however should not be confused with a high school diploma (te'udat g'mar tichon, Hebrew: תעודת גמר תיכון‬), which is a certificate awarded by the Ministry of Education attesting that a student has completed 12 years of study. (Source: Wikipedia)
Content
This file contains over 60,000 average Bagrut grades that were taken between 2013 and 2016 by over 1800 schools in various subjects.
This data was posted under the Israeli Freedom of Information law and was formatted by me"
Bollywood Movie Dataset,Movie details available before a movie's release in Bollywood,Mitesh Kumar Singh,4,"Version 4,2018-02-04|Version 3,2018-02-03|Version 2,2018-02-03|Version 1,2018-02-02","india
entertainment
classification",CSV,318 KB,CC0,934 views,63 downloads,,,https://www.kaggle.com/mitesh58/bollywood-movie-dataset,"Context
Indian Hindi Cinema, popularly known as Bollywood has witnessed exponential growth in terms of volume of business, manpower employed, number of movies produced each year and also the global reach. Hence, it could be of great commercial importance to develop a model which could predict the success of a movie before it's release. However, it is not easy to forecast demand for a movie. There are a number of factors like Actors, Directors, Time of Release, Genre, Production house etc. which affect the outcome of a movie.
The primary requirement to develop such a model would be the availability of Bollywood movie data. Thus, I created this dataset while working on my senior year research project, titled 'Predicting success of upcoming Bollywood movies'.
Content
The data has been created manually by visiting different websites. The primary ones being Wikipedia, boxofficeindia.com and IMDB. The data contains 1285 rows with movies released between the years 2001 to 2014.
The hitFlop column contains values from 1 to 9 with
1 - Disaster
2 - Flop
3 - Below Average
4 - Average
5 - Semi Hit
6 - Hit
7 - Super Hit
8 - Blockbuster
9 - All-Time Blockbuster
Acknowledgements
Research Guide - Dr. S.K. Saha
Inspiration
Can we save the time and money wasted by movie viewers on viewing flop and disaster movies?
Can we suggest must-watch movies to movie viewers even before movies release?
Can we classify upcoming movies into 1 of 9 categories even before their release?"
DenseNet-169,DenseNet-169 Pre-trained Model for Pytorch,PyTorch,4,"Version 1,2017-12-13","machine learning
pre-trained model",Other,52 MB,CC0,276 views,3 downloads,,0 topics,https://www.kaggle.com/pytorch/densenet169,"DenseNet-169
Densely Connected Convolutional Networks
Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and models are available at this https URL.
Authors: Gao Huang, Zhuang Liu, Kilian Q. Weinberger, Laurens van der Maaten
https://arxiv.org/abs/1608.06993
DenseNet Architectures
What is a Pre-trained Model?
A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.
Why use a Pre-trained Model?
Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it."
ResNet-18,ResNet-18 Pre-trained Model for PyTorch,PyTorch,4,"Version 2,2017-12-13|Version 1,2017-12-13","machine learning
pre-trained model",Other,41 MB,CC0,859 views,13 downloads,,0 topics,https://www.kaggle.com/pytorch/resnet18,"ResNet-18
Deep Residual Learning for Image Recognition
Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity.
An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.
The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.
Authors: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun
https://arxiv.org/abs/1512.03385
Architecture visualization: http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006
What is a Pre-trained Model?
A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.
Why use a Pre-trained Model?
Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it."
VGG-16,VGG-16 Pre-trained Model for PyTorch,PyTorch,4,"Version 1,2017-12-15","machine learning
pre-trained model",Other,490 MB,CC0,255 views,4 downloads,,0 topics,https://www.kaggle.com/pytorch/vgg16,"VGG-16
Very Deep Convolutional Networks for Large-Scale Image Recognition
In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.
Authors: Karen Simonyan, Andrew Zisserman
https://arxiv.org/abs/1409.1556
VGG Architectures
What is a Pre-trained Model?
A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.
Why use a Pre-trained Model?
Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it."
VGG-13 with batch normalization,VGG-13 Pre-trained model with batch normalization for PyTorch,PyTorch,4,"Version 1,2017-12-16","machine learning
pre-trained model",Other,471 MB,CC0,157 views,,,0 topics,https://www.kaggle.com/pytorch/vgg13bn,"VGG-13
Very Deep Convolutional Networks for Large-Scale Image Recognition
In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.
Authors: Karen Simonyan, Andrew Zisserman
https://arxiv.org/abs/1409.1556
VGG Architectures
What is a Pre-trained Model?
A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.
Why use a Pre-trained Model?
Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it."
UFC Fight Data Refactored,https://www.kaggle.com/calmdownkarm/ufcdataset,dgoke1,4,"Version 1,2017-12-01",sports,CSV,487 KB,CC0,192 views,32 downloads,,0 topics,https://www.kaggle.com/dgokeeffe/ufc-fight-data-refactored,"Context
This is a refactored version of https://www.kaggle.com/calmdownkarm/ufcdataset
The original dataset was separated only by fights, not by fights and fighters, replicating the metric columns and making it hard to work with.
Content
Additionally the statistics you would see would be the sum of all the fights BEFORE the fight that is referenced in the row itself. Every time a new fighter appeared their metrics would be NA for their first fight. If you're interested in predicting fights or knowing what metrics are the most important in a fight itself I think it's best separated. So I've undone this so every fight what you see is the statistics for the fight itself. If you want the sum that's easy enough to compute in R or Python.
Acknowledgements
All credit goes to https://www.kaggle.com/calmdownkarm for making this dataset
Inspiration
MMA is a highly unpredictable sport. It's the epitome of competition in my opinion where fighters pour their hearts out on the line for us."
Cars Data,,Lilit Janjughazyan,4,"Version 1,2017-12-11",,CSV,37 KB,Other,213 views,24 downloads,,0 topics,https://www.kaggle.com/ljanjughazyan/cars1,This dataset does not have a description yet.
Rome B&Bs reviews,A corpus of more than 220k reviews (and associated data) from TripAdvisor,Domenico Delle Side,4,"Version 1,2017-12-11","linguistics
internet",CSV,53 MB,CC4,721 views,29 downloads,2 kernels,0 topics,https://www.kaggle.com/nicodds/rome-b-and-bs,"Context
I started to scrape TripAdvisor reviews for a personal project on Sentiment Analysis. I thought it could be good to share my data on Kaggle, since this can help other with similar ideas.
Content
The dataset represents a corpus of more than 220k reviews scraped from TripAdvisor (in reviews.csv). Accompanying information on the B&Bs (in properties.csv) are also provided.
Each review has the following fields: - review_title, - review_date, - review_rating, - review_text, - review_language, - review_user, - review_id, - property_id
Each property has the following fields: - property_id, - property_name, - property_reviews (the total number of reviews of the B&B), - property_rating (the average rating of the B&B as displayed on TripAdvisor), - property_address, - property_latitude, - property_longitude
The reviews cover a time slice running from 2002 up to the first days of December 2017.
All data has been scraped from TripAdvisor website using CHeSF (the Chrome Headless Scraping Framework). CHeSF is really useful when you have to scrape javascript intensive web pages.
Inspiration
Other than Sentiment Analisys, this dataset could be useful to investigate strategies against fake reviews. I performed a simple analysis that identified several possibly fake users. Other people smarter than me could find more efficient strategies."
20k Tweets Relating to #JerusalemEmbassy,,Minerwa Min,3,"Version 1,2017-12-10","politics
demographics",CSV,1 MB,CC0,129 views,8 downloads,,,https://www.kaggle.com/minerva666/jerusalemembassy,"Context
I extracted 20,000 English language tweets relating to #JerusalemEmbassy . In the wake of USA's decision to move its embassy in Israel to Jerusalem (December 6, 2017), there has been an outpouring of tweets under this hashtag. This will be a good data set for data scientists interested in politics to carry out sentiment analysis and NLP. The data can be used for data visualization and geo-location based plotting. These tweets were collected on December 9, 2017.
Content
The CSV contains the tweets extracted under #JerusalemEmbassy . The column text contains the actual text of the tweets. Other columns are:
favorited : FALSE means the tweets was not favorited by anyone. TRUE means that people clicked the heart symbol to indicate they liked the content of the tweet
favoriteCount : Number of times the heart symbol was clicked, It will be 0 for the FALSE of the above
replytoSN
created: The date and time when the tweet was created isRetweet:
whether tweet is a re-tweet or not
retweetCount : Number of times a tweet was re-tweeted longitude & latitude: Geo-location info
Inspiration
Politics and current affairs interest me as as much as data science does (almost!! ). The world is in a state of flux and while maintaining a politically neutral stand I want to see how advances in data science can help us understand current affairs better. Some of the questions that can be answered include:
1) What are the overall sentiments associated with these tweets? (An analysis of 6K English tweets extracted on December 6, 2017 may surprise you: http://completestatisticaldatanalysis.com/o-jerusalem-jerusalemembassy-hashtag-on-december-6-2017/)
2) What are the dominant words that turn up in the word cloud?
3) Where are most of tweets coming from?
4) Which tweets were re-tweeted the most?
5) Which user received the most responses?
6) Was there any difference in the sentiments of tweets emanating from different countries?
7) How did the tweeting and re-tweeting intensity vary over a day?"
People and Character Wikipedia Page Content,"More than 110,000 people with matching descriptions",Stephen Thompson,3,"Version 1,2017-12-03","people
lists of people",CSV,222 MB,CC3,154 views,12 downloads,,0 topics,https://www.kaggle.com/coffeenmusic/people-on-wikipedia,"Context
This is a set of Wikipedia Title/Content pairs where the content is always based on a person. Maybe person is not the best word, because this set does not necessarily contain humans and the content can be about fictitious characters. I came across this data set by trying to remove this content from my primary set and thought it might be useful to someone else.
Content
There are two columns Wiki_Title and Content. This data was collected through the Wikipedia API and all the content is raw data from wikipedia.page. Content is from page.content and does contain the full page content. The people described can be characters, real people, fake people, humans, non-humans. Although, the majority of this data will be real people.
The Wiki_Title almost always contains the name of the person in the content or their nickname, but I know this is not true 100% of the time. It is too much data for me to verify the 100,000+ examples so I cannot verify all content is in fact about a person/character, but if there are improvements to be made I would like to do so.
Acknowledgements
Wikipedia
Python Wikipedia API https://github.com/goldsmith/Wikipedia
Photo by Paul Dufour on Unsplash
Inspiration
I think it would be interesting to try and match the celebrities in this data to the celebrity image data set and try and predict appearance of people based on their wikipedia content."
MLB Home Run Exit Velocity: 2015 vs. 2017,Home Run Ball Exit Speeds - 2015 vs. 2017,bitroy,3,"Version 1,2017-12-07","baseball
sports",CSV,377 KB,Other,171 views,21 downloads,,0 topics,https://www.kaggle.com/bitroy/mlb-home-run-exit-velocity-2015-vs-2017,"Everyone is wondering if the players (or balls) are 'Juiced' this year after we observed a significant uptick in the number of home runs. If so, would the Home Run Ball exit Velocity change? Take a Look."
Pretrain file,,Aydin Ayanzadeh,3,"Version 2,2017-12-11|Version 1,2017-12-10",,Other,1 GB,ODbL,47 views,0 downloads,,0 topics,https://www.kaggle.com/ayanzadeh93/pretrain-file,This dataset does not have a description yet.
Football Players,,Elen Vardanyan,3,"Version 1,2017-12-12",,Other,19 MB,Other,210 views,57 downloads,,0 topics,https://www.kaggle.com/lnvardanyan/football-players,This dataset does not have a description yet.
Pokemon Images Dataset,Dataset of 819 Pokemon images,kvpratama,3,"Version 1,2017-12-12",,Other,39 MB,CC0,377 views,54 downloads,,,https://www.kaggle.com/kvpratama/pokemon-images-dataset,"Context
I collected this dataset for my school project. The project is to train GAN to generate new Pokemon. I had a difficult time to find training dataset that is complete and clean. So I gather this collection of image and publish it here in hope that it will help others who need similar dataset.
You can find my project on my Github
Content
819 transparent Pokemon images in png format size 256x256.
Acknowledgements
I collected the image mostly from this website https://veekun.com/dex/downloads
Banner image is taken from https://viking011.deviantart.com/art/Pokemon-Poster-436455502
Inspiration
Since I failed to generate new Pokemon with clarity (I can only generate the shape) I wish there will be others that could do it with this dataset. If you managed to, please share it!"
Random Shopping cart,Random Shopping cart for assosation rules,Albert Costas,3,"Version 1,2017-12-06",product,CSV,565 KB,CC0,416 views,66 downloads,2 kernels,0 topics,https://www.kaggle.com/acostasg/random-shopping-cart,"Context
Random Shopping cart
Content
Date to add register
Id transaction
Product for id transaction
Acknowledgements
The dataset is transform from Random Shopping cart https://www.kaggle.com/fanatiks/shopping-cart
Inspiration
The inspiration is the testing the association rule learning https://en.wikipedia.org/wiki/Association_rule_learning"
Top 10 Cryptocurrencies,Updated till December 2017,Sudeepta Kkr,3,"Version 5,2018-01-24|Version 4,2017-12-20|Version 3,2017-12-20|Version 2,2017-12-08|Version 1,2017-12-01",finance,Other,267 KB,CC0,249 views,34 downloads,,0 topics,https://www.kaggle.com/skr912/top-10-cryptocurrencies-updateddecember-2017,"Introduction This file contains the values of the price for top 10 cryptocurrencies (including scams) recorded on daily base, I decide to include all coins. All this dataset come from coinmarketcap historical pages, grabbed using just an R script. Thanks coinmarketcap to making this data available for free (and for every kind of usage). The datasets will be updated on a regular basis (Once a week).
Available columns in the dataset:
Date - the day of recorded values
Open - the opening price (in USD)
High - the highest price (in USD)
Low - the lowest price (in USD)
Close - the closing price (in USD)
Volume - total exchanged volume (in USD)
Market.Cap - the total market capitalization for the coin (in USD)
coin - the name of the coin
Delta - calculated as (Close - Open) / Open
I m a student of BSc in IT. This semester i m having a course on ""Advanced Analytics"". As needed for my studies, i have to do analysis on a specific topic for which i have chosen Cryptocurrency. Due to lack of updated data i have worked in this and arranged this file. Hope this will be helpful."
IMDB movie rating,This is IMDB movie rating dataset,Bhuwan pandeya,3,"Version 1,2017-12-06",,CSV,2 KB,CC0,264 views,40 downloads,,0 topics,https://www.kaggle.com/pandeya/imdb-movie-rating,This dataset does not have a description yet.
US-based Jobs from Dice.com,"20,000+ job listings extracted from Dice.com",PromptCloud,3,"Version 1,2017-12-04",internet,CSV,17 MB,CC4,386 views,25 downloads,,0 topics,https://www.kaggle.com/PromptCloudHQ/usbased-jobs-from-dicecom,"Context
This is a pre-crawled dataset, taken as subset of a bigger dataset (close to 4 million job listings) that was created by extracting data from Dice.com, a leading US-based technology job board.
The complete dataset can be downloaded from DataStock — a repository of clean and ready to use web datasets with historical records.
Content
The dataset has the following data fields:
country_code
date_added
job_board
job_description
job_title
job_type
location
organization
page_url
phone_number
salary
sector
Time period: 2016
Acknowledgements
This dataset was created by PromptCloud's in-house web-crawling service.
Inspiration
Following are some of the analyses that can be performed
Job title depending on the location/sector
Job description with respect to the skills
Job type (full time, contractual, etc.) depending on the location/sector"
InceptionResNetV2,InceptionResNetV2 Pre-trained Model for Keras,Keras,3,"Version 2,2017-12-13|Version 1,2017-12-07","machine learning
pre-trained model",Other,392 MB,CC0,383 views,19 downloads,,0 topics,https://www.kaggle.com/keras/inceptionresnetv2,"Inception-Resnet-V2
Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning
Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge
Authors: Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi
https://arxiv.org/abs/1602.07261
InceptionResnetV2 Architecture
What is a Pre-trained Model?
A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.
Why use a Pre-trained Model?
Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it."
Xception,Xception Pre-trained Model for Keras,Keras,3,"Version 2,2017-12-13|Version 1,2017-12-07","machine learning
pre-trained model",Other,155 MB,CC0,599 views,21 downloads,2 kernels,0 topics,https://www.kaggle.com/keras/xception,"Xception
Xception: Deep Learning with Depthwise Separable Convolutions
We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.
Author: François Chollet
https://arxiv.org/abs/1610.02357
What is a Pre-trained Model?
A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.
Why use a Pre-trained Model?
Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it."
DenseNet-121,DenseNet-121 Pre-trained Model for PyTorch,PyTorch,3,"Version 2,2017-12-13|Version 1,2017-12-13","machine learning
pre-trained model",Other,29 MB,CC0,359 views,,,0 topics,https://www.kaggle.com/pytorch/densenet121,"DenseNet-121
Densely Connected Convolutional Networks
Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and models are available at this https URL.
Authors: Gao Huang, Zhuang Liu, Kilian Q. Weinberger, Laurens van der Maaten
https://arxiv.org/abs/1608.06993
DenseNet Architectures
What is a Pre-trained Model?
A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.
Why use a Pre-trained Model?
Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it."
DenseNet-161,DenseNet-161 Pre-trained Model for PyTorch,PyTorch,3,"Version 1,2017-12-13","machine learning
pre-trained model",Other,106 MB,CC0,206 views,2 downloads,,0 topics,https://www.kaggle.com/pytorch/densenet161,"DenseNet-161
Densely Connected Convolutional Networks
Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and models are available at this https URL.
Authors: Gao Huang, Zhuang Liu, Kilian Q. Weinberger, Laurens van der Maaten
https://arxiv.org/abs/1608.06993
DenseNet Architectures
What is a Pre-trained Model?
A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.
Why use a Pre-trained Model?
Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it."
SqueezeNet 1.0,SqueezeNet 1.0 Pre-trained Model for PyTorch,PyTorch,3,"Version 1,2017-12-15","machine learning
pre-trained model",Other,4 MB,CC0,242 views,2 downloads,,0 topics,https://www.kaggle.com/pytorch/squeezenet1,"SqueezeNet 1.0
SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size
Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet).
Authors: Forrest N. Iandola, Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, Kurt Keutzer
https://arxiv.org/abs/1602.07360
SqueezeNet Architectures
What is a Pre-trained Model?
A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.
Why use a Pre-trained Model?
Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it."
VGG-16 with batch normalization,VGG-16 Pre-trained model with batch normalization for PyTorch,PyTorch,3,"Version 1,2017-12-16","machine learning
pre-trained model",Other,490 MB,CC0,557 views,7 downloads,,0 topics,https://www.kaggle.com/pytorch/vgg16bn,"VGG-16
Very Deep Convolutional Networks for Large-Scale Image Recognition
In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.
Authors: Karen Simonyan, Andrew Zisserman
https://arxiv.org/abs/1409.1556
VGG Architectures
What is a Pre-trained Model?
A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.
Why use a Pre-trained Model?
Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it."
VGG-19 with batch normalization,VGG-19 Pre-trained model with batch normalization for PyTorch,PyTorch,3,"Version 1,2017-12-16","machine learning
pre-trained model",Other,509 MB,CC0,234 views,2 downloads,,0 topics,https://www.kaggle.com/pytorch/vgg19bn,"VGG-19
Very Deep Convolutional Networks for Large-Scale Image Recognition
In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.
Authors: Karen Simonyan, Andrew Zisserman
https://arxiv.org/abs/1409.1556
VGG Architectures
What is a Pre-trained Model?
A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.
Why use a Pre-trained Model?
Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it."
BBVA data challenge,,SebastianZanabria,3,"Version 1,2017-12-25",,CSV,3 MB,CC0,349 views,37 downloads,,0 topics,https://www.kaggle.com/seussz/bbva-data-challenge,This dataset does not have a description yet.
H1B Data Set 2017,Data provided by US Department of Labor about H1B Visa - 2017,Manoj,3,"Version 1,2018-01-30","united states
law
international relations
immigration",Other,48 MB,Other,227 views,29 downloads,,0 topics,https://www.kaggle.com/jonamjar/h1b-data-set-2017,"U.S. Department of Labor Employment and Training Administration Office of Foreign Labor Certification Public Disclosure File: Federal Fiscal Year: Reporting Period: H-1B iCERT LCA 2017 October 1, 2016, through September 30, 2017 Important Note: This public disclosure file contains administrative data from employers’ Labor Condition Applications (ETA Forms 9035 & 9035E) and the certification determinations processed by the Department’s Office of Foreign Labor Certification, Employment and Training Administration where the date of the determination was issued on or after October 1, 2016, and on or before September 30, 2017. All data were extracted from the Office of Foreign Labor Certification’s iCERT Visa Portal System; an electronic filing and application processing system of employer requests for H-1B nonimmigrant workers."
glove.6B.50d.txt,,DevjyotiChandra,3,"Version 1,2017-12-26",,Other,68 MB,CC0,456 views,125 downloads,,0 topics,https://www.kaggle.com/devjyotichandra/glove6b50dtxt,This dataset does not have a description yet.
My Clash Royale Ladder Battles,CSV file with my Clash Royale ladder battles,José Vicente,3,"Version 28,2018-02-25|Version 27,2018-02-24|Version 26,2018-02-24|Version 25,2018-02-22|Version 24,2018-02-18|Version 23,2018-02-18|Version 22,2018-02-17|Version 21,2018-02-09|Version 20,2018-02-06|Version 18,2018-02-03|Version 17,2018-01-23|Version 16,2018-01-21|Version 15,2018-01-19|Version 14,2018-01-16|Version 13,2018-01-12|Version 12,2018-01-10|Version 11,2018-01-08|Version 10,2018-01-06|Version 9,2018-01-05|Version 8,2018-01-03|Version 7,2018-01-02|Version 6,2017-12-31|Version 5,2017-12-30|Version 4,2017-12-29|Version 3,2017-12-28|Version 2,2017-12-26|Version 1,2017-12-25",video games,CSV,267 KB,CC4,465 views,23 downloads,2 kernels,0 topics,https://www.kaggle.com/pepe93/my-clash-royale-ladder-battles,"Clash Royale Ladder Battles dataset
I expect to update this dataset daily
I encourage you to make an EDA or even train a model to predict if I win or lose!
Notes:
""op"" means opponent
my/op_troops, my/op_buildings, my/op_spells, my/op_commons, my/op_rares, my/op_epics, my/op_legendaries: Number of cards of that type.
my/op_name_of_card: Level of that card. If 0 then that card wasn't used in that battle.
This is my second Clash Royale account and I don't play 2v2 matches, 1v1 ladder matches only."
NLP-Word2Vec-Embeddings(pretrained),Existing word2vec embeddings including glove and google news,pkugoodspeed,3,"Version 1,2018-02-02","linguistics
nlp",Other,2 GB,CC0,74 views,,,0 topics,https://www.kaggle.com/pkugoodspeed/nlpword2vecembeddingspretrained,"Context
Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space.
Content
Existing Word2Vec Embeddings. GoogleNews-vectors-negative300.bin glove.6B.50d.txt glove.6B.100d.txt glove.6B.200d.txt glove.6B.300d.txt
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
BikeShare Analysis,Analyze Bike Sharing Dataset,Samrat,3,"Version 1,2018-01-03",,CSV,12 MB,Other,381 views,42 downloads,,0 topics,https://www.kaggle.com/samratp/bikeshare-analysis,"Context
Over the past decade, bicycle-sharing systems have been growing in number and popularity in cities across the world. Bicycle-sharing systems allow users to rent bicycles for short trips, typically 30 minutes or less. With the latest technologies, it is easy for a user of the system to access a dock within the system to unlock or return bicycles. These technologies also provide a wealth of data that can be used to explore how these bike-sharing systems are used. The data consists of BikeShare information for three large cities in the US - New York City, Chicago, and Washington, DC.
Content
Sample Data
'tripduration', '839'
'starttime', '1/1/2016 00:09:55'
'stoptime', '1/1/2016 00:23:54'
'start station id', '532'
'start station name', 'S 5 Pl & S 4 St'
'start station latitude', '40.710451'
'start station longitude', '-73.960876'
'end station id', '401'
'end station name', 'Allen St & Rivington St'
'end station latitude', '40.72019576'
'end station longitude', '-73.98997825'
'bikeid', '17109'
'usertype', 'Customer'
'birth year', ''
'gender', '0'
Acknowledgements
Thanks to Udacity for the data.
Inspiration
This data set can be a very good inspirations for not only new users but experienced veterans who wants to explore more insights."
Medical Appointment,,Alvaro Flores,3,"Version 1,2017-12-18",,CSV,537 KB,GPL,224 views,17 downloads,,,https://www.kaggle.com/afflores/medical-appointment,"Context
The No Show problem is one of the bigest on the health industry, about 30% of the patient fail theirs appointments.
Content
61K points, from 2017.01.01 to 2017.04.30 and 19 features to work with
Data Dictionary
especialidad : what kind of specialist is going to. Ie dematologist, etc.
edad: Age
sexo: sex, 1: Male, 2: Female
reserva_mes_d : discrete value for the month of the appointment, 1: Jan, 2: Feb...
reserva_mes_c : continue value for the month of the appointment, the formula is COS(2*reserva_mes_d*Pi/12)
reserva_dia_d : day of the week for the appointment, 1: Mon... 7: Sun
reserva_dia_c : continous value for the day of the week, the formula is COS(2*reserva_dia_d*Pi/7)
reserva_hora_d : discrete value for hour of the appointment
reserva_hora_c : continous value for the hour of the appointment, the formula is COS(2*reserva_hora_d*Pi/24)
creacion_mes_d : discrete value for the month when the appointment was created
creacion_mes_c : continous value for the month when the appointment was created, the formula is COS(2*creacion_mes_d*Pi/12)
creacion_dia_d : same as reserva_dia_d, but considering the day when the appointment was created
creacion_dia_c : same as reserva_dia_c, but considering the day when the appintment was created
creacion_hora_d : hour when the appointment was created
creacion_hora_c : continous value for the creacion_hour_d, the formula is COS(2*creacion_hora_d*Pi/24)
latencia : number of days between the appointment and the date when it was created
canal : channel used for the creation of the apppointment, 1: call center, 2: Personal, 3: Web
tipo : type of appointment, 1: medical, 2: procedures
show : 0: no show, 1: show
Inspiration
Can we use it to predict if a patient is going to show up for his appointment?"
Indonesian Licence Plate,Number plate of West Java Indonesia,Imam Digmi,3,"Version 4,2018-02-02|Version 3,2018-01-20|Version 2,2018-01-02|Version 1,2018-01-02","politics
internet",Other,216 MB,GPL,186 views,15 downloads,,0 topics,https://www.kaggle.com/imamdigmi/indonesian-licence-plate,"Plate Number Recognition Datasets
This datasets provided for Plate Number Recognition in Indonesia and images are containing for Indramayu West Java and Yogyakarta region
Contribution
Yes! check out on my GitHub Repository"
Character Encoding Examples,Example text files for five popular text encodings,Rachael Tatman,3,"Version 1,2017-12-16","writing
languages
linguistics
+ 2 more...",Other,947 KB,CC0,480 views,21 downloads,2 kernels,0 topics,https://www.kaggle.com/rtatman/character-encoding-examples,"Context:
Character encodings are sets of mappings from raw bits (0’s and 1’s) to text characters. When a text encoded with a specific encoder is decoded with a different encoder, it changes the output text. Sometimes this results in completely unreadable text.
This dataset is intended to provide a list of example texts in different character encodings to help you diagnose which file encoding your source file actually in.
Content
This dataset is made up of six text files that represent five different character encodings and six different languages. The character encodings represented in this dataset are ISO-8859-1 (also known as Latin 1),
ASCII, Windows 1251, UTF-16 that has been successfully converted into the UTF-8 and BIG-5. More information on the files is available in the file_guide.csv file.
Each text file contains a header and footer. The body text is delimited by this text:
* START OF THE PROJECT GUTENBERG EBOOK [TITLE OF BOOK GOES HERE] *
* END OF THE PROJECT GUTENBERG EBOOK [TITLE OF BOOK GOES HERE]*
Acknowledgements:
The texts in this dataset were prepared by Project Gutenberg volunteers. These texts are in the public domain.
Inspiration:
Can you build an tool to automatically detect when a file in the wrong encoding is read in?
You can use this dataset to explore what happens when you read in text using different encoders."
Spoken Wikipedia Corpus (Dutch),224 hours of Dutch audio with transcriptions,Rachael Tatman,3,"Version 1,2018-01-04","languages
europe
acoustics
linguistics",Other,8 GB,CC4,429 views,29 downloads,,0 topics,https://www.kaggle.com/rtatman/spoken-wikipedia-corpus-dutch,"Context:
The Spoken Wikipedia project unites volunteer readers of Wikipedia articles. Hundreds of spoken articles in multiple languages are available to users who are – for one reason or another – unable or unwilling to consume the written version of the article. This is time-aligned corpus of these spoken articles, well suited to research and fostering new ways of interacting with the material.
Content:
All spoken articles use a template with slots: filename, speaker, date and revision spoken, ... to insert the audio player and a display of the meta-data on the Wikipedia page. The template also adds spoken articles to a root category. Templates, categories, and meta-data vary between language communities!
The Dutch language portion of this corpus contains 3073 articles read by 145 speakers. There are 224 hours of speech, of which 79 hours is aligned at the word level.
Each article is tokenized into sections, sentences, and tokens. Each token is normalized and the normalization is aligned to the audio. Complete information on the annotation schema can be found in the schema file. For each article, the corpus contains:
audio file(s)
original WikiText
HTML generated by MediaWiki
cleaned and normalized text
alignment between text and audio
meta-information (who, when, what)
For additional information and updates, please see the project website.
Acknowledgements:
This dataset was collected by Arne Köhn, Florian Stegen and Timo Baumann. If you use this dataset in your work, please cite the following paper:
Köhn, A., Stegen, F., & Baumann, T. (2016). Mining the Spoken Wikipedia for Speech Data and Beyond. In the Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016).
Inspiration:
Some possible uses for this corpus include:
train or evaluate automatic speech recognition systems
improve accessibility / spoken article navigation
top contributors speak >30 hours, which is enough audio to train synthesis voices
analyze prosody of reading (large amounts of diversely read text)
analyze prosody of information structure (accessible through links, research on semantic Wikipedia, Dbpedia, ...)"
Beijing PM2.5 concentration,,stawary,3,"Version 1,2017-12-16",,CSV,741 KB,Other,109 views,19 downloads,,0 topics,https://www.kaggle.com/stawary/beijing-pm25-concentration,This dataset does not have a description yet.
survey mental health 2016,,hellenandreea,3,"Version 5,2017-12-14|Version 4,2017-12-14|Version 3,2017-12-14|Version 2,2017-12-14|Version 1,2017-12-14",,CSV,160 KB,Other,141 views,26 downloads,,0 topics,https://www.kaggle.com/hellenandreea/survey-mental-health-2016,This dataset does not have a description yet.
PGA Tour 2016/2017 Leaderboards,,LE PALLEC Clément,3,"Version 1,2017-12-20","golf
sports",Other,941 KB,Other,182 views,47 downloads,,0 topics,https://www.kaggle.com/clementlepallec/pga-tour-20162017-leaderboards,"Context
Contains leaderboards of 2017 PGA Tour season.
Content
Location Course Yardage Purse Pos Player Earnings To Par THRU R1 R2 R3 R4 Total Score
Acknowledgements
Scrapped on Cssports.com
Inspiration
Best players regarding yardage ?"
Consolidated UFO and Weather Data,20 years UFO Sights data along with corresponding weather data,Eduardo Morelli,3,"Version 2,2018-02-08|Version 1,2018-01-28",weather,CSV,2 MB,CC0,92 views,10 downloads,,,https://www.kaggle.com/emorelli/consolidated-ufo-weather-data,"Context
Starting with 20 years data scrapped from NUFORC (from 97 to 2017), plus Mr. Ajayrana UFO report dataset (1949 to 2000), my idea was since the beginning to be able to predict, given sight data, which shape the UFO would be. Therefore, besides NUFORC data, I captured data from Wunderground related to those occurrences.
Content
First of all, I did some web scrapping over NUFORC website collecting data from 1997 to 2017, using R. Then I created another Python program running periodically (every 30 minutes) reading 10 rows at a time from Wunderground, but only for those cities, dates having sight records
Acknowledgements
Thanks to Wunderground for letting me capture small amounts of historical data every day. And thanks to Alura, one of the best online educating platforms I ever used, and for letting me host some of my courses about Big Data. Also, I must thank Mr. Ajayrana for his nice work
Inspiration
It is an ongoing work. Soon I will have more data and be able to create an awesome prediction model"
Chest X-Rays Dataset,Contains folder wise arranged images for 5 diseases.,Yash Prakash,3,"Version 1,2018-01-29","healthcare
diseases
medicine",Other,127 MB,CC0,174 views,28 downloads,,0 topics,https://www.kaggle.com/yashprakash13/chest-xrays-dataset,"This Dataset is comprised of the Chest X-Rays of 5 Different diseases. They are: Atelectasis Infiltration Effusion Cardiomegaly Fibrosis
All of the training sets contain 150+ images. All of the Validation/Test sets contain 30+ images. It contains folder wise arranged images for all diseases in both the training and Validation sets.
The aim is to train a Neural Network to classify a given Chest X-Ray image into one of these categories."
TripAdvisor Reviews for The Eiffel Tower,"53,000+ reviews posted by visitors from all over the world",PromptCloud,3,"Version 1,2018-01-31",internet,CSV,5 MB,CC4,125 views,4 downloads,,0 topics,https://www.kaggle.com/PromptCloudHQ/tripadvisor-reviews-for-the-eiffel-tower,"Context
The data extraction of the reviews was done for three iconic spots (The Eiffel Tower, The Statue of Libery and Taj Mahal) to analyze how people from different countries review different places. In this dataset we have released the reviews posted for the Eiffel Tower.
The data set for all the three locations can be downloaded from DataStock -- a repository of datasets containing historical records extracted from the web.
Content
Data extraction was done by PromptCloud's internal web crawling solution in December, 2017. It has the following fields:
Review text
Review Title
Rating (1-5)
Reviewer location
Landmark name
Initial analyses
We compared the reviews posted by the visitors from the UK and the US. It can be accessed here - https://goo.gl/CRu8Zj.
Inspiration
Text mining techniques can be applied on this data to unveil sentiment, word frequency, build network graph, perform topic modelling and more."
Articles extracted from a fashion blog,~2000 articles with title and content,PromptCloud,3,"Version 1,2018-02-01",internet,{}JSON,740 KB,CC4,73 views,5 downloads,,0 topics,https://www.kaggle.com/PromptCloudHQ/articles-extracted-from-a-fashion-blog,"Context
This dataset was created by machine learning-based WordPress crawler developed by PromptCloud.
Content
This a JSON file containing the following data:
Page URL
Page title
Post text content
Image URL
Timestamp of blog post
Inspiration
This dataset can be used to figure out how the fashion trend changes according to year and season."
VGG-13,VGG-13 Pre-trained Model for PyTorch,PyTorch,3,"Version 1,2017-12-15","machine learning
pre-trained model",Other,471 MB,CC0,192 views,,,0 topics,https://www.kaggle.com/pytorch/vgg13,"VGG-13
Very Deep Convolutional Networks for Large-Scale Image Recognition
In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.
Authors: Karen Simonyan, Andrew Zisserman
https://arxiv.org/abs/1409.1556
VGG Architectures
What is a Pre-trained Model?
A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.
Why use a Pre-trained Model?
Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it."
VGG-19,VGG-19 Pre-trained Model for PyTorch,PyTorch,3,"Version 1,2017-12-15","machine learning
pre-trained model",Other,508 MB,CC0,178 views,,,0 topics,https://www.kaggle.com/pytorch/vgg19,"VGG-19
Very Deep Convolutional Networks for Large-Scale Image Recognition
In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.
Authors: Karen Simonyan, Andrew Zisserman
https://arxiv.org/abs/1409.1556
VGG Architectures
What is a Pre-trained Model?
A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.
Why use a Pre-trained Model?
Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it."
All Podcast Episodes Published In December 2017,Meta data of 121k podcasts and 881k episodes with audio urls,Listen Notes,3,"Version 1,2018-01-30","mass media
media studies
sound technology
digital media",CSV,281 MB,CC4,"1,066 views",6 downloads,,0 topics,https://www.kaggle.com/listennotes/all-podcast-episodes-published-in-december-2017,"Context
Podcast exists for near two decades. But it really takes over in recent two years. The podcast meta data may be useful for research in fields like machine learning, social science, or media in general.
Content
Listen Notes is the podcast search engine that actually works. It has the most comprehensive podcast database that you can find on the Internet.
This dataset includes the meta data of (almost) all podcast episodes that were published in December 2017.
Data source: RSS feed of podcasts.
Acknowledgements
Thanks for all the podcasters who produce those inspiring / entertaining shows.
Inspiration
N/A"
Unofficial Holidays,"National Beach Day, Awkward Moments Day, International Ninja Day and so on",hwhy,3,"Version 1,2017-10-03",popular culture,CSV,19 KB,Other,585 views,54 downloads,,0 topics,https://www.kaggle.com/hanyan/unofficialholidays,"Context
From National Chicken Wings Day to National Video Game Day to National Clean Your Virtual Desktop Day, this dataset covers it all.
Content
event: name of the holiday/occasion
day: date the occasion is observed. A couple of occasions may appear more than once a year
type: activity, food, cause, family, etc
Acknowledgements
The dataset is scrapped with rvest from Nationaltoday.com which collected the data.
Inspiration
The dataset itself can be used for all sorts of timeline visualization. How this dataset can be merged with other public data is only limited by your imagination."
FCC Net Neutrality Comments,"90,000 public comments on FCC proceeding 17-108, ""Restoring Internet Freedom""",Megan Risdal,3,"Version 1,2017-11-22","internet
telecommunications",CSV,8 MB,CC0,173 views,5 downloads,,0 topics,https://www.kaggle.com/mrisdal/fcc-net-neutrality-comments,"Context
On November 21st, 2017, the US Federal Communications Commission (FCC) introduced a plan to repeal Net Neutrality rules.
Content
This dataset contains details of 90,000 public comments made on FCC proceeding 17-108, ""Restoring Internet Freedom"" between April and May 2017 while regulations regarding Net Neutrality were being considered.
Acknowledgements
This data was collected via the FCC ECFS public API (docs).
--
TODO: expand description"
Captcha Images,Test and Train Data to create a captcha solver,RajeshM,3,"Version 3,2017-10-06|Version 2,2017-10-06|Version 1,2017-10-06",internet,Other,2 MB,CC0,536 views,75 downloads,,0 topics,https://www.kaggle.com/codingnirvana/captcha-images,This dataset does not have a description yet.
Dataset of customer purchase,,UrvangPatel,3,"Version 1,2017-11-09",,CSV,33 MB,Other,295 views,50 downloads,,0 topics,https://www.kaggle.com/urvang/blackfriday,This dataset does not have a description yet.
Safebooru,1.9 million rows of tag-based anime image metadata,Alexander Lamson,3,"Version 1,2018-01-24","popular culture
visual arts
drawing
+ 2 more...",CSV,190 MB,CC0,126 views,2 downloads,,0 topics,https://www.kaggle.com/alamson/safebooru,"Context
Safebooru (safebooru.org) is a tag-based image archive maintained by anime enthusiasts. It allows users to post images and add tags, annotation, translations and comments. It's derived from Danbooru, and differs from it in that it disallows explicit content. It's quite popular, and there are more than 2.3 million posts as of January 24, 2018.
Content
The data was scraped via Safebooru's online API, then converted from XML to CSV (some attributes were discarded during the conversion to make the whole csv a little smaller). There are 1,934,214 rows of the metadata. Contains images uploaded to safebooru.org in the time range of 2010-01-29 through 2016-11-20.
Acknowledgements
Banner image taken from https://safebooru.org/index.php?page=post&s=view&id=1514244
Inspiration
What tags are highly correlated? Can you predict missing tags? Can you predict the score of an image based on its tags?"
Complete Kaggle Datasets Collection,"A dataset of Kaggle datasets, so you can explore while you explore",jvent,3,"Version 1,2018-01-16","communities
internet",CSV,391 KB,ODbL,718 views,46 downloads,,0 topics,https://www.kaggle.com/jessevent/all-kaggle-datasets,"Complete Kaggle Datasets Collection
A dataset of Kaggle datasets, so you can explore while you explore
Summary
> Observations: 8,036 unique datasets
> Variables: 14
> Current As: 16/01/2018
Description
For a bit of fun I thought i'd write a quick script to retrieve all of the Kaggle datasets and do a bit of analysis on it.
The dataset contains all the unique datasets hosted on Kaggle since existence, and each one links off to it.
Future Temptations
If the community is interested I am tempted to scrape over each one and retrieve each datasets metadata, consolidate a huge Kaggle data dictionary?
Data Structure
Observations: 8,036 
Variables: 14 
 $ title          <chr> ""Trending YouTube Video Statistics (UPDATED)"", ""7ecb8f4fe2ece9f4c8ffd2... 
 $ description    <chr> ""Daily statistics (views, likes, category, tags+) for trending YouTube... 
 $ url            <chr> ""https://www.kaggle.com/datasnaek/youtube-new"", ""https://www.kaggle.co.. 
 $ owner          <chr> ""Mitchell J"", ""Vera Lei"", ""chfly2000"", ""snow2011"", ""Tjb5670"", ""gabro"",... 
 $ kernels        <int> 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 
 $ discussions    <int> 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 
 $ views          <int> 9484, 55, 26, 12, 7, 6, 5, 5, 5, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3... 
 $ downloads      <int> 1668, 2, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0... 
 $ last_updated   <date> 2018-01-16, 2018-01-16, 2018-01-16, 2018-01-16, 2018-01-16, 2018-01-1... 
 $ license        <chr> ""CC0"", ""Other"", ""Other"", ""CC0"", ""CC0"", ""Other"", ""Other"", ""CC0"", ""Other... 
 $ size           <dbl> 35087677, 127264365, 0, 1635900, 18, 777566, 404381, 137847611, 807171... 
 $ featured       <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 
 $ super_featured <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 
 $ upvotes        <int> 46, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... 
Authors
Jesse Vent - Author - jessevent
Acknowledgments
Github - crypto R-Package
Kaggle - Kaggle; Need I say more?
Community Acknowledgements
Mkffl
U.S. Government Publishing Office
_Fkih Younes
_ilab
????
"">
<""xss'
0rangutan
1251
173050055
30CrMnSiA
361online
4d4stra
4Quant
73805
7Grandpa
A Grillo
A.C.vanderLinde
a.rocamora.terres
å§œä¸Šï¼ˆIntegï¼‰
A^b
å°¹é½ç‚œä¸­å—
å¤§æ¸…è¦å®Œ
Aadrika Singh
AAK
Aakaash Jois
AAKASH AGRAWAL
Aalborg University
Aamir Soni
aariyan panchal
Aaron Miles
Aaron
Aaron7sun
AaronMcKisic
AashaySachdeva
AashutoshAgrawal
abanil
Abbyasov Marat
Abdalla G Bakheet
Abdelhadi Kerfa
Abdelhaq El Aibi
Abdelkader Laraichi
Abderrahman (Abdou) Ait Ali
ABdhm
Abdul Basit
Abdul Qureshi
Abdul Somat Budiaji
Abdullah Karimi
Abhijeet Khandelwal
Abhilash Reddy
Abhilash
Abhinandan
Abhinav Ankit
Abhinav Maurya
Abhinav Moudgil
Abhinav Ralhan
Abhinav Walia
abhinav
AbhinavAgarwal
Abhishek Bera
abhishek jha
Abhishek Kumar
Abhishek Sharma
abhisheksharma
Abhrajyoti Pal
Abida Aslam
abidemi lorain grace
Abien Fred Agarap
Abineshkumar K
abiodun bayowa
abir
AbiyuG
AbliamitAbliamitov
Abo Sol
Academy of Motion Picture Arts and Sciences
achinta
Achmad WIldan Al aziz
Acruve15
ActiveGalaXy
acutesharpness
Ada Guo
ÃdÃ¡m MarkÃ³ja
adam kolodny
Adam Mathias Bittlingmayer
Adam Schroeder
Adam
AdamSkafi
Adarsh Chavakula
AdarshaShrivastava
Addy Naik
Ade Ihsan Hidayatullah
AdhokshajaPradeep
adign
Adithya Ganesh
adithya
Aditi Garg
Aditi
aditisingh
Aditya Bhati
Aditya Chetan
Aditya Gupta
Aditya Kirloskar
Aditya Mehndiratta
aditya pratap singh
Aditya Rajuladevi
Aditya Soni
Aditya Tandon
Aditya
AdityaDivakaruni
AdityaLodha
AdityaVamsiKiran
AdiVarma
ADM2752836
Admin admin
Adnan Rasheed
adong
Adriano Pylro
adrianulbona
adrien chevrier
Adrien
adu47249
Adult Survey Company
Adway S. Wadekar
Ady1
æž—æ¹§æ£® (Dyson Lin)
æŽç«‹å³°
Afeef k k
Afri
AGSantos
Agustin Montero
Aguy
Ahiale Darlington
Ahmad Delforouzi
Ahmad Obiedat
AhmadZaenal
Ahmed Abdelaal
Ahmed Nader
ahmed
ahmedeveloper
Ahmet Erkan
Ahmet Hamza Emra
AhmetAksoy
Ahn Kwang
Ahsan
Ahsan
AIFirst
AimeShangula
Airbnb
Airly
Aishwarya Deshpande
Aivar Annamaa
AJ_2017
Ajana
AjaxFB
AjaykumarManimala
ajayrana
Ajinkya Jumbad
Ajinkya Kolhe
Ajinkya Rasane
AjitBrar
ajmartinezm
AJS
Akash Gupta
AKASH JAISWAL
Akash Kumar
Akash
AkashPatel
Akhil Anto
Akhil Jain
Akhilesh
AkhileshwarReddyChennu
Akil Elkamel
akira.y
Akis Zervas
AkshatUppal
Akshay Babbar
Akshay Kumar Vikram
Akshay Sharma
akshay
AkshayAradhya
Akson
Alan ""AJ"" Pryor
Alan Du
Albert Costas
AlbertJiang
Alberto AlmuiÃ±a
Alberto Artasanchez
Alberto Barradas
Alberto Martinho
Albyati
Alec
AleenahKhan
Alejandro Taboada
Alejandro
Aleksandr Ivanov
Aleksandr Shevchenko
Aleksey Bilogur
Alessandro De Vito
Alex Acosta
Alex K
Alex Klibisz
Alex Korablev
Alex Lee
Alex Miasoedov
Alex Xiaotong Gui
Alex_deng
Alex
Alexander Kireev
Alexander Konshin
Alexander Long
Alexander Mamaev
Alexander Minushkin
Alexander Raboin
Alexander Shakhov
Alexander
AlexanderGlulkhovtsev
alexattia
Alexey Filimonchuk
Alexey Rozhnev
Alexis Carrillo
Alexis Fossart
AlexisGlennEspina
AlexLight
alexnavarrete
Alexstrasza
AlexZhang
ALFONSOREYES
AlfredoQuintana
Ali Ghafour
Ali Hussain
alifarsi
alifatemy
Aliia Salakheeva
alimbekovkz
Alin Secareanu
Alishan Kaisani
Allan Scott
Allan
Allen Institute for Artificial Intelligence
AllHailSammy
Allsmiles
Alok Nimrani
Alon
alopez247
Alp KoÃ§
alphaHaxor
alphajuliet
AltonLu
AlukoSayo
Alvaro Flores
Ãlvaro LÃ³pez GarcÃ­a
Alvaro Soares
Alvaro Trancon
Alvin Mbabazi
AlwaysChaCha
Alyssa
Aman Agarwal
Aman Ajmera
aman mahendra
Aman Shrivastava
Amandeep Rathee
Amar Basic
Amber Song
Amer
Amey Goel
Amil Khare
Amin Ghaderi
Amine GHERBI
AMiner
AminS
Amir Aharon
Amir Rezaei
Amit Maurya
amit
Amita Dhainje
AmitaAshokDhainje
amitani
Amlan Praharaj
Amol Naik
Amro
amrrs
Anand Jeyahar
Anand
AnantBhardwaj
Ananya Nayan
Anas Aboureada
Anastasios Zouzias
Ancient One
AndersKetelsen
Anderson Chaves
Andi Fauzi Firdaus
Andieminogue
Andre Holzner
Andre Sionek
Andrea Cesarini
Andrea Girardi
andrea leo
Andrea
Andreas Kappl
Andreas Klintberg
Andrei Dukhounik
Andres C
Andres Hernandez
AndresFelipeBayonaChinchilla
Andressa Coelho
Andrew Dacenko
Andrew Gross
Andrew Kirk
Andrew Kreimer
Andrew Thompson
Andrew Truman
Andrew wang
Andrew Yue Xie
andrew.chen
Andrew
Andrew
AndrewEhsaei
AndrewMalinow, PhD
andrewnachtigal
Andrey Dotsenko
Andrey
Andrey
Andrey
Andriy Gudziy
Andry Ml
Andy Friedman
Andy Harless
Andy Levitskyy
AndyKlyman
Angela Houston
AngelaLocoro
Angeline Pld
Angga Purnama
anil
AnilKumarPallekonda
AnimatronBot
Aniruddha Achar
Aniruddha Ghosh
Anirudh K. Muralidhar
Anish N Sharma
anjali reddy
Anji
Ankit Agarwal
Ankit Akash Jha
Ankit Biradar Crixus
Ankit Chaubal
ANKIT JINDAL
Ankit
ankita
Ankur Joshi
ankur
AnkurSaikia
AnkushAnshuman
anmol
Anna Montoya
Anna Montoya
AnnaMongillo
Annanya Pratap
annecool37
Annie Pi
anokas
ansh.g
Anshul Jain
Anshul Kwatra
Anthony DeLuca
Anthony Goldbloom
Anthony Nguyen
AnthonyAllen
Anton Bobanev
Anton Dmitriev
Anton Lytyakov
Anton Prokopyev
Anton Savchenko
Antonio Coelho
Antonio Domenzain
Antonio Guimarey MarÃ³n
Antonio Javier GonzÃ¡lez Ferrer
AntonioFeregrinoBolaÃ±os
AntonioIvanovski
antonyj
Antti-Paladin
anttip
Anu
Anubhav Dhiman
Anuj Anand Gagrai
Anuj Goyal
Anujay Saraf
Anupama Jha
Anurag Gothwal
anurag K
Anurag Maurya
Anurag Sharma
Anurag
AnuragPuri
Anuraj
Anvesh Tummala
APARAJITA TIWARI
apollonius
Apoorv Agnihotri
ApoorvaJha
AppleCrazy
Apratim Bhattacharya
Arasaraja
Arash
Aravindhan S
ArcGIS Open Data
Archana Khanal
Archangell
Arda Mavi
Arden Tran
areeves87
Ariful Ambia
Arihant Jain
Arijit Mukherjee
Arion AI
Arion
AritraSen
Arizona Secretary of State
ArjoonnSharma
Arjun
arkz
Armin Talic
Armineh Nourbakhsh
ArnaudLievin
Arnoud
arokkones
Arooj Anwar Khan
Arpan Dhatt
Arpi Sinanyan
arsenland
ArshadSiddhiqui
Arslan Zulfiqar
Artem Larionov
artemzraev
Arthur Stsepanenka
arthur163
artlee
Arun Joseph
Arun Kumar
Arun Menon
Arun
arvidzt
arvind bhatt
Arvindhan Rameshbabu
Arwin Neil Baichoo
AsadMahmood
asado23
asfdafaE
ashish bansal
Ashish Chauhan
Ashish gupta
Ashish Khanna
Ashish Sonavane
Ashita Gupta
ashleysmith
Ashok Kumar Pant
Ashok Lathwal
ASHUTOSH KUMAR
ashvinking
ashwani
Asim Irshad
AskarNurbekov
Asma BELHAOUA
asper
ASSO PAVIC - Angers Smart City
Astandri K
Atanas Atanasov
Atefeh Goodarzi
athabascaAI
Athni
athontz
Atikur Rahman
Atul A
Aty Rachmawati
Aubert Sigouin
August
AugustinPottier
Augusto Pertence
aumas
aurelian
Aurelio Agundez
auriml
AustinSonger
Australian Bureau of Statistics
Austro
autuanliu
Avani Gupta
AvirudhTheraja
Avkash
Awesome
aWright
AXA_FOSSOUO
ãŸã‹ã¨ã‚‚
AYAN MAITY
AyanTiwari
Aydin Ayanzadeh
AymanFawzy
Aysun
Ayush Sharma
ayush
AyushDewan
AyushThada
Azeem Bootwala
Babu Priyavrat
babuloseo
babybear
Bachi
Backblaze
bacon
Badari Vishal Madduluri
bader
Badri Adhikari
bagmanas
bahadir60
Bai Li
Baking Pi
Baligh Mnassri
Bank of England
BaptisteAmato
Bargava
Baris Simsek
Barney Farrell
BaronChen
Barton.news
Bas Hilgers
Basil
Bastien Javaux
Batangas
batzig
Bayarjargal
Bazinga
BB
Beavis Butthead
BEC14
bedy
behzadgolshan
Beili Zheng
Bello Gbadebo
beluga
belvederethecat
Ben Dilday
Ben Hamner
Ben Ho
Ben Rudolph
Ben
Benben Zhang
Benf
Benjamin Taylor
Benjamin Visser
BenjaminSwedlove
Berhane
Berkeley Earth
Bernardo Lares
Bert Carremans
BethTseng
Bhamin Patel
Bharadwaj Srigiriraju
Bharani
Bharath NR
Bharath Posa
Bhargav
Bhaskar Voleti
BhatNasir
bhavesh
Bhavna Chawla
Bhupen
Bhushan Sonawane
Bhuwan pandeya
Bianca Kramer
Bibin Paul
bielrv
BigBlessLee
bigdatachennai
bigzhao
BilalMahmood
Bill S
BillurEngin
Bin Ury
BingLi
Binks
BinRoot
BioSENSE @ UC Berkeley School of Information
birdie
biswa
bitroy
bkKaggle
BlackLee1994
BlairJennings
BlazeJ
Blissoft
Blitzer
Bo Ju
Bob Zhang
bob-li
bobbob
BobitaSingha
Bogdan Puida
Bojan Tunguz
BoltzmannBrain
Bongo
Boon P
BoraPajo
Boris Marjanovic
Bostjan Mrak
Botao_Deng
BOTSHOT
boyofans
bpali26
bqlearner
BrahanyaaSomasundaram
Brandon Lawrence
Brandon Trabuco
BrandtCowan
Brandy Chang
Brave
breadsh
BreanaMurphy
breandan
Brendan Finan
Brendan Murphy
BrendaSo
Breyonce Bugg
Brian Gonzalez
Brian Ho
Brian J
Brian Liao
Brian McGarry
Brian Roach
Brian Rouse
Brian Rushton
Brian W. Shreeves
Brian
Briane Paul Samson
BrianOn99
BrickettaSwiss
Brihi Joshi
BRIJ NANDA
Brijesh Singh
Brnt
bronson
brontosaur
Brouillette
brucelees
BruceRowan
Bruno Flores
Bryan Arnold
Bryan Chen
Bryan Park
bryandrive
BryanMaloney
Bryant Trombly
Bryce Freshcorn
Bryn Humphreys
bshivaani
bssasikanth
BTH Project
btolar1
buggs23
bughunter atgoogle
Buket Konuk Hirst
Bukun
bulblight
BurakH
BuryBuryZymon
Buzz Zhang
ç”³å°è™Ž
ç§‹ä¹‹çµç¾½
Caio Correia
Caio Lente
Caio Moreno
CaiqueCassemiro
Caitlin Furby
Caleb Willms
CalebFackler
California Environmental Protection Agency
Calvin Chan
Cam Nugent
Cameron Chandler
Cameron
CamilaSampaio
Camille Debrun
Campbell McGrouther
canuto
Caparrini
Caramba Donkey
Cards Against Humanity
Carl Jackson
Carl ThomÃ©
CarlesBalsach
Carlos Aguayo
Carlos BeltrÃ¡n Villamizar
Carlos Brioso
Carlos Paradis
Carlos Rafael
Carlos Vouking
CarlosMoncayo
Carly Wright
Caroline Cypranowska
Carrie
Carsten Behring
Cataras
Cathie So
Cauim
CCCHEUNG
cclark
cecil kim
cedrikfd
Celio Larcher
Cem Karabulut
Cenk BircanoÄŸlu
Center for Medicare and Medicaid
Centers for Disease Control and Prevention
Centers for Medicare & Medicaid Services
Central Bureau of Statistics
Ceshine Lee
cgaete
Chad Schirmer
Chaitanya Bapat
ChamberUnderground
Chandan Singh
chandlervan
Chandra Bhushan Roy
chansh
Chanwoo Kim
Chaochana Siparitat
Chara Remoundou
Charles Jansen
CharlesYang
Charlie H.
Charlie Monk
Charlie
Charmi
Chase Bank
Chase Willden
Chaton
Chekos
Chella Priyadharshini
Chen Chen
Chen Shuyao
Chen
Cheng ZHANG
Cheng
chengzhan
Chennai Kaggler's Forum
Chenxi_Ge
cheshire
Chester Cheng
Chetan Malhotra
chetan
Chewable
chfly2000
Chi
Chia Sáº½ Kinh Nghiá»‡m Äi Du Lá»‹ch Há»™i An ÄÃ  Náºµng
Chia Yi
Chicago Police Department
chickgod
Chidi
Chinelo Okpalaonwuka
Ching
ChinkiRai
ChinkitPatel
chip0001
Chippy
ChiragBalakrishna
Chithra MS
chloesh
ChNaveen
Chonlapat Patanajirasit
Chris Bartel
Chris Brent
Chris Buetti
Chris Crawford
Chris Cross
Chris Evi-Parker
Chris Formey
Chris G
Chris H.
Chris Murphy
Chris Pierse
Chris Roth
Chris Scott
Chris
Chris
ChrisAddy
chrisb
ChrisClark
ChrisDoil
ChrisM!
Christ
ChristenLucido
Christian Nygaard
Christian Safka
Christian Urcuqui
Christian Vorhemus
ChristianTrachsel
Christina Mak
Christophe Chabreuil
Christopher Clayford
Christopher Lambert
Christopher
ChristopherZerafa
ChristopheS
ChrisY1001
Chtholly
Chuck Ephron
Chuck-Yin
churandy
Cigil Achenkunju
Cindyyyyyy
CITIES
Citrahsagala
City of Chicago
City of Los Angeles
City of New York
citylines.co
ckeller
Clalby
Claudio Sanhueza
clayd
clement gauchy
clemetine
Cleuton Sampaio
Cliff Saito
clim
coconup
code_thief
CodingVanGogh
cogs
ColaCole
colemaclean
ColinMorris
College Board
Colliaux RÃ©mi
Colt Bauman
Committee to Protect Journalists
Connecticut Open Data
Conobrodel
Conor MacBride
ConoStabile
Consumer Financial Protection Bureau
CooperUnion
coplin
coredesign
Corentin Rdn
CorneliaVanDerWalt
Cornell University
CosmikAlpha
Cosmin Stamate
CostalAether
Costas Voglis
coulet.simon
Courtney Wanson
cpossehl
cricketsavant
CristhianBoujon
Cristiano
Cristina
criticalhits
Cro-Magnon
Crowdflower
csbond007
csgwon
csungroup67
Currie32
Curtis Chong
Cutechick
CWILOC
Cyphers
Cyril Ma
CYZhao0709
d pc
Ð–ÑƒÐ»Ð´Ñ‹Ð·Ð¶Ð°Ð½Ð¡Ð°Ð³Ð¸Ð¼Ð±Ð°ÐµÐ²
Daan Sterk
Dada123
Dahee
DAI GUANYU
Daia Alexandru
Daigo Miyoshi
Daisuke Ishii
Daisuke
Dale Matthews
dalgacik
Dalia Research
DALX555
Damian Denesha
Damian Eliel Aleman
Damiano
Damien BENESCHI
Dan Chrispine
Dan Emery
Dan Ofer
Dan Van Der Meulen
Dan Wilden
Dan Winchester
Dan Xu
dan_lo
Dan
Danai Avgerinou
dananos
DanB
danerbland
Daniel Esteves
Daniel Franch
Daniel Grijalva
Daniel Labbe
Daniel Pye
Daniel S. Panizzo
Daniel SÃ¡nchez
Daniel Silion
Daniel Sobrado
Daniele
DanielHKL
DanielVargas
DanielViray
danielwatabe
Danil Zherebtsov
danishxavier
Dano
DanyaKosmin
daoduySon
darcy
Daria Glebova
DarkLord
Data Hunter
Data Quantum
Data to Information to Knowledge to Wisdom
data-refinement
DataCanary
DataDopeBoy
Datafiniti
Datagraver
dataist
datamin2017
DataP
DataSF
dataspartan
Datastreamer
datatest84
datathÃ¨que
Dave D Harsh
Dave Fisher-Hickey
DaveRosenman
David Azria
David Baker
David Bialer
David Calloway
David Chudzicki
David Cohen
David Cooperberg
David de la Iglesia Castro
David Havera
David Odhiambo
David Prakash
David Rubal
David Schwertfeger
David Skipper Everling
David StrÃ¶m
david_becks
David
David
David
David
davide andreazzini
DavidParr
DavidShahrestani
DavidWesley-James
DavidYang
Dayana Moncada
dazhangyu
ÐÐ»Ð¸ÑÐ° ÐŸÑƒÐ³Ð°Ñ‡ÐµÐ²Ð°
ÐÐ»ÐµÐºÑÐ°Ð½Ð´Ñ€ (CMF DA)
ÐÐ½Ð´Ñ€ÐµÐ¹Ð¢Ð¸Ð¼Ð¾Ñ„ÐµÐµÐ²
dddhiraj
Death Penalty Information Center
deathmood
Debanjan
Debashish Dalal
DebayanDasgupta
DebdootSheet
DebiRath
dechavez005
deeley
Deena Liz John
deep
Deep
deepak gupta
deepak
DeepakGupta
DeepakKandasamy
DeepakMittal
DeepAnalytics
delepp
DeltoiX
deluxe hotel In dalhousie
deluXe
Demetri Pananos
Democracy Fund
DenisAfonin
Dennys Mallqui
Department of Defense
Department of Homeland Security
Department of Justice
Department of Transportation
Derek Chia
Derek Li
Derek Y
Derek Zhi
Derek
Derrick M
derrine
Destin
Devansh Besain
devashismohapatra
Developers Area
DeveshMaheshwari
Devin Anderson
Devious Dus
Devji Chhanga
DevjyotiChandra
Dexteritas
dgoke1
DhaferMalouche
Dhananjay Shembekar
Dhruv Desai
DhruvMangtani
Dian Purnama
Diego Villacreses
DigitalCowboy
Dileep Pandey
dilzeem
DimitriF
ding
Diogo Cortez
Dipanjan
Dipika Baad
dish
divyajain
Divyam Soni
Divyansh
DivyanshKumar
Divyojyoti Sinha
dmi3kno
Dmitrii Petukhov
dmitrijsc
Dmitriy Sakharov
Dmitry
DMPierre
ÐÑ€Ñ‚ÐµÐ¼ Ð›ÑÐ½
DobroeZlo
Documenting the American South (DocSouth)
Doe Jhon
ÐœÐ¸Ñ…Ð°Ð¸Ð»ÐœÐ°ÐºÑÑŽÑ‚ÐµÐ½ÐºÐ¾
Dom Hall
Domenico Delle Side
dominic
Dominik Gawlik
Don Browning
Donfuzius
DongGeun Oh
Dongwoo Kim
Donyoe
Dor Oppenheim
Doran Wu
Doug Friedman
Doug Hersak
Dr. Ahmad Al Sallab
Dr. Rich
Dr.D.Lakshmi
dr.priskott
Dragon
DrBenLyons
Drew Pope
DrGuillermo
DRISS AIT LABSIR
drjkuo
Dromosys
drop-out
Dryad Digital Repository
DSafonov
DSEverything
Dsloet
DuaaNasif
ducky
DucThanhNguyen
dust
Dversteele
Dyadya Bogdan
Dylan Amelot
Dylan Willow
Dylan
dzoulouvincisavitriDVSInformatique_ma_passion
E.Nikumanesh.Germany
ë²¤ìž ë¯¼
ê²½ë¦¼ ê³½
eagle
Eagles2F
Earless Abdul
Ebrahimi
eccc
ecerulm
ecodan
Ed King
Eden
Edern Haumont
edgano
Edi Mala
Edilson Augusto
Edit Osikovicz
Edith
Edmon
Edo Miyazaki
EdoardoPiccari
Eduardo
EduardoMagalhÃ£esOliveira
Edward Turner
Edwin Kestler
edX
Efrain Guzman
Eibriel
Eidan Cohen
EigenLaw
Ekansg Garg
Ekianjo
Eldar Tinjic
EleanorBlum
EleanorXu
Electoral Commission
Electronic Frontier Foundation
Elemente N
Elen Vardanyan
Elena Cuoco
ElenaCall
Eli Cerdan
Elias Barba Moral
elias8888
Eliezer Bourchardt
ElitCenkAlp
Eliud Kagema
Elizabeth Sam
Elkana Rosenblatt
EllaRabinovich
Elliptic to Quantum
ema
email365
EmersonPereiraBertolo
Emil Andreas Lund
Emil Nikolov
Emil.P
EmilioMC
Emily
emirozbir
Emma
Emmanuel Kens
EN Kim biokpc
éœœé›ªåƒå¹´
eoveson
epattaro
Epiphany
Eran Machlev
Eric Grinstein
Eric McCracken
Eric Oakley
Eric Vos
Eric You
EricFeng
Erich Rodrigues
Erik van de Ven
Erik
ErikHambardzumyan
eros
Erun Noid
Esha Somavarapu
Especuloide
Espen Sonneland
Espen
esperto
Eswar
Etherqua
Etienne LQ
Eugene
European Centre for Medium-Range Weather Forecasts
European Space Agency
Eurostat
Evan Jung
EvanPayne
Everton Seiei Arakaki
EveryPolitician
Evgeniy Malishev
Evgeniy Vasilev
evil.com
evren leet
EyyÃ¼b Sari
Ezequiel Bequet
Fabia
Fabiano Bizarro
Fabio Correa Cordeiro
Fabiola
fabiolux
Facebook
FAD2018
Faguilar-V
Faisal
Faizal Abd Kadir
Fan Fei Chong
fandang
Fangda
Far East Group
Faraz
Farhan Karim
faronFeng
Fatihah Ulya
fatima-ezzahra elaamraoui
FatimaLidia
FAUZI
fayomi
FCiceri
Federal Aviation Administration
Federal Bureau of Investigation
Federal Communications Commission
Federal Deposit Insurance Corporation
Federal Election Commission
Federal Emergency Management Agency
Federal Reserve
Federico BaylÃ©
Federico Soldo
FedericoSarrocco
Felipe Hoffa
FelipeArgolo
FelipeLeiteAntunes
Felix Gutierrez
FelixZhao
FelypeBastos
FeMO
FenilSuchak
Fernanda Castro
Fernando Lopez
FernandoBecerra
festa78
Fifth Tribe
figshare
Filemide
Filipe Morandi
Filippo
finintelligence.com
Firdha Amelia
FiveThirtyEight
Fizmath
Flaredown
flashthunder
FlavienGelineau
Florian Pydde
FlorianTHAUNAY
Florin Langer
flx1
foenix
folaraz
Food and Drug Administration
foolius
fordeletion
Fornax.ai
Fortune
Foxtrot
FrÃ©dÃ©ric Girod
Fracking Analysis
fran
Francis Paul Flores
Francisco Glez
Francisco Mendez
Francisco Penovi
FrancisGeek
Frank He
Frank Pac
Frank
FrankFernandes
frankie
Franklin Bradfield
freddie
Fredrik Jonsson
Free Code Camp
freeCodeCamp
Freedom House
French dude
FUNGYueHoi
FunnyMango
FuzzyFrogHunter
G1nG0
Gabriel Forsythe y Korzeniewicz
Gabriel Gutierrez Corral
Gabriel Joshua Miguel
Gabriel Moreira
Gabriel Preda
gabrielacaesar
GabrielAvellaneda
Gabriele Angeletti
Gabriele Baldassarre
gabro
Gael Kngm
Gagan
GaganBhatia
GajendraBadwal
ganesh
GaoweiWang
GarryKevin
Gary Ramah
Gasimov Aydin
Gaspare
Gaurav Arora
Gaurav Sharma
GAURAVJAIN
Gautam Doshi
GauthamSenthil
Gavin Cheng
GavinArmstrong
GAz113
geco
GECOdavide
gellowmellow
GeneBurin
genexpres
Gennadii
GeoffNoble
GeoNames
GeoNSoo Kim
George B
GeorgeMcIntire
Georgii Vyshnia
georginarose
Gerardo Suarez
GerardoSegura
GetTheData
Getting_started
Gevault
Gevorg Aghekyan
Gfan
Ggzet
GIANT: Machine learning for smart environments
Gibs
gift
giginim
giim
gilad
GilSousa
GilVolpe
Gin04kg
GiologicX
GiorgioRoffo
Giovanni Gonzalez
girish bansal
Girish Murthy
Github
Giulia Carra
Giuseppe
GKHI
GL_Li
Global Footprint Network
GMAdevs
Gnana Prasath
Gnanesh
gnania527
GodEater
Gokagglers
Gokul Alex
Gokul Alex
Golden Oak Research Group
GomteshHatgine
Goneee
Gonzalo Falloux
Google Brain
Google Natural Language Understanding Research
Google News Lab
Googleboy
Gopal
Gopal Chettri
GOPALJAISWAL
Gopi Vasudevan
Gor Khachatryan
Gorodec
gourabbhattacharyya
goutham
Government of France
GovLab
GowTham
gpoudel
Graham Daley
Grainsan
GreatImposter
greekygeek
greenet09
Greg
GregKondla
Gregory
GregorySmith
gregv
Greycop
GrishaSizov
group09
GroupLens
GrubenM
Gudang
guik
Guilherme Diaz-BÃ©rrio
Guilherme Diego
Guilherme Folego
Guillaume Vinet
GuillaumeTouzin
Gummula Srikanth
Gun Violence Archive
GunheePark
gunner38
Gurpreet Singh
GurpreetSingh
Gus Segura
Gus
Gustavo Bonesso
Gustavo Felhberg
Gustavo Palacios
Gustavo Torres
GustavoFelhberg
gutsyrobot
Guy T.
Gyan
GyanendraMishra
H1kkiGakki
H3MANT
Hacker News
hacker1
haemin Jeong
haho
Haibo
Haitam Abdoullah
Haitao Chen
Hakan Eren
Hakan Toguc
Hakeem Frank
Hakky
hakmesyo
Hakob Sukiasyan
HamaChi
Hamad42
Hammad A. Usmani
hamza el karoui
Hamza Zafar
Hani Ramadhan
Hanna
Hansel D'Souza
HansMaulwurf
Hao
Haohan Wang
HaoyuZhao
HaozhengNi
Hard_Core
Hari Krishna K
Hari prasath
haris21gr
harlfoxem
HarmanpreetSingh
Harold Almon
Haroon Ahmed
HarpieCrispi
Harry Peter
Harry
Harry
HarryQuake
HarryTan
Harsh B. Gupta
Harsh Mehta
Harsha
HarshaVardhan
Harshit Joshi
Harshit Sinha
HARSHITAGUPTA
HarshitMehta
HarshitSrivastava
Harshoday
HarshPandya
HarshVardhan
Harvard University
hashus
Hasil Sharma
HassanAftabMughal
hassankhanyusufzai
hatem
Hax S
Hazrat Ali
HDKIM
heatingSmoke
hectopascal
Hedi Ho
Hefen Zhou
heidogsdf
Heihei
Heiko
Heitor Tomaz
hellenandreea
Hello ML World
Hellrider
Hemant Sain
Hemanth k
Hemanth Kumar Veeranki
hemanthgowda
Hena
Hendrik Å uvalov
Hendrik Hilleckes
HenrikHeggland
Henry
HenryWConklin
Heraldo Reis
Herimanitra
Hervind
Heuristic
heymeredith
hhl028
hidark
Hidehisa Arai
hieuvt
Hill YU
Hillary Dawkins
Himanshu Chaudhary
Himanshu Garg
Himanshu Shekhar
himanshu0113
HimanshuRai
Hioki Ryuji
Hiro Ari
HiroyukiSHINODA
HIT_CS_LI BO
hitcs_1150310416
hitcs_jiangzhenfei
hitcs_yuhong_zhong
Hitesh Desai
hlnaima
HM Land Registry
Homo Deus
Hong
Honggu
honlamlai
Hossein Banki Koshki
How Toai
Howard Smith
hrfm
Hssan Driss
HTan
Hu Yao
huang xuan
Huang, Peng-Hsuan
Huangkai Yuãƒ¾(Â°Ð´Â°)ãƒŽ
HuayuanTu
Hubert Wassner
Hugh
Hugo Mathien
HugoDarwood
Hugues Talbot (ESIEE)
Huijun Zhao
huimin
Huiyu YE
Human Computation
Humberto BrandÃ£o
HungDo
Hunter Anderson
Hunter McGushion
Hunterr
Husein Zolkepli
HUSEYiNKiliC
Husnain wajid
Hussien El-Sawy
HustTiger
HuyNguyen
hwhy
hyuan
Hyun Ook Ryu
HYUNJUNGBYEON
I,Coder
i2i2i2
IagoDÃ­az
Ian Chu Te
Ian Nanez
ianmobbs
Iary Joseph
IbrahimAljarah
ibrahimkhaleelullah
ibrarhussain
icebear
idiosyncraticee
Iditarod Trail Committee
Idris Kuti
Ifechide Monyei
IfeoluwaAkande
Ignacio Chavarria
Igor Alexeev
Igor Lemes
Igor Nikolskiy
Igun
IHME
Ilenia
ilias sekkaf
IlknurIcke
Ilko Masaldzhiyski
Imad Khan
Imam Digmi
Imran Arif
InÃ¨s Potier
inaba
Indicium
InfiniteWing
INFINITYLABS
inooooooovation
inquisitor
Institute for Computing Education at Georgia Tech
Institute for Public Policy and Social Research
Institute of Museum and Library Services
Interaction Engineering Laboratory
interface
Internal Revenue Service
Internet Association
Internet Association
intest
InVinoVeritas
ÎœÎ±ÏÎ¹Î¿Ï‚ ÎœÎ¹Ï‡Î±Î·Î»Î¹Î´Î·Ï‚ KazAnova
IpLee
Irfan
IrfanWahyudin
Irina Kalatskaya
IrinaAchkasova
Irio Musskopf
Iryna
Isaac A.
Isaac Blinder
Isaac34
IsaacSim
Isabella Plonk
ishaan
Ishank Saxena
ishigen
ishiryish
Ishnoor
isildaaa
ismail turkmen
Itamar Mushkin
itest
Itzik Yohanan
Ivan Jakovcevic
Ivan Mazharov
Ivan Tsy
Ivan
ivanloginov
Ivo Penkov
Iwase Yuya
Izabella
Izzie Toren
izzuddin
Izzy
J from the Riverside
J.DavidCorrea
JÃ¶rg Eitner
JÃ¶rgen Sinka
Jaak Ungro
Jacco Jurg
Jack Blarr
Jack Cook
Jack Cosgrove
Jack Ho
Jack Miller
Jack Sunny
Jack
Jack
Jack
JackLiu
Jackson Harper
Jackson Raja
Jacky Wong
JackyD
Jaclyn A
Jaco de Groot
Jacob Boysen
JacobGoozner
jaehyeon yu
jaewonk
Jaffer Syed
Jagan
jagannath neupane
Jaime Valero
Jaish K
Jake Gnieser
Jake Rohrer
Jake Toffler
Jake Waitze
Jakub Pubrat
Jalaz Kumar
james ahn
James Clavin
James Condon
James D.
James Littiebrant
James Mathews
James Tollefson
James
jamesbasker
JamesG
JamesS
jamesyang96
JamesYuan
Jan Bodnar
Jan Charles Maghirang Adona
Jan Christian Blaise Cruz
Jan Nordin
jana
Janani Damodaran Gantal
Janek
janice
Janzen Liu
Jason A. Hatton
Jason A
Jason Benner
Jason Liu
Jason McNeill
Jason Nguyen
Jason Schenck
Jason.F_CN
Jason
Jason
JasonHuang
jasonzhang
jatin raina
Jatin Shah
Jaturong Kongmanee
javascript:alert(8007);
Javier Villanueva-Valle
Javier
Jay I
Jay Kulshreshtha
Jay Ravaliya
jay333
Jaya Gupta
Jayanth Yetukuri
Jayanth
Jayavardhan Reddy
jayjay
JayLee
JBD
jbfields
JD Torres
Jean Pierre Rukundo
Jean-MarcBouvier
Jean-Michel D.
Jean-NicholasHould
Jean-Phillipe
Jeanpat
JeevanNagaraj
Jeff Kao
Jeff Ussing
Jeff
JefferyT
JeffTennis
Jegs
JegyeongKim
Jekaterina Kokatjuhha
jekwon
Jemilu Mohammed
Jenkins Ruban
Jens Laufer
jenvo
Jeongmin Ha
Jerad Rose
Jeremy Seibert
Jeremy Wang
jeremymiles
JeremyWickman
Jerrin Joe Varghese
jerryg
JerryWang
Jesse Montgomery
Jessica Yung
Jessie-Raye Bauer
Jesus Jara LÃ³pez
Jesus Santander
Jguerreiro
JhonatanZubieta
JiachuanDeng
JiaJane
Jiaming Huang
Jian W
Jian Zhang
Jiang Yu
jiangzuo
JiansheFeng
Jibsgrl
Jigarkumar Patel
Jihane HAMMOUT
Jihye Sofia Seo
Jiji
Jill_M
JimmyMarguerite
Jin Liu
Jin-HwaChiu
Jindong Wang
Jindra Lacko
jinesh John
Jing Zhang
Jing
JingdaZhou
jingjuewang
jingli
jingwang
Jinner
Jinsoo Yeo
Jinze He
Jiri Roznovjak
Jirka Vrany
Jitendra Rajpurohit
JitendraKumarBansal
jiuzhang
jjjooo1
JLucas
jmataya
JO-Team
JoÃ£o Pedro Peinado
Joao Januario
Joao Pedro Evangelista
JobsPikr
Joe Kim
Joe Philleo
Joe Ramir
Joe Young
joejoe
Joel Jacobsen
Joel Lee
Joel Wilson
joeland209
Joerg Simon Wicker
joeymeyer
Joffles
Johannes Plambeck
JohannesBuchner
JohanneslaPoutre
John Bourassa
John Doe
John Doe
John Doe
john doe
john joe
John Jones
John Lin
John Mark
John Olafenwa
John Ostrowski
John Ruth
John Sumerel
John Traavis
John Wu
john
John
John
john2
JohnCurcio
Johnd
johndebugger
JohnHeyrich
JohnJayChou&MichelleZhuang
JohnnyHa
JohnWorne
JohnX
jolhe006
jomendes
Jon B
Jon Hong
jon.bill
Jonah Mary17
jonahelisio
Jonatan Cisneros
Jonathan
Jonathan
JonathanPhoon
Jones
Jonh Doe
JoniHoppen
JoostLubach
Jordan Goblet
Jordan Meta
Jordan Tremoureux
JorgeZazueta
JosÄ—AndrÄ—sAlvarezCabrera
JosÃ© Vicente
JosÃ©Prado
Jose Berengueres
Jose Fco Morales
Jose Lery Nunes
Jose Luis Juarez Ruelas
Jose Manuel Vera
Jose Toro
jose
Josep A.
Joseph Leichter
Joseph
JosephBae
Josh Haimson
Josh Wheeler
josh woulfe
josh777
joshkyh
JoshMcKenney
Joshua Schnessl
joshuaherman
JosS
jossssss
JPSS
jr91
jruots
jruvika
jrvalentin
jscharbach
Juan Corporan
Juan R
JUAN SOLER-COMPANY
JuanRodriguez
Juanu
jujuuu
Julian Christov
Julian Simon de Castro
julie
Julien Frisch
Jun Zhu
Juncheng ZHOU
Junfeng Zhang
JuniaGeorge
Junki Cho
Juran
Just try
Justin Pan
JustinMoore
JuturuPavan
jvent
jvm56
Jwuthrich
Jyothi Kamakshi
Jyoti Sharma
Jyun-Ting
jyzaguirre
JZ2771
k6box
KÃ¤rt
KÃ¢zÄ±m AnÄ±l Eren
Kaan Can
Kaan Ulgen
kaffes
kagami
Kaggle
KaggleRay
kaguser
kaho
Kai Wang
Kaique da Silva
Kairit
kajot
kalcal
KalyanYerra
Kamal raj
Kamau John
kambarakun
kamesh s
Kamil Jurek
Kamil Kaczmarek
Kamlesh
kamran
Kande Bonfim
Kane
Kanika Narang
KanikaChopra
Kanishka Misra
KanishkPratapSingh
KannanPiedy
Karamveer
Karan Thakkar
KaranSharma
KardoPaska
KarelVerhoeven
Karim Ardi
KarimBELAYATI
KarimNahas
Karmanya Aggarwal
KarmoT
Karolina Wullum
Kartheek
karthickveerakumar
KarthickVel
karthik
Karthiks
karthikziffer
Kartik
KartikPatnaik
kashif kaleem
Kashish Suneja
kashyap
Kasper Nielsen
Kate
Katrina Ni
katzwigmore
Kaus
Kaushik S
Kaveti Naveen Kumar
Kaylan Foster
Kayode Emmanuel Oluwatobi
Kazuki
KedanLi
Keelan Robinson
Keheira
Keik@
keisei
Keita Shimizu
Kelvin Wellington
Kelvin Xiao
Kemal Yilmaz
Kemical
Ken Yamaji
KendallGillies
KenichiNakatani
Kenji Kondo
Kenneth Benavides
Kenneth Chua
kenomaru
KentaroTakemoto
Kenton W. Murray
Keras
Keval M
Kevin
Kevin Chow
Kevin Mader
Kevin Mario Gerard
Kevin Moodley
Kevin Pertsovsky
Kevin Ree
Kevin Soucy
Kevin
kevin
Kevin
KevinH
kevv
Khac Bao Anh NGUYEN
Khai Xiang
khaled salah
Khashayar Baghizadeh Hosseini
Kheirallah Samaha
Khepry Quixote
Khushboo
Kiefer Smith
Kilian Batzner
Kilian. O
Kim Schreier
Kimos
Kimura
Kingsley Samuel
Kiran Ganji
Kiran Gutha
KiranKarri
KirthikaBabu
Kishan P
kishore
Kittisak
kiweee
KiyonariHarigae
KK
KK
KK16
KKDDAll
kktestin2'""
Kleber Bernardo
km1west
KMMR
Kmuvunyi
Kola Adebayo
Kondalarao Vonteru
Konstantin Lopuhin
Konstantin
Konstantinos Bazakos
Korakot Chaovavanich
Kory Becker
kosiewmm
Kostiantyn Isaienkov
Kostya
Kote42
Kotobotov
KOUASSI Konan Jean-Claude
Kozlova
KP
kpapamih
kravdiy
Krishna Agarwal
Krishna Bharadwaj
KrishnaDheeraj
Krishnan
KrishnaPraveen
KrishnaThiyagarajan
KrisMurphy
Kristian H
Kristjan PÃ¤rn
KristofferHess
Kristopher Sheets, PhD
Krithel
KrizsÃ³ Gergely
krsimons
ksayantani
Ksenia Sukhova
kso.
kumar abhishek
Kumar Nityan Suman
Kumar
Kumaran K
kumarbhrgv
KumarHalake
Kunal Kotian
Kunal Singh
Kunal Vaishnavi
kunalkumawat
kunimune
Kuntal Sardar
Kushal
Kushneryk Pavel
KutsalBaranÃ–zkurt
kveykva
kvpratama
Kwan Lowe
kwangrok lee
kwtcut
Kyle McClurg
kyle moon
L Sun
La Sul
LA Times Data Desk
Lacie
LacksonMundira
LAdams
lahouarami
LaiyiLin
lakshadvani
Lakshya Khandelwal
Lalit Khandelwal
Lalit Parihar
lalitsomnathe
lalthan
lamda-dev
langzi
Lantana Camara
LanVukuÅ¡iÄ
Lasteg
LastJedi76
Laura
Laurae
LauraMoen
Lauren BK
Laurenstc
LaurentBerder
lavi
LavishGulati
lazkol
LE PALLEC ClÃ©ment
Leandro dos Santos Coelho
Leandro Silva
Learner
Lee Worthington
LeeYun
lefant
Lehmaudar
Lei Ding
leigh
Leo Arruda
Leo
Leon Martin
Leon
Leonardo Ferreira
Leonidas
LeonPaul
Leroberge
Lesoler
LesPaulCustom
LeticiaFilgueiras
LeviMa
Lewis
Lexie Dempsey
Lgpatel
Liam Cusack
LiamLarsen
librahu
Lieven23
light-boat
lihan
Lihaoyang
Liisi
LiLi
Liling Tan
Lilit Janjughazyan
limi44
Liming
limmen
Limon M
Lin Gao
Lin Ying Lung
lincoln
Lindada
Lingzhi
LinkanRay
Lisa
lisjin
Lislejoem
Lissette Guzman
litianyi
Little Boat
Litu Rout
liuenda
liuxiaoliu
LiuYang
liuyongqi
liuzhe0125
livi
liwste
Liza Bolton
ljhuang
lkytal
lnicalo
LogHorizon
logwinner
lohith
lomungo
looo
Lorna Maria
Louis Marmet
louis
Louis
LouweAL
loyf
LPitre
LuanHo
LuÃ­s Gustavo Modelli
lubaroli
Lucas Astorian
Lucas Dixon
Lucas Erring
Lucas Venezian Povoa
Lucas Vergeest
Lucas
LucasVinze
Lucio LÃ³pez Lecube
Ludovic benistant
Lugark
Luigi
Luis Andre Dutra e Silva
Luis Bronchal
Luis Moneda
LuisaAPF
luistelmocosta
Luiz Gerosa
Luiz Gustavo Schiller
Luiz Henrique Amorim
Luiza Fontana
Luke Bunge
Luke Godwin-Jones
lukebyrne
LukeLee
Lumin
LunarLlama
Luu
LyAhmedTidiane
lyh19970409
Lynn dai
LynnPan
M Baddar
M Ganiyu
M.F.
maarten
Mabs
MACHINE LEARNING DATASETS
Maciej Witkowiak
Mad Hab
Madhan Varadhodiyil
Madhav Iyengar
Madhavi Burra
Madhur Inani
Madis_Lemsalu
Madison Curtis
MadScientist
Maghilnan
MagicK
Magsgiust
Mahadevan
MahdiJavid
Mahdy Nabaee
Mahek Hooda
Mahesh Sinha
Mahesh_PRS
MahirKukreja
Mahmoud Aljabary
MahreenAhmed
maik3141
Mainak kUNDU
Maitree Priyadarsini
MakarandVelankar
Maksim Mikhotov
Maksym
Malathi Arumugam
Malek Trabelsi
Malinee Fawcett
Malini
Mamun
Manan Jain
Manan Manwani
Manas
Manav Sehgal
mancml
Manfredi Federico Pivetta
Mani
MANIKANTA
ManikHossain
Manimala
Manish jain
Manish Kumar
Manjeet Singh
Manoj Kumar
manoj2891
ManojHariharan
MANOJKUMAR PARMAR
MANOJKUMAR
Manqiong
Manshubh Singh Rihal
Mansoor Iqbal
Mansour Movahhedinia
Mantas Zimnickas
Manuel Barrena
Mapik88
MarÃ­a Otero
Marc Kossa
marc moreaux
Marc Robert
Marc Slaughter
Marc Velmer
Marc
Marcel
Marcell ""Mazuh"" Guilherme Costa da Silva
Marcelo Santos
Marco Boaretto
Marco De Nadai
Marco Molina
Marco Zanchi
MarcoCarnini
Marcos Boaglio
MarcSchroeder
MarcTorrellas
Marcus Lin
Maria Bile
Maria Luiza
mariakatosvich
Mariehane
Marielen Ferreira
Mario Navas
Mario Pasquato
Marius
Mark DiMarco
Mark Eldridge
Mark
MarkArchieGamayan
Marketing As Is
Marko K
MarkSchultz
Markus Lang
Marlesson
Marouane Benmeida
Martin Enzinger
Martin Pereira
MartinBoyanov
MartJ
Marty
marvin
Marwa Saied
masahito429
masakt
Masato Hagiwara
Masato
Masood Hussain
Massachusetts Institute of Technology
Masseycre
Mateus
Mathew Savage
Mathias Meldgaard Pedersen
MathiasEdman
Mathieu Goutay
Mathijs Waegemakers
mathishammel
Mathurin AchÃ©
matiasfeld
matsueushi
Matt Hixon
Matt Rose
Matt Snell
Matt
Matteo Casadei
Matteo_Mazzola
Matthew Allbee
Matthew Anderson
Matthew Carter
matthew
MatthewHonnibal
Matthieu C
Mattia Gigliotti
mattilgale
maurice_f
Mauro Reverter
mavez DABAS
Max Candocia
Max Halford
Max Horowitz
Max Mind
Max Stanford-Taylor
Max.liu
Maxime Fuccellaro
Maximilian Hahn
Maximilian Kapsecker
Mayank Singla
MayankSiddharth
MayankTiwari
Maykon Ravy
McDonald's
MCrescenzo
Md Irfan Ali
Mearafat
Medicare
meep
Meetika Sharma
Meg Shields
Megan Risdal
Mehdi
Mehedi Shafi
mehrdad
mehrdadz007
Mehta
Meigang Gu
Meinertsen
Melody Z
melvincheung
Melvyn Drag
Mengfei Li
mengmengyong
mengyan
MengYe
meow
mepotts
Merilin KÃµrnas
Mesum Raza Hemani
mgkmgk
MGN
mharrys
MHouellemont
Miaomiao
MichaÅ‚ Jamry
MichaÅ‚Puchalski
Michael Clouting
Michael Ibrahim
Michael KS
Michael Nation
Michael Pang
Michael Pavlukhin
Michael Plohhotnichenko
Michael Skrzypiec
Michael
Michael
MichaelKirk
MichaelKlear
MichaelStone
Michal Januszewski
Michelle HY
MieMie Kurisu
Miguel LladÃ³
Miguel
Miguel
MiguelSalazar
Mihai Oltean
Mihir Garg
Mihkel Gering
MihwaHan
miinooo
mijim
Mike Chirico
Mike Johnson Jr
Mike Kim
Mike Mekilo
Mike Pastore
mike sebel
Mikhail Chesnokov
miki112
MikioKubo
mikr
MilindParadkar
Miljenko Bartulovic
mimic1
Mina
Minat Verma
MindaugasMejeras
Minerwa Min
mingming
minmind
Minso
Minx
Minxuan
minyao
Mir Ali
Miranda
Mircea Stanciu
Mirko MÃ¤licke
Miro Karpis
Miroslav Sabo
Miroslav Zoricak
MiroslavTyurin
MirrorLu
Mission San Jose AI Club
MistyMoo
Mitchell J
mithileshwaribhade
mitillo
mitsu
Mitusha
Miza'
mizosalah
MKMK
ML Coder
ML_CX
mlagunas
MLane
MLS
mlxd
mnakajima
Modeling Online Auctions
Moghazy
Mohamed Abul Danish
Mohamed Elsayed
Mohamed Loey
Mohamed Ramadan
Mohamed Shawky DG
MohamedSaidDaw
MohamedShawky
MohamedWasim
Mohammad Ali
Mohammad Ghahramani
Mohammad Kachuee
MohammadAmir
MohammadAseemUrRehman
Mohammed Alnemari
Mohit Balani
Mohit Sainani
Moi
Moko Sharma
Monika Munjal
MonishC
monkeyking
moon soo Lee
morcinim
MorganMazer
Moses Salifu
Moshfiqur Rahman
Motaz Saad
Moufid
Mouli
moxious
Mozilla
MphoGodfreyNkadimeng
Mr. Analytics
mrdeeds
MridulSharma
MritunjayMohitesh
mrjazz
MrNasalHazel
mrpantherson
mrsantos
Mrverde
mrzzheng
Ms Brown
msiebold
msjass
MsZombie
MT
Mudit Choraria
Mufti Mubarak
Muhamad Nady
Muhammad Abdul Rehman
Muhammad Alfiansyah
Muhammad Ali
Muhammad Aseem Ur Rehman
Muhammad Asif khan
Muhammad Jamil Moughal
MuhammadMahadTariq
MuhammadYasirAdnan
Mukarram Pasha
Mukesh Kumar
Muneeb ul Hassan
MuonNeutrino
Murali_Munna
MURALIDHAR ANUMULA
Murder Accountability Project
mureren
Murilo Siqueira
Murilo Viviani
MuskanBararia
Mustakim
Muthukumar.J
Muttaqi Ismail
My Khe Nguyen
Myles O'Neill
mypapit
n&n student
n01z3
Nabeel Raza
Nada Fathallah
Nadin Tamer
Nagabhushan S B
NAGARAJ RAMAKRISHNA
Nagendra Yadav
nailo
nami
Namory Koulibaly
Namsraijav Dugersuren
NAN JI
Nancy Lubalo
Nandagopal M
NaomiNguyen
narmeen
NASA
Nasir Mushtaq
Naszy
Nat T
Natalia
Natalia
Natalie Ha
Natasha Zope
Natasha
Natasha
Nate
Nathan Burns
Nathan Cohen
Nathan Zhang
Nathan
NathanGeorge
Nathaniel See
National Archives
National Health Service
National Institutes of Health Chest X-Ray Dataset
National Library of Medicine
National Park Service
National Snow and Ice Data Center
National UFO Reporting Center (NUFORC)
Navdeep Pal
naveen holla
Naveen Kumar
Naveen Pandian
navneethc
NavyashreeS
Nayan Bhattacharya
Nayan solanki
Nazimamzz
NCAA
Ncls byr
Neel Shah
Neeraj Kasturi
Neerav Kharche
neha singh
Neha
NeilS
neinei
neKsdrawkcaB
Nelson Chu
Nelson
Nema
Nerdiholic
NetanelMalka
Netflix
NeuroGuy
Never_die
New America
New York Philharmonic
New York Public Library
newman
Nguyen Tang Tri Duc
NHTSA
nic
Nicholas Zufelt
Nick DiGiulio
Nick Rose
Nick Schroeder
Nick Spadafora
Nick Torsky
Nick Wagner
Nick Wong
NickAchin
NickSehy
Niclas KjÃ¤ll-Ohlsson
Nico Belov
NicolÃ¡s
NicolaBernini
Nicolas P
Nigel Dalziel
nihal88
Nika Ioramishvili
Nikhil Akki
Nikhil Gargeya
Nikhil Gupta
Nikhil Jain
Nikhil Parihar
Nikhil Reddy
Nikhil
nikhil
Nikita Malyshev
Nikunj
Nilesh Sakpal
Nilesh
Nils Ponomarchuk
Nilzone
Ning Zhou
niniyan
Nirajk18
Niranjan Nakkala
NiranjanDeshpande
Nirav Nikunj Patel
nirmalelumalai
NirmalyaKumarMohanty
Nishant
Nishant Arora
Nishant Bhadauria
Nishant K
Nishant Kumar
nishantjain
NISHIO Hirokazu
Nishit Sehgal
Nitesh Tiwari
NItesh Yadav
NiteshSurana
Nitin Bisht
Nitin Venkateswaran
nitishaadhikari
Niwech Harnkham
Niyamat Ullah
NLSpdX
NLTK Data
NMIN
No more overfitting
No Re
NOAA
Noah Gift
Noah Schwartz
Noah Wang
Nodes
Noel Yoo
Nolan Conaway
Nooh
NorbertBudincsevity
NORC.org
Nosbielcs
Nowshin Nawar Arony
NPO 2799
Nuggs
Numerai
Nupur Warke
Nuraddin
NurÅŸenÃ–ÄŸÃ¼tveren
NV27
NYC Open Data
NYC Parks and Recreation
NYPD
Ø¹Ø¨Ø¯Ø§Ù„Ù„Ø·ÙŠÙØ£Ø­Ù…Ø¯ØºÙ„Ø§Ø¨
ObadiahJeshurenNaidoo
obandoruben
obey ismael
Ocelot
OctavioG
OfayMailey
Oh InQueue
Ohhm Prakash K I
Okus
Ole KrÃ¶ger
Oleg Brizhatiy
Oleg O
OlegSolomka
Oleksii Nidzelskyi
Olga Belitskaya
Olga Ivanova
Oliveira, L. O. V. B.
Oliver Collins
OliverMoralesLopez
olivia
Olivier Richard
olivier
ololo
Omajaykarthik
Omar
Omer Gozuacik
Omicron
OmkarP
OnkarKadam
Onno Eberhard
Onofrio_BIScience
Open Food Facts
Open Knowledge International
Open Source Sports
Open Sourcing Mental Illness, LTD
OpenAddresses
OpenFlights
ophelia1234
OrCo
Orges Leka
OrgodolDawaasuren
orgrimm9
Oscar Takeshita
Oscar Zamora
oscarleo
ostrokach
OSUBMI
Oswin Rahadiyan Hartono
ouissa souliman
ouyangxuan
Owais
Ozan Aygun
ozgur
P111110
Pablo
Pablo Escobar
Pablo Tabales
PabloMonleon
Padmavathi R
paesibassi
painkiller
Pakshal Jain
Palak Sharma
PalashShah
Pallav Routh
Pallavi Ramicetty
Panagiotis G. Togias
panchicore
Pancho
Panda974
pandataDelta
Pandey Nilesh Prasad
Panos Kostakos
panos
Panos
Paolo Campanelli
paolo
paosheng
Parallax
Paras Jindal
Paresh
Parichart
Parindsheel Singh
park thirty-two
Parmanand Sahu
Parole Hearing Data Project
parseltung
Parth Gupta
Parth Iramani
ParthMaheshwari
Pascal Brenner
Patatae
Patit Pawan Karmakar
Patrick Hyland
Patrick J
Patrick Murphy
Patryk NiedÅºwiedziÅ„ski
Paul Abramshe
Paul Curry
Paul Larmuseau
Paul Magda
Paul Rossotti
Paul Schale
Paul Tracey
Paul Watt
Paul Yang
Paul-Louis Hery
paul
Paula Ceccon
Paulo Henrique Vasconcellos
paultimothymooney
PaulZH
pavansubhash
PavelTroshenkov
Pavlin Bakalov
Pavlos Zitis
pawan
Pazookii
pbcquoc
PCMiners
Pedro Lima
Pedro Velez
PedroFrantz
PengM(MySaturdaySelf)
pengzha
People HR Analytics Repository
peppermintshake
perastikos
PerfectFit
Peter Joseph Arienza
Peter Klauke
Peter Ostroukhov
Peter Wittek
Peter Yang
Peter
Petit Ours
pguptha
phalaris
phatgamer
Philip Corr
PhilipHarmuth
philipjames11
Philipp Schmidt
PhillipChin
PhillipLiu
Phung Van Hoa
pickleChu
pickou
Pierre Sardin
Piks Ral
PiperGragg
pistachio_overlord
piyushgoyal443
piyushgupta
pjmonti
pkugoodspeed
PKylas
pmohun
Poetri Heriningtyas
Polina Vakhrusheva
Poorna
Poornima Ravishankar
PoornimaShanbhag
poquilia
portia brat
pourmehrab
Pradeep.narayanan
Pradeep
PradeepKumar
Pragya Goyal
Prajit Datta
prajwal
Prakash Tiwary
PrakashGawas
Prakhar Srivastava
Prakriti Iyengar
Pramit
Pramod Kumar
Pramud
Pranav
Pranay Aryal
Pranesh Kumar Palanisamy Padmavathy
PranjalGandhi
pranstar
Prasanna Nadimpalli
Prasanna steed
Prashant Singh Chauhan
Prashanth Poojary
Prashanth Sekar
prashanthsreepuram
Prateek Gupta
Prateek Joshi
Prateik
Pratibha Sharma
PratibhaSharma
Pratik Agrawal
Pratik K
Pratik Singh
Pratiksha Salimath
Pratiush Prasunn
pravallika
Pravesh_Ghorawat
preeth kumar
PreetSinghKhalsa
Prem Patrick
PreMon
PremTewari
Prince Grover
priscilla
Priya_ds
PriyaChowdary
Priyaljain
Priyank Shah
priyanka gagneja
Priyanka Kolli
priyanka Kukunuru
PriyanshJain
Progress Queens
Project Jupyter
proland
PromphongBandhuvara
PromptCloud
Pronto Cycle Share
Properati Data
prvns
ps
psparks
Pulkit Jha
PULKIT KHANDELWAL
puneet
puneeth019
Punxsutawney Groundhog Club
Purvank
Pushkar Jain
PushpendraPratap
pylyfe
PythonMython
PyTorch
Q82 Capital
QadeemKhan
Qishen Ha
qixiang109
qizheng
Quan Do
Quan Nguyen
Quang Nguyen
QuantScientist
Quentin Garnier
Quentin Mouton
QuinnCarver
Quoc Thang Nguyen
quoniammm
Quora
R.Venkatesh
R1q3
RÅÅ©KÄ©Ä…
raam
Raaz
Rachael Tatman
Rachit Sapra
Rachit Srivastava
RadociechBubuSierakowski
Radu Stoicescu
Rafael Novello
Rafal Cycon (blaine)
RafflesiaKhan
Raghavi
RaghuReddy
Rahi
Rahul Bagga
rahul batham
Rahul Chaudhary
rahul kumar
rahul patil
Rahul Sathyajit
rahul
Rahul
RahulBhambri
RahulMayuranath
RahulVerma
Raihan Kibria
Rainey
Raj Sharma
RajaGanapathy
Rajanand Ilangovan / à®‡à®°à®¾à®œà¯à®†à®©à®¨à¯à®¤à¯ à®‡à®³à®™à¯à®•à¯‹à®µà®©à¯
Rajasankar Viswanathan
rajat arora
Rajat Sharma
Rajeev kumar
Rajeev
rajesh kumar
RAJESH PURWAR
RajeshM
Rajiv Jeeva
Rajmund Mokso
Rajorshi Chaudhuri
RajSekhar
Raju Alluri
RakanNimer
Rakesh Raushan
RakeshSk
Rakuraku
RalicLo
Ram Ramrakhya
Ramakrishnan Srinivasan
ramamet
Ramanujam Allam
Ramesh
Ramiro
ramirobentes
RamNemani
Randy Betancourt
Ranjan Kumar
Ranjit kumar
RanjithaKorrapati
Ranjithkumar M
RaphaÃ«lMontaud
Raphael
Raquel Aoki
Rashid Ali
Rashid Khan
Rashmi Singh Chauhan
RatnaChowdary
Raul
Ravali
Ravi Rokhade
Ravi Verma
Ravi
ravi
RaviBhalala
Ravichandra Malapati
RaviJain
RaviKiran
Ravin
Ravindra Kompella
Ray
rayen
Raymond Delord
RaymondMak
raysar
Razib Mustafiz
RBakes
Rcaer
rdayala
RDizzl3
Reason Foundation
rechards
Recruit Institute of Technology
Reddit
RedRegressor
REE_
Regis Nunes Vargas
Reinhard
Remi Myers
Renata Barros
RenzoRamirez
Retailrocket
Reynald Riviere
Reza Agung Pambudi
Reza Javidi
Reza Katebi
REZA
rhammell
rhishikesh nepal
RhitamjeetSaharia
Rhostam
Ri_Nandiya
Ricardo Moya
Ricardo Suarez
Ricardo Zuccolo
Riccardo Bongiovanni
Riccardo Miccini
Riccardo Nizzolo
Richa Gautam
Richard Churchill
Richard Gu
Richard Nagyfi
richard
RichardBJ
RichardNguyen
Rick Chen
rickvenadata
Ricky
RickyMak
rickysaurav
Ridhi Adyanthaya
Rini
Rio 2016
Rippon
Riri
Rishab Gargeya
Rishabh Kumar Jha
Rishabh Mishra
Rishi Anand
Rishi Sankineni
RishiBarath
riti
RitikaJain
RITUSHARMA15BCE1347
River
Riyas
rjcampa
rjl2155
RM
rmsda2
Roam Analytics
Rob Harrand
Rob Wishart
Robbert Manders
RobbieS
Robert Hargraves
Robert Nolan
Robert Wexler
Roberto Sousa
Roberto Spadim
Roberto Williams
Robin E. Masliah
Robin Nicole
Robin Praet
RobinReni
robotcator
rocha
Rock Pereira
RockBottom
Rodrigo Ancavil
Rodrigo Domingos
Rodrigo Ramele
Rodrigo Salas
Rodrigo
Roel van den Boom
roger
Rogerio Lopes
RogierMonshouwer
Rohan Kale
Rohan Kayan
Rohan Patel
Rohit Sharma
Rohit Singh
RohithRPai
RohitMathur
Rohk
Roi Shikler
rojour
Rolandas Å imkus
Rolando P. Aguirre
Romain Loiseau
Romain LOURY
romainvincent
Roman Akhunov
Roman Semenyk
Roman
RomitDhamija
Romy
Ron Graf
Ron Leplae
Ronald Troncoso
rongruosong
ronnie
Ronny Kimathi kaimenyi
Rony Lussari
RoopaliKaujalgi
Rosanaider
rosegao
Roselyn Kinuthia
RoshaanKhan
Roshan
Rounak Banik
roundedup
rovilayjnr
Roy Garrard
Roy Kiran
Roy Klaasse Bos
RoyWWilson
RoyXss
RpyGamer
RShorty30
Rudd Fawcett
Ruhshan
Rui Romanini
ruijie li
Ruishen Lyu
Rumen Manev
rupali
Rush Kirubi
Ruslan Khalitov
Ruslan
Rutuj Gavankar
Ryan Bain
Ryan Buck
Ryan Chang
Ryan Cushen
Ryan Epp
Ryan Harrison
Ryan Li
Ryan Sloot
Ryan
RyanHuang
RyanLott
RyoOgata
RyuJiseung
ryvolum
S Sakarin
S. Zotos
s.ayadi
S.S. Tarek
S1M0N38
SÃ©bastien Aroulanda
SÃ©bastien MATHIEU
SÃ©bastien Pouilly
saagie_anthony
sab30226
Sabber Ahamed
Sabyasachi
SachGupta
Sachiemon
Sachin Kalsi
Sachin Patel
sachinumrao
SadhanaSingh
Safecast
Sagar Sarkar
Sagarnil Das
SagarSen
Sahil Gandhi
Sai C
Sai Pranav
Saida Antonyan
saigonapps
saikiran
SaiKumar
Saimagesh R
sainath
Saiprasad
Sajal
Sajid
SakinaDas
SakthiSiva
Sakti Prasad
Salil Gautam
Salim Dohri
SalimChouai
salmanpathan
Salomon
SalvadorDali
Sam Edelstein
Sam Harris
sam komo
Sam Shideler
Sam Stonesifer
Sam Wong
samael
SambitSekhar
samdeeplearning
SamDotson
Sameer Mahajan
Sameer
Sami Rahman
SamiraKlaylat
SamiTabet
Sammy Klasfeld
sammy123
Samrat
Samriddhi Sinha
Samuel Longwell
Samuel
Samyak Jain
SanD
Sandeep Kumar
Sandeep
SandeepRamesh
SandeepYadav
sandhya raghavan
Sandra Cristina Bustos Galvis
sandrarivera
Sandro Marcelo Peirano Gozalvez
sandsp
Sandy HE
SangamVerma
Sangeetha Sasikumar
sanjay kushwah
Sanjaya Wijeratne
Sanjeet Kumar Yadav
Sanjeev Upreti
Sanket Kumar
Santa Meilisa
SanthoshMurali
SantiagoVazquezGomez
Santosh Boina
Sanyam
Saqib Mujtaba
Sara G. Mille
Sarah Adsit
Sarah VCH
SarahZ
Sarai Rosenberg
Saravanan B
Saravanan Jaichandar
sariya
sarra zammit chatti
sarthak nautiyal
sarubhava
Sasan Jafarnejad
sash
sasi
Saswata Das
satadru5
Satavisha Mitra
satheeshperepu
Sathu79
Satish Karivedha
Satish Tiwari
Satya Patel
Satyaki Banik
satyasai
SaudAl-Zakwani
saurabh singh
Saurabh Singh
Saurabh
SaurabhBhagvatula
saurav ghosh
Saurav Kumar
SAURAV SUMAN
Sauro Grandi
savannahlogan
SavasYÄ±ldÄ±rÄ±m
Savioz
sawayaka
Saxinou
SazidurRahman
SciELO
Scott A. Miller
Scott Cole
Scott
Scott
Scottfree Analytics LLC
ScottHendrickson
sdorius
SeaGoat
Seagullbird
Sean Kelley
Sean Marjason
Sean Saito
Sean
SeanKim
SeanLahman
Seattle Public Library
Sebastian Mantey
Sebastian
sebastianmarkow
SebastianZanabria
Securities and Exchange Commission
security3test
securityteamvictim4
Seetharam Indurti
Sekar M G
Selah
Selfish Gene
selvakumar
Semin
SemionKorchevskiy
Seong-Jae Chu
SEPTA
Serena Chen
Sergei Fironov
Sergey Kosterin
Sergey Kuznetsov
Sergey
SergeyA
Sergio GQ
SergioGonzalez
SergioPerez
Sergiy Chumachenko
Serhiy Subota
Serigne
SeungHyun Jeon
sevaspb
seyvar
SGDE
sgDysregulation
SH Lee
shabeer
Shabu KC
Shahebaz
SHAHUMANGKAMLESHBHAI15BCE1303
Shaik Kamran
Shakaed Subin
Shakti Sharma
Shakti
ShalvaRai16MCB0025
Shams ul arfeen
shan
Shan
Shane Smith
Shang Pengxu
Shanger Lin
ShaniGershtein
Shankar
ShantamVijayputra
Shantanu Acharya
Shantanu
Shanth
Sharadhi V
Sharan Naribole
sharddha
Sharon Lin
Shashank
Shashank Kumar
Shashank Shekhar Shukla
Shashank Yadav
ShashankNainwal
Shatiel
ShaunakChadha
Shaurya Munshi
Shaurya Munshi
ShauryaChawla
Shawn Tian
Shayenne Moura
Shazad Udwadia
Sheik Mohamed Imran
Sheikh Asif Imran Shouborno
Sheil Naik
Sheng Guo
shengwei
shenjiawei
Sherry_CS
Shihao
Shikhar
shilpibhattacharyya
shilpitha
shiMu
Shiny
shirley
ShiSanCD
Shishir
Shitao Zeng
shiv gehlot
Shiv Santosh
Shiva Manhar
ShivajiAlaparthi
Shivam Panchal
Shivam Patel
Shivam Patel
shivamagrawal
ShivamGoel
shivamnijhawan
ShivinderKapil
shodiq
ShradhaJoshi
Shreeya Bhosale
Shreyams Jain
ShreyasSomashekara
shrihans giriraj meena
ShruthiShankar
Shruti Bhargava
Shubham
Shubham Barudwale
Shubham Deshmukh
SHUBHAM KARANDE
ShubhamAgarwal
ShubhamMaurya
ShubhamPawar
ShubhamThakur
shubhangi
Shuchi
Shuhei Fujiwara
Shunpoco
SHUNYA
shuwenz
Shwet Prakash
shweta
shwetabh123
sibappa
sichunlam
Sid Shetty
Siddartha
Siddhanth VInay
siddhartha sharan
Siddhartha
SidG
sidhant
SidhantDeka
siero
SiewKamOnn
Siim M
Sijo VM
silicon99
Silogram
Silvio Santana
Simo
Simon Asiimwe
Simon Fraser University - Summit
Simon Gurcke
Simon Plovyt
Simon Tse
Simon
Simon
Simon
Simone Seregni
SimoneDalessio
SimonRazniewski
Sindhu Rao
Sirish
Siva Kumar
Siva Swaminathan
SiyuanH
SIZZLE
skakki
Skalldihor
SkalskiP
Skiddie
SkyLord
sleight82
Smart Revolution
smeschke
smota
sna
Snehaa Ganesan
snehanshusengupta
SnehaReddy
Snow Dog
snow2011
Sofiya
Sohaib Ali
SohaibOmar
Soham Patel
sohel
Sohier Dane
SohiniBhattacharya
Somasundaram Sankaranaraynan
somesh
Sommenoob
Somnath Roy
Son Genacrys
Sonali Chawla
SonamSrivastava
Song WanG
soojung
soorajms
soroosh
Sotopia
Soufiane Fhiyil
soufianeorama
souhaiel
Souhail Toumdi
Soukaina
Souman Roy
Soumitra Agarwal
SourabhMittal
Sourav Nandi
Sourav Roy
Sourav Verma
SouravMaharana
SovBoc2018
sowhit
Sowmiya Nagarajan
soywu
SpaceX
Spencer Buja
Spider Pig
sprabakar
Sreeram Reddy Kasarla (SRK16113)
Sreyansh Jain
sri charan
Sri Kamma
SRI KANTH
Sri Manjusha
Sri Santhosh Hari
sridhar narasaiahgari
Sridhar
Srihari Vasudevan
SrihariRao
Srilakshmi
srilakshminandamuri
SriLBG
Srinath Sridharan
SrinivasRao
SriniVinnakota
SRK
Ssvitian
Stack Overflow
Stan
Stanford Network Analysis Project
Stanford Open Policing Project
Stanford University
Starbucks
starconf
starmine.ai
START Consortium
Startup Policy Lab
stawary
steal
SteeveHuang
Stefanie04736
Stephan Andre
Stephan Wessels
Stephane Bernadac
Stephanerappeneau
Stephanie Le Grange
Stephen Cranney
Stephen Huan
Stephen McGlennon
Stephen Thompson
Stephen
StephRouen
Steve Ahn
Steve Joly
Steve Palley
Steven Venezie
SteveN
Steven
Stoddy
stonepurple
Streichholz
Stuart Chan
Stuart Colianni
stytch
Styven Ponnusamy
SubarnaRana
Subham Das
Subhransu Sekhar Sahoo
Submarineering
Subra
Suchit Gupta
Sudarshan
Sudeepta Kkr
Sudheej Sudhakaran
Sudheer Sankar
Sudhir Thuppale
Sudip Das
Suhel
Sujan Ghimire
sujan
Sujay Khandagale
Sujith
sujithramkotagiri
Sulata Patra
sultan
sumanth
SumanthSRao
sumendar
Sumit Bhongale
Sumit Kant
Sumit Kothari
Sumit Kumar
Sumit
SunDai
SuneetSawant
Sungpil Han
Sunil Kumar SV
Sunil Neurgaonkar
Sunil Sethi
sunilp
sunmarkil
SuperDave
Suprabhat Tiwari
Suprabhat Tiwari
SupriyaDubey
Surabhi
Suraj
SurajPathak
Suresh Bhusare
SureshSrinivas
SuryaSista
Susan Noboa
Susan Wang
susanna
sushant
Sushant jha
Susmitha
Sustainable Development Solutions Network
Svidon
Swami Krishnamurthy
Swapnil
swapnilkale
SwaroopVenigalla
Swathi Priyadarsini
swati
Swatish Swaminathan
Swayam Mittal
Sweety
SwetaAgrawal
Sylas
Sylvia Mittal
Szery
Szkript
szrlee
T Byrnes
T McKetterick
T Michaels
T Peng
T. Scharf
T
T7 - Pokemon Challenge
TadashiNagao
Taffey Lewis
Taha Zerrouki
Tahsin Mayeesha
TaichiWang
Taimur Khan
TaiwoO.Adetiloye
Taka
takuoko
Tamber
Tamil Dhoni
Tamilselvan Sudalai
Tammy Rotem
Tan Kinh Bui
Tang Yiming
tanishk parihar
tankeestka
TANYA MAKKAR
TÃº Anh HoÃ ng
Tara Rutkowski
Taraprasanna Saha Babu
Taras
tarek benkhelif
Tarun Khanna
TARUN KUMAR
tdougherty223
Team AI
Team PuppyGoGo
techmn
teck44""><
tecperson
TehreemAnsari
tejasvagarwal
Temilade Adefioye Aina
Teng Lei
Teodosiy
TeraFlops
TerenceLiu
Terminal Security Agency
test "">
test"">
test
test
test2""><
testaccountkagglee
testbugmasooddd
tester
TESTIMON @ NTNU
testingshi
TetianaMyronivska
TetyanaLoskutova
TetyanaYatsenko
TEVEC Systems
Thais Rodrigues Neubauer
thaisalmeida
Thanakom Sangnetra
thanuj
Thao
Tharini Padmagirisan
The Bear
The BGU Cyber Security Research Center
The Fellow
The Flying Munkey
The Guardian
The Huffington Post
The Marshall Project
The Metropolitan Museum of Art
The Movie Database (TMDb)
The Museum of Modern Art
The Nobel Foundation
The Smithsonian Institution
The Wall Street Journal
The Washington Post
the1owl
Theo Ioa
TheScientistBR
thetraderrr
Theudas
Thiago Balbo
Thiago Oliveira
Thinh Uy Quang
THIRU MAALAVAN
Thobani Hlophe
Thomas De Jonghe
Thomas Nelson
Thomas Pappas
Thomas Ranvier
Thomas Wade Culbertson
Thomas
ThomasLuby
ThomasVoreyer
THORODINOVICH
Thought Vector
throne1032
ThuanHieu
Thulani Tembo
Tiago V. Melo
Tiago Vinhoza
Tiantian
Tianyi Wang
Tigran Davtyan
Tilak
Tim Hradil
Tim Kartawijaya
Tim Pearce
Time Magazine
Timo Bozsolik
Timothy Leung
TimRu
timsyang
Ting Zhou
tiredgeek
TirthGajjar
Tito Maraca
tivoli2
Tiziano Teso
Tjb5670
TK
tmthyjames
TobeyStrauch
toby jolly
Tolu Toluhi
Tom Bombadil
Tom Hill
TomÃ¡s Accini
TomÃ¡s Bustamante
Tomasz Bartczak
Tomato
Tomer Eldor
Tomi-Andre
tommert
Tommy Pompo
TomNeeld
Tomo
Tony Pino
Tony Xie
TonyChan
tophatsteve
torr
Toshnoue
TP
traceyvanp
tranndo
Transparency International
Trent Baur
Trey Kollmer
TripleFireYan
Trond Magne Lamprecht Haaland
TruMedicines
Truong An
Truth Lover
tsimins
Tuhin Saha
Tung Thanh Le
tusha kutusha
Tushar Dhyani
Tushar Gupta
Tushar Mahendra Patil
Tushar Makkar
Tushar Yadav
tvscitechtalk
TwistFateBOY
TY
tyjzhong
tylerfuller
TylerTuschhoff
u_kag
U.S. National Archives and Records Administration
UC San Diego
UCI Machine Learning
Udacity
UDAS
Uday
Uddeshya Singh
Udeme Udofia
ugocupcic
Ujjwal Kr Gupta
Ujjwal
Ujjwal
ultra-jack
Umakant
Umang Dhiman
Umberto
umut
Union of Concerned Scientists
United Nations Development Program
United Nations
United States Air Force
United States Department of Agriculture
United States Drought Monitor
University of Connecticut
University of Copenhagen
University of Michigan
University of North Texas
University of Pittsburgh
University of Virginia
uranio255
UrvangPatel
US Bureau of Labor Statistics
US Census Bureau
US Customs and Border Protection
US Department of Agriculture
US Department of Energy
US Department of Health and Human Services
US Environmental Protection Agency
US Geological Survey
US Patent and Trademark Office
US Senate
USB
usfundamentals
ushchent
Utagh
Utkarsh Aggarwal
utmhikari
uttahjazz
V.A. Freeman
V81msk
Vadim Shmelev
Vahe Andonians
Vahik95
vaibhav_varshney
vaibhavgeek
Vaibhavi Singh
Vaibs
Valentina C
Valeria BarÃ³n
ValerieSalazar
Valerio Luciani
Valerio Luzzi
ValerioVaccaro
vanAmsen
Vanessa
Vardan
VarDial
Varun Belliappa
Varun Bhargava
Varun Kashyap.K.S.
varunagarwal
vasilisnikolaou
VasyaVologdin
Vedant Ruparelia
VedapragnaReddy
Vein
Venkat Ramakrishnan
VenkataDuvvuri
VenkataSivaAbhishek
Venkatesh Madhava
Venkateshgopal
Vera Lei
Vered Shwartz
verginer
Veysel Kocaman
Vicc Alexander
Vicky1
VickyLee
Victor dos Santos
Victor Genin
Victor Hugo
Victor Paslay
Victor
victor7246
VictorElie
VictorGrobberio
Vidhu Shekhar Tripathi
viditjain
Vignesh Varadarajan
vihan
vijay dhameliya
vijay
Vijaykumar Ummadisetty
VijayN
Vikas Kamath M
Vikas Pandey
VikasSangwan
vikassrivastava
vikrant yadav
VikrantThakur
Viktor Malyi
ViktoriaSuponenko
vinay shanbhag
Vinayagam.D.Ganesh
vinceallenvince
Vincent Assoun
VincentLa
VineetKothari
vinodkumar
Viraj Bhambri
Virginie Do
VirgoData
viru
Vish Chekuri
Vish Vishal
VishakhHegde
VISHAL MODAGEKAR
VishnuRaghavan
Vishwas Shrikhande
Vishwesh S
viswateja gajulavarthy
Vitalii Peretiatko
Vitaly Burachyonok
Vitaly Korchagin
Vitor R. F.
Vivek Chutke
Vivek Kumar
Vivek Pandey
Vivek Singh
VivekGopinathlal
VivekMangipudi
Vivian l
Vivin Abraham
vl
Vlad Golubev
vlad.pambucol
VladB
vladifidchuk
Vladimir Alencar
Vladimir Belyaev
Vladimir Gmyzin
Vladimir Kiselev
Vladimir Kuznetsov
Vladislav Zavadskyy
Volodymyr Sadovyy
Vonage Garage
voronwe2007
VoteView
Vrushali Patel
vsmolyakov
Vyas
W. Yifan
Wal8800
walla2ae
Wally Atkins
Walter_Sam
wanglaiqi
WangQiucheng
wangtianju
WanqiWang
Waqas Malik
Ward Bradt
Warren Elder
warrentnt
Washim Ahmed
Washington University
Watts
Wayne Haubner
WayneC
Wazeed
Web IR
WebDev
weeliangng
Wei Chun Chang
Wei Ouyang
weibo
weisinhong
wellll
WENBOCAO
wenchen
Wenchi
Wendy Kan
wenlong
WesDuckett
wh0801
whosonit 1
WÎ”
Wijdan Aljumiah
wil o c ward
WildGrok
Wilian Osaku
Will Gao
will hunt
WillamGreen
William Cao
William Cukierski
William Hyde
William Straus
William Walter
williamnowak
Willie Liao
willinghorse
Winastwan Gora
Windson
Windy Torgerud
Winnie
WNYC
WojciechWÅ‚odarczyk
Wol4ara_Vio
Work1810
World Bank
World Bank
World Economic Forum
WorldValueSurvey
woutervh88
WrackShipParty
Wrong
WU Wuhui
WUZZUF
xachi
Xai Nano
xaliap
XavierBays
XavierMartinezBartra
Xavya
xgan
Xiang Zhang
xiaocongSonia
XiaojingLi
Xiaoxiao Wu
XIAOZHOU YANG
Ximing
Xin
xingzhangren
Xiong Songsong
xjtushilei
xss
xtyscut
Xuetao Shi
XuleiYang
xuseniayu
xuy2
xWang
xx
Yabir Canario
YachunCheng
YaGana Sheriff-Hussaini
Yagnesh Badiyani
YahyaCivelek
yaliTsai
yamuuu
Yan Ramos da Silva
Yan Zhu
Yang Lin
Yang Yunfan
Yanir
Yannis Pappas
YannMallegol
Yannsar
Yao Hu
Yao Lu
YaoHsiao
YaoSenYou
Yaoxiang Li
Yap Wei Yih
Yapi Donatien Achou
Yarden Sharon
Yasar Kocal
Yaser Ahmed
Yash Pradhan
Yash
yashjain
Yashna shravani
Yashu
YasmeenW
Yassine Marzougui
Yassine Morakkam
yassineameur
yasuhiro_121
Yaswanth Gosula
Yatishbn
yazi
Ye HuangJie
YeoMyungRo
yeongchan
yeongseok
Yevgeniya Migranova
Yexiaofeng
Yi Cao
Yi Jingyuan-é™è¿œ
Yi Su
Yichenâ€œEddieâ€Shen
Yifan Xie
YijieZhuang
Yin Zhang
YingHan
Yingzhu
YiqiZhang
yiweihuang
Yixin Sun
ykamikawa
ykatayama
YL
Ylan Kazi
yliu
ymlai87416
ymtoo
Yoann Pradat
Yochanan Scharf
Yogesh Gupta
yogeshsingh
Yogi
Yoka
Yonatan Vaizman
Yongho Choi
Young And Dumb
YourKingdomCome
ysaz
Yu Sheng Lu
YU_CHIH
Yuanjie Li
yuansaijie0604
Yueming
YueSu
YuhaoWang
YuhuaXiong
yujack
Yukarin
Yulia G
Yuncheng Li
Yunguan FU
yuqing01
Yura Shakhnazaryan
Yuranan
Yurii Biurher
Yury Kashnitsky
Yusuf
YuwenJin
Yuzie Yu
Yvon
yvonhk
Zach Barillaro
zach
zack
ZackCode
ZacKentonASI
zackthoutt
ZagarsurenSukhbaatar
Zain Baig
Zain Rizvi
Zakar H.
Zalando Research
Zan Huang
Zaruhi Avagyan
Zaur Begiev
zedd
Zeeshan-ul-hassan Usmani
zelhassn
Zeta
zhai kun
zhangchengwei
zhanglanqing
ZhaofengLi
zhaojingnan
Zhe LIN
ZheCJ
Zhengyi Zhu
ZhenyuBo
Zheye Yuan
Zhijin
zhiliang
zhixing
zhousheng
Zielak
Zillow
Zinuo Jia
ZiyuanZhong
zjf
zluckyH
ZoeRenwick
Zoey
ztyh0121
ZuhaibAli
Zuoyu Miao
Zurda
ZuSwi
Zwidofhelangani Gabara
zyaj"
California Housing Prices,Median house prices for California districts derived from the 1990 census.,Cam Nugent,3,"Version 1,2017-11-24","united states
housing
regression analysis",CSV,400 KB,CC0,"1,271 views",250 downloads,9 kernels,0 topics,https://www.kaggle.com/camnugent/california-housing-prices,"Context
This is the dataset used in the second chapter of Aurélien Géron's recent book 'Hands-On Machine learning with Scikit-Learn and TensorFlow'. It serves as an excellent introduction to implementing machine learning algorithms because it requires rudimentary data cleaning, has an easily understandable list of variables and sits at an optimal size between being to toyish and too cumbersome.
The data contains information from the 1990 California census. So although it may not help you with predicting current housing prices like the Zillow Zestimate dataset, it does provide an accessible introductory dataset for teaching people about the basics of machine learning.
Content
The data pertains to the houses found in a given California district and some summary stats about them based on the 1990 census data. Be warned the data aren't cleaned so there are some preprocessing steps required! The columns are as follows, their names are pretty self explanitory:
longitude
latitude
housing_median_age
total_rooms
total_bedrooms
population
households
median_income
median_house_value
ocean_proximity
Acknowledgements
This data was initially featured in the following paper: Pace, R. Kelley, and Ronald Barry. ""Sparse spatial autoregressions."" Statistics & Probability Letters 33.3 (1997): 291-297.
and I encountered it in 'Hands-On Machine learning with Scikit-Learn and TensorFlow' by Aurélien Géron. Aurélien Géron wrote: This dataset is a modified version of the California Housing dataset available from: Luís Torgo's page (University of Porto)
Inspiration
See my kernel on machine learning basics in R using this dataset, or venture over to the following link for a python based introductory tutorial: https://github.com/ageron/handson-ml/tree/master/datasets/housing"
Apartment data,10yrs time period real estate detail data (apartment),GunheePark,3,"Version 2,2017-10-20|Version 1,2017-10-20","housing
real estate",CSV,1019 KB,Other,412 views,55 downloads,2 kernels,0 topics,https://www.kaggle.com/gunhee/koreahousedata,"Context
What feature affects on housing price(specifically apartment price). Figuring out what patterns hidden in real estate market (apartment).
Content
For time period 10years traded apartment detailed data using API provided from data.go.kr
This data dealt only one specific district.
Acknowledgements
Inspiration
What patterns are hidden in data? If you can not find patterns or generalize, what feature should be added/consider?"
OSHA Accident and Injury Data,Injury records for 2015-2017,ruqaiyaship,3,"Version 1,2018-01-20",,CSV,869 KB,Other,99 views,10 downloads,,0 topics,https://www.kaggle.com/ruqaiyaship/osha-accident-and-injury-data-1517,"Context
Health, Safety, and Environment (HSE) is a dicspline centered on implementing practices for environmental protection and safety in a workplace. Energy companies place a strong emphasis on HSE when conducting day to day operations, whether it is on the field or in an office. A major challenge with HSE, however, is monitoring and managing HSE incidents across an enterprise. The common practice for incident management is analyzing detailed incident reports. This can be cumbersome and time-consuming, because in most cases, these reports contain unstructured text. To increase efficiency, companies are seeking technologies that allow them to derive valuable insights from unstructured HSE data efficiently.
Content
This dataset contains abstracts of the accidents and injuries of construction workers from 2015-2017. There is some structured data around the unstructured text abstracts, such as Degree of Injury, Body Part(s) Affected, and Construction End Use.
Acknowledgements
This is OSHA data which is publicly available.
Inspiration
What are the most buildings/structures to build? What trends do we see in injuries in terms of time of day, time of year, etc.? What is the reason injuries are occurring? Where do we need more training and safety measures in place?"
USDA plant database,Complete extraction from the Natural Resources Conservation Service,apollonius,3,"Version 1,2017-11-13","biology
plants
agriculture",Other,2 MB,CC0,388 views,45 downloads,,0 topics,https://www.kaggle.com/apollonius/usda-plant-database,"Context
The USDA Plant database extraction from the Natural Resources Conservation Service.
Content
It contains a wide variety of varieties in raw format.
Inspiration
There is currently no USDA plant information available via API. Using this data set I'm hoping that I can extract needed information for improved plant growth in controlled environments."
Hotel Reviews,Labelled hotel reviews,HarmanpreetSingh,3,"Version 1,2017-10-22","hotels
business",CSV,34 MB,CC0,713 views,102 downloads,,0 topics,https://www.kaggle.com/harmanpreet93/hotelreviews,"Data Description
Dataset is from one of the leading travel site containing hotel reviews provided by customers.
Variable | Description
--- | ---
User_ID | unique ID of the customer
Description | description of the review posted
Browser_Used | browser used to post the review
Device_Used | device used to post the review
Is_Response | target Variable"
Electric Power Consumption,,sabyasachi,3,"Version 1,2018-01-17",,Other,19 MB,CC0,116 views,10 downloads,,0 topics,https://www.kaggle.com/sabyasachi8/electric-power-consumption,This dataset does not have a description yet.
german_credit_data_with_risk,,Leonardo Ferreira,3,"Version 1,2018-01-09",,CSV,52 KB,CC0,156 views,74 downloads,,0 topics,https://www.kaggle.com/kabure/german-credit-data-with-risk,This dataset does not have a description yet.
World Energy Consumption Historical Data,,Sharad,3,"Version 1,2018-01-21",,CSV,19 KB,CC0,65 views,3 downloads,,0 topics,https://www.kaggle.com/shashank22/world-energy-consumption-historical-data,This dataset does not have a description yet.
Lead legs on chipset,Lead legs on chipset,Ggzet,3,"Version 1,2017-10-24",,Other,2 MB,CC4,441 views,31 downloads,,2 topics,https://www.kaggle.com/gagazet/lead-legs-on-chipset,"Context
Pretty simple and not huge dataset of png images from motherboard, lead legs of chipset. Created for training ML/DP models on detection and classification of defected parts. Contain 30 images with defect, and 323 of standard one.
Content
Collected at the August of 2017, by me. For it used photo microscope and 2 boards, one with defect parts, another is standard. As you can see, here is just 30 photos with problem parts. If someone need more, i can make and crop them by a short time, just msg me here or at the my email.
Acknowledgements
This is totally my dataset, used it for model training on detection and classification of lead parts at the motherboard.
Inspiration
This dataset can be useful for people, who trying to solve same problems with me, and cant create their own dataset. Soon, i will try to upload another dataset with solder parts! Not for commercial use, just for science~"
Cricinfo Statsguru Data,"For anyone who enjoys Cricket, and analyzing cricket stats.",Christopher Clayford,3,"Version 5,2017-12-19|Version 4,2017-12-07|Version 3,2017-11-30|Version 2,2017-10-20|Version 1,2017-10-20","cricket
sports
statistics",CSV,3 MB,CC0,841 views,107 downloads,,0 topics,https://www.kaggle.com/cclayford/cricinfo-statsguru-data,"Context
This is a data dump of the Statsguru section of Cricinfo's searchable cricket statistics database. I have only uploaded Test Match, and ODI data for now, and will upload T20I data soon.
Content
I have pulled details data on the following disciplines:
Team stats
Batting stats
Bowling stats
Fielding stats
All-Rounders stats
Batting partnerships
Acknowledgements
All data pulled can be found on the cricinfo website: http://stats.espncricinfo.com/ci/engine/stats/index.html
Inspiration
For anyone who enjoys Cricket, and analyzing cricket stats."
Daily Crypto Currency and Lunar Geocentric Data,"Daily crypto markets open, close, low, high data and Lunar Phases (2013-2018)",RudyMizrahi,3,"Version 1,2018-01-25","covariance and correlation
space
business
+ 2 more...",CSV,29 MB,Other,125 views,12 downloads,,0 topics,https://www.kaggle.com/rudymizrahi/daily-crypto-currency-and-lunar-geocentric-data,"Context
This data includes daily open, high, low, close values for all crypto currencies (since April 2013) as well as Daily Lunar Geocentric data (distance, declination, brightness, illumination %, and constellation). Please note that I have consolidated this data from the below two sources (originally submitted by MCrescenzo and jvent) after my mother asked me if there's a correlation between the lunar status and the financial markets.
Content
This data includes daily open, high, low, close values for all crypto currencies (since April 2013 until January 2018) as well as Daily Lunar Geocentric data (distance, declination, brightness, illumination %, and constellation) for same timeframe.
Acknowledgements
Lunar Daily Distance and Declination : 1800-2020. Click for original data submitted by MCrescenzo Every Cryptocurrency Daily Market Price. Click for original data submitted by jvent
Inspiration
Is there any correlation between cryptocurrencies and Lunar Phases?
Can we predict cryptocurrency movements by the Lunar Phases?"
Election News Headlines,A day's harvest of election headlines from Nepal's news homepages,dish,3,"Version 2,2017-11-18|Version 1,2017-11-14","politics
linguistics
internet",CSV,76 KB,CC0,471 views,14 downloads,,,https://www.kaggle.com/blogdish/election-news-headlines,"Context
The headlines, with links in most cases, were harvested for a quick viewing of the kinds of election talk the online news media were carrying as the campaign picked up steam ahead of Nepal's first federal and provincial elections less than a month away.
Content
The headlines, with links preceding them, were scraped from 24 news websites of Nepal on 11/14/2017. They comprise 510 lines of Nepali texts in UTF-8, after removing the links in English.
Acknowledgements
The dataset is part of a personal hobby of the author to take stock of the election talk going on in Nepali media at the moment. This was possible thanks to the Python libraries, requests and bs4, available with the Jupyter notebooks on Anaconda.
Inspiration
Media headlines tend to be a succinct gist of the election talk going on in the campaign period, with their potential to throw light on the kind of rhetoric and quality of arguments used for election gains. What can a text analysis of the headlines show?"
Call Tests Measurements for MOS prediction,Classification task with call test KPIs,Valerio Luciani,3,"Version 1,2018-01-11","telecommunications
engineering
machine learning",Other,6 MB,Other,120 views,7 downloads,2 kernels,0 topics,https://www.kaggle.com/valeriol93/predict-qoe,"### Context
Call Test Measurements for Mobile Network Monitoring and Optimization.
### Content
The measurements were performed with smartphones and collected on proprietary databases.
### Inspiration
The scope is to predict a level of ""Quality of Experience"" (e.g. Mean Opinion Score - MOS) with other measured KPIs (classification task). At the moment the best accuracy is about 60% obtained with a Random Forest classifier (implemented with Sci-kit Learn on Python)"
Chemical Substance Registry (CAS registry numbers),The EPA's Toxic Substances Control Act Chemical Substance Inventory,US Environmental Protection Agency,3,"Version 3,2017-10-19|Version 2,2017-10-19|Version 1,2017-10-19","science and culture
chemistry
health
chemical engineering",CSV,9 MB,CC0,824 views,28 downloads,,0 topics,https://www.kaggle.com/epa/cas-registry-numbers,"Context
What is the TSCA Chemical Substances Control Inventory?
Section 8 (b) of the Toxic Substances Control Act (TSCA) requires EPA to compile, keep current and publish a list of each chemical substance that is manufactured or processed, including imports, in the United States for uses under TSCA. Also called the “TSCA Inventory” or simply “the Inventory,” it plays a central role in the regulation of most industrial chemicals in the United States.
The initial reporting period by manufacturers, processors and importers was January to May of 1978 for chemical substances that had been in commerce since January of 1975. The Inventory was initially published in 1979, and a second version, containing about 62,000 chemical substances, was published in 1982. The TSCA Inventory has continued to grow since then, and now lists about 85,000 chemicals.
EPA’s compilation of the public TSCA Inventory information is updated twice a year to include new and corrected TSCA Inventory chemical listings, and it contains none of the chemical identities claimed as confidential. Thus it is not as complete nor current as the information contained in EPA's TSCA Master Inventory File, which includes the chemical identities claimed as confidential and is updated continuously as new and corrected information is received by EPA. Consequently, for the purposes of TSCA compliance, the TSCA Master Inventory File maintained by EPA's Office of Pollution Prevention and Toxics is the only complete and accurate source that can provide authoritative and conclusive information about which chemical substances are currently included in the TSCA Inventory.
Content
TSCAINV_062017.csv
ID: Record ID Number
RN: Chemical Abstracts Service (CAS) Registry Number
casregno: CAS registry number without ""-"" [dashes]
IN: Index Name (Chemical name)
DF: Chemical substance definition
FL: EPA TSCA Regulatory Flag
UV: UVCB Flag
CS: Commercial Status Designation
PMNACC_062017.csv
ID: Record Number ID
PMNNO: PMN Number/Form Number
ACCNO: EPA Accession Number
GN: Generic Name
FL: EPA TSCA Regulatory Flag
CS: Commercial Status Designation
Acknowledgements
The EPA updates this registry is twice per year. The version here was downloaded on Oct 18th, 2017. Check the EPA website for updated versions: https://www.epa.gov/tsca-inventory/how-access-tsca-inventory.
Inspiration
There are lots of air quality and pollution datasets that you can use in conjunction with this TSCA Registry to learn more about contaminants and chemicals in general."
Bitcoin Vericoin dataset (Poloniex + Mosquito),200 days 5-min ticker with 70 indicators generated with Mosquito Blueprint,Miro Karpis,3,"Version 1,2017-11-09",finance,CSV,52 MB,CC0,287 views,50 downloads,,0 topics,https://www.kaggle.com/kam1ro/btc-vrc-mosquito-blueprint,"Context
Dataset is provided to take a look and train model on historical crypto-currency data including extra financial indicators.
Content
Dataset contains 200 days of 5 Minute ticker dataset for Crypto Currency BTC (Bitcoin) and VRC (Vericoin) together w around 70 financial indicators. Additionally each row contains label, which describes buy, sell and hold action.
Small sample of labels:
Acknowledgements
Dataset is generated with Open Source Project mosquito-blueprint module.
Inspiration
Main point is give public (including me) the possibility to answer on following questions:
How well can we predict labeled price of given crypto-currency pair?
Does the dataset contain enough information to give good enough results?"
pima-indians-diabetes.data,pima-indians-diabetes-dataset,wanglaiqi,3,"Version 1,2017-10-12",healthcare,CSV,23 KB,ODbL,877 views,137 downloads,5 kernels,,https://www.kaggle.com/wanglaiqi/pimaindiansdiabetesdata,"Context
About whether the horse will be in the next 5 years, whether the occurrence of diabetes data set
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
Python-scripts,,HarryTan,3,"Version 1,2017-10-15",,Other,1 KB,Other,320 views,12 downloads,,0 topics,https://www.kaggle.com/harry688tan96/pythonscripts,This dataset does not have a description yet.
Blue Plaques,Almost 40000 blue plaque historical markers worldwide,Aleksey Bilogur,3,"Version 1,2017-11-06","culture and humanities
europe
history",CSV,26 MB,CC0,508 views,68 downloads,,0 topics,https://www.kaggle.com/residentmario/blue-plaques,"Context
Blue plaques are a well-known and very popular permanent historical marker scheme administered in the United Kingdom that has since spread to many other countries in Europe and the world. According to Wikipedia:
A blue plaque is a permanent sign installed in a public place in the United Kingdom and elsewhere to commemorate a link between that location and a famous person or event, serving as a historical marker. The brainchild of British politician William Ewart in 1863, it is the oldest such scheme in the world.
The world's first blue plaques were erected in London in the 19th century to mark the homes and workplaces of famous people. This scheme continues to the present day...the term ""blue plaque"" may be used narrowly to refer to the official English Heritage scheme, but is often used informally to encompass all similar schemes.""
Here's what a model blue plaque looks like:
(image via Wikimedia Commons)
This dataset contains data about most of the blue plaques installed in Europe as of June 2017, as reported by Open Plaques.
Content
This dataset contains information on the location of each plaque, who the subject is, and metadata about the person or organization being recognized.
Acknowledgements
This dataset is republished as-is from the original on Open Plauqes.
Inspiration
Where are the blue plaques located?
What kinds of people and places get awarded a plaque?
What is the geospatial distribution of these plaques throughout the UK? Worldwide?"
IBM HR,,E.Nikumanesh.Germany,3,"Version 1,2017-10-15",,CSV,223 KB,Other,778 views,127 downloads,,0 topics,https://www.kaggle.com/esmaeil391/ibm-hr,This dataset does not have a description yet.
California DDS Expenditures,Exploring Simpson's Paradox,WesDuckett,3,"Version 4,2017-09-30|Version 3,2017-09-29|Version 2,2017-09-29|Version 1,2017-09-29","ethnic groups
finance
health
demographics",CSV,41 KB,CC0,832 views,41 downloads,,0 topics,https://www.kaggle.com/wduckett/californiaddsexpenditures,"Context
This data set contains data regarding the allocation of funding from the Department of Developmental Services to developmentally-disabled individuals in California in 2014.
Content
The variables included are:
Id [int]
Age Cohort (age group) [factor]
Age [int]
Gender [factor]
Expenditures [int]
Ethnicity [factor]
This data set is well suited for exploring the effects of Simpson's Paradox and confounding variables.
Acknowledgements
The data was originally retrieved from the California Department of Developmental Services (http://www.dds.ca.gov) by Stanley Taylor and Amy Mickel from California State University, Sacremento. The names associated with each record have been removed to protect anonymity.
Taylor and Mickel explored Simpson's Paradox using this data set after a discrimination lawsuit was filed against the California DDS. The lawsuit claimed that White Non-Hispanics were receiving more funding than Hispanics. To learn more about the analysis and findings of Taylor and Mickel, read the paper they published together by following this link: www.amstat.org/publications/jse/v22n1/mickel.pdf
Inspiration
Is there any basis to the claim of discrimination? What are the confounding variables? What are other ways to organize this data to gain an alternate perspective?"
Users mobile banking transaction frequency,,HUSEYiNKiliC,3,"Version 2,2017-11-03|Version 1,2017-11-03","finance
banking
demographics
mobile web",CSV,310 KB,Other,654 views,146 downloads,,0 topics,https://www.kaggle.com/huseyinkilic/users-mobile-banking-transaction-frequency,"Context
This data collected via Survey. With this data, User or Items will be cluster. Based on cluster new minimalist applicatin will be designed.
Content
There are descripted data like gender, phone type etc. Main reason for that data is it possible to make cluster based on descripted data.
Inspiration
Is it possible to make cluster from this data ? Clustering Items together. I am not expecting seperate diffrent cluster. It should be overlap some feauters."
MOOC Dataset,,Chella Priyadharshini,3,"Version 1,2017-11-08",,CSV,12 MB,CC0,377 views,42 downloads,,,https://www.kaggle.com/chellaindu/mooc-dataset,This dataset does not have a description yet.
The UMass Global English on Twitter Dataset,"10,502 annotated tweets from 130 countries",Rachael Tatman,3,"Version 1,2017-09-27","languages
linguistics
twitter
internet",Other,1 MB,Other,290 views,30 downloads,,0 topics,https://www.kaggle.com/rtatman/the-umass-global-english-on-twitter-dataset,"Context:
It can difficult to identify the language that a tweet is written in. In addition to being very short, they often include code-switching, where the user uses two or more languages together, or names borrowed from a different language.
This dataset contains tweets from a variety of languages, tagged for whether they are in English or not, whether they contain code-switching, whether they includes names from a different language and whether they were generated automatically.
Content:
This dataset contains 10,502 tweets, randomly sampled from all publicly available geotagged Twitter messages, annotated for being in English, non-English, or having code switching, language ambiguity or having been automatically generated. It includes messages sent from 130 different countries.
The file all_annotated.tsv contains the dataset of 10,502 tweets used in the paper. Text is encoded as UTF-8.
The column headings (also given in the .tsv file) are: tweet ID, ISO country code, tweet date, tweet text, definitely English, ambiguous, definitely not English, code-switched, ambiguous due to named entities, and automatically generated tweets.
All annotations are binary; the definitely English, ambiguous, and definitely not English annotations are mutually exclusive.
Acknowledgements:
This dataset was collected by Su Lin Blodgett, Johnny Tian-Zheng Wei and Brendan O'Connor. It is redistributed here under the Creative Commons Attribution 4.0 International License. If you use this data in your work, please cite the following paper:
Blodgett, Su Lin, Johnny Wei, and Brendan O'Connor. ""A Dataset and Classifier for Recognizing Social Media English."" Proceedings of the 3rd Workshop on Noisy User-generated Text. 2017.
You can find more information on this dataset and related work on this website.
Inspiration:
Can you use this dataset to build a classifier that identifies whether a tweet is in English or not?
Can you use this dataset to build a language identifier? (You can check out the authors’ language identifier here.)"
Pokemon,,Salomon,3,"Version 1,2017-11-02",,CSV,682 KB,Other,379 views,36 downloads,,0 topics,https://www.kaggle.com/dollarbillio/pokemon,This dataset does not have a description yet.
Math Students,Math Scores for each student,Alexander,3,"Version 1,2017-10-11",,CSV,41 KB,CC0,"1,362 views",220 downloads,,,https://www.kaggle.com/janiobachmann/math-students,"Content
This is a dataset from the UCI datasets repository. This dataset contains the final scores of students at the end of a math programs with several features that might or might not impact the future outcome of these students.
Citation:
Please include this citation if you plan to use this database:
P. Cortez and A. Silva. Using Data Mining to Predict Secondary School Student Performance. In A. Brito and J. Teixeira Eds., Proceedings of 5th FUture BUsiness TEChnology Conference (FUBUTEC 2008) pp. 5-12, Porto, Portugal, April, 2008, EUROSIS, ISBN 978-9077381-39-7. [Web Link]
Attribute Information:
Attributes for both student-mat.csv (Math course) and student-por.csv (Portuguese language course) datasets:
1 school - student's school (binary: 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira)
2 sex - student's sex (binary: 'F' - female or 'M' - male)
3 age - student's age (numeric: from 15 to 22)
4 address - student's home address type (binary: 'U' - urban or 'R' - rural)
5 famsize - family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3)
6 Pstatus - parent's cohabitation status (binary: 'T' - living together or 'A' - apart)
7 Medu - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 â€“ 5th to 9th grade, 3 â€“ secondary education or 4 â€“ higher education)
8 Fedu - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 â€“ 5th to 9th grade, 3 â€“ secondary education or 4 â€“ higher education)
9 Mjob - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')
10 Fjob - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')
11 reason - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other')
12 guardian - student's guardian (nominal: 'mother', 'father' or 'other')
13 traveltime - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)
14 studytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)
15 failures - number of past class failures (numeric: n if 1<=n<3, else 4)
16 schoolsup - extra educational support (binary: yes or no)
17 famsup - family educational support (binary: yes or no)
18 paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)
19 activities - extra-curricular activities (binary: yes or no)
20 nursery - attended nursery school (binary: yes or no)
21 higher - wants to take higher education (binary: yes or no)
22 internet - Internet access at home (binary: yes or no)
23 romantic - with a romantic relationship (binary: yes or no)
24 famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)
25 freetime - free time after school (numeric: from 1 - very low to 5 - very high)
26 goout - going out with friends (numeric: from 1 - very low to 5 - very high)
27 Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)
28 Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)
29 health - current health status (numeric: from 1 - very bad to 5 - very good)
30 absences - number of school absences (numeric: from 0 to 93)
these grades are related with the course subject, Math:
31 G1 - first period grade (numeric: from 0 to 20)
31 G2 - second period grade (numeric: from 0 to 20)
32 G3 - final grade (numeric: from 0 to 20, output target)"
CAUSES OF DEATH IN THE WORLD 2014,"100,000 Best United Nation Dataset",Data Quantum,3,"Version 1,2017-10-31",,CSV,15 MB,CC0,168 views,16 downloads,,0 topics,https://www.kaggle.com/dataquantum/death2014,"Contain 95k rows and 11 columns
Content
This dataset is about the causes of death in 2014 release by UN for public.
Acknowledgements
Credit to United Nation
Inspiration
Data to make a world a better place"
Nineteenth Century Works On Nepal,For a quick viewing of how they portrayed the country in words and sentiments,dish,3,"Version 1,2017-11-02","languages
linguistics",Other,3 MB,CC0,297 views,12 downloads,,0 topics,https://www.kaggle.com/blogdish/nineteenth-century-works-on-nepal,"Context
These 19th century works on Nepal were downloaded from Project Gutenberg for a quick viewing of how the line graphs of their page-by-page sentiment compared with one another in applying the the text mining, analysis and visualization capabilities of R, inspired by the work on janeaustenr or gutenbergr.
Content
The books and collection of journals on Nepal of about 200 years ago are in text files.
Acknowledgements
These works have been available in machine readable format thanks to Project Gutenberg.
Inspiration
Although the volumes of books and journals are growing in the online repositories of Project Gutenberg, only a few works in English are about Nepal. How are their portrayals of Nepal similar or different in word-clouds and sentiments? Which R packages can be useful to make these comparisons?"
Stock Data,Stock prices of top banks throughout the financial crisis to early 2016.,Rohan Patel,3,"Version 3,2017-11-10|Version 2,2017-11-10|Version 1,2017-11-06",,CSV,1 MB,CC0,401 views,164 downloads,,0 topics,https://www.kaggle.com/rohan8594/stock-data,"Context
This dataset contains a pickle file and csv files that contain data of bank stocks of 6 top banks throughout the financial crisis all the way to early 2016.
Content
The stock information is from the following banks:
Bank of America
CitiGroup
Goldman Sachs
JPMorgan Chase
Morgan Stanley
Wells Fargo
In the dataset, each bank is represented by its ticker symbol. For example, Bank of America is represented by BAC and Wells Fargo is represented by its ticker symbol WFC.
Acknowledgements
This dataset has been collected from google finance as part of my data capstone project for my Udemy Datascience and ML Bootcamp."
Word Occurrences in Movies,"How many times did they say ""bee"" in Bee Movie?",Emma,3,"Version 2,2017-11-09|Version 1,2017-11-09","entertainment
linguistics",CSV,1 MB,CC0,259 views,5 downloads,,0 topics,https://www.kaggle.com/emmabel/word-occurrences-in-movies,"Context
Like most things in my life, it all started with the Bee Movie. I wanted to know what the most popular words were in the film. So I wrote a Python script that found the words used in the movie and the number of times they appear in the script.
Content
Each movie has an associated .csv file where the first column represents the words found in the script for that move and the second column is the number of times that word appears.
My Python code can be found at https://github.com/emmabel96/WordOccurrences
Acknowledgements
Most of the scripts used were collected by Alberto Acerbi and can be found at https://figshare.com/projects/imsdb_movie_scripts/18907
Inspiration
I'm looking forward to seeing the creative ways this dataset is used!"
Amazon Echo Dot 2 Reviews Dataset,Echo Dot 2 Reviews posted on Amazon in September and October 2017,PromptCloud,3,"Version 1,2017-11-03",internet,CSV,2 MB,CC4,594 views,15 downloads,,0 topics,https://www.kaggle.com/PromptCloudHQ/amazon-echo-dot-2-reviews-dataset,"Context
Since Amazon Echo Dot 2 has been the best selling Alexa product, we decided to extract the reviews posted on Amazon for this device. This particular dataset contains reviews posted in September and October 2017. The complete dataset with all the reviews from 2016 can be downloaded from DataStock - a repository of clean and structured web datasets with historical records.
Content
Given below are the data fields:
Pageurl
Title
Review Text
Device Color
User Verified
Review Date
Review Useful Count
Configuration
Rating
Declaration Text (Example: Vine Voice, Top 100 reviewer, etc.)
Acknowledgements
This dataset has been created via PromptCloud's in-house web data extraction solution.
Inspiration
The initial set of analyses can be access here - https://goo.gl/XHVe9b."
NIPS17 Adversarial learning - 3rd round results,"Scores, runtime statistics and intermediate results of the third DEV round.",Google Brain,3,"Version 2,2017-10-01|Version 1,2017-09-28",artificial intelligence,CSV,148 KB,Other,"1,207 views",221 downloads,5 kernels,,https://www.kaggle.com/google-brain/nips17-adversarial-learning-3rd-round-results,"This dataset contains results of the third development round of NIPS 2017 Adversarial learning competition.
Content
Matrices with intermediate results
Following matrices with intermediate results are provided:
accuracy_matrix.csv - matrix with number of correctly classified images for each pair of attack (targeted and non-targeted) and defense
error_matrix.csv - matrix with number of misclassified images for each pair of attack (targeted and non-targeted) and defense
hit_target_class_matrix.csv - matrix with number of times image was classified as specific target class for each pair of attack (targeted and non-targeted) and defense
In each of these matrices, rows correspond to defenses, columns correspond to attack. Also first row and column are headers with Kaggle Team IDs (or baseline ID).
Scores and run time statistics of submissions
Following files contain scores and run time stats of the submissions:
non_targeted_attack_results.csv - scores and run time statistics of all non-targeted attacks
targeted_attack_results.csv - scores and run time statistics of all targeted attacks
defense_results.csv - scores and run time statistics of all defenses
Each row of these files correspond to one submission. Columns have following meaning:
KaggleTeamId - either Kaggle Team ID or ID of the baseline.
TeamName - human readable team name
Score - raw score of the submission
NormalizedScore - normalized (to be between 0 and 1) score of the submission
MinEvalTime - minimum evaluation time of 100 images
MaxEvalTime - maximum evaluation time of 100 images
MedianEvalTime - median evaluation time of 100 images
MeanEvalTime - average evaluation time of 100 images
Notes about the data
Due to team mergers, team name in these files might be different from the leaderboard.
Not all attacks were used to compute scores of defenses and not all defenses were used to compute scores of attacks. Thus if you simply sum-up values in rows/columns of the corresponding matrix you won't obtain exact score of the submission (however number you obtain will be very close to actual score).
Few targeted and non-targeted attacks exceeded 500 seconds time limit on all batches of images. These submissions received score 0 in the official leaderboard. We still were able to compute ""real"" score for these submissions and include it into non_targeted_attack_results.csv and targeted_attack_results.csv files. However these scores are negated in the provided files to emphasize that these submissions violate the time limit."
Google news articles tagged under hate crimes,"Google news articles tagged under hate crimes in the US, Feb. 13-Oct. 28, 2017",Google News Lab,3,"Version 1,2017-11-02","news agencies
journalism
crime
internet",CSV,8 MB,CC4,166 views,13 downloads,,0 topics,https://www.kaggle.com/GoogleNewsLab/google-news-articles-tagged-under-hate-crimes,"Google news articles tagged under hate crimes in the US, Feb. 13-Oct. 28, 2017"
Frightgeist 2017: Costumes by State,Rankings for Halloween costumes by state in October 2017,Google News Lab,3,"Version 1,2017-11-02","journalism
united states
internet",CSV,3 KB,CC4,83 views,4 downloads,,0 topics,https://www.kaggle.com/GoogleNewsLab/frightgeist-2017-costumes-by-state,Rankings for Halloween costumes by state in October 2017
Safecast Radiation Measurements,80 million radiation readings from volunteers around the world,Safecast,3,"Version 2,2017-12-08|Version 1,2017-11-10",,CSV,3 GB,CC0,754 views,69 downloads,,0 topics,https://www.kaggle.com/safecast/safecast,"Context
Safecast is a volunteer driven non-profit organization whose goal is to create useful, accessible, and granular environmental data for public information and research. Begun in response to the nuclear disaster in Japan in March, 2011, Safecast collects radiation and other environmental data from all over the world. All Safecast data is published, free of charge, under a CC0 designation.
The official “SAFECAST data” published for others to use is collected by Safecast volunteers using professional quality devices. A combination of off the shelf commercial radiation monitors and devices are used in the collection process. Most devices are standardized on the same sensor, the LND7317 which is commonly referred to as the 2″ pancake. This is a highly sensitive piece of equipment that is used by nuclear professionals all over the world.
Content
""Presently we assume the radiation comes from cesium-137 which is the most prevalent isotope still around from nuclear weapons testing and from the accidents at Chernobyl and Fukushima. Based on calibration tests of multiple device designs using the same detector, we've settled on a conversion factor of 334. That is, 334CPM from a bGeigie equates to 1uSv/h.
It would be more accurate to have conversion factors tuned to each locale based on the spectrum of radiation present, but we don't have much of that data and in practice the error is estimated to be relatively small. ""
- Joe Moross from Safecast
Captured Time
Time data was captured
Latitude
Longitude
Value
Actual data, in whatever units are in the ""Unit"" field. About 130K out of over 80 million measurements are not in CPM (from a 2-inch pancake Geiger tube)
Unit
Describes the raw form of the data. The vast majority of the measurements in the database are from Safecast-designed bGeigies, which record radiation levels in CPM (counts per minute)
Location Name
Device ID
MD5Sum
Height
Height from the ground in meters
Surface
Denotes data recorded very close to a surface such as pavement. This is typically done at 1cm height so should be regarded as contamination density and displayed or analyzed in units such as Becquerels, not as dose data.
Radiation
Uploaded Time
Loader ID
Acknowledgements
Thank you to Safecast for collecting and sharing this dataset. The source files were downloaded from Safecast.org and have not been modified.
Inspiration
Safecast shared this datset for the public to have an un-biased source of radiation measurements. Use this dataset to see where Safecast volunteers have recorded data."
Australian Domestic Airline Traffic,Monthly flights between cities,alphajuliet,3,"Version 2,2017-11-19|Version 1,2016-08-20",,CSV,1 MB,Other,"1,505 views",137 downloads,6 kernels,0 topics,https://www.kaggle.com/alphajuliet/au-dom-traffic,"This dataset was downloaded from data.gov.au here in June 2016. It contains monthly aggregated data of flights between Australian cities. The CSV file has the following fields: City1, City2, Month, Passenger_Trips, Aircraft_Trips, Passenger_Load_Factor, Distance_GC_(km), RPKs, ASKs, Seats, Year, Month_num. See the link for full information.
It it released under Creative Commons Attribution 3.0 Australia."
Global Peace Index 2016,Each contry score in the Global Peace Index as of 2016,Tomasz Bartczak,3,"Version 1,2016-11-14",,CSV,9 KB,CC4,"2,343 views",113 downloads,3 kernels,0 topics,https://www.kaggle.com/kretes/gpi2008-2016,"This is a dataset scraped from global peace index wikipedia page and presents ""relative position of nations' and regions' peacefulness"".
It was created using this script: https://gist.github.com/kretes/2c191dddd78f8b5dcf20f3841eda24db
This can be used in various geopolitical analysis"
Selfies with Sunglasses,Contains 5536 images of selfies with sunglasses.,ShreyasSomashekara,3,"Version 2,2016-08-24|Version 1,2016-08-24",,Other,3 KB,CC0,"2,288 views",135 downloads,,,https://www.kaggle.com/shreyas0906/selfies-with-sunglasses,"The dataset is a collection of images of selfies with sunglasses and images with sunglasses with the scope for improving the accuracy of face recognition. The dataset contains 2768 unannotated images and a total of 5536 images. This dataset was created with the motive of removing reflection on sunglasses in a selfie and reconstruct the closest possible shade of the sunglasses. The images in the dataset are of varying dimensions and have to be resized for your research. Currently, the ownership of the dataset is shared between me and Dr. Boqing Gong, who is an Assistant Professor at the Centre for Research in Computer Vision at the University of Central Florida. More images will be added to the dataset in the future and I request fellow kagglers to send images to the author to increase the size of the dataset which will help the machine learning and computer vision engineers community.
About the dataset: The dataset is a collection of images of selfies with sunglasses and images with sunglasses to improve the accuracy of face recognition. The dataset contains 2768 unannotated images and a total of 5536 images. The repository also has the code for annotation which is ready to use.
Before executing the program: 1. make sure you have entered the correct address of the folder which contains the images and the saved_images folder.
Once everything works, a window should pop-up showing the image To annotate the image, 1. left-click and drag across the image where you wish to draw the box. 2. To set a dot on the image, double right-click on the image. 3. The coordinates of the red-dot will be stored in a .txt file and will also be displayed in the console. 4. To undo or redraw the rectangle press 'r' key. 5. To show the cropped image, press 'n' key. 6. To show the next image, press 'n' key again. 7. The output file saves the coordinates of the red-dot as a python dictionary so that it is easy to read and manipulate.
Since Kaggle won't support uploading more than 1000 images in a compressed format, you can get the complete repository of the images from https://github.com/shreyas0906/Dataset If you have any question, please email me at shreyas0906@gmail.com"
2012 Election- Obama vs Romney,"Data from the huffington post, concerning the 2012 general election",Abdelkader Laraichi,3,"Version 2,2016-12-09|Version 1,2016-12-06",,CSV,151 MB,Other,"1,660 views",90 downloads,3 kernels,,https://www.kaggle.com/kadser/2012-election-obama-vs-romney,This data set shows different types of polls and methods concerning the 2012 election.
TV Sales Forecasting,"Daily model wise TV sales data, having lunar calendar effect.",Ariful Ambia,3,"Version 1,2016-09-14",,CSV,806 KB,ODbL,"2,938 views",291 downloads,9 kernels,,https://www.kaggle.com/nomanvb/tv-sales-forecast,"Given data contains Jan-2014 to Aug-2016 daily TV sales quantity. There are total 124 Models. This data is collected from one of the leading brand of Bangladesh. Annually there are two big festivals (EID) which follows Islamic lunar calendar. provided data are in csv format. it contains only three columns.
Date: Date of Sale
Model: TV Model (not Original)
Count: Sale Quantity
I like to throw a challenge to Kagglers to device a effective sales forecasting model."
Anna University results May-June 2016,Contains the results of 97197 students in Tamil Nadu,GowTham,3,"Version 2,2016-09-15|Version 1,2016-09-14",,{}JSON,29 MB,CC0,"1,222 views",79 downloads,3 kernels,0 topics,https://www.kaggle.com/gowtham121/annauniversityresultsmay-june16,"The dataset contains the results of all the students studying under Anna University, Chennnai, Tamil Nadu. It contains the name, registernumber and department.
The dataset was scraped from the Anna University result site, the script to download it is here.
The data was collected to analyse the performance of colleges, subject performances.
Structure
The csv file has two columns, one is res, a dictionary containing the results and studentdetail, a list containing the student details."
Buenos Aires public WiFi access points,"List of free WiFi APs located in parks, libraries, etc",OctavioG,3,"Version 2,2016-10-08|Version 1,2016-10-07",,Other,1 MB,Other,"1,164 views",52 downloads,,0 topics,https://www.kaggle.com/octaviog/buenos-aires-public-wifi-access-points,"Context and acknowledgements: This dataset contains information about location of public WiFi APs in Buenos Aires city. It is an official dataset produced by Ministerio de Modernización - Unidad de Sistemas de Información Geográfica. The source of the data is Agencia de Sistemas de Información - Gerencia Operativa de Redes. The last update is from April 2015 despite being tagged as monthly updated.
Important: It is csv file type but it is separated by semi-colons "";""
Fields: - WKT: Geolocation coordinates. Type: text. - ID: Unique identifier. Type: numeric - NOMBRE: Name of the AP. Type: text. - TIPO: Category of location (e.g. museum, subway, etc). Type: text. - ETAPA: Stage of inauguration of the AP. Type: text - ETAPA_OBSE: Stage of secondary observations. Type: date. - ESTADO: Availability. Type: text. - CALLE: Main street. Type: text. - ALTURA: Number on main street. Type: numeric - CALLE2: Intersection street. Type: text. - DIRECCION: Address in one line. Type: text. - OBSERVACIO: Observations. Type: text. - OBSERVA_01: Private observations. Type: text. - PUBLICABLE: Publishable. Type: numeric. - VERIFICADO : Verified. Type: numeric. - DISTRITO: District of location. Type: text."
Montreal Street Parking,Street Side Coordinates and Parking Signals in Montreal,Mahdy Nabaee,3,"Version 1,2016-10-31",,Other,116 MB,CC0,"1,602 views",52 downloads,5 kernels,0 topics,https://www.kaggle.com/mnabaee/mtlstreetparking,"This dataset contains information about the streets and street parking signals in the City of Montreal. They are obtained from the City's portal at http://donnees.ville.montreal.qc.ca/group/transport.
In this database, you will see three different files, relevant to our problem of interest. gbdouble.json: This is a geo-json file which contains the geographical coordinates for each side of the streets in the City. Each street side is described by a number of line segments (the coordinates of a number of points). To open and read the coordinates of the street segments, you can have a look at https://www.kaggle.com/mnabaee/d/mnabaee/mtlstreetparking/load-street-side-coordinates/
signalisation.csv: This csv file includes all of the parking signals in the City. Each row will have the latitude, longitude, a text description as well as a number of other fields. One may need to parse/digest the text information field to understand what the signal means.
signalisation.pdf: This pdf contains all of the picture of each signal and their corresponding signal code (which can be matched with the codes provided in signalisation.csv).
The main goal in analyzing this dataset is to create an interactive map of the street parking spots at a given time (interval). The available parking spots should be found by analyzing the signals in a street and digesting their meaning.
A preliminary work on the data set is done on the data where you can see at GitHub."
Text for different industries,A industry list with 10 sample description text for most of them.,Barton.news,3,"Version 1,2016-08-25",,Other,518 KB,Other,"1,507 views",112 downloads,2 kernels,,https://www.kaggle.com/bartondotnews/text-for-different-industries,"The material is a industry list. And we provide 10 sample description text for most of them. The text is about some business for sale or investment projects. We need some one to use machine learning and natural language to build a classification script. But we hope the script can classify some other kind of text, such as news, summary, etc. So NLP might be as import as the machine learning.
We know the amount is far from enough to do some machine learning with good results. That's why we need some help.
For some of the industries, we didn't find enough text (red-color area), Just do it with less sample. And we will try fill them later.
Some description will appear in multiple industry, Such as a plastic company, their products are for automotive industry. So they will be both ""Plastic"" and ""Automotive general parts""
We need some one to train the machine with the material, and result we need is that after we input a ""description"" and it can tell us which industry(industries) it is.
And you might needs to deal with the following situation: ""A furniture manufacturer, they are selling themselves with the real estate and they have a rental vehicle."" Their industry should be only ""Household Light Industry"" instead of ""Household Light Industry, Real Estate, Automobile service and leasing""
We have more text material, but they are not classified. If you think they are also helpful. Just let us know."
Crashes 2014,Dataset gives details of crashes of 2014,Aditi,3,"Version 1,2016-11-07",,Other,78 MB,Other,"1,052 views",89 downloads,,,https://www.kaggle.com/leleadit/crashes-2014,"Crashes 2014 dataset describes whether conditions, number of vehicles included in the crash, number of fatalities etc. Dataset has 80 columns and 292020 rows. Currently dataset is raw. I would like to work on it if you accept."
Donald Trump Tweets,Having Fun searching through tweets,AustinSonger,3,"Version 1,2016-12-21",,Other,5 MB,CC0,"2,996 views",257 downloads,2 kernels,,https://www.kaggle.com/austinvernsonger/donaldtrumptweets,Donald Trump Tweets
NYC Open Data Metadata,Data about the data on the New York City Open Data Portal,Aleksey Bilogur,3,"Version 1,2016-11-24",,{}JSON,5 MB,CC4,"1,664 views",44 downloads,,,https://www.kaggle.com/residentmario/nyc-open-data-metadata,"One of the most compelling trends in technology today is the open data and open governance movement. It's not without reason that no less than Tim Berners-Lee himself, the creator of the worldwide web and one of the most preeminent scholars of the Internet, is doing his latest work in getting more government data on the web: in an interview with The New York Times a few years ago he spoke to how even records as mundane as traffic statistics or weather data could drive tinkerers to ""make government run better"".
New York City has been at the forefront of this movement: mayor Bloomberg formalized a citywide analytics team as the Mayor's Office for Data Analytics in 2013, and the effort has continued under Mayor De Blasio, with the city cementing its first Open Data Plan in July 2015. The resultant NYC Open Data Portal is populated with over 1500 datasets. It was, and is, the largest citywide open data portal in the world.
Nevertheless, a good open data platform is more than a count; it's a function also of all of the maintenance and structure that goes into it. What's a ""dataset"", who's publishing them, and how well-maintained are they?
This dataset contains the publicly available metadata about the datasets in the NYC Open Data portal, provided in a JSON format.
For an initial exploration of its contents see this blog post."
HSI-Futures,2014 Hang Seng Futures Index 1min Data,CCCHEUNG,3,"Version 1,2016-11-15",,CSV,4 MB,CC4,"1,663 views",44 downloads,2 kernels,,https://www.kaggle.com/cccheung/hsifutures,"This is a historical data of HangSeng Futures Index based in Hong Kong.
For non traders, the data is a time-series (sequential flow of numbers) describing the HangSeng Futures Index of HongKong. Every minute one line of data is created.
Each line has : open : first price at start of that minute high : highest price during that minute low : lowest price during that minute close : last price for that minute time frames volume : total number of units traded
This is can be called 'raw data' on a 1 minute time frame."
Banco Imobiliário,All relevant data from one of the most popular brazilian games,Kande Bonfim,3,"Version 1,2017-01-03",,CSV,4 KB,Other,483 views,25 downloads,,0 topics,https://www.kaggle.com/kandebonfim/banco-imobilirio,"Context
I used to play Banco Imobiliário a lot in my childhood and I just realized how interesting this game can be for statisticians. My main idea was to analyze the game data to try some reverse engineering and discover what is happening inside the game mathematically. Also, I was curious to see if there's any correlation between Banco Imobiliário and Monopoly's logic.
Content
I started listing all the properties cards in a spreadsheet (which is available in the properties.csv). Then, I created another file for the transportation companies (properties.csv) and finally the ""Sorte ou Revés"" cards (sorte-ou-reves.csv) (huge work to type the cards' descriptions).
Acknowledgements
I owe special thanks to my wife that dictate all the cards stats to me even falling asleep sometimes.
Inspiration
What's the most efficient way to give a property a score including all its stats like rent value, rent with multiple houses and hotel built, price per building, sell value and its color-neighbor properties? Also, now that you can see the game's big picture, what's the most effective strategy to beat everyone in this game?"
Dataset for collaborative filters,"It's a medium sized song list, ideal to practice collaborative filter algorithms",Jesus Santander,3,"Version 1,2016-12-23",,CSV,30 MB,CC4,"2,426 views",109 downloads,,,https://www.kaggle.com/rymnikski/dataset-for-collaborative-filters,"Medium sized song list
This dataset consists of a list of songs arranged as follow:
ID_user1,ID_song,rating
ID_user1,ID_song,rating
...
ID_user2,ID_song,rating
ID_user2,ID_song,rating
...
ID_usern,ID_song,rating
ID_usern,ID_song,rating
The main idea for this dataset is to implement recommendation algorithms based on collaborative filters. In addition to grouping data, reduce and compress lists. It is distributed under the CC 4.0 license. It's educational purpose.
In fact, as the music is coded in an ID, the dataset could be for anything else like, movies, places, etc. Use it for training your collaborative filters. (The data truly represent songs)"
MLB dataset 1870s-2016,"Including win rates, manager performances, etc.",timsyang,3,"Version 1,2016-12-19",,CSV,465 KB,Other,"3,413 views",282 downloads,6 kernels,,https://www.kaggle.com/timschutzyang/dataset1,"Context
This data set concerns data in team histories of MLB.
Content
This data set is 2594*23 in dimensions. It mainly keeps track of the existing 30 teams, with respect of winning records, managers and players chronically from 1870s to 2016.
Acknowledgements
We hereby appreciate professor Miles Chen at UCLA, for introducing us getting this dataset using nodes extraction from the MLB website ""baseball-reference.com"". This dataset is for solving Homework 3.
Inspiration
This dataset is for an analysis on coaching records of managers, and for figuring out the reason why managers switches jobs. Moreover, we are supposed to find out the big picture of MLB over the past one century and forty years. The dataset is expected to receive feedbacks on details of manager ratings and player ratings."
Medical Data,,KarimNahas,3,"Version 1,2017-11-05",,CSV,203 KB,Other,911 views,115 downloads,,,https://www.kaggle.com/karimnahas/medicaldata,This dataset does not have a description yet.
Handwritten Names,Images of handwritten first and last names for Optical Character Recognition,Crowdflower,3,"Version 1,2016-11-21",,CSV,9 MB,CC0,"2,837 views",155 downloads,2 kernels,,https://www.kaggle.com/crowdflower/handwritten-names,"NOTE: we're having some trouble uploading the actual images of the handwritten names. Stay tuned.
This dataset contains links to images of handwritten names along with human contributors’ transcription of these written names. Over 125,000 examples of first or last names. Most names are French, making this dataset of particular interest for work on dealing with accent marks in handwritten character recognition.
Acknowledgments
Data was provided by the Data For Everyone Library on Crowdflower.
Our Data for Everyone library is a collection of our favorite open data jobs that have come through our platform. They're available free of charge for the community, forever.
The Data
A file handwritten_names.csv that contains the following fields:
_unit_id: a unique id for the image
image_url: the path to the image; begins with ""images/""
transcription: the (typed) name
first_or_last: whether it's a first name or a last name
A folder images that contains each of the image files."
ERA-Interim 2m temperature anomalies,Monthly adjusted ERA-Interim 2m temperature anomalies (K) relative to 1981-2010.,European Centre for Medium-Range Weather Forecasts,3,"Version 3,2016-11-14|Version 2,2016-11-14|Version 1,2016-11-14",,CSV,10 KB,Other,"1,889 views",36 downloads,5 kernels,,https://www.kaggle.com/ECMWF/erainterim-2m-temperature-anomalies,"Monthly adjusted ERA-Interim 2m temperature anomalies relative to 1981-2010.
See https://climate.copernicus.eu/resources/data-analysis/average-surface-air-temperature-analysis/monthly-maps/october-2016 for more details of the data.
See also: Copernicus Climate Change Service
ERA-Interim data license"
Open Pubs,"UK pub names, addresses, positions and local authority as open data",GetTheData,3,"Version 1,2016-12-10",,CSV,7 MB,Other,"1,135 views",29 downloads,,0 topics,https://www.kaggle.com/getthedata/open-pubs,"Context
UK pubs as open data, including pub name, address, position and local authority.
Content
fsa_id
name
address
postcode
easting
northing
latitude
longitude
local_authority
fsa_id is the FSA's ID for the premises and allows you to link the pub to their Food Hygiene Ratings.
Acknowledgements
For latest version and documentation see the Open Pubs homepage.
Derived from the Food Standard Agency Food Hygiene Ratings database and licensed under their terms and conditions.
Local Authority field derived from the ONS Postcode Directory licensed under the OGL.
Contains OS data © Crown copyright and database right 2016
Contains Royal Mail data © Royal Mail copyright and database right 2016
Contains National Statistics data © Crown copyright and database right 2016
Published and maintained by GetTheData.
Inspiration
Create mashups with other geocoded open datasets: Pubs/Bus Stop Mashup
Optimise pubcrawls: World's longest pub crawl: Maths team plots route between 25,000 UK boozers"
Street Network Segmentation,A copy of the EPFL CVLabs Dataset of Aerial Images and a labeling of streets,Kevin Mader,3,"Version 1,2017-04-05",,Other,21 MB,Other,522 views,22 downloads,3 kernels,0 topics,https://www.kaggle.com/kmader/street-network-segmentation,"Overview
The data is taken from the EPFL CVLab's Library of Tree-Reconstruction Examples (http://cvlab.epfl.ch/data/delin)
The data are as images and some kind of SWC file which is basically a text file with columns for index, position and width. The kernels show how it can be loaded.
Tasks
The initial task is to segment the streets and their connectivity from the images. Ideally creating a list of points in a similar order to the SWC provided and with a minimal distance between the two center positions.
Secondary tasks include
Automatically Identifying Intersections (useful for self-driving cars and robots that need to navigate streets)
Representing the map as a graph with nodes and edges"
The files on your computer,Crab: A command line tool that scans file data into a SQLite database,cogs,3,"Version 3,2017-01-15|Version 2,2017-01-15|Version 1,2017-01-15",,SQLite,103 MB,Other,"1,096 views",32 downloads,7 kernels,0 topics,https://www.kaggle.com/cogitoe/crab,"Dataset: The files on your computer.
Crab is a command line tool for Mac and Windows that scans file data into a SQLite database, so you can run SQL queries over it.
e.g. (Win)       C:> crab C:\some\path\MyProject
or  (Mac)        $ crab /some/path/MyProject
You get a CRAB> prompt where you can enter SQL queries on the data, e.g. Count files by extension
SELECT extension, count(*) 
FROM files 
GROUP BY extension;
e.g. List the 5 biggest directories
SELECT parentpath, sum(bytes)/1e9 as GB 
FROM files 
GROUP BY parentpath 
ORDER BY sum(bytes) DESC LIMIT 5;
Crab provides a virtual table, fileslines, which exposes file contents to SQL
e.g. Count TODO and FIXME entries in any .c files, recursively
SELECT fullpath, count(*) FROM fileslines 
WHERE parentpath like '/Users/GN/HL3/%' and extension = '.c'
    and (data like '%TODO%' or data like '%FIXME%')
GROUP BY fullpath;
As well there are functions to run programs or shell commands on any subset of files, or lines within files e.g. (Mac) unzip all the .zip files, recursively
SELECT exec('unzip', '-n', fullpath, '-d', '/Users/johnsmith/Target Dir/') 
FROM files 
WHERE parentpath like '/Users/johnsmith/Source Dir/%'  and extension = '.zip';
(Here -n tells unzip not to overwrite anything, and -d specifies target directory)
There is also a function to write query output to file, e.g. (Win) Sort the lines of all the .txt files in a directory and write them to a new file
SELECT writeln('C:\Users\SJohnson\dictionary2.txt', data) 
FROM fileslines 
WHERE parentpath = 'C:\Users\SJohnson\' and extension = '.txt'
ORDER BY data;
In place of the interactive prompt you can run queries in batch mode. E.g. Here is a one-liner that returns the full path all the files in the current directory
C:>  crab -batch -maxdepth 1 . ""SELECT fullpath FROM files""
Crab SQL can also be used in Windows batch files, or Bash scripts, e.g. for ETL processing.
Crab is free for personal use, $5/mo commercial
See more details here (mac): http://etia.co.uk/ or here (win): http://etia.co.uk/win/about/
An example SQLite database (Mac data) has been uploaded for you to play with. It includes an example files table for the directory tree you get when downloading the Project Gutenberg corpus, which contains 95k directories and 123k files.
To scan your own files, and get access to the virtual tables and support functions you have to use the Crab SQLite shell, available for download from this page (Mac): http://etia.co.uk/download/ or this page (Win): http://etia.co.uk/win/download/
Content
FILES TABLE
The FILES table contains details of every item scanned, file or directory. All columns are indexed except 'mode'
COLUMNS
  fileid (int) primary key  -- files table row number, a unique id for each item
  name (text)               -- item name e.g. 'Hei.ttf'
  bytes (int)               -- item size in bytes e.g. 7502752
  depth (int)               -- how far scan recursed to find the item, starts at 0
  accessed (text)           -- datetime item was accessed
  modified (text)           -- datetime item was modified
  basename (text)           -- item name without path or extension, e.g. 'Hei'
  extension (text)          -- item extension including the dot, e.g. '.ttf'
  type (text)               -- item type, 'f' for file or 'd' for directory
  mode (text)               -- further type info and permissions, e.g. 'drwxr-xr-x'
  parentpath (text)         -- absolute path of directory containing the item, e.g. '/Library/Fonts/'
  fullpath (text) unique    -- parentpath of the item concatenated with its name, e.g. '/Library/Fonts/Hei.ttf'

PATHS
1) parentpath and fullpath don't support abbreviations such as ~ . or ..  They're just strings.
2) Directory paths all have a '/' on the end.
FILESLINES TABLE
The FILESLINES table is for querying data content of files. It has line number and data columns, with one row for each line of data in each file scanned by Crab.
This table isn't available in the example dataset, because it's a virtual table and doesn't physically contain data.
COLUMNS
  linenumber (int)  -- line number within file, restarts count from 1 at the first line of each file
  data (text)       -- data content of the files, one entry for each line
FILESLINES also duplicates the columns of the FILES table: fileid, name, bytes, depth, accessed, modified, basename, extension, type, mode, parentpath, and fullpath. This way you can restrict which files are searched without having to join tables.
Example Gutenberg data
An example SQLite database (Mac data), database.sqlite, has been uploaded for you to play with. It includes an example files table for the directory tree you get when downloading the Project Gutenberg corpus, which contains 95k directories and 123k files.
You can open it with any SQLite shell, or query it with any SQLite query tools, but the virtual tables such as fileslines and support functions such as EXEC() and WRITELN() only work from the Crab shell that you have to download from etia.co.uk.
Uses
Reporting and analysis of filesystem contents
Finding files and directories
Filesystem operations such moving, copying, deleting, unzipping files
ETL processing"
Pennsylvania Safe Schools Report,Self reporting statistics from Pennyslvania schools,Mike Chirico,3,"Version 2,2017-12-06|Version 1,2017-03-27","crime
education",CSV,7 MB,CC0,"1,758 views",136 downloads,4 kernels,0 topics,https://www.kaggle.com/mchirico/pennsylvania-safe-schools-report,"Introduction
Each year schools in Pennsylvania are required to report weapons violations, substance abuse, cyber harassment, and other crimes committed during school, at school events, or on a bus (or waiting at a bus stop) to and from school.
The raw data can be found at www.safeschools.state.pa.us
Important: (LEA Types)
The rows in this Dataset include several ""LEA Types"" (Legal Entity Types): ""School"", ""School District"", ""County""... etc. Please note that several Schools may fall under a single ""School District"", and there maybe several ""School Districts"" in a single county. Hence, if you include both ""LEA Types"", the counts could be off.
Key Pennsylvania Safe Schools Legislation
The Safe Schools Act of 1995 (Act 26) was amended in 1997 (Act 30) to mandate annual reporting of all incidents of violence, weapons, alcohol, drugs and tobacco possession to the Department of Education. Local education agencies also are required to develop a Memorandum of Understanding with local law enforcement agencies and provide for other procedural safeguards to enhance school safety. Another amendment to Act 26 (Act 36 of 1999) empowers schools to acquire the tools and resources needed to develop and enhance safe learning environments.
How this data was collected for Kaggle.
See the following gist."
School Exam,"25 questions, 23 students, but what are the right answers?",David,3,"Version 1,2017-03-17",,CSV,2 KB,Other,614 views,44 downloads,,0 topics,https://www.kaggle.com/davidbijl/school-exam,"Context
mlagunas asked a question here: ""what to do when data has less rows than columns?"". The real question is: ""Can we accurately predict the right answers to all the questions, based on the answers given and score received for only a few students"". (hint: no one student answered all questions correctly).
Content
23 students took the same test, answering 25 multiple choice questions. Each question has 5 possible answers (A-E). The last column shows the score the students received.
Acknowledgements
Data originated from mlagunas.
Inspiration
So, can you find out what the right answers are for each question?"
Administrative divisions of Moscow,"Shapefile of the 12 administrative okrugs, subdivided into 146 raions",Jordan Tremoureux,3,"Version 1,2017-05-03","russia
geography
politics",Other,682 KB,Other,"2,174 views",572 downloads,8 kernels,0 topics,https://www.kaggle.com/jtremoureux/administrative-divisions-of-moscow,"Context
The federal city of Moscow, Russia is divided into twelve administrative okrugs, which are in turn subdivided into districts (raions). (source Wikipedia)
Content
OKATO: Russian Classification on Objects of Administrative Division.
OKTMO: Russian Classification on Territories of Municipal Division.
RAION: Raion's name.
OKRUGS: Okrugs' name.
Acknowledgements
The shapefile data has been found here: http://gis-lab.info/qa/moscow-atd.html.
But I have translated the okrugs and raions names in English.
Inspiration
I hope you can use this shapefile to make interesting visualisations!
You can use OKATO and/or OKTMO codes to join your data to this shapefile."
r programming code,coding in actuarial science,dominic,3,"Version 1,2017-01-12",,Other,23 KB,CC0,735 views,43 downloads,,0 topics,https://www.kaggle.com/dominicondigo/r-programming-code,"Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
Obama Visitor Logs,About 5.9 million visitor logs released by the Obama White House,Jay Ravaliya,3,"Version 1,2017-04-15",politics,CSV,1 GB,CC0,"1,532 views",158 downloads,2 kernels,0 topics,https://www.kaggle.com/jayrav13/obama-visitor-logs,"Context
This dataset was released by the Obama White House consisting of about 5.9 million Visitor Logs. The more recent administration does not plan on releasing this dataset, so I thought it would be nice to move the Obama dataset to Kaggle to have this platform serve as an alternate home for this data.
Challenges
Note that the total dataset (5.9 million rows) is a total of 1.1 GB, so I split it into 6 files of 1 million rows each.
More Info
Source: https://obamawhitehouse.archives.gov/briefing-room/disclosures/visitor-records"
Sample of Car Data,Including multiple variables of car,EleanorXu,3,"Version 1,2016-12-25",,CSV,22 KB,ODbL,"2,282 views",237 downloads,14 kernels,0 topics,https://www.kaggle.com/jingbinxu/sample-of-car-data,"Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
Formula 1 points data. 2000-2016,All points according to new points system,viditjain,3,"Version 1,2017-03-30",,CSV,26 KB,ODbL,486 views,128 downloads,,0 topics,https://www.kaggle.com/viditj/formula1new,"Points scored by each player in each Formula 1 race for years 2000 to 2016 according to new points system. More data will be uploaded soon.
Filename Format: 'Year'_points_new.csv
Data obtained from Ergast API link: [Click Here][1]http://ergast.com/mrd/"
Speed Dating Experiment,What attributes influence the selection of a romantic partner?,Anna Montoya,3,"Version 1,2017-01-24",,Other,158 KB,Other,"1,382 views",187 downloads,,,https://www.kaggle.com/datafordays/speed-dating-experiment,"Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
Ultimate Beastmaster: First Season,All the competitors and performances,Kande Bonfim,3,"Version 4,2017-03-06|Version 3,2017-03-06|Version 2,2017-03-06|Version 1,2017-03-06","popular culture
music",CSV,29 KB,ODbL,"2,419 views",63 downloads,2 kernels,0 topics,https://www.kaggle.com/kandebonfim/ultimate-beastmaster,"Context
I like to compensate my frustration with sports analyzing sports data so I put together every data I could from the first season of this Netflix series and here we are."
ISP Contributions to Congress,Congressmen that voted to allow ISPs to sell consumer data and their legal bribe,mrpantherson,3,"Version 1,2017-03-30",,CSV,12 KB,CC0,397 views,17 downloads,3 kernels,0 topics,https://www.kaggle.com/mrpantherson/isp-contributions-to-congress,"Context
Simple analysis of how money affects the decisions of policy makers.
Acknowledgements
The data was taken from the website https://www.followthemoney.org/
Inspiration
Mainly I was frustrated with the lack of consumer protection, so I wanted to see how much money ISPs donated to congress."
Average Fuel Consumption,The weighted average specific fuel consumption of new private cars,Anton Bobanev,3,"Version 1,2017-05-20",,CSV,4 KB,CC0,605 views,48 downloads,,0 topics,https://www.kaggle.com/antfarol/average-fuel-consumption,"Context
I gethered this data while providing some investigation about used car market. This dataset helps to transfer from annual mileage and engine volume/type to fuel consumption and cost.
Content
Represents the weighted average specific fuel consumption of new private cars first registered in the years 2000 to 2011 in Ireland.
Acknowledgements
The core data was collected by Sustainable Energy Authority of Ireland and freely available on its web page (http://www.seai.ie/)
Inspiration
Though there are plenty of notes and news about car fuel consumption it is not so easy to find clear and simple data to use in fuel spendings estimations."
PSL data,Ball by Ball commentary and Scorecards,AsadMahmood,3,"Version 2,2017-05-24|Version 1,2017-05-20",,Other,2 MB,ODbL,484 views,36 downloads,,,https://www.kaggle.com/asad1m9a9h6mood/psl-data,"Context
I was wandering the Kaggle datasets and then i came across the dataset of IPL. So i just thought that why not have a dataset for PSL as well.
Content
It has 3 basic csv files for every match in both the PSL edition 1 and PSL edition 2. First file is the file that has the ball by ball commentary of Inning1 Second file is the file that has the ball by ball commentary of Inning2 and then finally the third file contains the Scorecard of the whole match.
Acknowledgements
I would like to Acknowledge ESPNcricinfo for having such a detailed data for every match that has been played on earth for quite a while.
Inspiration
I want people to find out one specific thing from the data . That is to use NLP and determine the reasons behind batsman getting out and the remedy that what might have been done differently to avoid the wicket."
Pre-processed Twitter tweets,"The dataset is divided into positive, negative and neutral pre-processed tweets",Shashank Yadav,3,"Version 2,2017-05-18|Version 1,2017-05-14",,CSV,188 KB,CC0,"1,208 views",213 downloads,,,https://www.kaggle.com/shashank1558/preprocessed-twitter-tweets,"Context
When i was working on Twitter sentiment analysis, i found it very difficult to look for a dataset. Many that i found were too complicated to process or could not be used. This inspired me to create my own dataset and pre-process it .
I have provided pre-processed tweets divided into positive, negative and neutral categories . The positive and negative tweets were retrieved using emoticons. Neutral tweets are the timeline tweets of The Telegraph.
Future works
I would be expanding the dataset in future."
IMDB Data,Data for 5000 movies,Suchit Gupta,3,"Version 1,2017-05-14",,CSV,1 MB,Other,"1,853 views",305 downloads,6 kernels,2 topics,https://www.kaggle.com/suchitgupta60/imdb-data,IMDB dataset for 5000 movies
Three years of my search history,This is the most random and useless dataset i've ever uploaded. Oh well.,LiamLarsen,3,"Version 1,2017-04-16",,CSV,596 KB,Other,464 views,17 downloads,,0 topics,https://www.kaggle.com/kingburrito666/three-years-of-my-search-history,"Context
This is just my archived search history . This actually took me a solid 3 hours to put together because I had no idea how to deal with JSON files (and I still dont)
Content
JSON timestamp
Text
the most recent is at index 0 then 1, 2, and so on"
Basic Computer Data,A dataset of basic computer stats,LiamLarsen,3,"Version 1,2017-05-06","statistics
computer science",CSV,290 KB,Other,"2,696 views",266 downloads,7 kernels,0 topics,https://www.kaggle.com/kingburrito666/basic-computer-data-set,"For what?
This dataset is for basic data analysis. Student Statisticians or Data-Analysists (like myself) could use this as a basic learning point. Even ML students could predict future prices and speeds of computers.
Unfortunately, this dataset doesn't come with dates. (which are a pain to work with anyway), But the computers are in order from earliest to latest.
I will be uploading another version with this and a more detailed CSV that has the computer name, date, and other stats. This dataset is free to use for any purpose.
This is simply to gain understanding in analyzing data. At least for me.
Content
price, speed, hd, ram, screen, cd, multi, premium, ads, trend
Something glorious is coming
The largest computer CSV? Maybe? Maybe im scrapping it right now? Who knows? ;)"
Academic Research from Indian Universities,Collection of 1387 SCOPUS journal papers from Indian authors and institutions,Neel Shah,3,"Version 1,2017-03-15","research
education",CSV,13 MB,CC0,"3,499 views",211 downloads,2 kernels,0 topics,https://www.kaggle.com/neelshah18/scopusjournal,"Context
The main aim of this data analysis is to identify the ongoing research in Indian Universities and Indian Industry. It gives a basic answer about research source and trend with top authors and publication. It also shows the participation of Industry and Universities in research.
Content
It is a collection of 1387 paper dataset from SCOPUS journal between 2001 to 2016 published by Indian Universities or India based research center of any industry.
If a paper has multiple authors from Industry and Indian University, we count that paper as university paper.
If a paper published by industry and non-Indian university, we count that paper as Industry paper.
During cleaning of data, we consider the different name of Institute as single Institute. For example IIT-Madras, Indian Institute of Technology and IIT-M count as the same institute.
We also consider the different name of same industry as single industry, For example, TCS and tata consultancy service count as the same industry.
Acknowledgements
This dataset is available as open source on Scopus journal. We took only Indian researcher's detail from it.
Detail of analysis and Blog : scopus journal blog"
Random Shopping cart,Contains a list of products separated in carts,kso.,3,"Version 1,2017-05-29",,CSV,325 KB,Other,992 views,148 downloads,2 kernels,,https://www.kaggle.com/fanatiks/shopping-cart,Is a dataset that contains a list of items sorted into a set of shopping carts.
People Walking with No Occlusion,Useful for Object Tracking,Spencer Buja,3,"Version 2,2017-02-01|Version 1,2017-02-01",,Other,66 B,Other,"1,025 views",30 downloads,,2 topics,https://www.kaggle.com/csbuja/people-walking-with-no-occlusion,"Context
For an undergraduate project at the University of Michigan, my team collected this data to see if we could classify gender using images of people. Then to try to improve performance, we used machine learning and computer vision methods to track the person over time, automatically and used the additional tracking data to boost the performance of our convolutional neural network. It turned out that tracking improved our performance on our gender classifier.
Content
70 sequences of people walking with tight bounding boxes and gender labels.
CURRENT STATUS
Can't upload data due, since the site detects the contents of my .zip files as uncompressed."
"Results from Running Events in Porto, Portugal","Collection of results from running events in Porto, Portugal between 2011-2017",Pedro Lima,3,"Version 2,2017-06-18|Version 1,2017-06-18",running,CSV,37 MB,CC4,954 views,44 downloads,3 kernels,2 topics,https://www.kaggle.com/pvlima/results-from-running-events-in-porto,"Context
Collection of results from running events in Porto, Portugal.
Content
Each row corresponds to the race name and year, runner name, official time (clock time), net time, place, age class, sex and country. Data collected from the website runporto.com."
Spam filter,Identifying spam using emails,karthickveerakumar,3,"Version 1,2017-07-14",,CSV,9 MB,Other,929 views,148 downloads,2 kernels,0 topics,https://www.kaggle.com/karthickveerakumar/spam-filter,"Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
NYC Rejected Vanity Plates,Rejected vanity license plates in NYC from 2010 - 2014,Chris Crawford,3,"Version 1,2017-07-15","linguistics
automobiles",CSV,2 MB,Other,"1,114 views",69 downloads,,0 topics,https://www.kaggle.com/crawford/nyc-rejected-vanity-plates,"Context
This is a small dataset New York personalized license plate applications received from the New York DMV in response to a July 2014 Freedom of Information Law (FOIL) request. Covers applications from 10/1/2010 to 9/26/2014.
Content
accepted-plates.csv: CSV of plate applications that were accepted and issued, with order date and plate configuration.
rejected-plates.csv: CSV of plate applications that were rejected by the department, with order date and plate configuration.
red-guide.csv: A copy of the Red Guide, the list of ""inappropriate"" plate configurations that are automatically disallowed by the New York DMV as of July 2015. procedure.pdf: A document listing the DMV's plate review and cancellation procedures as of June 2014.
Acknowledgements
This dataset is from a FOIL request by Noah Veltman at Data News. Here are some notes about the process, but check the original source for more information. Thanks to Noah for letting us share this dataset with the Kaggle community!
https://github.com/datanews/license-plates
This data may contain explicit or offensive language.
Plate configurations in accepted-plates.csv may have since been revoked by the DMV.
Plate configurations in rejected-plates.csv were rejected by the department. It does not include plates that were reserved, banned by the Red Guide, or cancelled for administrative reasons.
Some plate configurations may exist in multiple applications.
A small number of rows may contain erroneous data because of Excel cell formatting in the original prepared files.
Although the DMV collects an explanation of the requested combination from each online applicant, that information is not preserved.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?https://github.com/datanews/license-plateshttps://github.com/datanews/license-plates"
American Presidency Project,A JSON file with all documents released by US presidents,Jay Ravaliya,3,"Version 1,2017-01-27",politics,{}JSON,328 MB,Other,"2,890 views",150 downloads,,0 topics,https://www.kaggle.com/jayrav13/american-presidency-project,"Context
I ran into the American Presidency Project and was inspired by the incredible amount of data that the founders of this project had accumulated. Further, I ran into a few key projects such as The Wordy Words of Hillary Clinton and Donald Trump and IBM Watson Compares Trump's Inauguration Speech to Obama's that used this data.
The site itself, however, simply has a PHP page that individually returns every one of the 120,000+ documents in an HTML format. My goal was to extract the almost 60,000 documents released by the offices of all of the presidents of the United States, starting with George Washington, in an effort to make this data available to anyone interested in diving into this data set for unique studies and experimentation.
Content
The data is normalized using two key properties of a document: President and Document Category. Document categories can include, but are not limited to: Oral, Written, etc.
Each document has a variety of properties:
category - This category field is a further detailed categorial assignment, such as Address, Memo, etc.
subcategory - Inaugural, etc.
document_date - Format: 1861-03-04 00:00:00
title - Title of the released document.
pid - This value, stored as an integer, can be used to access the original document at the following URL: http://www.presidency.ucsb.edu/ws/index.php?pid={}. where {} can be replaced with the value in this field.
content - This is the full text of the released document.
A markdown version of this JSON structure can be found on GitHub.
Acknowledgements
A HUGE thank you for the data and inspiration to the American Presidency Project."
First Quora Dataset Release: Question Pairs,Quora Duplicate or not,SambitSekhar,3,"Version 1,2017-02-14",,CSV,58 MB,ODbL,"3,307 views",148 downloads,13 kernels,0 topics,https://www.kaggle.com/sambit7/first-quora-dataset,"Today, we are excited to announce the first in what we plan to be a series of public dataset releases. Our dataset releases will be oriented around various problems of relevance to Quora and will give researchers in diverse areas such as machine learning, natural language processing, network science, etc. the opportunity to try their hand at some of the challenges that arise in building a scalable online knowledge-sharing platform. Our first dataset is related to the problem of identifying duplicate questions.
An important product principle for Quora is that there should be a single question page for each logically distinct question. As a simple example, the queries “What is the most populous state in the USA?” and “Which state in the United States has the most people?” should not exist separately on Quora because the intent behind both is identical. Having a canonical page for each logically distinct query makes knowledge-sharing more efficient in many ways: for example, knowledge seekers can access all the answers to a question in a single location, and writers can reach a larger readership than if that audience was divided amongst several pages.
To mitigate the inefficiencies of having duplicate question pages at scale, we need an automated way of detecting if pairs of question text actually correspond to semantically equivalent queries. This is a challenging problem in natural language processing and machine learning, and it is a problem for which we are always searching for a better solution.
The dataset that we are releasing today will give anyone the opportunity to train and test models of semantic equivalence, based on actual Quora data. We are eager to see how diverse approaches fare on this problem.
Our dataset consists of over 400,000 lines of potential question duplicate pairs. Each line contains IDs for each question in the pair, the full text for each question, and a binary value that indicates whether the line truly contains a duplicate pair. Here are a few sample lines of the dataset:
Here are a few important things to keep in mind about this dataset:
Our original sampling method returned an imbalanced dataset with many more true examples of duplicate pairs than non-duplicates. Therefore, we supplemented the dataset with negative examples. One source of negative examples were pairs of “related questions” which, although pertaining to similar topics, are not truly semantically equivalent. The distribution of questions in the dataset should not be taken to be representative of the distribution of questions asked on Quora. This is, in part, because of the combination of sampling procedures and also due to some sanitization measures that have been applied to the final dataset (e.g., removal of questions with extremely long question details).
links for download data: http://qim.ec.quoracdn.net/quora_duplicate_questions.tsv
source: https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs"
O'Reilly Strata London 2017 Talks and Ratings,Scraped results from the conference website,Felipe Hoffa,3,"Version 1,2017-07-01",,CSV,52 KB,Other,392 views,5 downloads,,0 topics,https://www.kaggle.com/fhoffa/oreilly-strata-london-2017-talks-and-ratings,"Content
Data scraped from the famous big data conference site:
https://conferences.oreilly.com/strata/strata-eu/public/schedule/grid/public/2017-05-24
Acknowledgements
Kiyoto Tamura did a similar analysis back in 2015:
https://blog.treasuredata.com/blog/2015/03/10/sponsored-talks-at-strata/
Inspiration
What topics do people prefer at the conference?
Do ratings change depending on time of day?"
Ski Resorts - Daily Snowfall,Daily Snowfall Records for various ski resorts from 2009 to 2017,Sean Marjason,3,"Version 1,2017-07-09",,CSV,66 KB,Other,465 views,53 downloads,2 kernels,,https://www.kaggle.com/mrmarjo/resort-daily-snowfall-20092017,"This data was stripped manually from OnTheSnow.com as part of work to understand how closely related snowfall is to daily temperature.
The set contains the following:
Date (DD-Mon-YY)
Daily Snowfall (24 hr New Snow)
Cumulative Seasonal Snowfall (Season Snowfall Total)
Base Depth for the Resort being measured (Base Depth)
Each resort measured has been loaded in its own isolated csv file.
NOTE: The accuracy and validity of the data contained has not been verified in any way."
Iris datasets,Iris datasets for annalysis,Chuck-Yin,3,"Version 1,2017-03-10",,CSV,5 KB,Other,"2,381 views",361 downloads,33 kernels,0 topics,https://www.kaggle.com/chuckyin/iris-datasets,"Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
Executive Orders,Since Ronald Reagan with dates and titles,BrandtCowan,3,"Version 3,2017-02-17|Version 2,2017-02-08|Version 1,2017-02-03",politics,CSV,194 KB,Other,"1,908 views",121 downloads,5 kernels,0 topics,https://www.kaggle.com/brandtcowan/executiveorders,"Context
With the events unfolding within the new administration, I was curious if any President had been this active this early. I set out in search of a data set for executive orders, but didn't have a whole lot of easy success. pulling from the American Presidency Project (which doesn't really make pulling mass amounts of data easy with my skill level), and adding ""memorandums"" from wikipedia, I compiled this CSV to analyze and share. If anyone has datasets including full text of executive orders, that would be fun to dive into!
Content
The data attached has the president's name, date signed, executive order id, and title.
Acknowledgements
Data pulled from The American Presidency Project : http://www.presidency.ucsb.edu/executive_orders.php?year=2017&Submit=DISPLAY
and wikipedia
Cabinet confirmations pulled from https://www.senate.gov/reference/resources/pdf/cabinettable.pdf and wikipedia
Inspiration
One last thing I'd like to have added to this data set: classification! Would be nice to analyze topics. Which president had more orders concering economy? Gov't? War? International relations?"
Straits Times index Data,This is a time series data to perform prediction and classification,PradeepKumar,3,"Version 1,2017-07-18",finance,CSV,143 KB,ODbL,503 views,44 downloads,3 kernels,,https://www.kaggle.com/contactprad/straits-times-index-data,"Context
This is an example dataset of Straits times index.
This is a time series data and could be used to explore the trends, index and other things.
Content
Straits index is a financial time series of a basket of top stocks in Singapore, the equivalent of the American DOW Jones or S&P 500.
You will explore and notice that volume is missing for a notable time period. What do you, should we discard volume in our model. Sholud we impute values?
Acknowledgements
This dataset is provided by the university to explore NNet
Inspiration
This is a practice data set if someone wants to explore and understand the time series based data which is usually non-linear. Some of the answers we could try to predict will be: 1. What is going to be tomorrow closing price 2. In general what is going to be trend tomorrow 3. Imagine a person in test data set placing bet 1 dollar for each row. Do you think he is going to make money if he follow the prediction of your model."
TOP 1000 City Betwen Distance Lookup,North American Cities Only,JohnHeyrich,3,"Version 1,2017-03-14",,CSV,31 MB,ODbL,340 views,20 downloads,,0 topics,https://www.kaggle.com/johnney12/namericancitytop1000distancelookup,The dataset contains the To-From name of between distance and the distance in imperial miles. There are 1000 cities it is referencing so it contains 1 Million lines in database. Thank You!
Movie lens,IMDB Movies Dataset for clustering,VedapragnaReddy,3,"Version 1,2017-07-14",,Other,231 KB,CC0,556 views,56 downloads,,0 topics,https://www.kaggle.com/vedapragnareddy/movie-lens,"Context
MovieLens data sets were collected by the GroupLens Research Project at the University of Minnesota.
This data set consists of: * 100,000 ratings (1-5) from 943 users on 1682 movies. * Each user has rated at least 20 movies. * Simple demographic info for the users (age, gender, occupation, zip)
The data was collected through the MovieLens web site (movielens.umn.edu) during the seven-month period from September 19th, 1997 through April 22nd, 1998. This data has been cleaned up - users who had less than 20 ratings or did not have complete demographic information were removed from this data set. Detailed descriptions of the data file can be found at the end of this file.
Neither the University of Minnesota nor any of the researchers involved can guarantee the correctness of the data, its suitability for any particular purpose, or the validity of results based on the use of the data set.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
Canadian Car Accidents 1994-2014,Car accidents in Canada from 1999-2014 with various features,steal,3,"Version 1,2017-07-10",,Other,353 MB,ODbL,"1,124 views",251 downloads,,,https://www.kaggle.com/tbsteal/canadian-car-accidents-19942014,"Context
This data set contains collision data for car accidents in Canada from 1999-2014 as provided by Transport Canada. This dataset provides various features such as time of day, whether or not there were fatalities, driver gender, etc. The codes for the different categories can be found in 'drivingLegend.pdf'. The original csv file is no longer available, however it can be downloaded in portions by selecting the various features using this portal.
Content
Each feature is 100% categorical data, with some features having 2 categories, while others can have 30+. The data is not completely imputed appropriately (you can thank Stats Canada), so some data preprocessing is required. For instance, categories may have duplicates in the form of '01' and '1', or some data may be formatted as integers while others are formatted as strings. Some data is not known and is marked accordingly in 'drivingLegend.pdf'. Unfortunately, features such as location and impaired driving are not a part of this feature set, however there are plenty of others to work with.
Acknowledgements
This data is provided by Transport Canada and Statistics Canada. This data is provided under the Statistics Canada Open License Agreement.
Inspiration
Questions of particular interest: - What are the main contributing factors to accident fatalities? - Can a machine learning classifier be used to predict fatalities? Note: If attempting to predict fatalities, the data is highly skewed towards non-fatalities."
Nutrition,Food Nutrition By NSSO,Sandeep Kumar,3,"Version 1,2017-06-22",,Other,9 MB,ODbL,944 views,176 downloads,,0 topics,https://www.kaggle.com/sanbelief/nutrition,"Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
Trump Financial Disclosure 2016,Donald Trump's 2016 Form 278e,Sohier Dane,3,"Version 1,2017-07-11","presidents
finance
politics",CSV,197 KB,Other,952 views,62 downloads,,0 topics,https://www.kaggle.com/sohier/trump-financial-disclosure-2016,"Context
All US presidential candidates are required to fill out form 278e, a general disclosure of their assets, debts, and sources of income. This is an unpacked version of the pdf of Trump's form that was made available by the Federal Election Commission in mid 2016. It contains some information about his financial interests, but not enough to paint a complete picture of his net worth. It may be possible to use some of these forms to identify his foreign business partners.
Acknowledgements
This dataset unpacked from the original pdf and kindly made available by Quartz. Please see their original article for the full background on what this form does and does not contain.
You might also like
Trump's World
Trump's Tweets
Trump Campaign Expenditures"
Pharmaceutical Tablets Dataset,"252 speckled pill images convoluted to 20,000 image training set",TruMedicines,3,"Version 1,2017-07-07",,Other,89 MB,ODbL,910 views,127 downloads,,,https://www.kaggle.com/trumedicines/pharmaceutical-tablets-dataset,"Context
TruMedicines has trained a deep convolutional neural network to autoencode and retrieve a saved image, from a large image dataset based on the random pattern of dots on the surface of the pharmaceutical tablet (pill). Using a mobile phone app a user can query the image datebase and verify the query pill is not counterfeit and is authentic, additional meta data can be displayed to the user: manf date, manf location, drug expiration date, drug strength, adverse reactions etc.
Content
TruMedicines Pharmaceutical images of 252 speckled pill images. We have convoluted the images to create 20,000 training database by: rotations, grey scale, black and white, added noise, non-pill images, images are 292px x 292px in jpeg format
In this playground competition, Kagglers are challenged to develop deep Convolutional Neural Network and hash codes to accurately identify images of pills and quickly retrieved from our database. Jpeg images of pills can be autoencoded using a CNN and retrieved using a CNN hashing code index. Our Android app takes a phone of a pill and sends a query to the image database for a match, then returns meta data abut the pill: manf date, expiration date, ingredients, adverse reactions etc. Techniques from computer vision alongside other current technologies can make recognition of non-counterfeit, medications cheaper, faster, and more reliable.
Acknowledgements
Special Thanks to Microsoft Paul Debaun and Steve Borg and NWCadence, Bellevue WA for their assistance
Inspiration
TruMedicines is using machine learning on a mobile app to stop the spread of counterfeit medicines around the world. Every year the World Health Organization WHO estimates 1 million people die or become disabled due to counterfeit medicine."
FAspell,Naturally-occurring Persian (Farsi) spelling mistakes,Rachael Tatman,3,"Version 1,2017-07-15",linguistics,CSV,146 KB,Other,540 views,18 downloads,,0 topics,https://www.kaggle.com/rtatman/faspell,"Context:
FASpell dataset was developed for the evaluation of spell checking algorithms. It contains a set of pairs of misspelled Persian (Farsi) words and their corresponding corrected forms similar to the ASpell dataset used for English.
Content:
The dataset consists of two parts:
faspell_main: list of 5050 pairs collected from errors made by elementary school pupils and professional typists.
faspell_ocr: list of 800 pairs collected from the output of a Farsi OCR system.
Acknowledgements:
Based on a work at http://pars.ie/lr/FAspell_Dataset. Please acknowledge the use of this dataset by referencing one of the following papers:
Barari, L., & QasemiZadeh, B. (2005). CloniZER spell checker adaptive language independent spell checker. In AIML 2005 Conference CICC, Cairo, Egypt (pp. 65-71).
QasemiZadeh, B., Ilkhani, A., & Ganjeii, A. (2006, June). Adaptive language independent spell checking using intelligent traverse on a tree. In Cybernetics and Intelligent Systems, 2006 IEEE Conference on (pp. 1-6). IEEE.
License
FASpell by Behrang QasemiZadeh is licensed under a Creative Commons Attribution 4.0 International License. Based on a work at http://pars.ie/lr/FAspell_Dataset.
Inspiration:
Which kinds of misspellings occurs more often?
Are certain characters more likely to be misspelled? Certain words?
Can you construct a finite state automaton spell checker for Persian based on this data?"
Cuss words and Deaths in Quentin Tarantino Films,A tally of every cuss word and death in Tarentino's films up to 2012,FiveThirtyEight,3,"Version 1,2017-07-20","news agencies
film
death
linguistics",CSV,62 KB,Other,984 views,69 downloads,2 kernels,0 topics,https://www.kaggle.com/fivethirtyeight/cuss-words-and-deaths-in-quentin-tarantino-films,"Context
I found this dataset after reading a Five Thirty Eight article. The author got a tally of every death and cuss word in Tarantino's movies. That's no small feat considering the content of Tarantino flicks! Such endurance!
Content
movie: Film title
type: Whether the event was a profane word or a death
word: The specific profane word, if the event was a word
minutes_in: The number of minutes into the film the event occurred
Acknowledgements
Thanks to FiveThirtyEight for throwing this dataset up on github and sharing with everyone.
The original article can be found on FiveThirtyEight's website here: https://fivethirtyeight.com/features/complete-catalog-curses-deaths-quentin-tarantino-films/
And the dataset is can be found here: https://github.com/fivethirtyeight/data/tree/master/tarantino
Inspiration
Try some word counting and see how Tarantino's murder:death:cuss ratios have changed over time. What are his favorite cuss words? Which movies have the most deaths?
Shared under MIT License"
Stanford Open Policing Project - South Carolina,Data on Traffic and Pedestrian Stops by Police in South Carolina,Stanford Open Policing Project,3,"Version 1,2017-07-11","government agencies
crime
law
violence",Other,2 GB,Other,674 views,42 downloads,,,https://www.kaggle.com/stanford-open-policing/stanford-open-policing-project-south-carolina,"Context:
On a typical day in the United States, police officers make more than 50,000 traffic stops. The Stanford Open Policing Project team is gathering, analyzing, and releasing records from millions of traffic stops by law enforcement agencies across the country. Their goal is to help researchers, journalists, and policymakers investigate and improve interactions between police and the public.
If you'd like to see data regarding other states, please go to https://www.kaggle.com/stanford-open-policing.
Content:
This dataset includes 1.7 gb of stop data from South Carolina, covering all of 2010 onwards. Please see the data readme for the full details of the available fields.
Acknowledgements:
This dataset was kindly made available by the Stanford Open Policing Project. If you use it for a research publication, please cite their working paper: E. Pierson, C. Simoiu, J. Overgoor, S. Corbett-Davies, V. Ramachandran, C. Phillips, S. Goel. (2017) “A large-scale analysis of racial disparities in police stops across the United States”.
Inspiration:
How predictable are the stop rates? Are there times and places that reliably generate stops?
Concerns have been raised about jurisdictions using civil forfeiture as a funding mechanism rather than to properly fight drug trafficking. Can you identify any jurisdictions that may be exhibiting this behavior?"
Email Status Tracking,Targeted Marketing Email Campaign Datasets,Gokagglers,3,"Version 1,2017-08-09",internet,CSV,3 MB,CC0,539 views,57 downloads,,,https://www.kaggle.com/loveall/email-status-tracking,"Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
Vehicles - Nepal,This dataset contains 4800 images of Nepalese vehicles,Sudarshan,3,"Version 1,2017-06-25",,Other,23 MB,ODbL,638 views,46 downloads,,,https://www.kaggle.com/sdevkota007/vehicles-nepal,"Introduction
This image dataset was collected as a part of my final year undergraduate project Vehicle Detection and Road Traffic Congestion Mapping Using Image Processing. A total of 30 traffic videos, each of approx. 4 mins, from different streets of Kathmandu were taken and images of vehicles were manually cropped out from the video frames.
Content
Total images: 4800 , No. of two-wheeler vehicles: 1811 , No. of four-wheeler vehicles: 2989 , Size of each image: varies , Resolution of each image: varies , Image format: "".jpg""
Acknowledgement
This dataset wouldn't be here without the help of my project mates Anup Adhikari, Binish Koirala and Sparsha Bhattarai. Thank you lads for your wonderful contribution.
Note
Since Kaggle won't support uploading more than 1000 images in a compressed format, you can get the complete repository of the images from https://github.com/sdevkota007/vehicles-nepal-dataset If you have any question, please email me at sdevkota007@gmail.com"
New York City Bike Share Dataset,Predict Gender of the riders,Nikhil Akki,3,"Version 1,2017-07-25",,CSV,126 MB,CC4,556 views,46 downloads,,0 topics,https://www.kaggle.com/akkithetechie/new-york-city-bike-share-dataset,"The New York City Bike Share enables quick, easy, and affordable bike trips around the New York city boroughs. They make regular open data releases (this dataset is a transformed version of the data from this link). The dataset contains 735502 anonymised trips information made from Jan 2015 to June 2017.
Acknowledgements -
This dataset is the property of NYC Bike Share, LLC and Jersey City Bike Share, LLC (“Bikeshare”) operates New York City’s Citi Bike bicycle sharing service for T&C click here
Objectives -
EDA
Feature Engineering
Predict Gender of the riders"
Sales of Shampoo Over a Three Year Period,Monthly number of sales of shampoo over a 3-year period.,Samriddhi Sinha,3,"Version 1,2017-07-30",time series,CSV,604 B,ODbL,747 views,106 downloads,,0 topics,https://www.kaggle.com/djokester/sales-of-shampoo-over-a-three-year-period,"Context
This dataset describes the monthly number of sales of shampoo over a 3-year period.
Content
The units are a sales count and there are 36 observations. The original dataset is credited to Makridakis, Wheelwright, and Hyndman (1998).
Acknowledgements
Source: Time Series Data Library (citing: Makridakis, Wheelwright and Hyndman (1998))"
Executed Inmates 1982 - 2017,A dataset of inmates executed in Texas. Includes inmate's last statement.,Evan Payne,3,"Version 2,2017-08-01|Version 1,2017-07-26","united states
death
crime",CSV,637 KB,CC0,192 views,20 downloads,,0 topics,https://www.kaggle.com/jpayne/executedoffenders,"Context
Between the years of 1982 and 2017, the state of Texas has executed approximately 543 inmates. During this time, the TDCJ(Texas Department of Criminal Justice) recorded data regarding each execution.
Content
Each row in the data set includes the executed inmate's age, last statement, date of his/her execution, first and last name, race and county. The data was scraped from the TDCJ 's website: here
Acknowledgements
Thank you to the TDCJ for recording this dataset
Inspiration
I would like to see some analysis on the demographics of the prisoner and their last statement(or lack of one). Is age associated with the length of the last statement? Do the demographics of the prisoner have an association with whether or not the prisoner left a last statement? How many times, on average is the word ""sorry"" used?"
North Carolina Schools: Report Cards and Metadata,NC School Report Cards for all North Carolina Public Schools,JustinMoore,3,"Version 1,2017-08-01","united states
schools and traditions
education",CSV,2 MB,CC0,"1,661 views",116 downloads,2 kernels,,https://www.kaggle.com/lazyjustin/ncschools,"Context
North Carolina school report cards provide an efficient method for comparing and reviewing student academic performance across all public schools in North Carolina. The following data set combines school report card (SPG) grades and scores with other school metadata gathered by the state and other local organizations.
Content
School report card grades and scores for three consecutive school years of state testing (2013/14, 2014/15, and 2015/16). Additional school metadata is also included (addresses, geo-codes, poverty indicators, transportation and budget information, etc...)
Acknowledgements
All data sourced from public resources: https://ncreportcards.ondemand.sas.com/src/#/?_k=mpdibp
For additional information, please consult a full data dictionary here: http://www.ncpublicschools.org/docs/src/researchers/data-dictionary.pdf
Special thanks to gmaps D. Kahle and H. Wickham. ggmap: Spatial Visualization with ggplot2. The R Journal, 5(1), 144-161. http://journal.r-project.org/archive/2013-1/kahle-wickham.pdf
Inspiration
This data set was constructed as part of a data science project related to my Master's degree studies. Some of my findings and basic information can be found here: https://ncschoolreportcard.wordpress.com/. Additional ideas include:
Better visualizations for counties, cities, etc...
Prediction of future school performance
Feature engineering for school performance"
Oil price and share price of a few companies,Just a daily oil price dataset and a few companies dataset,Brave,3,"Version 1,2017-07-27","finance
energy",CSV,3 MB,CC4,927 views,158 downloads,,3 topics,https://www.kaggle.com/javierbravo/oil-price-and-share-price-of-a-few-companies,"Context
This is the oil price and the share price of a few companies.
Content
The share price contains the following columns: Date,Open,High,Low,Close,Adj Close,Volume The oil price contains the following columns: date and brent oil price
Acknowledgements
The oil price is obtained from the website of the U.S Energy Information administration: https://www.eia.gov/dnav/pet/hist/LeafHandler.ashx?n=PET&s=rbrte&f=D
Share price dataset in a daily frequency from a few companies:
To download the dataset I use Yahoo finance. The following link is an example to download the share price dataset: https://uk.finance.yahoo.com/quote/RDSB.L/history?period1=946684800&period2=1499122800&interval=1d&filter=history&frequency=1d
Inspiration"
New York Traffic Accidents 2016,Accident data to be used for New York taxi competition,oscarleo,3,"Version 1,2017-08-03",,CSV,25 MB,ODbL,762 views,109 downloads,,,https://www.kaggle.com/oscarleo/new-york-traffic-accidents-2016,"Context
This dataset contains all reported traffic accidents in New York 2016. The data is from New York Open Data. I have preprocessed the data to work well with the New York City Taxi Trip Duration and the New York City Taxi with OSRM dataset. I have made made sure that the street names have the same format so that the datasets can be used together."
2011 - 2013 NYC Traffic Volume Counts,Hourly Traffic Counts in NYC,MihwaHan,3,"Version 1,2017-07-22",,CSV,1 MB,Other,"1,039 views",124 downloads,,2 topics,https://www.kaggle.com/hanriver0618/2011-2013-nyc-traffic-volume-counts,"Context
In order to optimize my estimate for the NYC Taxi Trip Duration Competition, I thought it would be helpful to include historical traffic volume data in various parts of NYC. This dataset was compiled from 2011 -2013, and consists of over 100 days in which traffic counts were recorded on an hourly basis for numerous locations (anywhere from a few to hundreds of locations).
Content
The traffic counts are measured on a particular road (Roadway Name) from one intersection to another (From and To). The data set also specifies a direction of traffic that is being measured (Direction - NB, SB, WB, EB).
Acknowledgements
I found this dataset on the NYC Open Data(https://opendata.cityofnewyork.us/).
Inspiration
My hope is that with these data one can better predict the patterns of traffic in NYC on hourly and daily basis."
California Housing Prices,California Housing Prices based on the data collected from 1990 census,abanil,3,"Version 1,2017-08-06",,CSV,1 MB,Other,619 views,89 downloads,,0 topics,https://www.kaggle.com/abhilashanil/california-housing-prices,"Context
Census data is essential in gaining insights into how the population is distributed within a certain geographic area.
Content
This census dataset contains details related to the housing of the population in California, including the number of people in a certain area, geographic area, their proximity to the bay and kind of house that the population is living in currently.
Acknowledgements
I have taken this data from the link mentioned below https://github.com/ageron/handson-ml/tree/master/datasets/housing
Inspiration
Through this data I want to gain insight in to the housing prices in California."
Stock Market Data,Play with all the stock market data needed by a stock researcher,Sourav Roy,3,"Version 1,2017-05-29",,CSV,473 KB,Other,"1,768 views",202 downloads,2 kernels,,https://www.kaggle.com/souravroy1/stock-market-data,"Context
This dataset contains data from a list of Indian stocks in NSE. It includes a collection of well performing stocks with all the data necessary to predict which stocks to buy, hold, or exit.
Acknowledgements
I work in a stock research firm. This stock data is for all Kaggle users to play and experiment with in order to learn more about stock research.
Inspiration
The second column, ""Category"", gives a list of all the stocks that a user needs to buy, hold, or exit . We challenge you to develop an algorithm to see if your result matches ours."
Beginner Projects - Ergonomic Study on Chopsticks,Evaluate the effects of chopsticks length on food-serving performance,Priya_ds,3,"Version 1,2017-06-12","food and drink
psychometrics",CSV,3 KB,Other,"1,390 views",100 downloads,5 kernels,2 topics,https://www.kaggle.com/priya2908/chopsticks-1992,"Question
As per the 1992 ergonomics study, What is the optimum length of chopsticks usable by adults & children?
Acknowledgements
An investigation for determining the optimum length of chopsticks. Hsu SH, Wu SP. Appl Ergon. 1991 Dec;22(6):395-400. PMID: 15676839
Inspiration
Data Sets shared by a beginner, for Beginners :) Thanks to David Venturi and Udacity, that I was able to get started off on this. More details here : Link"
Countries ISO Codes,List of countries of the world with their ISO codes,Juanu,3,"Version 1,2017-08-10",,CSV,9 KB,CC0,303 views,42 downloads,4 kernels,0 topics,https://www.kaggle.com/juanumusic/countries-iso-codes,"Context
This dataset was uploadedto be able to link the Countries ISO codes to any data in a better way than just names. This Dataset can give the opportunity to improve current and new Notebooks as well as other datasets. Libraries like plotly use country codes to easily identify the data linked to the country. This dataset can help with that task.
Content
The dataset contains a list of ALL the states and their codes. Columns: - Alpha-2 code: The alpha-2 code of the country (2 characters) - Alpha-3 code: The alpha-3 code of the country (3 characters) - Numeric code: The numeric code of the country (int) - ISO 3166-2: The ISO 3166-2 code. Formatted as: ISO 3166-2:[2 characters]
Acknowledgements
https://gist.github.com/radcliff/f09c0f88344a7fcef373
Inspiration
Any dataset that contains a country column, can be linked to this dataset and be used to link other data, as well as plotting MAPS. Libraries like plotly use country codes to easily identify the data linked to the country. This dataset can help with that task."
Mercedes Benz Us car sales data 06/May - 09/March,Us luxury car sales data pre and during economic crises time,Luigi,3,"Version 1,2017-06-09","business
economics
marketing",CSV,3 KB,CC0,"1,013 views",115 downloads,2 kernels,0 topics,https://www.kaggle.com/luigimersico/mercedes-benz-us-car-sales-data-06may-09march,Data are acquired from mercedes Benz monthly sales report. The price is the average MSRP (aka sticker price) stands for the manufacturer suddested retail price.
Exchange rate BRIC currencies/US dollar,historical data monthly frequencies 01/07/1997 - 1/12/2015,Luigi,3,"Version 1,2017-06-15","business
finance
economics",CSV,9 KB,Other,368 views,41 downloads,2 kernels,0 topics,https://www.kaggle.com/luigimersico/exchange-rate-bric-currenciesus-dollar,"Content
over 10 years of historical exchange rate data of BRIC countries currencies/ U.S. dollar"
housing,Housing Details For Melbourne,Sandeep Kumar,3,"Version 1,2017-06-08",,CSV,1 MB,CC0,"1,421 views",181 downloads,,0 topics,https://www.kaggle.com/sanbelief/housing,"Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
PDD Graph,"Patient-Disease-Drug Graph, Bridging MIMIC-III and Linked Data Cloud",xjtushilei,3,"Version 1,2017-06-15",,Other,3 KB,Other,876 views,25 downloads,,,https://www.kaggle.com/xjtushilei/pdd-graph,"Online
Website
Github
DataHub
SPARQL endpoint
You can query some of the data online there. There is also the download link. Of course you can download it here.
Context
Electronic medical records contain multi-format electronic medical data that consist of an abundance of medical knowledge. Facing with patients symptoms, experienced caregivers make right medical decisions based on their professional knowledge that accurately grasps relationships between symptoms, diagnosis, and treatments. We aim to capture these relationships by constructing a large and high-quality heterogeneous graph linking patients, diseases, and drugs (PDD) in EMRs.
Content
Specifically, we extract important medical entities from MIMIC-III (Medical Information Mart for Intensive Care III) and automatically link them with the existing biomedical knowledge graphs, including ICD-9 ontology and DrugBank. The PDD graph presented is accessible on the Web via the SPARQL endpoint, and provides a pathway for medical discovery and applications, such as effective treatment recommendations.
A subgraph of PDD is illustrated in the followng figure to betterunderstand the PDD graph.
Acknowledgements
Author
Data set belongs to Meng Wang, Jiaheng Zhang, Jun Liu,Wei Hu, Sen Wang, , Wenqiang Liu and Lei Shi
They come from： 1. MOEKLINNS lab, Xi’an Jiaotong University, Xi’an, China 2. State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China 3. Griffith Universtiy, Gold Coast Campus, Australia
Some Email： - Meng Wang：wangmengsd@stu.xjtu.edu.cn - Lei Shi：xjtushilei@foxmail.com - Jun Liu：liukeen@xjtu.edu.cn
Research
The paper is being reviewed and is not easily disclosed.So it can't be linked here.
Inspiration
If you have any questions, please contact the email address above.
Do you have any suggestions ? And send them to an e-mail address above.
License
This work is licensed under a Creative Commons Attribution 4.0 International License.
If your article needs to be reference our work , you can reference our github."
Iris Dataset,Best dataset for small project,Manimala,3,"Version 1,2017-08-03",,CSV,4 KB,Other,494 views,70 downloads,4 kernels,0 topics,https://www.kaggle.com/vikrishnan/iris-dataset,"Context
Based on Fisher's linear discriminant model, this data set became a typical test case for many statistical classification techniques in machine learning such as support vector machines.
Content
The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis.[1] It is sometimes called Anderson's Iris data set because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers of three related species.[2] Two of the three species were collected in the Gaspé Peninsula ""all from the same pasture, and picked on the same day and measured at the same time by the same person with the same apparatus"".[3]
The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimetres. Based on the combination of these four features, Fisher developed a linear discriminant model to distinguish the species from each other.
Acknowledgements
description taken from Wiki Would like to thank Dr. Jason Brownlee who has explained all the examples very nicely and clearly!"
Connecticut inmates awaiting trial,Inmates being held in correcitonal facilities until trial,Connecticut Open Data,3,"Version 1,2017-07-27",,CSV,115 MB,CC0,592 views,34 downloads,,,https://www.kaggle.com/Connecticut-open-data/connecticut-inmates-awaiting-trial,"Context
Since July 1, 2016, Connecticut has updated this nightly dataset of every inmate held in jail while awaiting trial. At the time of download, this dataset contains just over one year of data with 1132352 rows of data, where one row is one inmate.
Content
Field Descriptions: * DOWNLOAD DATE: Date in which the data were extracted and reflecting the population for that day.
IDENITIFIER: Individual Inmate Identifier
LATEST ADMISSION DATE: Most recent date in which the inmate has been admitted. In some instances, this may reflect an original date of admission to a correctional facility. Generally, if a date is more than one year old, an inmate should not be considered to have been held for the entire duration of that time.
RACE: Race of inmate
AGE: Age of inmate
BOND AMOUNT: Amount of bond for which the inmate is being held. In some instances, for particularly low (less than $100), this bond amount may be considered a place holder value
OFFENSE: Controlling offense for which the bond amount has been set.
FACILITY: Department of Correction facility where the inmate is currently held.
DETAINER: Denotes whether inmate is being held at the request of another criminal justice agency, or if another agency is to be notified upon release.
Acknowledgements
Thanks to [http://dataispluralc.om] for the tip on this dataset! This dataset was downloaded on July 26, 2017 - Chekc the original source for more up-to-date data (updated nightly) [https://data.ct.gov/Public-Safety/Accused-Pre-Trial-Inmates-in-Correctional-Faciliti/b674-jy6w]
Inspiration
This dataset contains information about inmate's race and the nature of the crimes. The Minority Report is a sci-fi story pretty well known for predicting crimes and arresting people before they happen. Can you do the same? Would you dare do the same?"
The Correlates of State Policy Project,One-stop shop for anyone studying state policies and politics,Institute for Public Policy and Social Research,3,"Version 1,2017-07-27",,Other,15 MB,CC0,598 views,76 downloads,,,https://www.kaggle.com/ippsr/correlates-state-policy,"Context
The Correlates of State Policy Project aims to compile, disseminate, and encourage the use of data relevant to U.S. state policy research, tracking policy differences across and change over time in the 50 states. We have gathered more than nine-hundred variables from various sources and assembled them into one large, useful dataset. We hope this Project will become a “one-stop shop” for academics, policy analysts, students, and researchers looking for variables germane to the study of state policies and politics.
Content
The Correlates of State Policy Project includes more than nine-hundred variables, with observations across the U.S. 50 states and time (1900 – 2016). These variables represent policy outputs or political, social, or economic factors that may influence policy differences across the states. The codebook includes the variable name, a short description of the variable, the variable time frame, a longer description of the variable, and the variable source(s) and notes.
Take a look at the codebook PDF to get more information about each column
Acknowledgements
This aggregated data set is only possible because many scholars and students have spent tireless hours creating, collecting, cleaning, and making data publicly available. Thus if you use the dataset, please cite the original data sources.
Jordan, Marty P. and Matt Grossmann. 2016. The Correlates of State Policy Project v.1.10. East Lansing, MI: Institute for Public Policy and Social Research (IPPSR).
This dataset was originally downloaded from
http://ippsr.msu.edu/public-policy/correlates-state-policy"
Southern Ocean Microbial Concentrations,Voyage 3 of the Aurora Australis (2005/2006),Myles O'Neill,3,"Version 1,2017-09-06","antarctica
ecology",CSV,29 KB,Other,259 views,11 downloads,,0 topics,https://www.kaggle.com/mylesoneill/microbial-concentrations,"This data set contains concentrations of phytoplankton, protozoa, total bacteria and metabolically active bacteria assessed by flow cytometry on transects 12, 1, 3, 5, 7, 9 and 11 of the BROKE-West survey of the Southern Ocean between January and March 2006. Only total bacterial concentrations were assesed for transect 11.
Between 4 and 12 depths were sampled for marine microbes and concentations were assesed using FACScan flowcytometer. Phytoplankton were identified and counted based on the autofluorescense of chlorophyll a when excited by the 488 nm laser of the FACScan. Protozoa were identified and counted after staining with the acid vacuole stain Lysotracker Green. Total bacteria were identified and counted using the cell permeant SYTO 13 nucleic stain. Metabolically active bacteria were identified and counted after staining for intracellular esterases with the esterase stain 6CFDA.
Data collected by the Australian Antarctic Division. Sourced from: http://data.gov.au/dataset/broke-west-microbial-concentrations-voyage-3-of-the-aurora-australis-2005-2006"
Tools Testing and Community Prototyping,Machine learning skunkworks ground for your pleasure,the1owl,3,"Version 4,2017-09-17|Version 3,2017-08-08|Version 2,2017-08-07|Version 1,2017-08-06",puzzles,SQLite,201 MB,Other,308 views,36 downloads,4 kernels,0 topics,https://www.kaggle.com/the1owl/dataset01,"Context
Test, discover, suggest, and repeat.
Inspiration
You inspire me. Yes, you. No not behind you, I mean you. Yes you. Thats it. Take credit. Enjoy the feeling. Relax. Create a Kernel now.
Acknowledgements
Licenses intact for due credit. Enjoy the ride."
supply chain data,Import and shipment data,sunilp,3,"Version 1,2017-09-20",,CSV,152 MB,ODbL,"1,518 views",186 downloads,,,https://www.kaggle.com/sunilp/walmart-supply-chain-data,This dataset does not have a description yet.
India 🇮🇳 - Habitation Info (6.65m observations),Village and caste wise habitation information of India. (2009 - 2012),Rajanand Ilangovan / இராஜ்ஆனந்த் இளங்கோவன்,3,"Version 1,2017-08-06","bodies of water
india",Other,89 MB,CC4,268 views,33 downloads,,0 topics,https://www.kaggle.com/rajanand/habitation,"Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks.
Context
Basic Habitation Information: The data refers to the list of habitations, its population in different caste category (SC, ST and GENERAL) and status of availability of potable drinking water (Covered or Partially covered) all over India.
Water Quality Affected Habitations: The data refers to the list of drinking water quality affected habitations all over India due to contamination such as Fluoride, Arsenic, Iron, Salinity and Nitrate.
Content
Basic Habitation Information from 2009 and 2012. Water Quality Affected Habitations from 2009 and 2012.
Acknowledgements
Ministry of Drinking Water and Sanitation (MDWS), Govt of India has shared this data in Open Govt Data Platform India portal under Govt. Open Data License - India."
Football Delphi,Predicting soccer match outcomes,Jörg Eitner,3,"Version 2,2017-08-16|Version 1,2017-08-15","association football
sports",SQLite,6 MB,CC0,"1,688 views",149 downloads,3 kernels,0 topics,https://www.kaggle.com/laudanum/footballdelphi,"Context
As many others I have asked myself if it is possible to use machine learning in order to create valid predictions for football (soccer) match outcomes. Hence I created a dataset consisting of historic match data for the German Bundesliga (1st and 2nd Division) as well as the English Premier League reaching back as far as 1993 up to 2016. Besides the mere information concerning goals scored and home/draw/away win the dataset also includes per site (team) data such as transfer value per team (pre-season), the squad strength, etc. Unfortunately I was only able to find sources for these advanced attributes going back to the 2005 season.
I have used this dataset with different machine learning algorithms including random forests, XGBoost as well as different recurrent neural network architectures (in order to potentially identify recurring patterns in winning streaks, etc.). I'd like to share the approaches I used as separate Kernels here as well. So far I did not manage to exceed an accuracy of 53% consistently on a validation set using 2016 season of Bundesliga 1 (no information rate = 49%).
Although I have done some visual exploration before implementing the different machine learning approaches using Tableau, I think a visual exploration kernel would be very beneficial.
Content
The data comes as an Sqlite file containing the following tables and fields:
Table: Matches
Match_ID (int): unique ID per match
Div (str): identifies the division the match was played in (D1 = Bundesliga, D2 = Bundesliga 2, E0 = English Premier League)
Season (int): Season the match took place in (usually covering the period of August till May of the following year)
Date (str): Date of the match
HomeTeam (str): Name of the home team
AwayTeam (str): Name of the away team
FTHG (int) (Full Time Home Goals): Number of goals scored by the home team
FTAG (int) (Full Time Away Goals): Number of goals scored by the away team
FTR (str) (Full Time Result): 3-way result of the match (H = Home Win, D = Draw, A = Away Win)
Table: Teams
Season (str): Football season for which the data is valid
TeamName (str): Name of the team the data concerns
KaderHome (str): Number of Players in the squad
AvgAgeHome (str): Average age of players
ForeignPlayersHome (str): Number of foreign players (non-German, non-English respectively) playing for the team
OverallMarketValueHome (str): Overall market value of the team pre-season in EUR (based on data from transfermarkt.de)
AvgMarketValueHome (str): Average market value (per player) of the team pre-season in EUR (based on data from transfermarkt.de)
StadiumCapacity (str): Maximum stadium capacity of the team's home stadium
Table: Unique Teams
TeamName (str): Name of a team
Unique_Team_ID (int): Unique identifier for each team
Table: Teams_in_Matches
Match_ID (int): Unique match ID
Unique_Team_ID (int): Unique team ID (This table is used to easily retrieve each match a given team has played in)
Based on these tables I created a couple of views which I used as input for my machine learning models:
View: FlatView
Combination of all matches with the respective additional data from Teams table for both home and away team.
View: FlatView_Advanced
Same as Flatview but also includes Unique_Team_ID and Unique_Team in order to easily retrieve all matches played by a team in chronological order.
View: FlatView_Chrono_TeamOrder_Reduced
Similar to Flatview_Advanced, however missing the additional attributes from team in order to have a longer history including years 1993 - 2004. Especially interesting if one is only interested in analyzing winning/loosing streaks.
Acknowledgements
Thanks to football-data.co.uk and transfermarkt.de for providing the raw data used in this dataset.
Inspiration
Please feel free to use the humble dataset provided here for any purpose you want. To me it would be most interesting if others think that recurrent neural networks could in fact be of help (and even maybe outperform classical feature engineering) in identifying streaks of losses and wins. In the literature I mostly only found example of RNN application where the data were time series in a very narrow sense (e.g. temperature measurements over time) hence it would be interesting to get your input on this question.
Maybe someone also finds additional attributes per team or match which have substantial impact on match outcome. So far I have found the ""Market Value"" of a team to be by far the best predictor when two teams face each other, which makes sense as the market value usually tends to correlate closely with the strength of a team and it's propects at winning"
Baton Rouge Crime Incidents,"Through September 21st, 2017",John Ruth,3,"Version 1,2017-09-22",crime,CSV,66 MB,CC0,760 views,95 downloads,,0 topics,https://www.kaggle.com/johnruth/baton-rouge-crime-incidents-through-09212017,"Context
Crimes reported in Baton Rouge and handled by the Baton Rouge Police Department. Crimes include Burglaries (Vehicle, Residential and Non-residential), Robberies (Individual and Business), Theft, Narcotics, Vice Crimes, Assault, Nuisance, Battery, Firearm, Homicides, Criminal Damage to Property, Sexual Assaults and Juvenile.
Content
Dataset only includes records through September 21st, 2017
Columns included: FILE NUMBER, OFFENSE DATE, OFFENSE TIME, CRIME, COMMITTED, OFFENSE, OFFENSE DESC, ADDRESS, ST NUMBER, ST DIR, ST NAME, ST TYPE, CITY, STATE, ZIP, DISTRICT, ZONE, SUBZONE, COMPLETE DISTRICT, GEOLOCATION
Acknowledgements
This public domain data is provided by Open Data BR through Socrata. See this dataset's official page for more information. Public domain licensed banner image provided by GoodFreePhotos.com."
HCC dataset,Hepatocellular Carcinoma Dataset,mrsantos,3,"Version 1,2017-09-09","healthcare
hospitals
survival analysis
medicine",Other,83 KB,CC4,"1,003 views",96 downloads,,0 topics,https://www.kaggle.com/mrsantos/hcc-dataset,"Data Set Name: Hepatocellular Carcinoma Dataset (HCC dataset)
Abstract: Hepatocellular Carcinoma dataset (HCC dataset) was collected at a University Hospital in Portugal. It contains real clinical data of 165 patients diagnosed with HCC.
Donors: Miriam Seoane Santos (miriams@student.dei.uc.pt) and Pedro Henriques Abreu (pha@dei.uc.pt), Department of Informatics Engineering, Faculty of Sciences and Technology, University of Coimbra Armando Carvalho (aspcarvalho@gmail.com) and Adélia Simão (adeliasimao@gmail.com), Internal Medicine Service, Hospital and University Centre of Coimbra
Data Type: Multivariate Task: Classification, Regression, Clustering, Casual Discovery Attribute Type: Categorical, Integer and Real
Area: Life Sciences Format Type: Matrix Missing values: Yes
Instances and Attributes: Number of Instances (records in your data set): 165 Number of attributes (fields within each record): 49
Relevant Information: HCC dataset was obtained at a University Hospital in Portugal and contais several demographic, risk factors, laboratory and overall survival features of 165 real patients diagnosed with HCC. The dataset contains 49 features selected according to the EASL-EORTC (European Association for the Study of the Liver - European Organisation for Research and Treatment of Cancer) Clinical Practice Guidelines, which are the current state-of-the-art on the management of HCC.
This is an heterogeneous dataset, with 23 quantitative variables, and 26 qualitative variables. Overall, missing data represents 10.22% of the whole dataset and only eight patients have complete information in all fields (4.85%). The target variables is the survival at 1 year, and was encoded as a binary variable: 0 (dies) and 1 (lives). A certain degree of class-imbalance is also present (63 cases labeled as “dies” and 102 as “lives”).
A detailed description of the HCC dataset (feature’s type/scale, range, mean/mode and missing data percentages) is provided in Santos et al. “A new cluster-based oversampling method for improving survival prediction of hepatocellular carcinoma patients”, Journal of biomedical informatics, 58, 49-59, 2015."
Energy Efficiency Dataset,This study looked into assessing the energy efficiency of buildings,Ahiale Darlington,3,"Version 1,2017-09-04",,CSV,40 KB,CC0,532 views,35 downloads,,0 topics,https://www.kaggle.com/elikplim/eergy-efficiency-dataset,"Source:
The dataset was created by Angeliki Xifara (angxifara '@' gmail.com, Civil/Structural Engineer) and was processed by Athanasios Tsanas (tsanasthanasis '@' gmail.com, Oxford Centre for Industrial and Applied Mathematics, University of Oxford, UK).
Data Set Information:
We perform energy analysis using 12 different building shapes simulated in Ecotect. The buildings differ with respect to the glazing area, the glazing area distribution, and the orientation, amongst other parameters. We simulate various settings as functions of the afore-mentioned characteristics to obtain 768 building shapes. The dataset comprises 768 samples and 8 features, aiming to predict two real valued responses. It can also be used as a multi-class classification problem if the response is rounded to the nearest integer.
Attribute Information:
The dataset contains eight attributes (or features, denoted by X1...X8) and two responses (or outcomes, denoted by y1 and y2). The aim is to use the eight features to predict each of the two responses.
Specifically: X1 Relative Compactness X2 Surface Area X3 Wall Area X4 Roof Area X5 Overall Height X6 Orientation X7 Glazing Area X8 Glazing Area Distribution y1 Heating Load y2 Cooling Load
Relevant Papers:
A. Tsanas, A. Xifara: 'Accurate quantitative estimation of energy performance of residential buildings using statistical machine learning tools', Energy and Buildings, Vol. 49, pp. 560-567, 2012
Citation Request:
A. Tsanas, A. Xifara: 'Accurate quantitative estimation of energy performance of residential buildings using statistical machine learning tools', Energy and Buildings, Vol. 49, pp. 560-567, 2012 (the paper can be accessed from [Web Link])
For further details on the data analysis methodology: A. Tsanas, 'Accurate telemonitoring of Parkinsonâ€™s disease symptom severity using nonlinear speech signal processing and statistical machine learning', D.Phil. thesis, University of Oxford, 2012 (which can be accessed from [Web Link])"
Delpher Dutch Newspaper Archive (1618-1699),Can you identify linguistic features that predict a market crash?,Rachael Tatman,3,"Version 1,2017-08-11","languages
europe
finance
+ 2 more...",Other,144 MB,Other,738 views,29 downloads,,0 topics,https://www.kaggle.com/rtatman/delpher-dutch-newspaper-archive-16181699,"Context:
""Tulip mania, tulipmania, or tulipomania (Dutch names include: tulpenmanie, tulpomanie, tulpenwoede, tulpengekte and bollengekte) was a period in the Dutch Golden Age during which contract prices for bulbs of the recently introduced tulip reached extraordinarily high levels and then dramatically collapsed in February 1637. It is generally considered the first recorded speculative bubble (or economic bubble)."" -- From Wikipedia, CC BY-SA
Market forecasting is difficult. There are many factors that may affect the market, and a high degree of uncertainty. One thing that some researchers have been investigating is whether natural language processing (NLP) of news texts can help with market forecasting. Recent publications suggest that it can be.
Peng, Y., & Jiang, H. (2016). Leverage Financial News to Predict Stock Price Movements Using Word Embeddings and Deep Neural Networks. In Proceedings of NAACL-HLT (pp. 374-379).
Fraiberger, S. P. (2016). News Sentiment and Cross-Country Fluctuations. NLP+ CSS 2016, 125.
This dataset an interesting test case for these methodologies. It contains Dutch-language newspapers from the years immediately preceding and following tulip mania. Can you use NLP techniques to model the tulip market over time?
Content:
This dataset contains the texts of 8,559 newspaper deliveries from the 17th century, from June 14th, 1618 to December 31, 1699. The text is in Dutch. Since the text was scraped from old newspapers using OCR (optical character recognition), there are some errors in the text.
Acknowledgments:
This dataset was compiled by Delpher, an archive service provided by the National Library of the Netherlands. It is provided under a CC-BY 4.0 license. For more information, and newspapers from other years, please visit their website (in Dutch). If you use this dataset in your work, please include this citation:
Delpher open newspaper archive (1.0). Creative Commons Attribution 4.0 , The Hague, 2017 ."
Obama White House Budgets,2016 & 2017 OMB Proposals,Jacob Boysen,3,"Version 1,2017-09-08","government agencies
finance
government",CSV,7 MB,CC0,822 views,82 downloads,,0 topics,https://www.kaggle.com/jboysen/obama-budgets,"Context:
Each year, after the President's State of the Union address, the Office of Management and Budget (OMB) releases the Administration's Budget, offering proposals on key priorities and newly announced initiatives. In 2016 & 2017 Obama’s OMB released all of the data included in the President's budget in a machine-readable format here on GitHub. “The budget process should be a reflection of our values as a country, so we think it's important that members of the public have as many tools as possible to see the data behind the President's proposals. And, if people are motivated to create their own visualizations or products from the data, they should have that chance as well.”
Content:
This branch includes three data files that contain an extract of the Office of Management and Budget (OMB) budget database. These files can be used to reproduce many of the totals published in the budget and examine unpublished details below the levels of aggregation published in the budget. The user guide file contains detailed information about this data, its format, and its limitations.
Acknowledgements:
Datasets were compiled by Obama White House officials and released at this Github repo.
Inspiration:
What significant changes were there between 2016 and 2017 proposals?
How was the federal budget distributed across agencies?
Where there any interesting changes in federal receipts?"
Shanghai stock composite index,Shanghai stock composite index,R1q3,3,"Version 1,2017-09-12","finance
internet",CSV,542 KB,CC0,241 views,33 downloads,,0 topics,https://www.kaggle.com/ruanqian/shanghai-stock-composite-index,上证综合指数前复权日线数据
Rosary Prayers in Latin,"All traditional Latin prayers of the Rosary, and the associated Mysteries",Allan Scott,3,"Version 1,2017-09-16","languages
faith and traditions
christianity",CSV,4 KB,CC0,435 views,17 downloads,,0 topics,https://www.kaggle.com/allanscott/rosary-prayers-in-latin,"Context
This dataset has been uploaded primarily to help me, a novice learning Python, to practice coding before attempting the tutorials on Kaggle. By all means, anyone may make use of it. For my part, I'm trying to code a simple program that will print all prayers applicable to whatever sets of Rosary Mysteries someone wishes to pray.
Content
A traditional Dominican Crown Rosary comprises 15 'decades' of 10 Hail Mary prayers with a few others. In addition, there are variable introductory prayers ('the Drop') and concluding prayers. These 15 decades are made up of three groups of 'Mysteries' - Joyful, Sorrowful and Glorious. Usually, one prays a five decade Rosary focusing on just one group, according to the day of the week. One CSV file contains the prayers, and the other contains the mysteries.
Acknowledgements
I cobbled this public domain prayers from various sites; a few good ones are:
http://www.preces-latinae.org/thesaurus/BVM/Rosarium.html https://www.fisheaters.com/rosary.html
Inspiration
I've been having difficulties making simple input scripts work in Python (repeated EOF errors), asking the user to indicate how many decades he/she wishes to pray, and which set of mysteries should be prayed first. I have some private code uploaded, but would be interested in how others do it."
Independent Election Expenditures,Spending by groups other than the candidates themselves,Federal Election Commission,3,"Version 1,2017-09-07",politics,CSV,77 MB,CC0,519 views,36 downloads,,0 topics,https://www.kaggle.com/fec/independent-campaign-expenditures,"This file contains ""24-hour"" and ""48-hour"" reports of independent expenditures filed during the current election cycle and for election cycles through 2010. The file contains detailed information about independent expenditures, including who was paid, the purpose of the disbursement, date and amount of the expenditure and the candidate for or against whom the expenditure was made.
Independent expenditures represent spending by individual people, groups, political committees, corporations or unions expressly advocating the election or defeat of clearly identified federal candidates. These expenditures may not be made in concert or cooperation with or at the request or suggestion of a candidate, the candidate's campaign or a political party.
Any time up to 20 days before an election, if these independent expenditures by a person or organization aggregate more than $10,000 in a race they must be reported to the Commission before the end of the second day after the communication is publicly distributed. If the communications are distributed within the last 19 days before the election, the expenditure must be reported within one day if they aggregate more than $1,000 in any race.
Acknowledgements
This data comes from the US Federal Election Commission. You can find the original dataset here.
If you like...
If you enjoyed this dataset, you might also like the Congressional Election Disbursements dataset."
Korean War Bombing Runs,Details on 12.8k Bombing Runs,United States Air Force,3,"Version 1,2017-09-14","military
war",CSV,4 MB,CC0,885 views,74 downloads,,0 topics,https://www.kaggle.com/usaf/korean-war-bombing-runs,"Context:
THOR is a painstakingly cultivated database of historic aerial bombings from World War I through Vietnam. THOR has already proven useful in finding unexploded ordinance in Southeast Asia and improving Air Force combat tactics. Our goal is to see where public discourse and innovation takes this data. Each theater of warfare has a separate data file, in addition to a THOR Overview.
Content:
By June 1950, the U.S. Air Force had constructed a comprehensive historical program. Over half the records in the Air Force Historical Archives consisted of World War II artifacts, including unit histories and combat reports compiled by field historians as they received a steady flow of documents from operational squadrons and wings. The archives team developed experience pouring through intelligence reports, target folders, bomb damage assessments, and statistics to develop hard earned lessons on modern warfare. So from the first day of combat, 25 June, historians embedded within operational commands in Korea knew recording events from the start would be important. In particular, Albert F. Simpson, the Archives' Director, picked up the phone and directly called the headquarters of the Far East Air Forces (FEAF) to request they begin collecting data on all sorties generated in theater. Their statistical services agreed, and began regularly sending typed reports on 20 essential data items:
Group and Squadron designations
Operating base location
Type and model of aircraft
Aborted, airborne, and effective sorties
Number of aircraft lost or damaged to enemy ground, aircraft, or other action
Personnel Killed, Wounded, or Missing in Action
Number of enemy aircraft destroyed or damaged
Number of bombs, rockets, and bullets expended
Read more here on the Exteter database and consult the data dictionary here.
Acknowledgements:
THOR is a dataset project initiated by Lt Col Jenns Robertson and continued in partnership with Data.mil, an experimental project, created by the Defense Digital Service in collaboration with the Deputy Chief Management Officer and data owners throughout the U.S. military.
Inspiration:
Which campaigns saw the heaviest bombings?
Which months saw the most runs?"
NIPS17 Adversarial learning - 2nd round results,"Scores, runtime statistics and intermediate results of the second DEV round.",Google Brain,3,"Version 1,2017-09-12",,CSV,89 KB,Other,700 views,167 downloads,2 kernels,0 topics,https://www.kaggle.com/google-brain/nips17-adversarial-learning-2nd-round-results,"This dataset contains run time statistics and details about scores for the second development round of NIPS 2017 Adversarial learning competition
Content
Matrices with intermediate results
Following matrices with intermediate results are provided:
accuracy_matrix.csv - matrix with number of correctly classified images for each pair of attack (targeted and non-targeted) and defense
error_matrix.csv - matrix with number of misclassified images for each pair of attack (targeted and non-targeted) and defense
hit_target_class_matrix.csv - matrix with number of times image was classified as specific target class for each pair of attack (targeted and non-targeted) and defense
In each of these matrices, rows correspond to defenses, columns correspond to attack. Also first row and column are headers with Kaggle Team IDs (or baseline ID).
Scores and run time statistics of submissions
Following files contain scores and run time stats of the submissions:
non_targeted_attack_results.csv - scores and run time statistics of all non-targeted attacks
targeted_attack_results.csv - scores and run time statistics of all targeted attacks
defense_results.csv - scores and run time statistics of all defenses
Each row of these files correspond to one submission. Columns have following meaning:
KaggleTeamId - either Kaggle Team ID or ID of the baseline.
TeamName - human readable team name
Score - raw score of the submission
NormalizedScore - normalized (to be between 0 and 1) score of the submission
MinEvalTime - minimum evaluation time of 100 images
MaxEvalTime - maximum evaluation time of 100 images
MedianEvalTime - median evaluation time of 100 images
MeanEvalTime - average evaluation time of 100 images
Notes about the data
Due to team mergers, team name in these files might be different from the leaderboard.
Not all attacks were used to compute scores of defenses and not all defenses were used to compute scores of attacks. Thus if you simply sum-up values in rows/columns of the corresponding matrix you won't obtain exact score of the submission (however number you obtain will be very close to actual score).
Few targeted and non-targeted attacks exceeded 500 seconds time limit on all batches of images. These submissions received score 0 in the official leaderboard. We still were able to compute ""real"" score for these submissions and include it into non_targeted_attack_results.csv and targeted_attack_results.csv files. However these scores are negated in the provided files to emphasize that these submissions violate the time limit."
North American Slave Narratives,First-hand Accounts of Slaves from the United States,Documenting the American South (DocSouth),3,"Version 1,2017-08-15","united states
north america
slaves
+ 2 more...",CSV,52 MB,Other,763 views,49 downloads,,0 topics,https://www.kaggle.com/docsouth-data/north-american-slave-narratives,"""North American Slave Narratives"" collects books and articles that document the individual and collective story of African Americans struggling for freedom and human rights in the eighteenth, nineteenth, and early twentieth centuries. This collection includes all the existing autobiographical narratives of fugitive and former slaves published as broadsides, pamphlets, or books in English up to 1920. Also included are many of the biographies of fugitive and former slaves and some significant fictionalized slave narratives published in English before 1920.
Context
The North American Slave Narratives collection at the University of North Carolina contains 344 items and is the most extensive collection of such documents in the world.
The physical collection was digitized and transcribed by students and library employees. This means that the text is far more reliable than uncorrected OCR output which is common in digitized archives.
More information about the collection and access to individual page images can be be found here: http://docsouth.unc.edu/neh
The plain text files have been optimized for use in Voyant and can also be used in text mining projects such as topic modeling, sentiment analysis and natural language processing. Please note that the full text contains paratextual elements such as title pages and appendices which will be included in any word counts you perform. You may wish to delete these in order to focus your analysis on just the narratives.
The .csv file acts as a table of contents for the collection and includes Title, Author, Publication Date a url pointing to the digitized version of the text and a unique url pointing to a version of the text in plain text (this is particularly useful for use with Voyant: http://voyant-tools.org/).
Copyright Statement and Acknowledgements
With the exception of ""Fields's Observation: The Slave Narrative of a Nineteenth-Century Virginian,"" which has no known rights, the texts, encoding, and metadata available in Open DocSouth are made available for use under the terms of a Creative Commons Attribution License (CC BY 4.0:http://creativecommons.org/licenses/by/4.0/). Users are free to copy, share, adapt, and re-publish any of the content in Open DocSouth as long as they credit the University Library at the University of North Carolina at Chapel Hill for making this material available.
If you make use of this data, considering letting the holder of the original collection know how you are using the data and if you have any suggestions for making it even more useful. Send any feedback to wilsonlibrary@unc.edu.
About the DocSouth Data Project
Doc South Data provides access to some of the Documenting The American South collections in formats that work well with common text mining and data analysis tools.
Documenting the American South is one of the longest running digital publishing initiatives at the University of North Carolina. It was designed to give researchers digital access to some of the library’s unique collections in the form of high quality page scans as well as structured, corrected and machine readable text.
Doc South Data is an extension of this original goal and has been designed for researchers who want to use emerging technology to look for patterns across entire texts or compare patterns found in multiple texts. We have made it easy to use tools such as Voyant (http://voyant-tools.org/) to conduct simple word counts and frequency visualizations (such as word clouds) or to use other tools to perform more complex processes such as topic modeling, named-entity recognition or sentiment analysis."
The Church in the Southern Black Community,144 primary texts about the Church in the Southern Black Community,Documenting the American South (DocSouth),3,"Version 1,2017-08-15","united states
history
faith and traditions
linguistics",CSV,38 MB,Other,422 views,33 downloads,,0 topics,https://www.kaggle.com/docsouth-data/the-church-in-the-southern-black-community,"""The Church in the Southern Black Community"" collects autobiographies, biographies, church documents, sermons, histories, encyclopedias, and other published materials. These texts present a collected history of the way Southern African Americans experienced and transformed Protestant Christianity into the central institution of community life. Coverage begins with white churches' conversion efforts, especially in the post-Revolutionary period, and depicts the tensions and contradictions between the egalitarian potential of evangelical Christianity and the realities of slavery. It focuses, through slave narratives and observations by other African American authors, on how the black community adapted evangelical Christianity, making it a metaphor for freedom, community, and personal survival.
Context
The North American Slave Narratives collection at the University of North Carolina contains 344 items and is the most extensive collection of such documents in the world.
The physical collection was digitized and transcribed by students and library employees. This means that the text is far more reliable than uncorrected OCR output which is common in digitized archives.
More information about the collection and access to individual page images can be be found here: http://docsouth.unc.edu/neh
The plain text files have been optimized for use in Voyant and can also be used in text mining projects such as topic modeling, sentiment analysis and natural language processing. Please note that the full text contains paratextual elements such as title pages and appendices which will be included in any word counts you perform. You may wish to delete these in order to focus your analysis on just the narratives.
The .csv file acts as a table of contents for the collection and includes Title, Author, Publication Date a url pointing to the digitized version of the text and a unique url pointing to a version of the text in plain text (this is particularly useful for use with Voyant: http://voyant-tools.org/).
Copyright Statement and Acknowledgements
With the exception of ""Fields's Observation: The Slave Narrative of a Nineteenth-Century Virginian,"" which has no known rights, the texts, encoding, and metadata available in Open DocSouth are made available for use under the terms of a Creative Commons Attribution License (CC BY 4.0:http://creativecommons.org/licenses/by/4.0/). Users are free to copy, share, adapt, and re-publish any of the content in Open DocSouth as long as they credit the University Library at the University of North Carolina at Chapel Hill for making this material available.
If you make use of this data, considering letting the holder of the original collection know how you are using the data and if you have any suggestions for making it even more useful. Send any feedback to wilsonlibrary@unc.edu.
About the DocSouth Data Project
Doc South Data provides access to some of the Documenting The American South collections in formats that work well with common text mining and data analysis tools.
Documenting the American South is one of the longest running digital publishing initiatives at the University of North Carolina. It was designed to give researchers digital access to some of the library’s unique collections in the form of high quality page scans as well as structured, corrected and machine readable text.
Doc South Data is an extension of this original goal and has been designed for researchers who want to use emerging technology to look for patterns across entire texts or compare patterns found in multiple texts. We have made it easy to use tools such as Voyant (http://voyant-tools.org/) to conduct simple word counts and frequency visualizations (such as word clouds) or to use other tools to perform more complex processes such as topic modeling, named-entity recognition or sentiment analysis."
New York City - Certificates of Occupancy,New and newly reconstructed buildings in New York City.,City of New York,3,"Version 1,2017-09-01","cities
civil engineering",CSV,14 MB,CC0,570 views,52 downloads,,0 topics,https://www.kaggle.com/new-york-city/nyc-certificates-of-occupancy,"Context:
The City of New York issues Certificates of Occupancy to newly constructed (and newly reconstructed, e.g. “gut renovated”) buildings in New York City. These documents assert that the city has deemed the building habitable and safe to move into.
Content:
This dataset includes all temporary (expirable) and final (permanent) Certificates of Occupancies issues to newly habitable buildings in New York City, split between new (Job Type: NB) and reconstructed (Job Type: A1) buildings, issued between July 12, 2012 and August 29, 2017.
Acknowledgements:
This data is published as-is by the New York City Department of Buildings.
Inspiration:
In what areas of New York City are the newly constructed buildings concentrated?
What is the difference in distribution between buildings that are newly built and ones that are newly rebuilt?
In combination with the New York City Buildings Database dataset, what are notable differences in physical characteristics between recently constructed buildings and existing ones?"
Indirect Food Additives,Chemicals indirectly added during processing regulated by the FDA,Food and Drug Administration,3,"Version 1,2017-09-12",food and drink,CSV,955 KB,CC0,"1,535 views",109 downloads,,0 topics,https://www.kaggle.com/fda/indirect-food-additives,"Context
The vast majority of food and food ingredients eaten today is processed in some way before they arrived at the kitchen or dinner table. Food processing equipment may leave trace amounts of various industrial chemical compounds in the foods we eat, and these chemicals, classed indirect food additives, are regulated by the United States Food and Drug Administration. This dataset is a list of indirect food additives approved by the FDA.
Content
This dataset contains the names of chemical compounds and references to the federal government regulatory code approving and controlling their usage.
Acknowledgements
This dataset is published by the FDA and available online as a for-Excel CSV file. A few errant header columns have been cleaned up prior to upload to Kaggle, but otherwise the dataset is published as-is.
Inspiration
What tokens most commonly appear amongst the names contained in this list?
Any identifiable elements or compounds?"
Industrial Security Clearance Adjurations,Over 20000 security clearance appeals made to the Department of Defense,Department of Defense,3,"Version 1,2017-09-14",military,CSV,10 MB,CC0,592 views,66 downloads,,0 topics,https://www.kaggle.com/usdod/dod-clearance-adjurations,"Context
Industry contractors that work for or with the United States Department of Defense and comes into contact with secret or privileged information must submit to a background check by the government as a part of their contractual obligations. Any employee who fails to get the necessary clearance will be unable to work.
Employees may however appeal their decision; in this case the decision will be reviewed and finalized (or reversed) by the Department of Defense Office of Hearings and Appeals (DOHA). This dataset contains summaries of the deliberations and results of such hearings, and provides a window into getting security clearance to work as a defense contractor in the United States.
Content
This data contains dates, case numbers, decisions, and decisions summaries for over 20,000 cases submitted for review between late 1996 and early 2016.
Acknowledgements
This data was published in an HTML format by the US Department of Defense. It has been converted into a CSV format before upload to Kaggle.
Inspiration
What percentage of appeals were declined or upheld?
What were the dominant reasons decisions were made? Have the factors behind decisions changed over times?
What kinds of words appear in decision texts?"
One week of Betfair data: 23 sports,A detailed history of prices traded for each event,Foxtrot,3,"Version 1,2017-08-31",,CSV,322 MB,Other,603 views,53 downloads,,0 topics,https://www.kaggle.com/zygmunt/betfair-sports,"A sample of Betfair data, normally available to those who spend a lot of money wagering. All sports except horse racing (for horse racing, there is a twin dataset at https://www.kaggle.com/zygmunt/betfair-horses).
See http://data.betfair.com/ for a description.
The file has 1306731 data rows. It is 321 MB uncompressed.
Full list of sport IDs is available at http://data.betfair.com/sportids.htm. Sports present in this file are:
1 - Soccer
2 - Tennis
3 - Golf
4 - Cricket
5 - Rugby Union
6 - Boxing
8 - Motor Sport
10 - Special Bets
11 - Cycling
1477 - Rugby League
3503 - Darts
3988 - Athletics
4339 - Greyhound Racing
6231 - Financial Bets
6422 - Snooker
6423 - American Football
7511 - Baseball
7522 - Basketball
7524 - Ice Hockey
61420 - Australian Rules
104049 - ? [1 row]
468328 - Handball
998917 - Volleyball
2152880 - Gaelic Games
26420387 - UFC
Columns:
EVENT_ID
FULL_DESCRIPTION
SCHEDULED_OFF
EVENT
ACTUAL_OFF
SELECTION
SETTLED_DATE
ODDS
LATEST_TAKEN (when these odds were last matched on the selection)
FIRST_TAKEN (when these odds were first matched on the selection)
IN_PLAY (IP - In-Play, PE - Pre-Event, NI - Event did not go in-play)
NUMBER_BETS (number of individual bets placed)
VOLUME_MATCHED (sums the stakes of both back and lay bets)
SPORTS_ID
SELECTION_ID
WIN_FLAG (1 if the selection was paid out as a full or partial winner, 0 otherwise)"
Fertility Rate By Race,Predict Rate of Fertility Over Years,Gokagglers,3,"Version 1,2017-08-23",demographics,CSV,16 KB,Other,395 views,29 downloads,,0 topics,https://www.kaggle.com/loveall/fertility-rate-by-race,"Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
Daikon (Diachronic Corpus),Historic texts from the British Spector news magazine,Liling Tan,3,"Version 1,2017-08-17",,{}JSON,113 MB,CC0,547 views,22 downloads,,0 topics,https://www.kaggle.com/alvations/daikon,"Context
The Daikon Corpus was created during the Diachronic Text Evaluation task in SemEval-2015. The task was to create a system that can date a piece of text.
For example, given a text snippet:
“Dictator Saddam Hussein ordered his troops to march into Kuwait. After the invasion is condemned by the UN Security Council, the US has forged a coalition with allies. Today American troops are sent to Saudi Arabia in Operation Desert Shield, protecting Saudi Arabia from possible attack.”
The text has clear temporal evidence with reference to a
historical figure (“Saddam Hussein”),
notable organization (“UN Security Council”)
factual event (“Operation Desert Shield”).
Historically, we know that
Saddam Hussein lived between 1937 to 2006,
UN Security Council has existed since 1946
Operation Desert Shield (i.e. the Gulf War) occurred between 1990-1991
Given the specific chronic deicticity (“today”) that indicates that the text is published during the Gulf War, we can conceive that the text snippet should be dated 1990-1991.
Content
The Daikon Corpus is made up of articles from the British Spectator news magazine from year 828 to 2008.
The corpus contains 24,280 articles with 19 million tokens; the token count is calculated by summing the number of whitespaces plus 1 for each paragraph.
The Daikon corpus is saved in the JSON format, where the outer most-structure is a list and the inner data structure is a key-value dictionary/hashmap that contains the:
url: URL where the original article resides
date: Date of the article
body: A list of paragraphs
title: Title of the text
Note: If the url is broken, try removing the .html suffix of the url. e.g. change
http://archive.spectator.co.uk/article/24th-september-2005/57/doctor-in-the-house.html 
to
http://archive.spectator.co.uk/article/24th-september-2005/57/doctor-in-the-house
Citations
Liling Tan and Noam Ordan. 2015. 
USAAR-CHRONOS:  Crawling the Web for Temporal Annotations. 
In Proceedings of Ninth International Workshop on 
Semantic Evaluation (SemEval 2015). Denver, USA.
Task reference:
Octavian Popescu and Carlo Strapparava. 
SemEval 2015, Task 7: Diachronic Text Evaluation. 
In Proceedings of Ninth International Workshop on 
Semantic Evaluation (SemEval 2015). Denver, USA.
Dataset image comes from Jonathan Pielmayer
Inspiration
Let's make an artificially intelligent ""Flynn Carsen"" !!"
Crowdfunding Data (Reg CF),Recent crowdfunding data from Edgar.,Dan,3,"Version 1,2017-08-16","business
finance",Other,322 KB,CC0,574 views,70 downloads,,0 topics,https://www.kaggle.com/dan195/regcf,"Context
This is data on Reg CF. Reg CF is a form of crowdfunding that enables start-ups and small businesses to seek money directly from both accredited and non-accredited investors.
Content
The data is taken directly from the Edgar website. The data is from August 3rd, 2017.
The data is in the Reg
Inspiration
Although some sites have sections catered to reg CF data, I have not yet seen any place that also includes the financial information of all the companies that have filed. This dataset is an attempt to aggregate both to see if any insights can be drawn."
Wikipedia Edits,Dataset containing list of wikipedia edits over a period of 20 minutes,ShradhaJoshi,3,"Version 1,2017-08-21",,CSV,118 KB,ODbL,699 views,61 downloads,,0 topics,https://www.kaggle.com/shradhapj/wikipedia-edits,"Context
Hey everyone out there! Wikipedia is a publicly available encyclopedia which can be modified by anyone. Some of these modifications are useful whereas some are not. This data set captures all the edits done to English Wikipedia by anyone across the globe. As there are two edits per second, the data which I have collected is for just 20 minutes.
Content
I have revised the original data set, removed the duplicates and included only the relevant and useful columns. This data set has below mentioned columns: a) action : only edits action is captured. Other actions maybe Talk, etc. b) change_size : the number of characters added or deleted. Positive size means the change was added and negative means the change was deleted. c) geo_ip : This is null if the user is registered in Wikipedia otherwise it is a JSON object containing city, latitude, country_name, region_name and longitude d) is_anonymous : This is a flag/boolean value(true/false) that notifies whether the user is registered or unregistered(anonymous) e) is_bot : This flag/boolean value(true/false) determines if the user is a bot(robot) or a human. f) is_minor: Thus flag/boolean value(true/false) identifies whether the change made to Wikipedia article was minor or major one. g) page_title : This is the title of the Wikipedia article edited by the user. h) url : This field has the URL or link which compares the Wikipedia article before and after the change. i) user : If the user is unregistered, this field will have IP Address either in IPv4 or IPv6 format and if the user is register it will contain the username used when registering on Wikipedia.
Acknowledgements
I would like to thank hatnote.com from which I could get this data. If you need the original data you may visit www.hatnote.com or directly connect this WebSocket - ws://wikimon.hatnote.com/en/"
Automatic generation of Guard roles,lets build an primer for an automated guard distribution system,Paul Larmuseau,3,"Version 1,2017-08-29","healthcare
business",CSV,189 KB,ODbL,446 views,18 downloads,2 kernels,0 topics,https://www.kaggle.com/plarmuseau/geowacht,"Context
There are 4933 pharmacies in Belgium, and each pharmacy (in groups) are obliged to create a network of night-guard pharmacies covering complete Belgium. Compare it with a hospital that has 2000 nurses and want's to distribute the burden of the 'night' shift or 'weekend' shifts over the 2000 nurses on an 'equal foot' basis, but here we add a geographical aspect. So its a maximal covering location problem combined with an typical 'personel' planning problem
The challenge...
The distribution of the pharmacies follows certain rules: 11million inhabitants having access to 5000 pharmacies, you can estimate that each pharmacy serves 2200 inhabitants. This is approximately true. You see a glimpse of the guard kalender : blackpoints, feast days, day/night guard (sun/moon)
Each pharmacy is equal and has to do equal number of guards. That is in this description rounded 12 days guard. We give each pharmacy a guard-capital. Meaning when at the end of the year one pharmacy has done 10 days guard, the next year the pharmacy has to do two days guard more. On average each pharmacy is doing 12 days per year. So starting with an equal guard capital. We try to minimize the difference from the mean (mse).
The guard is divided in a day part from 9:00u-22:00 and from 22:00pm to 9:00am as night guard. Each Pharmacy can choose to do guard during 1 day, having 12 days and 6 nights distributed over the year. With at least 2 sunday guards per year and one sunday night. Or each pharmacy can choose to do his guard in blocks of 4 weekdays (Mo-Tu-We-Th /// Fr-Sa-So) Doing at least two midweek blocks and two weekendblocks ending up with 2 day's too much guard capital. From those blocks he get alternating fe the Mo and We a nightshift. Or by example the Fr/So or Sa as nightshift . The nightshifts are also equally distributed. The choice for midweek/weekend or day guard is a freedom indicated in the database. We filled in a random example. Usually the freedom collides with regions. So a dayblock and a nightblock each get one guard point.
Each customer has to find a pharmacy within 20 minutes from his home. On average this rule is easily obeyed, since its possible to find 3-5 pharmacies within 20 minutes in 'city' zones. Its only in very rural zones this rule can be violated. We use a GISS database to correctly calculate the distance and travelling time between each pharmacy. You can use google-api or haversine, internally we have exact data. But within this proof of concept this doesn't matter. Actually highway's are draining more people to a pharmacy, and the algorithm shows the pharmacy as a faster alternative than geographic haversine closer pharmacies. So a very fine tuned model takes this driving speed into account in function of that 20 minutes rule. But here the haversine distance between the closest clusters should give a good approximation. It actually counts only for the case where the 20 minutes rule is 'violated'
If we divide Belgium in 165 clusters there are 30 pharmacies per cluster. Each cluster has 1 pharmacy available for 66000 inhabitants within 20 minutes. This during DAYTIME. (Daytime is defined until 22:00u) At night the scheme HALVES. 82 clusters, with 60 pharmacies per cluster. Each night cluster has then 132.000 inhabitants. The same rule each cluster has 1 pharmacy available. We search to MAXIMIZE the DISTANCE between each guard-pharmacy , so that there is an maximal SPREAD for the guard. This guarantees that customers find very fast a pharmacy. If you think about it,on the Belgium card you can superimpose a 'grid' that is shifting each day and each night selecting a pharmacy in the intersections of the grid. The only interfering element here is that 50% of the pharmacies chooses to have guard in weekend/midweek scheme, and 50% wants day/night guards, hence you have to swap the guard between neighbouring pharmacy's, so the distance rule remains respected.
Each Pharmacy can block 3 weeks of vacation, that is typical during school vacations periods that pharmacy's tend to block periods. We call it black-points; You can generate random 3 weeks school/holiday vacation weeks that are blocked for each pharmacy. The database is filled in with a manual created sample.. Actually the pharmacist can block 3weeks, or 6 weekend and mid/week blocks. Here i simplified to three week (number of week , week of year)
Doing guard on a holiday like Christmas, New Year, Eastern, Sinksen, National Feast Day, is rewarded with an extra guard capital point. Those pharmacies can do as such 1 day less guard. This as a last twitch
What do we need at the end
1° a database of all pharmacy's and their guard capital points. We tend to minimize the difference with the mean capital points. And usually the current algorithm selects the first the pharmacies in a cluster with the lowest capital points as prime candidate.
2° a list of guards for all the 165clusters for all 365days, or 60225 guards per year. And a measurement that estimates the distance between all pharmacies. for that day.. The Mean square error of the distance between the pharmacies has to be minimized"
Top 100 Global Steel Producers (2011-2016),Steel is being produced on a global level in places that might surprise you,David Rubal,3,"Version 1,2017-08-17",,CSV,6 KB,CC0,930 views,103 downloads,,0 topics,https://www.kaggle.com/drubal/top-100-global-steel-producers-20112016,"Context
The production of steel has shifted from a just few primary countries to many countries all over the world. This dataset provides statistics and insight into the locations and volumes of steel production for years 2011-2016 with specific ranking in 2015 and 2016.
Acknowledgements
The data source is worldsteel Association. www.worldsteel.org
Inspiration
Some questions are trends, predictive analytics/forecasting/consolidation possibilities and global supply chain. While the steel industry is 'flat' where steel is produced and shipped globally. There may an opportunity for a new model where steel is increasingly produced locally to save shipping and logistics costs."
US Tariff Rates,Harmonized Tariff Rates as of July 2017,Sohier Dane,3,"Version 2,2017-09-15|Version 1,2017-09-15",economics,Other,9 MB,CC0,931 views,77 downloads,,,https://www.kaggle.com/sohier/us-tariff-rates,"This dataset includes the applicable tariff rates and statistical categories for all merchandise imported into the United States. It is based on the international Harmonized System, the global system of nomenclature that is used to describe most world trade in goods.
Although the USITC publishes and maintains the HTSA in its various forms, Customs and Border Protection is the only agency that can provide legally binding advice or rulings on classification of imports. Contact your nearest Customs office with questions about how potential imports should be classified. For a binding ruling on classification, contact the Bureau of Customs and Border Protection.
Content
The csv is a somewhat condensed version of a series of pdf documents. The row by row contents are generally comprehensive, but the pdf chapters often contain general information that is not included here.
Acknowledgements
This dataset was made available by the United States International Trade Commission. You can find the original dataset, updated regularly, here."
Geographically Annotated Civil War Corpus,Texts from the War of The Rebellion American Civil War archives,Rachael Tatman,3,"Version 1,2017-08-18","united states
geography
history
+ 2 more...",{}JSON,48 MB,Other,616 views,38 downloads,,0 topics,https://www.kaggle.com/rtatman/geographically-annotated-civil-war-corpus,"WarOfTheRebellion is an annotated corpus of data from War of The Rebellion (a large set of American Civil War archives). It was built using GeoAnnotate.
It consists of two parts: a toponym corpus and a document-geolocation corpus.
Document geolocation corpus
The document geolocation corpus is found in two JSON files.
wotr-docgeo-jan-5-2016-625pm-by-vol.json gives the spans by volume.
wotr-docgeo-jan-5-2016-625pm-80-0-20-by-split.json gives the spans by split, with an 80-20 training/test split.
In both cases, the JSON data for an individual span consists of the following information:
The volume number, from War of the Rebellion.
The span character offsets, from corrected OCR'd text.
The text of the span in question.
The counts of individual words, using the tokenization algorithm followed in the paper (FIXME, name of paper). They are stored in a string, with a space separating word-count pairs and a colon separating the word from the count. The word itself is URL-encoded, i.e. a colon is represented as %3A and a percent character as %25.
The date (if available), extracted from the text using regular expressions.
The full GeoJSON of the points and polygons annotated for the span.
The centroid of the points and polygons, computed first by taking the centroid of each polygon and then taking the centroid of the resulting set of annotated points and polygon-centroid points. The centroid is in the form of a size-2 array of longitude and latitude (the same as how points are stored in GeoJSON).
Toponym corpus
The Toponym corpus, otherwise known as WoTR-Topo, is given in two different formats. The first format is JSON format files, split into train and test. Geographic information for toponyms is given by the geojson standard, with annotations done in a stand-off style.
Not everything that has been annotated is guaranteed to be correct. The creators encourage others to correct errors that they find in a branched repository and submit pull requests when corrections are made.
For questions regarding the corpus, please contact its creators Ben Wing (ben@benwing.com) and Grant DeLozier (grantdelozier@gmail.com). This data is reproduced here under the MIT license. Please see the file “LICENSE” for more information.
The ACL LAW paper describing the corpus and performing benchmark evaluation"
Kwici Welsh Wikipedia Corpus,A 4 million word corpus of contemporary Welsh,Rachael Tatman,3,"Version 1,2017-08-30","languages
europe
linguistics",CSV,26 MB,CC4,479 views,24 downloads,,0 topics,https://www.kaggle.com/rtatman/kwici-welsh-wikipedia-corpus,"Content:
Kwici is a 4m-word corpus drawn from the Welsh Wikipedia as it was on 30 December 2013.
The final pages and articles dump for 2013 was downloaded from the Wikimedia dump page. The WikiExtractor tool written by Giuseppe Attardi and Antonio Fuschetto was then used to extract plain text (discarding markup etc) from the 165Mb dump, resulting in a 33Mb output file. This was tidied by removing remaining XML, blank lines, and blocks of English text.
The text was then split to give into a total of 360,477 sentences, and these were imported into a PostgreSQL database table. The sentences were pruned by removing all items less than 50 characters long, all items containing numbers only (eg timelines), and all duplicates, to give a final total of 204,789 sentences in the corpus.
The file contains the following fields:
id: unique identifier for the sentence;
welsh: the sentence in Welsh;
word_w: the number of words in the Welsh sentence.
Acknowledgements:
This dictionary was created by Kevin Donnell. If using Kwici in research, please use the following citation
Kevin Donnelly (2014). ""Kwici: a 4m-word corpus drawn from the Welsh Wikipedia."" http://cymraeg.org.uk/kwici. (BibTeX)
Inspiration:
Can you use this corpus to add frequency information to this Welsh dictionary?
Can you use this corpus to create a stemmer for Welsh?"
Trial and Terror,Database of US Terrorism Prosecutions and Sentencing Information,Jacob Boysen,3,"Version 2,2017-08-16|Version 1,2017-08-16",,CSV,487 KB,CC4,929 views,65 downloads,,0 topics,https://www.kaggle.com/jboysen/trial-and-terror,"Context:
This database of terrorism prosecutions and sentencing information was created using public records including three lists of prosecutions from the U.S. Department of Justice (from 2010, 2014, and 2015), court files available through the federal judiciary’s case management system, DOJ press releases, and inmate data from the Bureau of Prisons.
Content:
Trevor Aaronson created the first iteration of this database as part of a project funded by the Investigative Reporting Program at the University of California, Berkeley. Mother Jones magazine published that data in 2011, along with accompanying articles, in a package that is still available online. Beginning in 2016, Aaronson and Margot Williams collaborated to update and expand the database, with a new emphasis to include Bureau of Prisons data because so many post-9/11 terrorism defendants had been released. The cases include any prosecutions after September 11, 2001, that the U.S. government labeled as related to international terrorism. The Intercept first published this database on April 20, 2017. For each defendant in the database, U.S. criminal code data related to charges has been categorized according to this legend
Acknowledgements:
This database is licensed under Creative Commons for noncommercial uses with appropriate attribution. If you publish this database, in part or whole, you must credit Trevor Aaronson and Margot Williams.
Inspiration:
What are the most common charges?
Are the sentence lengths similar?"
Mapping the KKK 1921-1940,Location and Charter Date of over 2000 “Klaverns”,Jacob Boysen,3,"Version 1,2017-09-16","united states
sociology",CSV,304 KB,CC4,580 views,35 downloads,,0 topics,https://www.kaggle.com/jboysen/mapping-the-kkk,"Context:
Mapping the Klan is a rough timeline of the rise of the second Ku Klux Klan between 1915 and 1940. Each red dot shows a local unit or ""Klavern."" The official numbers for each Klavern indicate a basic chronology for the chartering of the Klaverns, and they also reveal patterns of Klan organizing.
Content:
The data for Mapping the Klan is based on a variety of sources, mostly newspapers sponsored by or sympathetic to the Ku Klux Klan. These publications reported on the activities of local units, known officially as Klaverns. Data includes approximate date of charter, location(lat/lon), nickname, source for data, and related notes.
Dates: The dates for each Klavern come from the publication listed for that entry. So, it is likely that the Klaverns identified were established even earlier than the date indicated. The Klan’s recruitment methods make it harder to accurately date the beginning of a Klavern. Each local group had to recruit a set number of members before it could get its charter and number.
Numbers: The Klaverns in each state were numbered in chronological order of their chartering. So we can assume that if a Klan number 40 is dated October 1923, Klans 1 to 39 were established before 1923.
As historians agree, the busiest years of Klan expansion were 1922-1924, with big declines thereafter. The large number of klaverns established after 1925, when the Ku Klux Klan largely disappeared from the national news media, is intriguing. The continued organizing of Klaverns after 1925 is more difficult to study, for lack of sources. That history remains to be explored. Learn more.
Acknowledgements:
Source data here available through the VCU Library site. Data was compiled by:
John Kneebone, lead author and professor of History, VCU
Shariq Torres, lead web developer and data co-author, VCU Libraries
Erin White, project manager, VCU Libraries
Lauren Work, digital collections, VCU Libraries
Alison Tinker, web designer, VCU Libraries
John Glover, digital humanities consultant, VCU Libraries
Inspiration:
Where was the densest concentrations of KKK?
What years saw the biggest rises?"
WMT15 Evaluation,Machine Translation Evaluation for WMT15,NLTK Data,3,"Version 1,2017-08-20",,Other,1 MB,Other,557 views,16 downloads,,0 topics,https://www.kaggle.com/nltkdata/wmt15-eval,"Context
The wmt15_eval dataset contains the files to machine translation evaluation output from Workshop on Machine Translation (WMT15).
NLTK uses this dataset to validate the machine translation BLEU score implementations.
Content
The wmt15_eval directory contains the files to evaluate MT evaluation metrics, it's not production standards data, neither will it be helpful in shared task participation but it provides a good testbed for new metrics implementation and comparison against metrics already available in nltk.translate.*_score.py to validate the numbers.
It includes the first 100 sentences from the newstest 2015 development set for the English-Russian language part, made available at Workshop for Machine Translation 2016 (WMT16) and the Google Translate of the English source sentences.
[Plaintext]
newstest-2015-100sents.en-ru.src.en
newstest-2015-100sents.en-ru.ref.ru
newstest-2015-100sents.en-ru.google.ru
[SGM]
newstest2015-100sents-enru-ref.ru.sgm
newstest2015-100sents-enru-src.en.sgm
newstest2015-100sents-enru-google.ru.sgm
And the original ,sgm files from WMT16:
newstest2015-enru-ref.ru.sgm
newstest2015-enru-src.en.sgm
The plaintext are converted from the .sgm files from the development sets in WMT with the following command:
sed -e 's/<[^>]*>//g; /^\s*$/d' newstest-2015.enru.src.en.sgm | head -n100 > newstest-2015-100sents.en-ru.src.en
sed -e 's/<[^>]*>//g; /^\s*$/d' newstest-2015.enru.ref.ru.sgm | head -n100 > newstest-2015-100sents.en-ru.ref.en
The tokenized versions of the natural text files above are processed using Moses tokenizer.perl:
~/mosesdecoder/scripts/tokenizer/tokenizer.perl -l ru < newstest-2015-100sents.en-ru.ref.ru > ref.ru
~/mosesdecoder/scripts/tokenizer/tokenizer.perl -l ru < newstest-2015-100sents.en-ru.google.ru > google.ru
~/mosesdecoder/scripts/tokenizer/tokenizer.perl -l en < newstest-2015-100sents.en-ru.src.en > src.en
The Google translate outputs are created on 25 Oct 2016 10am. using the English source sentences.
The newstest2015-100sents-enru-google.ru.sgm is created using the wrap-xml.perl tool in Moses:
~/mosesdecoder/scripts/ems/support/wrap-xml.perl ru newstest2015-100sents-enru-src.en.sgm Google < google.ru > newstest2015-100sents-enru-google.ru.sgm
The BLEU scores output from multi-bleu.perl is as such:
~/mosesdecoder/scripts/generic/multi-bleu.perl ref.ru < google.ru 
BLEU = 23.17, 53.8/29.6/17.6/10.3 (BP=1.000, ratio=1.074, hyp_len=1989, ref_len=1852)
The mteval-13a.output file is produced using the mteval-v13a.pl
~/mosesdecoder/scripts/generic/mteval-v13a.pl -r newstest2015-100sents-enru-ref.ru.sgm -s newstest2015-100sents-enru-src.en.sgm -t newstest2015-100sents-enru-google.ru.sgm  > mteval-13a.output
Acknowledgements
Credits go to the organizers of WMT15 and WMT16."
Perluniprops,Perl Unicode Properties,NLTK Data,3,"Version 1,2017-08-20",,Other,133 KB,Other,308 views,9 downloads,,0 topics,https://www.kaggle.com/nltkdata/perluniprops,"Context
The perluniprops dataset in NLTK is a subset of the index of Unicode Version 7.0.0 character properties in Perl
The Pythonic equivalence of the Perl Uniprops is created primarily to ease the porting of regex related Perl code to NLTK, inspired by this Stackoverflow question.
Content
The NLTK port of the Perl Uniprops contains the following character sets:
Close_Punctuation.txt
Currency_Symbol.txt
IsAlnum.txt
IsAlpha.txt
IsLower.txt
IsN.txt
IsSc.txt
IsSo.txt
IsUpper.txt
Line_Separator.txt
Number.txt
Open_Punctuation.txt
Punctuation.txt
Separator.txt
Symbol.txt"
CONLL Corpora,"CONLL Corpora (2000, 2002, 2007)",NLTK Data,3,"Version 1,2017-08-20",,Other,17 MB,Other,643 views,81 downloads,,0 topics,https://www.kaggle.com/nltkdata/conll-corpora,"Context
The canonical metadata on NLTK:
<package id=""conll2000"" name=""CONLL 2000 Chunking Corpus""
         webpage=""http://www.cnts.ua.ac.be/conll2000/chunking/""
         contact=""Erik Tjong Kim Sang (erikt@uia.ua.ac.be)""
         unzip=""1""
         />

<package id=""conll2002"" name=""CONLL 2002 Named Entity Recognition Corpus""
         webpage=""http://www.cnts.ua.ac.be/conll2002/ner/""
         unzip=""1""
         />


<package id=""conll2007"" name=""Dependency Treebanks from CoNLL 2007 (Catalan and Basque Subset)""
         webpage=""http://nextens.uvt.nl/depparse-wiki/DataDownload""
         contact=""Kepa Sarasola""
         copyright=""Copyright (C) 2007 The University of the Basque Country""
         license=""Creative Commons Attribution-NonCommercial-NoDerivativeWorks license""
         unzip=""0""
         />"
Genesis,The Genesis Book of the Bible,NLTK Data,3,"Version 1,2017-08-20",,Other,1 MB,Other,158 views,5 downloads,,0 topics,https://www.kaggle.com/nltkdata/genesis,"Context
The canonical metadata on NLTK:
<package id=""genesis"" name=""Genesis Corpus""
     copyright=""public domain""
     license=""public domain""
     unzip=""1""
     />"
Twitter Sample,Twitter Samples (subject to Twitter Developer Agreement),NLTK Data,3,"Version 1,2017-08-21",,Other,117 MB,Other,847 views,132 downloads,,0 topics,https://www.kaggle.com/nltkdata/twitter-sample,"Context
The canonical metadata on NLTK:
<package id=""twitter_samples"" name=""Twitter Samples""
     copyright=""Copyright (C) 2015 Twitter, Inc""
     license=""Must be used subject to Twitter Developer Agreement
      (https://dev.twitter.com/overview/terms/agreement)""
 note=""Sample of Tweets collected from the Twitter APIs,
       observing the 50k limit required by https://dev.twitter.com/overview/terms/policy#6._Be_a_Good_Partner_to_Twitter ""
     unzip=""1""
     />"
Web Text Corpus,A Corpus of Web Text,NLTK Data,3,"Version 1,2017-08-21",,Other,2 MB,Other,579 views,40 downloads,,0 topics,https://www.kaggle.com/nltkdata/web-text-corpus,"Context
The canonical metadata on NLTK:
 <package id=""webtext""
     name=""Web Text Corpus""
     unzip=""1""
     />"
Penn Tree Bank,A Sample of the Penn Treebank Corpus,NLTK Data,3,"Version 5,2017-11-16|Version 4,2017-11-16|Version 3,2017-11-16|Version 2,2017-08-21|Version 1,2017-08-21",,Other,2 MB,Other,"1,604 views",115 downloads,,0 topics,https://www.kaggle.com/nltkdata/penn-tree-bank,"Context
The canonical metadata on NLTK:
<package id=""ptb"" name=""Penn Treebank""
     copyright=""Copyright (C) 1995 University of Pennsylvania""
     license=""This is a stub for the full Penn Treebank Corpus version 3.""
     unzip=""1""
     />"
Reuters,Reuters-21578 benchmark corpus,NLTK Data,3,"Version 2,2017-11-16|Version 1,2017-08-21",,Other,6 MB,Other,460 views,33 downloads,,0 topics,https://www.kaggle.com/nltkdata/reuters,"Context
The canonical metadata on NLTK:
<package id=""reuters"" 
     name=""The Reuters-21578 benchmark corpus, ApteMod version""
     webpage=""http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html""
     license=""The copyright for the text of newswire articles and Reuters 
     annotations in the Reuters-21578 collection resides with Reuters Ltd. 
     Reuters Ltd. and Carnegie Group, Inc. have agreed to allow the free 
     distribution of this data *for research purposes only*.  
     If you publish results based on this data set, please acknowledge its use, 
     refer to the data set by the name 'Reuters-21578, Distribution 1.0', and 
     inform your readers of the current location of the data set.""
     unzip=""0""
     />"
GloVe: Global Vectors for Word Representation,"All Pre-trained word vectors from Twitter (25d, 50d, 100d, 200d)",JdPaletto,3,"Version 1,2018-02-23",nlp,Other,1 GB,CC0,100 views,21 downloads,,0 topics,https://www.kaggle.com/jdpaletto/glove-global-vectors-for-word-representation,"Overview
GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.
Content
Pre-trained word vectors. This data is made available under the Public Domain Dedication and License v1.0 whose full text can be found at: http://www.opendatacommons.org/licenses/pddl/1.0/
Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased, 25d, 50d, 100d, & 200d vectors)
Acknowledgements
Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation https://nlp.stanford.edu/pubs/glove.pdf
https://nlp.stanford.edu/projects/glove/
Inspiration
NLP Endeavors
Usage in Kaggle Kernels"
Asbestos Clean-up in Poland,Is Poland on track to be free of asbestos by 2032?,Obrocka,3,"Version 1,2018-02-14",pollution,CSV,122 KB,CC0,232 views,27 downloads,,0 topics,https://www.kaggle.com/pinsleepe/asbestos-cleanup-in-poland,"Context
First, what is asbestos? Well it is a mineral that can be pulled into fine fibres with high resistance to heat, electricity and chemical corrosion. In the past it was a common ingredient in construction materials (caveat: this is at least true for the European Union). Why in the past? Asbestos is a threat to health due to its very fibre structure. Those microscopic fibers can become trapped in the respiratory system, causing cancer and other disease decades after exposure.
Second, where is Poland?! The answer depends on how grumpy the internet is on that day. My home country, Poland, is located in the Eastern or Central Europe. Poland joined the European Union in 2004 and suddenly stuff was required of her. Strangely enough, Poland is the only European country that plans to be free of asbestos by 2032. The National Asbestos Cleaning Program program was initiated in 2009 with one of the aims to create a complete database of asbestos contamination by 2012. In this blog post I’m hoping to shed some light on the progress of this ambitious plan.
Content
The database is run by the Ministry of Development and should be updated yearly. It was originally uploaded on March 21st 2016 and then updated 8 months later. As far as I can see they don’t keep older versions. The spreadsheet contains columns with the total number of asbestos in the given location (in kilograms), how much of that has been utilised (also in kilograms) and how much still needs to be utilised (not kidding). There is also name of the place and its code TERYT. TERYT translates as the National Official Register of the Territorial Division of the Country. It is a very useful thing in identifying cities and regions, especially for languages that include certain letters with diacritics, the overdot, the tail and the stroke. As a side note, TERYT code for asbestos dataset was incomplete i.e. missing the last digit (!). In addition, there was no metadata that describes the data collection process or time when it was taken.
Acknowledgements
This dataset was downloaded from the Polish Public Data and is considered public data and can be used under following restrictions: - One should inform about the source of this data and the creation time of reused information as well
Inspiration
Is Poland on track to be free of asbestos by 20132?"
Grand Theft Auto 2017 Sao Paulo,Data from Sao Paulo State of all car thefts in 2017,Felipe Brunholi,3,"Version 2,2018-02-07|Version 1,2018-02-07","brazil
crime",CSV,21 MB,CC0,51 views,,,0 topics,https://www.kaggle.com/linhobru/grand-theft-auto-2017-sao-paulo,"Context
Car theft numbers in Brazil are ridiculously high. Sao Paulo reports numbers for the state in two different datasets: robbery and theft, every month.
Content
I will describe each column in next update
Acknowledgements
All data is available at SSP website: http://www.ssp.sp.gov.br/transparenciassp/ When reading the csv files, it helps using encoding='utf-16le', sep=""\t"", dayfirst=True as arguments in pd.read_csv()
Inspiration
I compiled all data from 2017 to study patterns, models of cars that are robbed the most, which brands are robbers favourites, cities and neighborhoods that are the most dangerous, etc. Feel free to explore it."
Crimes in Boston,"More than 2,60,760 crimes in Boston (2015- 2018)",AnkurJain,3,"Version 1,2018-02-07","united states
crime",CSV,9 MB,CC0,103 views,7 downloads,,0 topics,https://www.kaggle.com/ankkur13/boston-crime-data,"Context-
This is a dataset containing records from the new crime incident report system, which includes a reduced set of fields focused on capturing the type of incident as well as when and where it occurred.
Content-
This dataset has 2,60,760 rows and 17 columns.
INCIDENT_NUMBER:
OFFENSE_CODE:
OFFENSE_CODE_GROUP:
OFFENSE_DESCRIPTION:
DISTRICT:
REPORTING_AREA:
SHOOTING:
OCCURRED_ON_DATE:
YEAR:
MONTH:
DAY_OF_WEEK:
HOUR:
UCR_PART:
STREET:
LATITUDE:
LONGITUDE:
LOCATION:
Acknowledgements-
I would like to thank the Boston Police Department for making this dataset available to everyone.
Inspiration
How has crime changed over the years?
Is it possible to predict where or when a crime will be committed?
Which areas of the city have evolved over this time span?
In which area most crimes are committed?"
Valid US Addresses by Zip Code,Dataset containing all US Zip Codes with 10 Valid US Addresses,Christopher Lambert,3,"Version 1,2018-02-08","united states
north america
real estate
demographics",{}JSON,1 MB,CC0,781 views,22 downloads,,,https://www.kaggle.com/theriley106/valid-addresses-by-us-zip-code,"I needed to create this Dataset to scrape pricing info from Xfinity. I didn't find anything like this anywhere, so I decided to host it on Kaggle in case someone else needs it.
Here is the script used to scrape this data: https://gist.github.com/theriley106/b4fdb027c6bdc36e9f7109547348e147"
Unicode Samples,Demonstrate unicode encoding in chapter 10 of NLTK book,NLTK Data,3,"Version 1,2017-08-21",,Other,643 B,Other,169 views,7 downloads,,0 topics,https://www.kaggle.com/nltkdata/unicode-samples,"Context
The canonical metadata on NLTK:
<package id=""unicode_samples"" 
     name=""Unicode Samples""
     note=""A very small corpus used to demonstrate unicode encoding in chapter 10 of the book""
     unzip=""1""
     />"
Unix Words,A list of words that somehow exists on all Unix systems,NLTK Data,3,"Version 1,2017-08-21",,Other,2 MB,Other,164 views,14 downloads,,0 topics,https://www.kaggle.com/nltkdata/unix-words,"Context
 <package id=""words"" name=""Word Lists""
     webpage=""http://en.wikipedia.org/wiki/Words_(Unix)""
     license=""public domain""
     copyright=""public domain""
     unzip=""1""
     />
The nltk.corpus.words are words a list of words from http://en.wikipedia.org/wiki/Words_(Unix)
Which in Unix, you can do:
 ls /usr/share/dict/
See also:
https://unix.stackexchange.com/questions/286787/who-or-what-compiled-usr-share-dict-words
https://stackoverflow.com/questions/44449284/nltk-words-corpus-does-not-contain-okay"
WNCAATouneyChampions,WNCAA Past Champions for Google Cloud & NCAA® ML Competition 2018-Women's,Kathakali Seth,2,"Version 2,2018-02-24|Version 1,2018-02-24",,CSV,55 KB,GPL,29 views,5 downloads,,0 topics,https://www.kaggle.com/kathakaliseth/wncaatouneychampions,This dataset does not have a description yet.
USArrests,,Deepak Gupta,2,"Version 3,2018-02-01|Version 2,2018-02-01|Version 1,2018-02-01",,CSV,1 KB,CC0,80 views,16 downloads,,0 topics,https://www.kaggle.com/deepakg/usarrests,"Context
Violent Crime Rates by US State
Content
This data set contains statistics, in arrests per 100,000 residents for assault, murder, and rape in each of the 50 US states in 1973. Also given is the percent of the population living in urban areas.
Acknowledgements
World Almanac and Book of facts 1975. (Crime rates).
Statistical Abstracts of the United States 1975. (Urban rates).
References
McNeil, D. R. (1977) Interactive Data Analysis. New York: Wiley.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
Hawkeye Tennis Matches,,Frédéric Kosmowski,2,"Version 3,2018-02-13|Version 2,2018-02-13|Version 1,2018-02-13","tennis
sports",CSV,1020 KB,CC4,44 views,2 downloads,,0 topics,https://www.kaggle.com/fkosmowski/hawkeye-tennis-matches,This dataset does not have a description yet.
Advanced Pandas Exercises,Dataset with exercises for the Advanced Pandas Learn tutorial,Aleksey Bilogur,2,"Version 10,2018-02-14|Version 9,2018-02-14|Version 8,2018-02-14|Version 7,2018-02-11|Version 6,2018-02-11|Version 5,2018-02-11|Version 4,2018-02-11|Version 3,2018-02-10|Version 2,2018-02-10|Version 1,2018-02-10",,Other,16 KB,CC0,49 views,4 downloads,,0 topics,https://www.kaggle.com/residentmario/advanced-pandas-exercises,This dataset contains exercises for the Advanced Pandas Learn tutorial. It is not meant to be consumed separately.
OSM Russia. Central District,"OpenStreetMap Files (osm, geojson, pbf)",Olga Belitskaya,2,"Version 1,2018-02-06",geography,Other,579 MB,ODbL,42 views,0 downloads,,0 topics,https://www.kaggle.com/olgabelitskaya/osm-russia-central-district,"Context
OSM Russia. Central District is a real-world dataset with geo points.
Content
Files are in the .osm, . geojson and .pbf formats.
The files map.osm and map.geojson have a small size for quick training and preprocessing.
Acknowledgements
The database contains files from open internet sources:
OpenStreetMap;
OpenStreetMap Data Extracts.
All license conditions are the same with the original data.
Inspiration
Map preprocessing and analyzing are really important in data science and machine learning practice."
Philippine Stock Exchange Data,Daily OHLCV stock data from the Philippines,Ian Chu Te,2,"Version 1,2018-02-20","time series
business
money
regression analysis",CSV,17 MB,CC4,215 views,5 downloads,,,https://www.kaggle.com/ianchute/philippine-stock-exchange-data,"Philippine Stock Exchange Data
Daily OHLCV stock data from the Philippines"
Panic! at the Dataset,Panic at the Disco Discography w/ Lyrics &amp; Sentiment,Christopher Lambert,2,"Version 1,2018-02-09",,{}JSON,147 KB,CC0,566 views,6 downloads,,0 topics,https://www.kaggle.com/theriley106/panic-at-the-dataset,"Why not?
Here is a link to the script used to scrape this dataset"
700 Kumar Sangakkara Face Annotations,Annotations of the face of sporting star Kumar Sangakkara,Mirantha Jayathilaka,2,"Version 1,2018-02-13","cricket
sports
image processing
+ 2 more...",Other,6 MB,ODbL,64 views,,,0 topics,https://www.kaggle.com/mirantha/sangaface,"700 annotations of Kumar Sangakkara's face
Context
Recently I have been working on some object localization problems using Convolutional Nets and I wanted to try train the model on a new dataset other than the very common COCO or PASCAL VOC datasets. While pondering on what object to compile a small dataset around, I thought of pushing the challenge a bit more to see if the same model can be trained to localize faces. Having this in mind I wanted a dataset of a person's face annotations.
As you may know with Deep Learning models, the more data you have the more accuracy you reach. So considering the challenge to detect a face I wanted a considerable number of images of the same face that the model should be trained on.
Hence, I needed many pictures of the same person. So the person had to be famous so I could easily find many pictures. So being in Sri Lanka where else to look other than our Cricket stars. So I chose the living legend in Sri Lankan Cricket, Kumar Sangakkara.
Content
I downloaded around 1000 images from google images and after manual cleaning ended up with 704, which are contained here. I manually annotated all the pictures using a python script to generate the xml files. (Yeah, I couldn't find a better thing to do in that 2 hours.) Now here is the dataset for anyone to make use of.
The zip file attached contains two folders, images and annotations.
Inspiration
So as I mentioned in the above description, my goal with this dataset was to see if an object localization model can be used to detect a face of a person. Even though I have the pipeline, I couldn't still thoroughly test its performance using a GPU. So anyone whose interested can use this dataset to test those results. Also if these annotations can be useful for any other application, feel free to use it and share it. Have fun!"
French Tragedies,,SharkcpN,2,"Version 1,2018-02-12",,Other,2 MB,CC4,237 views,50 downloads,,0 topics,https://www.kaggle.com/sharkcpn/french-tragedies,This dataset does not have a description yet.
test.json,,Prayan,2,"Version 1,2018-02-10",,{}JSON,4 MB,CC0,35 views,,,0 topics,https://www.kaggle.com/rabinandan/testjson,This dataset does not have a description yet.
Data Breaches 2004-2017 (EN),English version of Data Breaches 2004-2017 greater than 30K records,Carlos E. Jimenez-Gomez,2,"Version 2,2018-02-20|Version 1,2018-02-19","databases
computer security
computing and society
+ 2 more...",CSV,96 KB,CC4,88 views,9 downloads,,0 topics,https://www.kaggle.com/estratic/data-breaches-2004-2017-en-20180218,"Context
Data breaches. Incidents in the world, that compromised more than 30000 records, between 2004 and 2017. English version. I wanted to visualize the data including the possibility to compare numbers between variable levels. I did some improvements in levels of variables as well as data, and I did a visualization. I also uploaded this version of the dataset in Spanish. I did the visualization with Tableau software.
In this post in my blog, you can read more about it: Spanish version and English version. You can also see the visualization in this link: Spanish version and English version.
Content
The dataset has 270 observations and 11 variables. Most of them, are categorical variables. Incidents happened between 2004 and 2017. Last updated: February 2018. Format: CSV2.
Variables (columns) [EN]:
Entity: name of the organization (public or private) that had the breach. String
Alternative Name: other known names of the entity. String
Story: tells a summary of what happened. String
Year: year of the breach. Date
Records Lost: number of records that the breach compromised.Integer
Sector: organization's main sector (or field of business). String
Method of Leak: main cause of the breach. String
1st source (link): 1st. url with more info about the breach. String
2nd source (link): 2nd. url with more info about the breach. String
3rd source (link): 3rd. url with more info about the breach. String
Source name: name of the source of news, official reports, blog, etc. included. Note that some of them have changed after I replaced some previous broken links that the original dataset had. String
Acknowledgements
Informationisbeautiful.net. Before the improvements, a first dataset was downloaded from this site, by the end of 2017.
Inspiration
The main question to be answered with the data visualization was ""What quantities of records were compromised by important data breaches, in organizations and sectors, between 2004 and 2017, and what was the reason?"". I wanted to have a visual answer that allows to compare numbers between year, sector, and method of leak. It would be great to improve the dataset adding new variables for data mining in the future. Achieving a complete and exhaustive ""Data Breaches 2004-2017"" dataset, would help to an in-depth analysis of incidents in this period. 2017 has been the worst year in the history."
Statlog (German Credit Data) Data Set,classifies people described by a set of attributes as good or bad credit risks,Miri Choi,2,"Version 1,2018-02-13","business
finance",CSV,92 KB,ODbL,116 views,21 downloads,,0 topics,https://www.kaggle.com/mirichoi0218/statlog-german-credit-data-data-set,"First of all, this dataset is not mine!
I just want to use this dataset to approve my machine learning skills.
So I upload this one! :-)
Hope you like it!
========================================================================================
This dataset classifies people described by a set of attributes as good or bad credit risks. Comes in two formats (one all numeric). Also comes with a cost matrix.
Source: Professor Dr. Hans Hofmann Institut f""ur Statistik und ""Okonometrie Universit""at Hamburg FB Wirtschaftswissenschaften Von-Melle-Park 5 2000 Hamburg 13
============================================================================================
Data Set Information:
Two datasets are provided. the original dataset, in the form provided by Prof. Hofmann, contains categorical/symbolic attributes and is in the file ""german.data"".
For algorithms that need numerical attributes, Strathclyde University produced the file ""german.data-numeric"". This file has been edited and several indicator variables added to make it suitable for algorithms which cannot cope with categorical variables. Several attributes that are ordered categorical (such as attribute 17) have been coded as integer. This was the form used by StatLog.
This dataset requires use of a cost matrix (see below)
..... 1 2
1 0 1
2 5 0
(1 = Good, 2 = Bad)
The rows represent the actual classification and the columns the predicted classification.
It is worse to class a customer as good when they are bad (5), than it is to class a customer as bad when they are good (1).
========================================================================================
UCI German Credit Data original version
You can find more details about variance, etc. > https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)
UCI German Credit Data Modify version
Here is an modify version of the original one. > https://github.com/stedy/Machine-Learning-with-R-datasets/blob/master/credit.csv"
Indoor Temp Over an Oven and Cooktop,Monitoring usage of cooking appliance to identify risks,Ray Dickenson,2,"Version 2,2018-02-14|Version 1,2018-02-14",,CSV,110 KB,CC0,112 views,3 downloads,2 kernels,,https://www.kaggle.com/rdickenson/cooktoptemp,"Context
Using measurements from an indoor temperature sensor mounted over a cooktop, can you help monitor the welfare of an elderly person who wants to live at home later in life?
Content
Indoor Temperature over the Cooktop
The dataset contains temperature measurements from an indoor sensor mounted 30cm over a combination cooktop-oven. Data were collected over a 100 day period. Measurements were recorded only when the temperature changed by 1 degree C or more and at a minimum of every 15 minutes.
Outside Temperature and Relative Humidity
Also included are outside air temperatures and relative humidity collected in the same region during the same period.
Other factors include:
The data include round-the-clock measurements including typical meal preparation times and long periods (overnight) when the kitchen is not being used.
Ambient temperature in the house varies between 15C - 30C.
The home may be under air conditioning or with the windows and sliding doors open when outside temperature and humidity allow it; use the associated climate data to help determine which state the house is in.
When the air conditioning is in use, the thermostat keeps the house between 24C - 25C.
There is an incandescent light bulb about 20cm over the sensor that may be on or off for long periods.
First use of the cooktop is typically to prepare coffee between 05:30 and 07:00; any absence of this event is likely an indicator of trouble.
Acknowledgements
Indoor temperatures were collected in a home setting by the poster. Outdoor temperature and humidity are accessed from the National Climatic Data Center, U.S. Climate Reference Network (USCRN/USRCRN), via anonymous ftp at: ftp://ftp.ncdc.noaa.gov/pub/data/uscrn/products/hourly02
Inspiration
Can you help monitor the welfare of elderly residents of a home so they can safely live independently later in life? Can you establish a profile of typical use of the oven and cooktop and then detect anomalies that signal the occupant(s) are in trouble and may need help? If so, a simple monitoring system can alert family members to check on their elderly parents or grand parents.
Can you detect these trouble events, that exist in this dataset and possibly in future data? - A saucepan was left on the cooktop too long creating a fire hazard - Normal meal preparation patterns have been interrupted requiring a check on the occupant(s)"
School Shootings US 1990-present,Record of all school shooting incidents since 1990,ecodan,2,"Version 3,2018-02-18|Version 2,2018-02-18|Version 1,2018-02-18",,Other,103 KB,CC0,153 views,30 downloads,,0 topics,https://www.kaggle.com/ecodan/school-shootings-us-1990present,"Context
Another week, sadly another school shooting.
To better understand the facts I went looking for data and found it difficult to come by - often embedded in other datasets or fragmented and unusable. I decided to create my own compilation based on a mashup of the Pah/Amaral/Hagan research on school shootings with the Wikipedia article from 1990 to present.
Content
pah_wikp file: A list of all school shooting incidents from 1990 to present.
Fields:
Date: date of incident
City: location of incident
State: location of incident
Area Type: urban or suburban (only in Pah dataset)
School: C = college, HS = high school, MS = middle school, ES = elementary school, - = unknown
Fatalities: # killed
Wounded: # wounded (only in Wikipedia dataset)
Dupe: whether this incident appears in both datasets. Note: only the ""Pah"" version of the incident is marked.
Source: Pah or Wikp
Desc: text description of incident (only in Wikipedia dataset)
cps file: US census data on school populations. Fields should be fairly self explanatory.
Acknowledgements
Thanks to the authors referenced above as well as the Wikipedia contributors!
Inspiration
Why are school shootings (and death counts) increasing over time?
How does the risk of being killed in a school shooting compare with other risks?
Are some schools / cities / states at higher risk?
Is there a correlation between countermeasures and a decrease in fatalities?
What else correlates with school shooting risks? In addition to firearms and the people who wield them, is there any clear causality?"
Austin Animal Center Shelter Outcomes,"30,000 shelter animals",AaronSchlegel,2,"Version 1,2018-02-17",animals,CSV,3 MB,ODbL,118 views,5 downloads,,0 topics,https://www.kaggle.com/aaronschlegel/austin-animal-center-shelter-outcomes-and,"Context
The Austin Animal Center is the largest no-kill animal shelter in the United States that provides care and shelter to over 18,000 animals each year and is involved in a range of county, city, and state-wide initiatives for the protection and care of abandoned, at-risk, and surrendered animals.
As part of the City of Austin Open Data Initiative, the Austin Animal Center makes available its collected dataset that contains statistics and outcomes of animals entering the Austin Animal Services system.
The dataset was explored in a series of Jupyter Notebooks that end with a pipeline prediction model written in scikit-learn. The series of notebooks can be viewed here:
Part One - Downloading, Cleaning and Feature Engineering the AAC Shelter Outcome dataset
Part Two - Exploratory Data Analysis of Shelter Cat Outcomes with Pandas and Seaborn
Content
The dataset contains shelter outcomes of several types of animals and breeds from 10/1/2013 to the present with a hourly time frequency. The data is updated daily.
The Austin Animal Center's original dataset has the following column entries:
Animal ID
Name
DateTime
MonthYear
Date of Birth
Outcome Type
Outcome Subtype
Animal Type
Sex upon Outcome
Age upon Outcome
Breed
Color
The additional data set filtered for only shelter cat outcomes has the additional columns added as part of the cleaning and feature engineering steps.
Sex
Spay/Neuter
Periods
Period Range
outcome_age_(days)
outcome_age_(years)
Cat/Kitten (outcome)
sex_age_outcome
age_group
dob_year
dob_month
dob_monthyear
outcome_month
outcome_year
outcome_weekday
outcome_hour
breed1
breed2
cfa_breed
domestic_breed
coat_pattern
color1
color2
coat
Acknowledgements
The dataset is provided by the wonderful folks at the Austin Animal Center, the largest no-kill animal shelter in the United States. The AAC makes available the data on the Austin Open Data Portal. More information on the dataset can be found one the Shelter Outcomes page.
Inspiration
The inspiration for sharing this dataset and the associated notebooks is to spread awareness and provide another set of data to help support and care for the animals who need it most. By increasing the amount of data and knowledge around best practices and data analysis, those in the animal welfare community can more effectively respond and identify animals that need more support to avoid unwanted outcomes."
Movement coordination in trawling bats,Explore how bats interact during flight,Rob Harrand,2,"Version 1,2017-08-24",,CSV,14 MB,CC0,529 views,25 downloads,,0 topics,https://www.kaggle.com/tentotheminus9/movement-coordination-in-trawling-bats,"Context
This dataset comes from a study into the movement of bats by researchers at the University of Bristol, UK. I found it whilst exploring the open datasets at the Movebank Data Repository, a site dedicated to animal tracking data.
Content
The datasets contain information on the position and timestamps for multiple bats. The type of movement (individual or paired) is also included. See the readme.txt file for much more information.
Acknowledgements
I did not create this data. Full citations are below,
Data, Holderied M, Giuggioli L, McKetterick TJ (2015) Data from: Delayed response and biosonar perception explain movement coordination in trawling bats. Movebank Data Repository. doi:10.5441/001/1.62h1f7k9
Associated paper (open access), Giuggioli L, McKetterick TJ, Holderied M (2015) Delayed response and biosonar perception explain movement coordination in trawling bats. PLOS Computational Biology. doi:10.1371/journal.pcbi.1004089.t001"
Finance ₹ - India,Statewise India's finance detail from 1980 to 2015.,Rajanand Ilangovan / இராஜ்ஆனந்த் இளங்கோவன்,2,"Version 1,2017-08-27",,CSV,48 KB,CC4,704 views,251 downloads,,0 topics,https://www.kaggle.com/rajanand/finance-india,"Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks.
Content
This dataset contains the various finance detail of India.
Aggregate expenditure.
Capital expenditure.
Social sector expenditure.
Revenue expenditure.
Revenue deficit.
Gross fiscal deficit.
Own tax revenues.
Nominal GSDP series.
Granularity: Annual Time period: 1980-81 to 2015-16. Amount: in crore rupees (i.e, 1 crore = 10 million)
Acknowledgements
National Institution for Transforming India (NITI Aayog)/Planning commission, Govt of India has published this data on their website."
Ebay Motorcycle Prices,Ebay Motorcycle Listings,Dan,2,"Version 1,2017-08-23","business
internet",Other,1 MB,CC0,463 views,76 downloads,,0 topics,https://www.kaggle.com/dan195/ebaymotorcycles,"Content
The dataset was scraped from Ebay's site on August 21st. It shows all listings available at that time.
Inspiration
What are the most popular bikes? What are the most popular types of bikes? Any trends with respect to sellers?"
UCDP Georeferenced Event Dataset,UCDP Georeferenced Event Dataset Version 17.1,Kheirallah Samaha,2,"Version 1,2017-08-26",,CSV,67 MB,CC0,536 views,37 downloads,,0 topics,https://www.kaggle.com/khsamaha/ucdp-georeferenced-event-dataset,"Context
The basic unit of analysis for the UCDP GED dataset is the “event”, i.e. an individual incident (phenomenon) of lethal violence occurring at a given time and place. This version authored by: Mihai Croicu, Ralph Sundberg, Ph. D.
http://www.ucdp.uu.se/downloads/
please check the attached PDF Codebook
Content
The dataset contains 135 181 events. GED 17.1 is a global dataset that covers the entirety of the Globe (excluding Syria) between 1989-01-01 and 2016-12-31. The maximum (best) spatial resolution of the dataset is the individual village or town. The dataset is fully geocoded. The maximum (best) temporal resolution of the dataset is the day.
Only events linkable to a UCDP/PRIO Armed Conflict, a UCDP Non-State Conflict or a UCDP One-Sided Violence instance are included. Events are included for the entire period, i.e. both for the years when such conflicts were active and for the years when such conflicts where not active.
UCDP GED 17.1 is compatible with the 17.1 series of UCDP datasets
The UCDP GED 17.1 is (mostly) backwards compatible with UCDP GED versions 1.0-5.0. Check the compatibility notes below for further details. Significant changes have been made in the actor, dyad and actor/side id meaning these identifiers are no longer backwards compatible.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
The maximum (best) spatial resolution of the dataset is the individual village or town. The dataset is fully geocoded. The maximum (best) temporal resolution of the dataset is the day.
Inspiration
Hopefully, learn from wars."
Sentiment Labelled Sentences Data Set,Public Data Set By University Of California For Sentiment Analysis,rahul kumar,2,"Version 1,2017-08-31",,Other,200 KB,ODbL,"1,280 views",139 downloads,,0 topics,https://www.kaggle.com/rahulin05/sentiment-labelled-sentences-data-set,"Context
Data for Sentiment analysis
Content
The Data has sentences from 3 sources IMDB Reviews Yelp Reviews Amazon Reviews Each line in Data Set is tagged positive or negative
Acknowledgements
https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences#
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
OECD macroeconomic data,Macroeconomic indicators for all OECD member states,XavierMartinezBartra,2,"Version 3,2017-08-26|Version 2,2017-08-26|Version 1,2017-08-26","business
politics
economics",CSV,37 MB,ODbL,342 views,60 downloads,2 kernels,,https://www.kaggle.com/xavier14/oecd-macroeconomic-data,OECD countries full macroeconomic indicators
Eurfa Welsh Dictionary,"212,403 word dictionary of Welsh",Rachael Tatman,2,"Version 1,2017-08-30","languages
europe
linguistics",CSV,15 MB,GPL,384 views,27 downloads,,0 topics,https://www.kaggle.com/rtatman/eurfa-welsh-dictionary,"Context:
Welsh is a member of the Brittonic branch of the Celtic languages. It is spoken natively in Wales, by some in England, and in Y Wladfa (the Welsh colony in Chubut Province, Argentina). Historically, it has also been known in English as ‘Cambrian’, ‘Cambric’ and ‘Cymric’. The current number of Welsh speakers in Wales is over 562,000.
Content:
Eurfa is the largest Welsh dictionary under a free license, and it was the first dictionary of a Celtic language to list verbal inflections and mutated forms. It also includes in-context citations for most words from a number of corpora:
Bilingual (Welsh-English, Welsh-Spanish):
The 18m-word Kynulliad3 corpus (K3). This contains formal written Welsh (the majority of it translated from English).
The 450k-word Siarad corpus (S). These transcribed conversations contain ""Welsh as she is spoke"", including English codeswitches. For readability, the version here (download) removes much of the transcription marking.
The 200k-word Patagonia corpus (P). These transcribed conversations contain spoken Welsh from Patagonia. This has fewer codeswitches, and many of them are in Spanish rather than English. For readability, the version here (download) removes much of the transcription marking.
The 200k-word Korrect/Kywiro corpus (Ko). This contains Welsh translations of English text in free/open software programs.
Monolingual (Welsh only)
A 220k-word subset of the 300k-word CIG1 child (18-30 months) language acquisition corpus (Kig1), containing non-child utterances only. The version here removes much of the transcription marking.
A 100k-word subset of the 560k-word CIG2 child (3-7 years) language acquisition corpus (Kig2), containing non-child utterances only. The version here removes much of the transcription marking.
Acknowledgements:
This dictionary was created by Kevin Donnelly and is redistributed here under the GNU General Public License. For more information, see the attached LICENSE file.
You may also like:
4 million word corpus of contemporary Welsh"
WNBA Player stats Season 2016-2017,"Points, Assists, Height, Weight and other personal details and stats",Thomas De Jonghe,2,"Version 1,2017-08-25",sports,CSV,20 KB,Other,"1,036 views",144 downloads,2 kernels,0 topics,https://www.kaggle.com/jinxbe/wnba-player-stats-2017,"Context
Scraped and copied from http://www.wnba.com/stats/player-stats/#?Season=2017&SeasonType=Regular%20Season&PerMode=Totals + http://www.wnba.com/ in general for the bio data.
Content
Stats from all games of season 2016-2017
G = Games Played
MIN = Minutes Played
FGM = Field Goals Made
FGA = Field Goals Attempts
FG% = Field Goals %
3PM = 3Points Made
3PA = 3Points Attempts
3P% = 3Points %
FTM = Free Throws made
FTA = Free Throws Attempts
FT% = Free Throws %
OREB = Offensive Rebounds
DREB = Defensive Rebounds
REB = Total Rebounds
AST = Assists
STL = Steals
BLK = Blocks
TO = Turnovers
PTS = Total points
DD2 = Double doubles
TD3 = Triple doubles
Inspiration
Compare WNBA to NBA in best players, average heights, ..."
World Countrywise Population Data 1980 - 2010,Open EI countrywise population data set for exploration,Prasanna Nadimpalli,2,"Version 1,2017-08-31","world
populated places
humans",CSV,58 KB,Other,340 views,29 downloads,,0 topics,https://www.kaggle.com/dataswimmer/population19802010,"Context
A download of the population data for different countries from Open EI data sets.
Content
Pretty simple, the First column is Country name and then follow the population figures (In Millions) for the years 1980-2010
Acknowledgements
I don't own anything. This is for pure exploration. Source -> https://openei.org/datasets/dataset/population-by-country-1980-2010
Inspiration
There is a great deal of information here. Explore this data set exclusively or join with other data sets that need this information."
notMNIST dataset,Used in Udacity's Deep Learning MOOC,lubaroli,2,"Version 1,2017-08-22",,Other,8 MB,Other,836 views,75 downloads,,0 topics,https://www.kaggle.com/lubaroli/notmnist,"Context
This dataset was created by Yaroslav Bulatov by taking some publicly available fonts and extracting glyphs from them to make a dataset similar to MNIST. There are 10 classes, with letters A-J.
Content
A set of training and test images of letters from A to J on various typefaces. The images size is 28x28 pixels.
Acknowledgements
The dataset can be found on Tensorflow github page as well as on the blog from Yaroslav, here.
Inspiration
This is a pretty good dataset to train classifiers! According to Yaroslav:
Judging by the examples, one would expect this to be a harder task than MNIST. This seems to be the case -- logistic regression on top of stacked auto-encoder with fine-tuning gets about 89% accuracy whereas same approach gives got 98% on MNIST. Dataset consists of small hand-cleaned part, about 19k instances, and large uncleaned dataset, 500k instances. Two parts have approximately 0.5% and 6.5% label error rate. I got this by looking through glyphs and counting how often my guess of the letter didn't match it's unicode value in the font file.
Enjoy!"
WEATHER ANALYSIS,A dataset for a specific period of time to predict the fututre.,Raghavi,2,"Version 1,2017-08-23",,CSV,78 KB,CC0,643 views,81 downloads,,0 topics,https://www.kaggle.com/raghavi9607/weather-analysis,"Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
Wikidata Property Ranking,Relevance judgments for properties of 350 Wikidata entities,SimonRazniewski,2,"Version 2,2017-08-23|Version 1,2017-08-22",,CSV,385 KB,CC4,402 views,14 downloads,,0 topics,https://www.kaggle.com/srazniewski/wikidatapropertyranking,"A set of preference judgements among generated random property pairs for 350 random Wikidata persons. For each (entity, property1, property2) record, 10 annotators judged which of the two properties is more interesting for the respective entity.
The goal is then to predict the annotator judgments as good as possible.
Current state-of-the-art methods (Wikidata Property Suggester and others) achieve 61% precision in this task, while methods based on linguistic similarity get to 74%, still significantly below annotator agreement (87.5%).
Further details are in the paper ""Doctoral Advisor or Medical Condition: Towards Entity-specific Rankings of Knowledge Base Properties"", ADMA 2017, available at http://www.simonrazniewski.com/2017_ADMA.pdf"
Europarl annotated for speaker gender and age,Proceedings of the European Parliament annotated for speaker gender and age,EllaRabinovich,2,"Version 1,2017-08-26",,Other,380 MB,Other,289 views,18 downloads,,0 topics,https://www.kaggle.com/ellarabi/europarl-annotated-for-speaker-gender-and-age,"Context
Proceedings of the European Parliament (http://statmt.org/europarl/) annotated for speaker gender and age at the sentence-level. The dataset was used for work on personalized machine-translation; specifically, preserving authorial gender traits during the automatic translation process. The corpus contains two parallel subcorpora: English-French and English-German. For further details about the dataset please refer to http://aclanthology.coli.uni-saarland.de/pdf/E/E17/E17-1101.pdf."
Global Terrorism Database_compact version,"A slimmer version, 58 columns vs 135, of the GTD 1970-2016 dataset.",Jan Nordin,2,"Version 1,2018-02-15",terrorism,CSV,17 MB,Other,72 views,7 downloads,,0 topics,https://www.kaggle.com/northon/globalterrorismdatabase-compact,"Context
This is the data set of the 170.000+ global terrorist attacks during 1970-2016. I've removed many of the text columns, since these can be found as categorized data as well. The number of columns in this data set is 58. In the original source file it was 135.
Content
The columns in this data set are the following: eventid iyear imonth iday extended country country_txt region region_txt provstate city latitude longitude specificity vicinity location summary crit1 crit2 crit3 doubtterr alternative alternative_txt multiple success suicide attacktype1 attacktype1_txt targtype1 targtype1_txt targsubtype1 targsubtype1_txt corp1 target1 natlty1 natlty1_txt gname motive guncertain1 individual nperps nperpcap claimed compclaim weaptype1 weaptype1_txt weapsubtype1 weapsubtype1_txt nkill nkillter nwound nwoundte property propextent propextent_txt ishostkid nhostkid dbsource"
UDHR Corpus,Universal Declaration of Human Rights Corpus,NLTK Data,2,"Version 1,2017-08-21",,Other,9 MB,Other,163 views,7 downloads,,0 topics,https://www.kaggle.com/nltkdata/udhr-corpus,"Context
The canonical metadata on NLTK:
<package id=""udhr2"" name=""Universal Declaration of Human Rights Corpus (Unicode Version)""
     webpage=""http://unicode.org/udhr/""
     license=""public domain""
     copyright=""public domain""
     unzip=""1""
     />"
VerbNet,"VerbNet Lexicon, Version 2.1",NLTK Data,2,"Version 1,2017-08-21",,Other,2 MB,Other,402 views,11 downloads,,0 topics,https://www.kaggle.com/nltkdata/verbnet,"Context
The canonical metadata on NLTK:
<package id=""verbnet""
     name=""VerbNet Lexicon, Version 2.1""
     version=""2.1""
     author=""Karin Kipper-Schuler""
     webpage=""http://verbs.colorado.edu/~mpalmer/projects/verbnet.html""
     license=""Distributed with permission of the author.""
     unzip=""1""
     />"
Universal Treebank,Universal Treebanks Version 2.0,NLTK Data,2,"Version 1,2017-08-21",,Other,114 MB,Other,209 views,11 downloads,,0 topics,https://www.kaggle.com/nltkdata/universal-treebank,"Context
The canonical metadata on NLTK:
 <package id=""universal_treebanks_v20"" name=""Universal Treebanks Version 2.0""
     license=""Creative Commons Attribution-NonCommercial-ShareAlike 3.0 United States""
     webpage=""https://code.google.com/p/uni-dep-tb/""
     unzip=""0""
     />"
WordNet,WordNet 3.0 and Information Content,NLTK Data,2,"Version 1,2017-08-21",,Other,67 MB,Other,259 views,20 downloads,,0 topics,https://www.kaggle.com/nltkdata/wordnet,"Context
 <package id=""wordnet"" name=""WordNet""
     version=""3.0""
     license=""Permission to use, copy, modify and distribute this software and 
     database and its documentation for any purpose and without fee or royalty is 
     hereby granted, provided that you agree to comply with the following copyright 
     notice and statements, including the disclaimer, and that the same appear on ALL 
     copies of the software, database and documentation, including modifications that 
     you make for internal use or for distribution.... [see webpage for full license]""
     copyright=""WordNet 3.0 Copyright 2006 by Princeton University.  All rights reserved.""
     webpage=""http://wordnet.princeton.edu/""
     unzip=""1""
     />



 <package id='wordnet_ic' name='WordNet-InfoContent'
     version='3.0'
     webpage='http://wn-similarity.sourceforge.net'
     unzip=""1""
     />"
Subtitles of The Eleventh House podcast,Find out hidden meaning behind current affairs,Binks,2,"Version 1,2017-08-20","popular culture
storytelling
politics
+ 2 more...",CSV,19 MB,Other,518 views,26 downloads,,0 topics,https://www.kaggle.com/binksbiz/robert,"Context:
Youtube has introduced automatic generation of subtitles based on speech recognition of uploaded video. This dataset provides collection of subtitles Robert Phoenix The 11th House uploaded podcasts. It serves as database for an introduction to algorithmic analysis of spoken language.
From the podcasts author description: “The Eleventh House is the home of Robert Phoenix, a journalist, blogger, interviewer, astrologer and psychic medium with over 30 years experience in personal readings and coaching, and has been a media personality in TV and radio. The 11th house delves into the supernatural, geopolitics, exopolitics, conspiracy theories, and pop culture.”
Content:
The 11th House speeches dataset consists of 543 subtitles (sets of words) retrieved from Youtube playlists: https://www.youtube.com/user/FreeAssociationRadio/videos
This dataset consists of a single CSV file RobertPhoenixThe11thHouse.csv. The columns are: 'id', 'playlist', 'upload_date', 'title', 'view_count', 'average_rating', 'like_count', 'dislike_count', 'subtitles', which are delimited with a comma.
Text data in columns 'subtitles' is not sentence based, there are not commas or dots. It is only stream of words being translated from speech into text by GoogleVoice (more here https://googleblog.blogspot.com.au/2009/11/automatic-captions-in-youtube.html).
Acknowledgements:
The data was downloaded using youtube-dl package.
Inspiration:
I'm interested in a deeper meaning behind current affairs. (For example see http://www.blogtalkradio.com/freeassociationradio)"
New York Taxi Trip enriched by Mathematica,Support dataset for New York Taxi Trip Duration Playground,Wol4ara_Vio,2,"Version 1,2017-08-21",,CSV,382 MB,CC0,250 views,16 downloads,,0 topics,https://www.kaggle.com/wol4aravio/ny-taxi-trip-duration-enriched-by-mathematica,"Context
This data set was created to help Kaggle users in the New Your City Taxi Trip Duration competition. New features were generated using Wolfram Mathematica system.
Hope that this data set will help both young and experienced researchers in their data mastering path.
All sources can be found here.
Content
Given dataset consists of both features from initial dataset and generated via Wolfram Mathematica computational system. Thus, all features can be split into following groups:
Initial features (extracted from initial data),
Calendar features (contains of season, day name and day period),
Weather features (information about temperature, snow, and rain),
Travel features (geo distance with estimated driving distance and time).
Dataset contains the following columns:
id - a unique identifier for each trip,
vendorId - a code indicating the provider associated with the trip record,
passengerCount - the number of passengers in the vehicle (driver entered value),
year,
month,
day,
hour,
minute,
second,
season,
dayName,
dayPeriod - day period, e.g. late night, morning, and etc.,
temperature,
rain,
snow,
startLatitude,
startLongitude,
endLatitude,
endLongitude,
flag - this flag indicates whether the trip record was held in vehicle memory before sending to the vendor because the vehicle did not have a connection to the server - Y=store and forward; N=not a store and forward trip,
drivingDistance - driving distance, estimated via Wolfram Mathematica system,
drivingTime - driving time, estimated via Wolfram Mathematica system,
geoDistance - distance between starting and ending points,
tripDuration - duration of the trip in seconds (value -1 indicates test rows)."
Reviews - TripAdvisor (hotels) & Edmunds (cars),"Full reviews from Tripadvisor (~259,000) & Edmunds (~42,230)",Emil Nikolov,2,"Version 1,2017-08-18",,Other,341 MB,CC0,359 views,45 downloads,,0 topics,https://www.kaggle.com/enikolov/reviews-tripadvisor-hotels-and-edmunds-cars,"From: http://kavita-ganesan.com/entity-ranking-data
Downloads: - [Dataset] (https://code.google.com/p/dataset/downloads/detail?name=OpinRankDataset.zip&can=2&q=) - Only reviews (~98MB) [[ readme ] (http://kavita-ganesan.com/sites/default/files/OpinRankDataset.pdf)]
[Dataset with Judgments] (http://kavita-ganesan.com/modules/pubdlcnt/pubdlcnt.php?file=https://github.com/kavgan/OpinRank/releases/download/OpinRank/OpinRankDatasetWithJudgments.zip&nid=141) - Reviews and Relevance Judgments (~100MB) [[ readme ] (http://kavita-ganesan.com/sites/default/files/OpinRankDatasetWithJudgments.pdf)]
OpinRank Dataset - Reviews from TripAdvisor and Edmunds Dataset Type: Text Format: Full reviews from Tripadvisor (~259,000 reviews) and Edmunds (~42,230 reviews) Domain: hotels, cars How to cite dataset: [ bib ]
Citing Dataset [ bib ] If you use this dataset for your own research please cite the following to mark the dataset:
Ganesan, K. A., and C. X. Zhai, ""Opinion-Based Entity Ranking"", Information Retrieval.
@article{ganesan2012opinion, title={Opinion-based entity ranking}, author={Ganesan, Kavita and Zhai, ChengXiang}, journal={Information retrieval}, volume={15}, number={2}, pages={116--150}, year={2012}, publisher={Springer} }
Dataset Overview This data set contains full reviews for cars and and hotels collected from Tripadvisor (~259,000 reviews) and Edmunds (~42,230 reviews).
Car Reviews Dataset Description
Full reviews of cars for model-years 2007, 2008, and 2009 There are about 140-250 cars for each model year Extracted fields include dates, author names, favorites and the full textual review Total number of reviews: ~42,230 Year 2007 -18,903 reviews Year 2008 -15,438 reviews Year 2009 - 7,947 reviews Format There are three different folders (2007,2008,2009) representing the three model years. Each file (within these 3 folders) would contain all reviews for a particular car. The filename represents the name of the car. Within each car file, you would see a set of reviews in the following format:
06/15/2009 The author The review goes here.. What are my favorites about this car
Note that each review is enclosed within a element as shown above and all the extracted items are within this element.
Hotel Reviews Dataset Description
Full reviews of hotels in 10 different cities (Dubai, Beijing, London, New York City, New Delhi, San Francisco, Shanghai, Montreal, Las Vegas, Chicago) There are about 80-700 hotels in each city Extracted fields include date, review title and the full review Total number of reviews: ~259,000 Format There should be 10 different folders representing the 10 cities mentioned earlier. Each file (within these 10 folders) would contain all reviews related to a particular hotel. The filename represents the name of the hotel. Within each file, you would see a set of reviews in the following format:
Date1Review title1Full review 1 Date2Review title2Full review 2 ................ ................
Each line in the file represents a separate review entry. Tabs are used to separate the different fields."
"13,000 Screen Capture Images + How to Get More","13,000 Screen Caps Scraped from prnt.sc, Plus Description of How to Collect More",Mitchell J,2,"Version 1,2017-08-31","artificial intelligence
internet",Other,498 MB,Other,411 views,28 downloads,,0 topics,https://www.kaggle.com/datasnaek/lightshot,"Story
This is a set of 13,000 images from the site https://prnt.sc/. It is a site that enables users to easily upload images, either through the web interface, or, most commonly, through the downloadable screen cap tool which enables easy selection and uploading of an area of your screen. As you can see on their homepage, at the point of posting this, they have almost a billion images uploaded. The amount of information in there will be incredible, it’s an information enthusiast dream. Around 2 years ago I discovered this, and I thought it was interesting to mass download these images with a tool I created, but I was manually looking at every single image. As I became more interested in machine learning, I figured experimenting with the 20,000 or so images I had downloaded at the time from the site would be interesting, especially since because of the nature of the site and its ease of access, it gets used for a few very specific purposes which is very useful for image categorisation. Video games are an extremely popular use, pictures of people, animations, screenshots of chats and the most popular; debugging or technical help. Anyone in this field knows people are not particularly conscious of where they put information. I’m sure you can imagine some of the interesting nuggets of info in here just waiting to be found. I was able to find a fair amount just using a retrained Inception CNN and some OCR. I manually categorised just over 5,600 images into 6 categories:
Animations
Games
Objects
People
Text
A very specific kind of animated game character uploaded frequently enough to deserve its own category
I was able to achieve around 85% accuracy for categorisation with the rest of the image set (I have 1,000,000 images downloaded from the site total) using just those manually categorised 5,609 images.
Image Collection
The way an image is assigned its link on their site is what made it easy to scan and scrape images from their site. The URL codes are generated sequentially out of a combination letters and numbers of length 6, i.e. prnt.sc/jkl123 prnt.sc/jkl124 prnt.sc/jkl125 prnt.sc/jkl126 Would represent images uploaded one after another. This is of course very easy to write a script to scrape and collect images from. So far I have collected 1,000,000 images in total, and I am now making as many of them as I could available here, but of course you can imagine how easy it would be to collect massive amounts of images from this site. My images were collected over the last 2 years, although the vast majority were collected within the last 4 months.
Copyright
The LightShot terms of service, including their stance on copyright: https://app.prntscr.com/en/privacy.html
Inspiration
This image set is interesting because it represents a balance between a completely random and chaotic set of images and a very structured set of images, due to the fact that there is a great amount of variance image to image, but essentially every image can be categorised into a set of very specific purposes that the site is used for. Because of this, it is good for learning and testing out image processing models.
It is essentially an information gatherers gold mine. There are 1 billion screenshots uploaded by everyday users to be collected and processed on this site, all available sequentially. I’m sure you can imagine the kinds of easily accessible (to a machine learning enthusiast) information waiting to be collected on this site."
Spam Text Message Classification,Let's battle with annoying spammer with data science.,Team AI,2,"Version 1,2017-08-20",,CSV,474 KB,CC0,"1,033 views",112 downloads,8 kernels,,https://www.kaggle.com/team-ai/spam-text-message-classification,"Context
Coming Soon
Content
Coming Soon
Acknowledgements
Special thanks to; http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/
Inspiration
Coming soon"
Universal Tagset,Mappings to the Universal Part-of-Speech Tagset,NLTK Data,2,"Version 1,2017-08-19",,Other,36 KB,Other,347 views,15 downloads,,0 topics,https://www.kaggle.com/nltkdata/universal-tagset,"Context
The Universal Tagset (Petrov et al. 2012) was created to facilitate future research in unsupervised induction of syntactic structure and to standardize best-practices. An up-to-date documentation can be found on http://universaldependencies.org/u/pos/
Content
The files in this repository contain mappings from treebank specific tagsets
to a set of 12 universal part-of-speech tags. The 12 universal tags are:

VERB - verbs (all tenses and modes)
NOUN - nouns (common and proper)
PRON - pronouns 
ADJ - adjectives
ADV - adverbs
ADP - adpositions (prepositions and postpositions)
CONJ - conjunctions
DET - determiners
NUM - cardinal numbers
PRT - particles or other function words
X - other: foreign words, typos, abbreviations
. - punctuation

See ""A Universal Part-of-Speech Tagset""
by Slav Petrov, Dipanjan Das and Ryan McDonald
for more details:
http://arxiv.org/abs/1104.2086
The zipfile contains the <lang>-<tagset>.map files that maps the respective <tagset> POS tagsets in <lang> to the Universal Tagset, e.g. the en-ptb.map contains the mapping from the English Penn Tree Bank tagset to the universal tagset.
The list of mappings includes:
ar-padt.map
bg-btb.map
ca-cat3lb.map
cs-pdt.map
da-ddt.map
de-negra.map
de-tiger.map
el-gdt.map
en-brown.map
en-ptb.map
en-tweet.map
es-cast3lb.map
es-eagles.map
es-iula.map
es-treetagger.map
eu-eus3lb.map
fi-tdt.map
fr-paris.map
hu-szeged.map
it-isst.map
iw-mila.map
ja-kyoto.map
ja-verbmobil.map
ko-sejong.map
nl-alpino.map
pl-ipipan.map
pt-bosque.map
ru-rnc.map
sl-sdt.map
sv-talbanken.map
tu-metusbanci.map
zh-ctb6.map
zh-sinica.map
Additionally, it contains:
README: A README file
universal_tags.py: A script use to convert tags to the universal tagset using the mappings by Nathan Schneider
en-tweet.README: A description of the tweeter tag mappings from (Noah et al. 2011)
Citations
Slav Petrov,  Dipanjan Das, and Ryan McDonald. 
A universal part-of-speech tagset.  In LREC 2012"
MaxEnt Treebank POS Tagger,Maximum Entropy POS Tagger,NLTK Data,2,"Version 1,2017-08-19",,Other,17 MB,Other,498 views,15 downloads,,0 topics,https://www.kaggle.com/nltkdata/maxent-treebank-pos-tagger,"Context
This was the original pre-trained POS tagger that nltk.pos_tag used.
This is the infamous maximum entropy POS tagger that gained a lot of heat when no one knew where exactly the model came from.
Acknowledge
We would like to know who to acknowledge too ;P"
Vader Lexicon,Lexicon use for the Vader Sentiment Algorithm,NLTK Data,2,"Version 1,2017-08-19",linguistics,Other,424 KB,Other,"1,392 views",59 downloads,,0 topics,https://www.kaggle.com/nltkdata/vader-lexicon,"Context
VADER Sentiment Analysis. VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media, and works well on texts from other domains.
The VADER lexicon is an empirically validated by multiple independent human judges, VADER incorporates a ""gold-standard"" sentiment lexicon that is especially attuned to microblog-like contexts.
The documentation of the lexicon and the algorithm can be found from the original implementation: https://github.com/cjhutto/vaderSentiment
The NLTK port has slight modification to integrate with the NLTK interfaces and it comes with better Python3 support: https://github.com/nltk/nltk/blob/develop/nltk/sentiment/vader.py
Citation
Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for
Sentiment Analysis of Social Media Text. Eighth International Conference on
Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.
Inspiration
We look forward to more sentiment analysis datasets, perhaps a JEDI (Joint Entropy Decay Iteration) algorithm someday?"
BLLIP Parser Model,Charniak-Johnson parser,NLTK Data,2,"Version 1,2017-08-20",,Other,52 MB,Other,394 views,8 downloads,,0 topics,https://www.kaggle.com/nltkdata/bllip,"Context
NLTK provides an interface to the BLLIP reranking parser (aka Charniak-Johnson parser, Charniak parser, Brown reranking parser).
NLTK redistribute the pre-trained model trained on the WSJ section of the Penn Treebank without auxillary.
The full list of models can be found on https://github.com/BLLIP/bllip-parser/blob/master/MODELS.rst
Acknowledgements
Parser and reranker:
Eugene Charniak and Mark Johnson. 2005. ""Coarse-to-fine n-best 
parsing and MaxEnt discriminative reranking."" In ACL.

Eugene Charniak. 2000. ""A maximum-entropy-inspired parser."" In ACL.
Self-training:
David McClosky, Eugene Charniak, and Mark Johnson. 2006. 
""Effective Self-Training for Parsing."" In HLT-NAACL.
Syntactic fusion:
Do Kook Choe, David McClosky, and Eugene Charniak. 2015. 
""Syntactic Parse Fusion."" In EMNLP."
Moses Sample,Moses sample MT models,NLTK Data,2,"Version 1,2017-08-20",,Other,10 MB,Other,532 views,12 downloads,,0 topics,https://www.kaggle.com/nltkdata/moses-sample,"Context
NLTK redistributes the Moses machine translation models to test the nltk.translate functionalities, originally from http://www.statmt.org/moses/?n=Development.GetStarted
Content
The Moses sample contains the following subdirectories:
lm: pre-trained N-gram language models using europarl and SRILM
phrase-model: pre-trained Moses phrase-based model
string-to-tree: pre-trained Moses String-to-Tree model
tree-to-tree: pre-trained Moses Tree-to-Tree model
Acknowledgements
Credit goes to the Moses developers who distribute this as a regression test to check that Moses Statistical Machine Translation tool is successfully installed."
MaxEnt NE Chunker,ACE Named Entity Chunker (Maximum entropy),NLTK Data,2,"Version 1,2017-08-20",,Other,23 MB,Other,587 views,14 downloads,,0 topics,https://www.kaggle.com/nltkdata/maxent-ne-chunker,"Context
The maxent_ne_chunker contains two pre-trained English named entity chunkers trained on an ACE corpus (perhaps ACE ACE 2004 Multilingual Training Corpus?)
It will load an nltk.chunk.named_entity.NEChunkParser object and it is used by the nltk.ne_chunk() function.
Robert M. Johnson has written an excellent expository of what the pre-trained model is doing under the hood
From the relation extraction code function in NLTK, it lists the following tags for the ACE tagset:
LOCATION
ORGANIZATION
PERSON
DURATION
DATE
CARDINAL
PERCENT
MONEY
MEASURE
FACILITY
GPE
Content
The maxent_ne_chunker.zip contains - english_ace_binary.pickle: Chunks the input POS tagged sentence and labeled positive NEs as NE. - english_ace_multiclass.pickle: Chunks the input POS tagged sentence and outputs the repsective NE labels under the ACE tagset. - PY3: Subdirectory that contains the Python3 compatiable pickles as above
Acknowledgements
We're not sure who exactly to credit for this pre-trained model, it'll be great if anyone who knows help to document this on https://github.com/nltk/nltk/issues/1783"
Gutenberg,Hot of the printing press...,NLTK Data,2,"Version 1,2017-08-20",,Other,11 MB,Other,293 views,11 downloads,,0 topics,https://www.kaggle.com/nltkdata/gutenberg,"Context
The canonical metadata on NLTK:
<package id=""gutenberg"" name=""Project Gutenberg Selections""
     webpage=""http://gutenberg.net/""
     license=""public domain""
     copyright=""public domain""
     unzip=""1""
     />"
NPS Chat,NPS Chat Corpus (http://faculty.nps.edu/cmartell/NPSChat.htm),NLTK Data,2,"Version 1,2017-08-21",,Other,2 MB,Other,575 views,68 downloads,,0 topics,https://www.kaggle.com/nltkdata/nps-chat,"Context
The canonical metadata on NLTK:
<package id=""nps_chat"" name=""NPS Chat""
     author=""Craig Martell (cmartell@nps.edu)""
     webpage=""http://faculty.nps.edu/cmartell/NPSChat.htm""
     license=""This corpus is distributed solely for non-commercial, non-profit educational and research use. It is a derivative compilation work of multiple works whose copyrights are held by the respective original authors.""
     unzip=""1""
     />"
Paradigm,a collection of (mostly) morphological paradigms in linguistic literature,NLTK Data,2,"Version 1,2017-08-21",,Other,353 KB,Other,194 views,7 downloads,,0 topics,https://www.kaggle.com/nltkdata/paradigm,"Context
The canonical metadata on NLTK:
<package id=""paradigms"" name=""Paradigm Corpus""
     author=""Cathy Bow, University of Melbourne""
     license=""Distributed with the permission of the author""
     unzip=""1""
     />"
Product Reviews,from http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#datasets,NLTK Data,2,"Version 1,2017-08-21",,Other,816 KB,Other,445 views,65 downloads,,0 topics,https://www.kaggle.com/nltkdata/product-reviews,"Context
The canonical metadata on NLTK:
<package id=""product_reviews_1""
 name=""Product Reviews (5 Products)""
 author=""Bing Liu""
     copyright=""Copyright (C) 2004 Bing Liu""
     license=""Creative Commons Attribution 4.0 International""
 licenseurl = ""http://creativecommons.org/licenses/by/4.0/""
 webpage=""http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#datasets""
     unzip=""1""
     />

<package id=""product_reviews_2""
 name=""Product Reviews (9 Products)""
 author=""Bing Liu""
     copyright=""Copyright (C) 2007 Bing Liu""
     license=""Creative Commons Attribution 4.0 International""
 licenseurl = ""http://creativecommons.org/licenses/by/4.0/""
 webpage=""http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#datasets""
     unzip=""1""
     />"
Shakespeare,Shakespeare XML Corpus Sample,NLTK Data,2,"Version 1,2017-08-21",,Other,2 MB,Other,235 views,22 downloads,,0 topics,https://www.kaggle.com/nltkdata/shakespeare,"Context
The canonical metadata on NLTK:
<package id=""shakespeare"" 
     name=""Shakespeare XML Corpus Sample""
     license=""public domain""
     copyright=""public domain""
     webpage=""http://www.andrew.cmu.edu/user/akj/shakespeare/""
     sample=""True""
     unzip=""1""
     />"
Stopwords,List of stopwords in 16 languages,NLTK Data,2,"Version 1,2017-08-21",,Other,20 KB,Other,247 views,14 downloads,,0 topics,https://www.kaggle.com/nltkdata/stopwords,"Context
The canonical metadata on NLTK:
<package id=""stopwords"" name=""Stopwords Corpus""
     webpage=""ftp://ftp.cs.cornell.edu/pub/smart/english.stop and http://snowball.tartarus.org/ and others""
     unzip=""1""
     />"
Toolbox Sample,Sample files from http://software.sil.org/toolbox/,NLTK Data,2,"Version 1,2017-08-21",,Other,810 KB,Other,161 views,7 downloads,,0 topics,https://www.kaggle.com/nltkdata/toolbox-sample,"Context
The canonical metadata on NLTK:
<package id=""toolbox""
     name=""Toolbox Sample Files""
     unzip=""1""
     />"
Movie Reviews,Sentiment Polarity Dataset Version 2.0,NLTK Data,2,"Version 2,2017-11-16|Version 1,2017-08-20",,Other,4 MB,Other,"1,312 views",157 downloads,3 kernels,0 topics,https://www.kaggle.com/nltkdata/movie-review,"Context
The [Sentiment Polarity Dataset Version 2.0](http://www.cs.cornell.edu/people/pabo/movie-review-data/ ) is created by Bo Pang and Lillian Lee. This dataset is redistributed with NLTK with permission from the authors.
This corpus is also used in the Document Classification section of Chapter 6.1.3 of the NLTK book.
Content
This dataset contains 1000 positive and 1000 negative processed reviews.
Citation
Bo Pang and Lillian Lee. 2004. A Sentimental Education: Sentiment Analysis 
Using Subjectivity Summarization Based on Minimum Cuts. In ACL.
Bibtex:
@InProceedings{Pang+Lee:04a,
  author =       {Bo Pang and Lillian Lee},
  title =        {A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts},
  booktitle =    ""Proceedings of the ACL"",
  year =         2004
}"
Brown Corpus,Brown Corpus of Standard American English,NLTK Data,2,"Version 2,2017-11-16|Version 1,2017-08-20",,Other,3 MB,Other,341 views,26 downloads,,0 topics,https://www.kaggle.com/nltkdata/brown-corpus,"Context
The corpus consists of one million words of American English texts printed in 1961.
The canonical metadata on NLTK:
<package id=""brown"" name=""Brown Corpus""
         author=""W. N. Francis and H. Kucera""
         license=""May be used for non-commercial purposes.""
         webpage=""http://www.hit.uib.no/icame/brown/bcm.html""
         unzip=""1""
         />

<package id=""brown_tei"" name=""Brown Corpus (TEI XML Version)""
     author=""W. N. Francis and H. Kucera""
     license=""May be used for non-commercial purposes.""
     webpage=""http://www.hit.uib.no/icame/brown/bcm.html""
     contact=""Lou Burnard -- lou.burnard@oucs.ox.ac.uk""
     unzip=""1""
     />"
Similar Sentences Clustered Data,Similar Sentences data from 183 Million sentences,Rajasankar Viswanathan,2,"Version 1,2017-08-17","languages
linguistics",{}JSON,579 MB,CC4,558 views,43 downloads,,0 topics,https://www.kaggle.com/rajasankar/similar-sentences-clustered-data,"Description
This dataset has two parts, one is created using 100 Million sentences from last 18 months of US Patent data downloaded from USPTO site. Second is from 83 Million sentences extracted from NCBI PMC open access articles.
Preparation of Data
Text from original XML extracted using Python lxml module. Extracted text split into sentences using simple regex. Resulting 83 Million sentences from PMC articles, 120 Million sentences from Patents.
Clustering of Sentences
Sentences with less than 5 words and more than 70 words were removed. Stop words and unwanted characters were removed. Extracted sentences were fed to the Clustering Algorithm to generate Hierarchical Clusters.
Sentences are clustered as if each sentence is compared with all other sentences, ie all against all match. However, all against all match for 100 Million sentences computationally may not be possible, even extracting features would be costly, graph mining algorithms used to cluster the data.
Files Description
NCBI PMC
File : NCBI_data.json ; Mapping : {count: cluster_id, val:[list of line ids] } File : NCBI_lines_map.json ; Mapping : {pmc: line id, line: } split pmc with _, first part is the doc id File : NCBI_doc_titles_map.json ; Mapping : {pmc: line id, article-title: , journal-title: , article-id:}
US Patent Data
File : USPTO_data.json ; Mapping : {count: cluster_id, val:[list of line ids] } File : USPTO_lines_map.json ; Mapping : {num: line id, line : } split num with _, first part is the doc id File : USPTO_doc_titles_map.json ; Mapping : {num: Patent ID extracted from xml xpath //publication-reference//document-id//doc-number, title: Invention title from Patent XML }
Where this data can be used?
This can be used in Search Ranking of Similar Documents, Conceptual Search, Plagiarism Detection etc. Having similar sentences can improve the accuracy of the prediction models. Machine translation and other areas where alignment is needed, running models on already aligned data can boost the accuracy while reducing the time.
Details about the uploaded data
Original Data size uncompressed is 32 GB for NCBI PMC data, 30 GB for USPTO data. Clustered data size is slightly bigger than original, so it cant be uploaded in Kaggle.
Only Clusters with less than 5/8 lines were uploaded, consisting of 400k and 358 clusters for NCBI and USPTO respectively. Each has more than 800k lines in 250k and 135k documents.
Examples from data
NCBI
pmc : 3045422_8
 article-title : Association of a-Adducin and G-Protein b3 Genetic Polymorphisms with Hypertension: A Meta-Analysis of Chinese Populations
 journal-title : PLoS ONE
line : As the genomic sequences of a-adducin and GNB3 genes are highly polymorphic, it is of added interest to identify which polymorphism(s) in these genes might have functional potentials of affecting their bioavailability

 pmc : 3142626_5  
 article-title :Evaluation of Transforming Growth Factor Beta-1 Gene 869T/C Polymorphism with Hypertension: A Meta-Analysis
  journal-title : International Journal of Hypertension
  line : Since the genomic sequence of TGFB1 gene is highly polymorphic, it is of added interest to confirm which TGFB1 polymorphism(s) might have functional potentials to influence the final bioavailability of TGF- b 1, thus the development of hypertension

pmc : 3166328_6
article-title : An Updated Meta-Analysis of Endothelial Nitric Oxide Synthase Gene: Three Well-Characterized Polymorphisms with Hypertension
journal-title : PLoS ONE
line : Since the genomic sequence of eNOS is highly polymorphic, it is of added interest to confirm which polymorphism(s) at eNOS might have functional potentials to affect the final bioavailability of eNOS, and thus the development of hypertension
pmc : 362850_30
article-title : De-Orphaning the Structural Proteome through Reciprocal Comparison of Evolutionarily Important Structural Features
journal-title : PLoS ONE
line : Since a small root mean squared deviation (RMSD) alone is not sufficient to guarantee the functional relevance of a match , , a support vector machine (SVM) trained on enzymes () considers in addition to RMSD whether the matches also fall on evolutionarily important regions of T i 

pmc : 4651773_184
article-title : Crystal structure of group II intron domain 1 reveals a template for RNA assembly
journal-title : Nature chemical biology
line : All of the root mean square deviation (RMSD) values were calculated by Pymol without allowing removal of non-fitting residues to minimize RMSD. For calculating the simulated annealing omit map, the region of interest was first deleted from the model, and then this partial model was subject to simulated annealing refinement in phenix.refine
US Patent data
num : 09226555_77
title: Cane structure
line : As such, when the negative terminal of a battery is located at the holder with the L-shaped blocking wall 2112 , the battery will be blocked from being electrically connected with the associated conductive sheet 213 therein to prevent an incorrect electrical connection that may cause a damage or failure to the circuit board 22 

num : 09520536_507
title : Light emitting diode chip having electrode pad
line : Therefore, the second electrode extensions 39 a and the transparent conductive layer 33 are not electrically connected to each other in the positions in which the holes h are formed, Thus, an electrical flow is blocked in the corresponding positions in which the holes h are formed, and the second electrode extensions 39 a are located on the current blocking layer 31 b
num : 09233144_346
title : Tyrosine kinase receptor TYRO3 as a therapeutic target in the treatment of cancer
line : In order to investigate the role of TYRO3 in cell growth and tumorigenic properties, TYRO3 expression was blocked using RNA interference technology or TYRO3 activity was inhibited using a blocking antibody directed against the extracellular domain of TYRO3 or a soluble receptor consisting of the recombinant extracellular domain of TYRO3 produced in bacteria

num : 09283245_194
title : Composition containing PIAS3 as an active ingredient for preventing or treating cancer or immune disease
line : In order to measure the amount of produced IL-17 cytokine, the supernatant of the cell culture medium was collected and level of IL-17 expression was investigated using human IL-17 and sandwich ELISA. After reaction on a 96 well plate with 2 mg/mL of monoclonal anti-IL-17 at 4deg C., overnight, non-specific binding was blocked with blocking solution (1% BSA/PBST)

num : 09546210_330
title : Cripto antagonism of activin and TGF-b signaling
line : In contrast, the Cripto DEGF mutant blocked roughly half of the luciferase activity induced by activin-B ( FIG. 10 ), indicating an independent role for the CFC domain in blocking activin-B signaling"
European Soccer Database Supplementary,"Data Supplementary to ""European Soccer Database""",willinghorse,2,"Version 1,2017-09-10",,CSV,59 MB,ODbL,849 views,107 downloads,,0 topics,https://www.kaggle.com/jiezi2004/soccer,"Context
This dataset was built as a supplementary to ""[European Soccer Database][1]"". It includes data dictionary, extraction of detailed match information previously contains in XML columns.
Content
PositionReference.csv: A reference of position x, y and map them to actual position in a play court.
DataDictionary.xlsx: Data dictionary for all XML columns in ""Match"" data table.
card_detail.csv: Detailed XML information extracted form ""card"" column in ""Match"" data table.
corner_detail.csv: Detailed XML information extracted form ""corner"" column in ""Match"" data table.
cross_detail.csv: Detailed XML information extracted form ""cross"" column in ""Match"" data table.
foulcommit_detail.csv: Detailed XML information extracted form ""foulcommit"" column in ""Match"" data table.
goal_detail.csv: Detailed XML information extracted form ""goal"" column in ""Match"" data table.
possession_detail.csv: Detailed XML information extracted form ""possession"" column in ""Match"" data table.
shotoff_detail.csv: Detailed XML information extracted form ""shotoffl"" column in ""Match"" data table.
shoton_detail.csv: Detailed XML information extracted form ""shoton"" column in ""Match"" data table.
Acknowledgements
Original data comes from [European Soccer Database][1] by Hugo Mathien. I personally thank him for all his efforts.
Inspiration
Since this is a open dataset with no specific goals / objectives, I would like to explore the following aspects by data analytics / data mining:
Team statistics Including overall team ranking, team points, winning possibility, team lineup, etc. Mostly descriptive analysis.
Team Transferring Track and study team players transferring in the market. Study team's strength and weakness, construct models to suggest best fit players to the team.
Player Statistics Summarize player's performance (goal, assist, cross, corner, pass, block, etc). Identify key factors of players by position. Based on these factors, evaluate player's characteristics.
Player Evolution Construct model to predict player's rating of future.
New Player's Template Identify template and model player for young players cater to their positions and characteristics.
Market Value Prediction Predict player's market value based on player's capacity and performance.
The Winning Eleven Given a season / league / other criteria, propose the best 11 players as a team based on their capacity and performance."
Tailpipe Emissions for sedan vehicle,Analysis for emission data,Irfan,2,"Version 3,2017-09-02|Version 2,2017-09-02|Version 1,2017-09-02",,Other,222 KB,Other,315 views,17 downloads,,,https://www.kaggle.com/irfanazeem/tailpipe-emissions-for-sedan-vehicle,"Context
Tailpipe emissions for BMW 3series sedan
Content
Analysis for tailpipe omissions data for BMW 3 Series
Data is recorded for car travelling at every second with following variables: Vehicle_speed (km/h), HC_tailpipe (g/s), CO_tailpipe (g/s) and NOx_tailpipe (g/s). It is expected that the value of theses emissions are close to zero, so the recommended emissions are Speed = 39.9 , HC = 0.0005, CO = 0.0200 𝑎𝑛𝑑 NO = 0.0004"
Kannada Word Set,Meaningful words in the Kannada language,KB,2,"Version 1,2017-09-05",,Other,483 KB,Other,150 views,17 downloads,,,https://www.kaggle.com/kumarbhrgv/kannada-dataset,"Context
Kannada basic word set for Natural Language Processing
Content
Words which has meaning in Kannada Language"
FAA laser with days of week,altered: https://www.kaggle.com/crawford/laser-incident-report,Cam Nugent,2,"Version 1,2017-08-16",,CSV,1 MB,CC0,127 views,6 downloads,,0 topics,https://www.kaggle.com/camnugent/faa-laser-with-days-of-week,"Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
State of the Nation Corpus (1990 - 2018),Full texts of the South African State of the Nation addresses,Allan,2,"Version 2,2018-02-21|Version 1,2017-09-06",,Other,427 KB,ODbL,666 views,32 downloads,,0 topics,https://www.kaggle.com/allank/state-of-the-nation-1990-2017,"Context
The State of the Nation Address of the President of South Africa (abbreviated SONA) is an annual event in the Republic of South Africa, in which the President of South Africa reports on the status of the nation, normally to the resumption of a joint sitting of Parliament (the National Assembly and the National Council of Provinces).
Content
Full text of all the speeches, from 1990 through to 2018. In years that elections took place, a State of the Nation Address happens twice, once before and again after the election."
Ecoli Data Set,Abstract: This data contains protein localization sites,Ahiale Darlington,2,"Version 1,2017-09-01",,CSV,19 KB,CC0,490 views,44 downloads,,0 topics,https://www.kaggle.com/elikplim/ecoli-data-set,"Title: Protein Localization Sites
Creator and Maintainer: Kenta Nakai Institue of Molecular and Cellular Biology Osaka, University 1-3 Yamada-oka, Suita 565 Japan nakai@imcb.osaka-u.ac.jp http://www.imcb.osaka-u.ac.jp/nakai/psort.html Donor: Paul Horton (paulh@cs.berkeley.edu) Date: September, 1996 See also: yeast database
Past Usage. Reference: ""A Probablistic Classification System for Predicting the Cellular Localization Sites of Proteins"", Paul Horton & Kenta Nakai, Intelligent Systems in Molecular Biology, 109-115. St. Louis, USA 1996. Results: 81% for E.coli with an ad hoc structured probability model. Also similar accuracy for Binary Decision Tree and Bayesian Classifier methods applied by the same authors in unpublished results.
Predicted Attribute: Localization site of protein. ( non-numeric ).
The references below describe a predecessor to this dataset and its development. They also give results (not cross-validated) for classification by a rule-based expert system with that version of the dataset.
Reference: ""Expert Sytem for Predicting Protein Localization Sites in Gram-Negative Bacteria"", Kenta Nakai & Minoru Kanehisa,
PROTEINS: Structure, Function, and Genetics 11:95-110, 1991.
Reference: ""A Knowledge Base for Predicting Protein Localization Sites in Eukaryotic Cells"", Kenta Nakai & Minoru Kanehisa, Genomics 14:897-911, 1992.
Number of Instances: 336 for the E.coli dataset and
Number of Attributes. for E.coli dataset: 8 ( 7 predictive, 1 name )
Attribute Information.
Sequence Name: Accession number for the SWISS-PROT database
mcg: McGeoch's method for signal sequence recognition.
gvh: von Heijne's method for signal sequence recognition.
lip: von Heijne's Signal Peptidase II consensus sequence score. Binary attribute.
chg: Presence of charge on N-terminus of predicted lipoproteins. Binary attribute.
aac: score of discriminant analysis of the amino acid content of outer membrane and periplasmic proteins.
alm1: score of the ALOM membrane spanning region prediction program.
alm2: score of ALOM program after excluding putative cleavable signal regions from the sequence.
Missing Attribute Values: None.
Class Distribution. The class is the localization site. Please see Nakai & Kanehisa referenced above for more details.
cp (cytoplasm) 143 im (inner membrane without signal sequence) 77
pp (perisplasm) 52 imU (inner membrane, uncleavable signal sequence) 35 om (outer membrane) 20 omL (outer membrane lipoprotein) 5 imL (inner membrane lipoprotein) 2 imS (inner membrane, cleavable signal sequence) 2"
Forest Fires Data Set,predict the burned area of forest fires using meteorological and other data,Ahiale Darlington,2,"Version 1,2017-09-04",,CSV,25 KB,Other,"2,088 views",124 downloads,2 kernels,0 topics,https://www.kaggle.com/elikplim/forest-fires-data-set,"Source: https://archive.ics.uci.edu/ml/datasets/forest+fires
Citation Request: This dataset is public available for research. The details are described in [Cortez and Morais, 2007]. Please include this citation if you plan to use this database:
P. Cortez and A. Morais. A Data Mining Approach to Predict Forest Fires using Meteorological Data. In J. Neves, M. F. Santos and J. Machado Eds., New Trends in Artificial Intelligence, Proceedings of the 13th EPIA 2007 - Portuguese Conference on Artificial Intelligence, December, Guimaraes, Portugal, pp. 512-523, 2007. APPIA, ISBN-13 978-989-95618-0-9. Available at: http://www.dsi.uminho.pt/~pcortez/fires.pdf
Title: Forest Fires
Sources Created by: Paulo Cortez and An�bal Morais (Univ. Minho) @ 2007
Past Usage:
P. Cortez and A. Morais. A Data Mining Approach to Predict Forest Fires using Meteorological Data. In Proceedings of the 13th EPIA 2007 - Portuguese Conference on Artificial Intelligence, December, 2007. (http://www.dsi.uminho.pt/~pcortez/fires.pdf)
In the above reference, the output ""area"" was first transformed with a ln(x+1) function. Then, several Data Mining methods were applied. After fitting the models, the outputs were post-processed with the inverse of the ln(x+1) transform. Four different input setups were used. The experiments were conducted using a 10-fold (cross-validation) x 30 runs. Two regression metrics were measured: MAD and RMSE. A Gaussian support vector machine (SVM) fed with only 4 direct weather conditions (temp, RH, wind and rain) obtained the best MAD value: 12.71 +- 0.01 (mean and confidence interval within 95% using a t-student distribution). The best RMSE was attained by the naive mean predictor. An analysis to the regression error curve (REC) shows that the SVM model predicts more examples within a lower admitted error. In effect, the SVM model predicts better small fires, which are the majority.
Relevant Information:
This is a very difficult regression task. It can be used to test regression methods. Also, it could be used to test outlier detection methods, since it is not clear how many outliers are there. Yet, the number of examples of fires with a large burned area is very small.
Number of Instances: 517
Number of Attributes: 12 + output attribute
Note: several of the attributes may be correlated, thus it makes sense to apply some sort of feature selection.
Attribute information:
For more information, read [Cortez and Morais, 2007].
X - x-axis spatial coordinate within the Montesinho park map: 1 to 9
Y - y-axis spatial coordinate within the Montesinho park map: 2 to 9
month - month of the year: ""jan"" to ""dec""
day - day of the week: ""mon"" to ""sun""
FFMC - FFMC index from the FWI system: 18.7 to 96.20
DMC - DMC index from the FWI system: 1.1 to 291.3
DC - DC index from the FWI system: 7.9 to 860.6
ISI - ISI index from the FWI system: 0.0 to 56.10
temp - temperature in Celsius degrees: 2.2 to 33.30
RH - relative humidity in %: 15.0 to 100
wind - wind speed in km/h: 0.40 to 9.40
rain - outside rain in mm/m2 : 0.0 to 6.4
area - the burned area of the forest (in ha): 0.00 to 1090.84 (this output variable is very skewed towards 0.0, thus it may make sense to model with the logarithm transform).
Missing Attribute Values: None"
Argentina's Private Neighborhoods,,Juanu,2,"Version 1,2017-09-21","cities
geography
demographics",CSV,170 KB,CC0,202 views,3 downloads,,0 topics,https://www.kaggle.com/juanumusic/argentinas-private-neighborhoods,"This dataset contains a collection of the Private Neighborhoods in Argentina.
Context
I created this dataset mainly for a challenge at school on which we had to predict house prices. Houses that are located on a private neighborhood, have prices that are completely different than the price of houses that are on the same city, but outside the neighborhoods. I used this dataset to detect whether a house was inside or outside a private neighborhood.
GitHub Project: https://github.com/HarkDev/barrios_privados_argentina
Source: http://www.guiacountry.com/countries/imagenes/listado.php"
Estimated speed using fastest route,Estimated speed and trip durations of trips in NYC using the fastest route data,Peter Klauke,2,"Version 1,2017-09-07",,CSV,106 MB,CC0,146 views,10 downloads,,0 topics,https://www.kaggle.com/pepeeee/estimated-speed-using-fastest-route,"Context
These datasets contain estimated speed and estimated trip durations for all trips generated by the fastest route dataset from oscarleo. The estimated trip durations are a better approximation for the actual trip duration than the total_travel_time variable from the fastest route dataset is.
How these features are generated in detail can be found in this kernel: https://www.kaggle.com/pepeeee/nyc-estimating-avg-speed-using-fastest-route/notebook
Content
estimated_speed - This variable contains the estimated average speed of the respective trip. The unit of this variable is meter per second.
estimated_trip_duration - This variable is generated by dividing the total_distance variable of the fastest route dataset by estimated_speed. The unit of the variable is seconds.
Acknowledgements
I'm very thankful to oscarleo for sharing the great dataset about the fastest routes. Without that dataset mine wouldn't have been possible. I would also like to thank saihttam for sharing the holidays package.
Inspiration
The fastest route contains a lot of very valuable data for predicting the trip durations of taxis in New York City. Just using the variables total_travel_time, total_distance and number_of_steps didn't feel like using its full potential to me."
Copy of wikipedia-language-iso639,Simply list from wikipedia,sgDysregulation,2,"Version 1,2017-09-01",,CSV,2 KB,CC0,143 views,8 downloads,3 kernels,0 topics,https://www.kaggle.com/sophieg/wikipedia-language-iso639,"Context
Simply list of ISO_639-1_codes
Content
Needed to access through kaggle
Acknowledgements
https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes
Image from Goran Ivos at unplash"
Mannanafnaskrá,The legally binding list of Icelandic names,Sohier Dane,2,"Version 1,2017-09-11",linguistics,CSV,36 KB,CC0,478 views,9 downloads,,0 topics,https://www.kaggle.com/sohier/icelandic-names,"Trying to name a child? Looking for something a little different? Something that will force a preschool's database to support unicode? Look no farther! From Aðdal to Ösp to Ben, this list has you covered.
The Icelandic Naming committee maintains an official register of approved Icelandic given names and is the governing body of introduction of new given names into the culture of Iceland. In many cases parents use the database as a guide when choosing a name. If the name they have in mind is not in the register they can fill out a special form and request whether the name will be considered allowable by law. If the committee rules positively on a request the name will be added to the Personal Names Register. If the committee denies the request, the child may not be allowed to get an Icelandic passport with that name.
The register is stored and maintained at Registers Iceland and is accessible through the national portal Ísland.is.
Note on the column headers: Drengir = boys, Millinöfn = girls, Stúlkur = either."
NY State Lotto Winning Numbers,Winning numbers since 2001,Sohier Dane,2,"Version 1,2017-09-14",money,CSV,54 KB,CC0,"1,118 views",78 downloads,,0 topics,https://www.kaggle.com/sohier/ny-state-lotto-winning-numbers,"Winning numbers from the New York State Lotto since 2001.
Acknowledgements
This dataset was kindly made available by the state of New York. You can find the original dataset here.
Inspiration
Some other state lotteries have proven to be predictable and ended up being gamed. It's extremely unlikely that any real patterns exist in a large and long running lotto like New York's, but can you find any?"
emotion recognition,,ashish bansal,2,"Version 1,2017-09-19",,CSV,287 MB,CC0,"1,288 views",196 downloads,,0 topics,https://www.kaggle.com/ashishbansal23/emotion-recognition,This dataset does not have a description yet.
Car Emissions data,"A dataset of over 5,000 vehicles and their Emissions",dananos,2,"Version 1,2017-09-08",,CSV,803 KB,ODbL,551 views,73 downloads,,,https://www.kaggle.com/dananos/car-emissions-data,"Context
Data sourced from the UK government, and used in the Car Registration API http://www.regcheck.org.uk - Correct as of August 2017"
Area and Geography,text files describing different places around the world,bshivaani,2,"Version 1,2017-09-17","geography
linguistics
internet",Other,493 KB,CC0,227 views,18 downloads,,0 topics,https://www.kaggle.com/bsivavenu/area-and-geography,"Context
This Dataset was downloaded from internet few years ago.
Content
This dataset contains many individual word/ text files describing different places and their history/geography around the world.
Acknowledgements
As this dataset is downloaded from internet few years ago and i don't remember the site/ particular authors of this file.But any how thanks to those people who created these files.
Inspiration
This might be helpful for those who practise NLP/NLTK, Text mining. Each individual file contains many paragraphs which describes about the place in a lengthy manner. with text mining one should be able to remove all stop words and describe the data in a brief way."
"Armors, Exoskeletons & Mecchas","300 heroes listed, 80 fully detailed",NMIN,2,"Version 3,2017-09-08|Version 2,2017-09-06|Version 1,2017-09-03",,CSV,43 KB,Other,960 views,117 downloads,,,https://www.kaggle.com/nicolasmin/armors-exoskeletons-mecchas,"Context
The file presents a listing of characters wearing powered armor / mini or giant meccha in movies, comics, animation etc.
The purpose was to analyse our imaginaries in a specific field (i.e armors in this case) in order to see what are the macro elements, see how they evolve around time and if they are close to what is used in real life.
Content
Each armor is analyzed according to 13 characteristics (uses an AI or not, what kind or power, where is the weapon, its capacities (does is fly, gives enhanced strength etc.). Being a social science professor and not a data analysts, I went on marvel wikia, DC wikia etc. to compile it. Something like 80 heroes are fully presented, and a list of almost 300 been found.
Inspiration
Coming from social science I compiled that data during my free time, but I understand that it is highly limiting and that there must be a way to aggregate much more data, & faster. Also, I am sure that it does not meet some of the standards for such work. Being a newbie here, please tell me how to improve this & I will.
The question after is to know if we can ""predict"" what future armors will look like : is there a trend showing that AI is used more and more ? That they all fly ? Once this done, it would allow to ""delineate"" the ideal characteristics of a super hero and hence, where we could innovate if we do not want to reproduce things that already done while imagining them ?
The last questions correlate to social trends : do some characteristics appear during a certain period ? If yes, is it correlated to some specific social context ? (new type of wars impacting how we imagine our heroes ?)."
Tutorial,http://trevorstephens.com/kaggle-titanic-tutorial/r-part-1-booting-up/,edgano,2,"Version 1,2017-09-13",,CSV,3 KB,CC0,633 views,7 downloads,,0 topics,https://www.kaggle.com/edgano/tutorial,This dataset does not have a description yet.
NYC City Hall Library Catalog,All official reports published by the City of New York,City of New York,2,"Version 1,2017-09-09","cities
government",CSV,5 MB,CC0,505 views,30 downloads,,0 topics,https://www.kaggle.com/new-york-city/nyc-city-hall-library-catalog,"Context
This dataset compiles the titles, publication dates, and other data about all reports published in the official capacities of New York City government agency work are listed in the City Hall Library catalog. The catalog functions like a city-level equivalent of the national Library of Congress, and goes back very far --- at least to the 1800s.
Content
Columns are provided for the report name and report sub-header, the year the report was issued, the name of the publisher compiling the report, and some other smaller fields.
Acknowledgements
This data was originally published in a pound (""#"") delimited dataset on the New York City Open Data Portal. It has been restructured as a CSV and lightly cleaned up for formatting prior to being uploaded to Kaggle.
Inspiration
Can you separate reporting publications by the City of New York into topics?
Who are the most common report issuers, and what causes do they represent?
What are some common elements to report titles?"
Cocktail Ingredients,Cocktails and their respective ingredients,AIFirst,2,"Version 1,2017-09-24","food and drink
alcohol",CSV,208 KB,CC0,557 views,61 downloads,5 kernels,,https://www.kaggle.com/ai-first/cocktail-ingredients,"Context
It would be fun to have an system which could generate new cocktails from just the name and if the cocktail already exists produce the right result
Content
The data is a scraped collection of cocktails and their ingredients
Acknowledgements
The idea was made at an AI-First meetup and the scraping idea came from the group who worked on it"
Telangana Hospitals,"Number of Health Centres, Hospitals.. etc in each of the district of Telangana",sumendar,2,"Version 1,2017-06-15",,CSV,2 KB,CC0,340 views,21 downloads,,,https://www.kaggle.com/sumendar/telangana-hospitals,"Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
NYC hourly car accidents 2013-2016,Dataset contains information about traffic collisions in NYC in the given period,PavelTroshenkov,2,"Version 1,2017-08-09","transport
taxi services",CSV,232 MB,Other,485 views,69 downloads,,0 topics,https://www.kaggle.com/pavetr/nypdcollisions,"Context
Dataset contains infromation about car accidents in 2013-2016
Main columns: DATE TIME
BOROUGH LATITUDE
LONGITUDE
LOCATION"
NYC boroughs shapes,Contains json with boroughs shapes,PavelTroshenkov,2,"Version 1,2017-08-11",,{}JSON,3 MB,Other,85 views,6 downloads,,0 topics,https://www.kaggle.com/pavetr/nycgeoshapes,Json file with NYC boroughs geoshapes
Realtime GTFS,"Realtime GTFS St.Petersburg, Russia",Vadim Shmelev,2,"Version 1,2017-06-13",,Other,2 GB,Other,277 views,23 downloads,,0 topics,https://www.kaggle.com/wadims/realtime-gtfs,"Context
Realtime GTFS data collected from 2017_5_29_11_30_15 till 2017_6_6_13_42_11 at Saint-Petersburg, Russia. Data requested from: http://transport.orgp.spb.ru/Portal/transport/internalapi/gtfs
Content
Archive contain real-time GTFS files (https://developers.google.com/transit/gtfs-realtime/)
Acknowledgements
Thanks to http://transport.orgp.spb.ru for implementing API
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
US state county name & codes,"State name state code, county name , county fips code. Not for analysis",VivekMangipudi,2,"Version 2,2017-06-07|Version 1,2017-06-07",,CSV,124 KB,Other,328 views,30 downloads,,0 topics,https://www.kaggle.com/stansilas/us-state-county-name-codes,"Context
There is no story behind this data.
These are just supplementary datasets which I plan on using for plotting county wise data on maps.. (in particular for using with my kernel : https://www.kaggle.com/stansilas/maps-are-beautiful-unemployment-is-not/)
As that data set didn't have the info I needed for plotting an interactive map using highcharter .
Content
Since I noticed that most demographic datasets here on Kaggle, either have state code, state name, or county name + state name but not all of it i.e county name, fips code, state name + state code.
Using these two datasets one can get any combination of state county codes etc.
States.csv has State name + code
US counties.csv has county wise data.
Acknowledgements
Picture : https://unsplash.com/search/usa-states?photo=-RO2DFPl7wE
Counties : https://www.census.gov/geo/reference/codes/cou.html
State :
Inspiration
Not Applicable."
1000 Genome Data for Complete Beginners,DNA Analysis from public data. Lightweight CSV file.,Daisuke Ishii,2,"Version 1,2017-07-30",,CSV,271 KB,CC0,713 views,45 downloads,,,https://www.kaggle.com/daiearth22/1000-genome-data,"Context
Taken from; http://www.internationalgenome.org/data
Content
Coming soon
Acknowledgements
Special thanks to international genome org.
Inspiration
Coming soon"
Film Locations in San Francisco,"If you love movies, and you love San Francisco, you're bound to love this!",Suchit Gupta,2,"Version 1,2017-06-01","film
geography",CSV,313 KB,CC0,987 views,62 downloads,2 kernels,,https://www.kaggle.com/suchitgupta60/film-locations-in-san-francisco,"Introduction
If you love movies, and you love San Francisco, you're bound to love this -- A listing of filming locations of movies shot in San Francisco starting from 1924. You'll find the titles, locations, fun facts, names of the director, writer, actors, and studio for most of these films.
Inspiration
Combine with the popular IMDB 5000 dataset on Kaggle to see how movies filmed in San Francisco are rated. Click on ""New Kernel"" and add this dataset source by clicking on ""Input Files"".
Start a new kernel"
NYC Weather,Collecting Info From Jan 2016 - June 2016,Edward Turner,2,"Version 1,2017-07-22",,CSV,2 MB,Other,489 views,44 downloads,,0 topics,https://www.kaggle.com/eaturner/nycweather,"Context
This dataset contains a subset of information, pertaining to the weather patterns during Januaray 2016 - June 2016 in NYC. This may be important to those who are looking to add columns to their dataset.
Content
Preciptation, snowfall, temperatures, latitude and longitude, along with dates. All the important information we need to know. Unforunately, this is not minute to minue information, but strictly daily information.
Acknowledgements
I would like to thank mcrepy94 for sharing the link https://www.ncdc.noaa.gov/cdo-web/search?datasetid=GHCND. Please upvote his post if you upvote this.
Inspiration
I want people to look at this dataset, and create more features for their uses."
Titanic,For Binary logistic regression,Azeem Bootwala,2,"Version 1,2017-06-05",,CSV,54 KB,ODbL,"1,157 views",193 downloads,6 kernels,0 topics,https://www.kaggle.com/azeembootwala/titanic,"Context
This Data was originally taken from Titanic: Machine Learning from Disaster .But its better refined and cleaned & some features have been self engineered typically for logistic regression . If you use this data for other models and benefit from it , I would be happy to receive your comments and improvements.
Content
There are two files namely:- train_data.csv :- Typically a data set of 792x16 . The survived column is your target variable (The output you want to predict).The parch & sibsb columns from the original data set has been replaced with a single column called Family size.
All Categorical data like Embarked , pclass have been re-encoded using the one hot encoding method .
Additionally, 4 more columns have been added , re-engineered from the Name column to Title_1 to Title_4 signifying males & females depending on whether they were married or not .(Mr , Mrs ,Master,Miss). An additional analysis to see if Married or in other words people with social responsibilities had more survival instincts/or not & is the trend similar for both genders.
All missing values have been filled with a median of the column values . All real valued data columns have been normalized.
test_data.csv :- A data of 100x16 , for testing your model , The arrangement of test_data exactly matches the train_data
I am open to feedbacks & suggesstions"
Social Network Ads,A categorical dataset to determine whether a user purchased a particular product,Rakesh Raushan,2,"Version 1,2017-08-06",,CSV,11 KB,Other,769 views,107 downloads,,0 topics,https://www.kaggle.com/rakeshrau/social-network-ads,"Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
FacialSemanticAnalysis.csv,Dataset for semantic analysis using facial expressions,ShaunakChadha,2,"Version 1,2017-07-30",,CSV,287 MB,Other,656 views,100 downloads,,,https://www.kaggle.com/hanumanstark/facialsemanticanalysiscsv,"Context
I found this dataset after exploring numerous datasets for my personal project.
Content
It contains images(in pixel format) along with emotions .
Acknowledgements
Well, I found it on my own.
Inspiration"
U.S. College Scorecard Data 1996-2015,An overview of higher-ed in the U.S.,Roy Garrard,2,"Version 2,2017-08-23|Version 1,2017-07-29",education,Other,194 MB,CC0,"1,478 views",185 downloads,,0 topics,https://www.kaggle.com/noriuk/us-college-scorecard-data-19962015,"Context
The College Scorecard was created by the U.S. Department of Education in an attempt to better understand the efficacy of colleges in the United States. The Scorecard reports information such as the cost of tuition, undergraduate enrollment size, and the rate of graduation. Further details can be found in the file ""FullDataDescription.pdf"".
Content
U.S. College statistics from 1996 to 2015, organized by year. Data was pulled from the official website (https://collegescorecard.ed.gov/data/) in June of 2017. It was reportedly last updated in January 2017."
Stopword Lists for African Languages,Stopword Lists & Frequency Information for 9 African Languages,Rachael Tatman,2,"Version 1,2017-07-28","languages
africa
linguistics",CSV,209 KB,Other,721 views,103 downloads,,0 topics,https://www.kaggle.com/rtatman/stopword-lists-for-african-languages,"Context:
Some words, like “the” or “and” in English, are used a lot in speech and writing. For most Natural Language Processing applications, you will want to remove these very frequent words. This is usually done using a list of “stopwords” which has been complied by hand.
Content:
This project uses the source texts provided by the African Storybook Project as a corpus and provides a number of tools to extract frequency lists and lists of stopwords from this corpus for the 60+ languages covered by ASP.
Included in this dataset are the following languages:
Afrikaans: stoplist and word frequency
Hausa: stoplist and word frequency
Lugbarati: word frequency only
Lugbarati (Official): word frequency only
Somali: stoplist and word frequency
Sesotho: stoplist and word frequency
Kiswahili: stoplist and word frequency
Yoruba: stoplist and word frequency
isiZulu: stoplist and word frequency
Files are named using the language’s ISO code. For each language, code.txt is the list of stopwords, and code_frequency_list.txt is word frequency information. A list of ISO codes the the languages associated with them may be found in ISO_codes.csv.
Acknowledgements:
This project therefore attempts to fill in the gap in language coverage for African language stoplists by using the freely-available and open-licensed ASP Source project as a corpus. Dual-licensed under CC-BY and Apache-2.0 license. Compiled by Liam Doherty. More information and the scripts used to generate these files are available here.
Inspiration:
This dataset is mainly helpful for use during NLP analysis, however there may some interesting insights in the data.
What qualities do stopwords share across languages? Given a novel language, could you predict what its stopwords should be?
What stopwords are shared across languages?
Often, related languages will have words with the same meaning and similar spellings. Can you automatically identify any of these pairs of words?
You may also like:
Stopword Lists for 19 Languages (mainly European and South Asian)"
"ACLED Asian Conflicts, 2015-2017",35k Conflicts Across Developing Asian Countries,Jacob Boysen,2,"Version 1,2017-08-09",war,CSV,13 MB,CC0,564 views,35 downloads,,,https://www.kaggle.com/jboysen/asian-conflicts,"Context:
The Armed Conflict Location and Event Data Project is designed for disaggregated conflict analysis and crisis mapping. This dataset codes the dates and locations of all reported political violence and protest events in developing Asian countries in. Political violence and protest includes events that occur within civil wars and periods of instability, public protest and regime breakdown. The project covers 2015 to the present.
Content:
These data contain information on:
Dates and locations of conflict events;
Specific types of events including battles, civilian killings, riots, protests and recruitment activities;
Events by a range of actors, including rebels, governments, militias, armed groups, protesters and civilians;
Changes in territorial control; and
Reported fatalities.
Event data are derived from a variety of sources including reports from developing countries and local media, humanitarian agencies, and research publications. Please review the codebook and user guide for additional information: the codebook is for coders and users of ACLED, whereas the brief guide for users reviews important information for downloading, reviewing and using ACLED data. A specific user guide for development and humanitarian practitioners is also available, as is a guide to our sourcing materials.
Acknowledgements:
ACLED is directed by Prof. Clionadh Raleigh (University of Sussex). It is operated by senior research manager Andrea Carboni (University of Sussex) for Africa and Hillary Tanoff for South and South-East Asia. The data collection involves several research analysts, including Charles Vannice, James Moody, Daniel Wigmore-Shepherd, Andrea Carboni, Matt Batten-Carew, Margaux Pinaud, Roudabeh Kishi, Helen Morris, Braden Fuller, Daniel Moody and others. Please cite:
Raleigh, Clionadh, Andrew Linke, Håvard Hegre and Joakim Karlsen. 2010. Introducing ACLED-Armed Conflict Location and Event Data. Journal of Peace Research 47(5) 651-660.
Inspiration:
Do conflicts in one region predict future flare-ups? How do the individual actors interact across time?"
Music notes,contains wav files of different notes,adithya,2,"Version 1,2017-08-03",,Other,86 MB,CC0,"1,036 views",89 downloads,,,https://www.kaggle.com/adithyakag/musicnotedetection,"Context
I want to create an app that could generate instrumentals of songs that we listen daily.
Content
I have generated wav files of different notes using garageband. I will use this data to classify musical notes.
Acknowledgements
I took stanford paper on sheet music from audio files by Jan Dlabal and Richard Wedeen
Inspiration
Can we even generate midi files of complicated melodies just using wav files of the song."
NIPS17 Adversarial learning - 1st round results,"Scores, runtime statistics and intermediate results of the first DEV round.",Google Brain,2,"Version 1,2017-08-16",,CSV,48 KB,Other,755 views,139 downloads,,0 topics,https://www.kaggle.com/google-brain/nips17-adversarial-learning-1st-round-results,"This dataset contains run time statistics and details about scores for the first development round of NIPS 2017 Adversarial learning competition
Content
Matrices with intermediate results
Following matrices with intermediate results are provided:
accuracy_matrix.csv - matrix with number of correctly classified images for each pair of attack (targeted and non-targeted) and defense
error_matrix.csv - matrix with number of misclassified images for each pair of attack (targeted and non-targeted) and defense
hit_target_class_matrix.csv - matrix with number of times image was classified as specific target class for each pair of attack (targeted and non-targeted) and defense
In each of these matrices, rows correspond to defenses, columns correspond to attack. Also first row and column are headers with Kaggle Team IDs (or baseline ID).
Scores and run time statistics of submissions
Following files contain scores and run time stats of the submissions:
non_targeted_attack_results.csv - scores and run time statistics of all non-targeted attacks
targeted_attack_results.csv - scores and run time statistics of all targeted attacks
defense_results.csv - scores and run time statistics of all defenses
Each row of these files correspond to one submission. Columns have following meaning:
KaggleTeamId - either Kaggle Team ID or ID of the baseline.
TeamName - human readable team name
Score - raw score of the submission
NormalizedScore - normalized (to be between 0 and 1) score of the submission
MinEvalTime - minimum evaluation time of 100 images
MaxEvalTime - maximum evaluation time of 100 images
MedianEvalTime - median evaluation time of 100 images
MeanEvalTime - average evaluation time of 100 images
Notes about the data
Due to team merging these files contain slightly more submissions than reflected in leaderboard.
Also not all attacks were used to compute scores of defenses and not all defenses were used to compute scores of attacks. Thus if you simply sum-up values in rows/columns of the corresponding matrix you won't obtain exact score of the submission (however number you obtain will be very close to actual score)."
OpenAddresses - North America (excluding U.S.),Addresses and geolocations for North American countries,OpenAddresses,2,"Version 1,2017-08-04",,CSV,5 GB,Other,239 views,37 downloads,,0 topics,https://www.kaggle.com/openaddresses/openaddresses-north-america-excluding-us,"Context
OpenAddresses's goal is to connect the digital and physical worlds by sharing geographic coordinates, street names, house numbers and postal codes.
Content
This dataset contains one data file for each of these countries:
Bermuda - bermuda.csv
Canada - canada.csv
Curaçao - curaçao.csv
Jamaica - jamaica.csv
Mexico - mexico.csv
Field descriptions:
LON - Longitude
LAT - Latitude
NUMBER - Street number
STREET - Street name
UNIT - Unit or apartment number
CITY - City name
DISTRICT - ?
REGION - ?
POSTCODE - Postcode or zipcode
ID - ?
HASH - ?
Acknowledgements
Data collected around 2017-07-25 by OpenAddresses (http://openaddresses.io).
Address data is essential infrastructure. Street names, house numbers and postal codes, when combined with geographic coordinates, are the hub that connects digital to physical places.
Data licenses can be found in LICENSE.txt.
Data source information can be found at https://github.com/openaddresses/openaddresses/tree/9ea72b079aaff7d322349e4b812eb43eb94d6d93/sources
Inspiration
Use this dataset to create maps in conjunction with other datasets to map weather, crime, or how your next canoing trip."
OpenAddresses - South America,Addresses and geolocations for South American countries,OpenAddresses,2,"Version 2,2017-08-05|Version 1,2017-08-04","cities
internet",CSV,6 GB,Other,294 views,60 downloads,,0 topics,https://www.kaggle.com/openaddresses/openaddresses-south-america,"Context
OpenAddresses's goal is to connect the digital and physical worlds by sharing geographic coordinates, street names, house numbers and postal codes.
Content
This dataset contains one data file for each of these countries:
Argentina - argentina.csv
Brazil - brazil.csv
Chile - chile.csv
Columbia - columbia.csv
Uraguay - uraguay.csv
Field descriptions:
LON - Longitude
LAT - Latitude
NUMBER - Street number
STREET - Street name
UNIT - Unit or apartment number
CITY - City name
DISTRICT - ?
REGION - ?
POSTCODE - Postcode or zipcode
ID - ?
HASH - ?
Acknowledgements
Data collected around 2017-07-25 by OpenAddresses (http://openaddresses.io).
Address data is essential infrastructure. Street names, house numbers and postal codes, when combined with geographic coordinates, are the hub that connects digital to physical places.
Data licenses can be found in LICENSE.txt.
Data source information can be found at https://github.com/openaddresses/openaddresses/tree/9ea72b079aaff7d322349e4b812eb43eb94d6d93/sources
Inspiration
Use this dataset to create maps in conjunction with other datasets to map weather, crime, or plan your next canoeing trip."
Retail Sales Forecasting,Short term forecasting to optimize in-store inventories,TEVEC Systems,2,"Version 1,2017-08-01",,CSV,22 KB,CC4,"2,081 views",217 downloads,,0 topics,https://www.kaggle.com/tevecsystems/retail-sales-forecasting,"Context
This dataset contains lot of historical sales data. It was extracted from a Brazilian top retailer and has many SKUs and many stores. The data was transformed to protect the identity of the retailer.
Content
[TBD]
Acknowledgements
This data would not be available without the full collaboration from our customers who understand that sharing their core and strategical information has more advantages than possible hazards. They also support our continuos development of innovative ML systems across their value chain.
Inspiration
Every retail business in the world faces a fundamental question: how much inventory should I carry? In one hand to mush inventory means working capital costs, operational costs and a complex operation. On the other hand lack of inventory leads to lost sales, unhappy customers and a damaged brand.
Current inventory management models have many solutions to place the correct order, but they are all based in a single unknown factor: the demand for the next periods.
This is why short-term forecasting is so important in retail and consumer goods industry.
We encourage you to seek for the best demand forecasting model for the next 2-3 weeks. This valuable insight can help many supply chain practitioners to correctly manage their inventory levels."
First Person Narratives of the American South,Personal accounts of Southern life between 1860 and 1920,Documenting the American South (DocSouth),2,"Version 1,2017-08-15","united states
slaves
history
linguistics",CSV,43 MB,Other,573 views,54 downloads,,0 topics,https://www.kaggle.com/docsouth-data/first-person-narratives-of-the-american-south,"""First-Person Narratives of the American South"" is a collection of diaries, autobiographies, memoirs, travel accounts, and ex-slave narratives written by Southerners. The majority of materials in this collection are written by those Southerners whose voices were less prominent in their time, including African Americans, women, enlisted men, laborers, and Native Americans.
The narratives available in this collection offer personal accounts of Southern life between 1860 and 1920, a period of enormous change. At the end of the Civil War, the South faced the enormous challenge of re-creating their society after their land had been ravaged by war, many of their men were dead or injured, and the economic and social system of slavery had been abolished. Many farmers, confronted by periodic depressions and market turmoil, joined political and social protest movements. For African Americans, the end of slavery brought hope for unprecedented control of their own lives, but whether they stayed in the South or moved north or west, they continued to face social and political oppression. Most African Americans in the South were pulled into a Darwinistic sharecropper system and saw their lives circumscribed by the rise of segregation. As conservative views faced a growing challenge from Modernist thought, Southern arts, sciences, and religion also reflected the considerable tensions manifested throughout Southern society. Admidst these dramatic changes, Southerners who had lived in the antebellum South and soldiers who had fought for the Confederacy wrote memoirs that and strived to preserve a memory of many different experiences. Southerners recorded their stories of these tumultuous times in print and in diaries and letters, but few first-person narratives, other than those written by the social and economic elite found their way into the national print culture. In this online collection, accounts of life on the farm or in the servants' quarters or in the cotton mill have priority over accounts of public lives and leading military battles. Each narrative offers a unique perspective on life in the South, and serves as an important primary resource for the study of the American South. The original texts for ""First-Person Narratives of the American South"" come from the University Library of the University of North Carolina at Chapel Hill, which includes the Southern Historical Collection, one of the largest collections of Southern manuscripts in the country and the North Carolina Collection, the most complete printed documentation of a single state anywhere. The DocSouth Editorial Board, composed of faculty and librarians at UNC and staff from the UNC Press, oversees this collection and all other collections on Documenting the American South.
Context
The North American Slave Narratives collection at the University of North Carolina contains 344 items and is the most extensive collection of such documents in the world.
The physical collection was digitized and transcribed by students and library employees. This means that the text is far more reliable than uncorrected OCR output which is common in digitized archives.
More information about the collection and access to individual page images can be be found here: http://docsouth.unc.edu/neh
The plain text files have been optimized for use in Voyant and can also be used in text mining projects such as topic modeling, sentiment analysis and natural language processing. Please note that the full text contains paratextual elements such as title pages and appendices which will be included in any word counts you perform. You may wish to delete these in order to focus your analysis on just the narratives.
The .csv file acts as a table of contents for the collection and includes Title, Author, Publication Date a url pointing to the digitized version of the text and a unique url pointing to a version of the text in plain text (this is particularly useful for use with Voyant: http://voyant-tools.org/).
Copyright Statement and Acknowledgements
With the exception of ""Fields's Observation: The Slave Narrative of a Nineteenth-Century Virginian,"" which has no known rights, the texts, encoding, and metadata available in Open DocSouth are made available for use under the terms of a Creative Commons Attribution License (CC BY 4.0:http://creativecommons.org/licenses/by/4.0/). Users are free to copy, share, adapt, and re-publish any of the content in Open DocSouth as long as they credit the University Library at the University of North Carolina at Chapel Hill for making this material available.
If you make use of this data, considering letting the holder of the original collection know how you are using the data and if you have any suggestions for making it even more useful. Send any feedback to wilsonlibrary@unc.edu.
About the DocSouth Data Project
Doc South Data provides access to some of the Documenting The American South collections in formats that work well with common text mining and data analysis tools.
Documenting the American South is one of the longest running digital publishing initiatives at the University of North Carolina. It was designed to give researchers digital access to some of the library’s unique collections in the form of high quality page scans as well as structured, corrected and machine readable text.
Doc South Data is an extension of this original goal and has been designed for researchers who want to use emerging technology to look for patterns across entire texts or compare patterns found in multiple texts. We have made it easy to use tools such as Voyant (http://voyant-tools.org/) to conduct simple word counts and frequency visualizations (such as word clouds) or to use other tools to perform more complex processes such as topic modeling, named-entity recognition or sentiment analysis."
Satellite Imagery,These images have been uploaded so that they can be used in a kernel.,Prateek Joshi,2,"Version 1,2017-06-19",,Other,5 MB,ODbL,622 views,42 downloads,,0 topics,https://www.kaggle.com/pjoshi15/satellite-imagery,"Feel free to use this data.
Thanks"
Total Expenditure on Health per Capita,Total expenditure on health per capita in USD per country,vikassrivastava,2,"Version 1,2017-02-14",,CSV,39 KB,ODbL,840 views,49 downloads,,,https://www.kaggle.com/onlyricks/healthspending,"Context
Per capita total expenditure on health at average exchange rate (US$)
Content
Per capita total expenditure on health expressed at average exchange rate for that year in US$. Current prices.
Acknowledgements
It is downloaded from WHO, Gapminder.
Inspiration
It seems good to me for forecasting the total spending of money on health around the world, It can be good use case for prediction of money spending on health spending."
Amazon baby dataset,Amazon baby dataset - product reviews,RoopaliKaujalgi,2,"Version 1,2017-02-02",,CSV,47 MB,Other,"1,892 views",142 downloads,,,https://www.kaggle.com/roopalik/amazon-baby-dataset,"Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
US Casualties of the Korean War,Data from The U.S. National Archives and Records Administration,0rangutan,2,"Version 1,2017-02-17","history
war",CSV,7 MB,Other,"1,483 views",128 downloads,,0 topics,https://www.kaggle.com/orangutan/koreanconflict,"Context
Information reproduced from the National Archives:
The Korean Conflict Extract Data File of the Defense Casualty Analysis System (DCAS) Extract Files contains records of U.S. military fatal casualties of the Korean War. These records were transferred into the custody of the National Archives and Records Administration in 2008. The Defense Casualty Analysis System Extract Files were created by the Defense Manpower Data Center (DMDC) of the Office of the Secretary of Defense. The records correspond to the Korean War Conflict statistics on the DMDC web site, which is accessible online at https://www.dmdc.osd.mil/dcas/pages/main.xhtml .
A full series description for the Defense Casualty Analysis System (DCAS) Extract Files is accessible online via the National Archives Catalog under the National Archives Identifier 2240988. The Korean War Conflict Extract Data File is also accessible for direct download via the National Archives Catalog file-level description, National Archives Identifier 2240988.
Content
The raw data files have been cleaned and labelled as best as I can with reference to the accompanying Supplemental Code Lists. Names and ID numbers have been removed out of respect and to provide anonymity.
Data fields: * SERVICE_TYPE * SERVICE_CODE * ENROLLMENT * BRANCH * RANK * PAY_GRADE * POSITION * BIRTH_YEAR * SEX * HOME_CITY * HOME_COUNTY * HOME_STATE * STATE_CODE * NATIONALITY * MARITAL_STATUS * ETHNICITY * ETHNICITY_1 * ETHNICITY_2 * DIVISION * FATALITY_YEAR * FATALITY_DATE * HOSTILITY_CONDITIONS * FATALITY * BURIAL_STATUS
Acknowledgements
Data provided by The U.S. National Archives and Records Administration.
Raw data can be accessed via the following link: https://catalog.archives.gov/id/2240988
Inspiration
By cleaning the data I hope to give wider access to this resource."
US Representation by Zip Code,"The entire nation's representatives, consolidated.",Jay Ravaliya,2,"Version 1,2017-01-31",,{}JSON,228 MB,Other,939 views,22 downloads,,,https://www.kaggle.com/jayrav13/us-representation-by-zip-code,"Why?
https://twitter.com/lindsaylee13/status/826298008824328192
I expressed my interest in using my technical skills anywhere I could. One of the suggestions made was to have a running list of representation throughout the nation.
My approach for this case was to retrieve a list of zip codes and return representation via Google's Civic Information API.
My source for zip codes is: https://www.aggdata.com/node/86
This data set currently has just over 29,000 of the total 43,000 listed here. This is due to:
1) Rate Limit 2) Zip codes not found
My goal is to continue to expand on the list of zip codes to get as comprehensive of a view of representation as possible.
This is not possible without Google's Civic Information API!"
perishable products Colombian markets,Prices of perishable products in the main Colombian markets,Andres Felipe Bayona Chinchilla,2,"Version 1,2017-02-19",,CSV,3 MB,Other,708 views,57 downloads,,,https://www.kaggle.com/afbayonac/perishable-products-colombian-markets,"Content
The bulletins were downloaded from the DANE web platform and organized by products in cvs files.
Acknowledgements
SIPSA"
Classifying wine varieties,Great practice for testing out different algorithms,Bryn Humphreys,2,"Version 1,2017-06-21",,CSV,11 KB,Other,"1,875 views",100 downloads,4 kernels,0 topics,https://www.kaggle.com/brynja/wineuci,"Context
Wine recognition dataset from UC Irvine. Great for testing out different classifiers
Labels: ""name"" - Number denoting a specific wine class
Number of instances of each wine class
Class 1 - 59
Class 2 - 71
Class 3 - 48
Features:
Alcohol
Malic acid
Ash
Alcalinity of ash
Magnesium
Total phenols
Flavanoids
Nonflavanoid phenols
Proanthocyanins
Color intensity
Hue
OD280/OD315 of diluted wines
Proline
Content
""This data set is the result of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines""
Acknowledgements
Lichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.
@misc{Lichman:2013 , author = ""M. Lichman"", year = ""2013"", title = ""{UCI} Machine Learning Repository"", url = ""http://archive.ics.uci.edu/ml"", institution = ""University of California, Irvine, School of Information and Computer Sciences"" }
UC Irvine data base: ""https://archive.ics.uci.edu/ml/machine-learning-databases/wine""
Sources: (a) Forina, M. et al, PARVUS - An Extendible Package for Data Exploration, Classification and Correlation. Institute of Pharmaceutical and Food Analysis and Technologies, Via Brigata Salerno, 16147 Genoa, Italy. (b) Stefan Aeberhard, email: stefan@coral.cs.jcu.edu.au (c) July 1991 Past Usage: (1) S. Aeberhard, D. Coomans and O. de Vel, Comparison of Classifiers in High Dimensional Settings, Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of Mathematics and Statistics, James Cook University of North Queensland. (Also submitted to Technometrics).
The data was used with many others for comparing various classifiers. The classes are separable, though only RDA has achieved 100% correct classification. (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) (All results using the leave-one-out technique)
(2) S. Aeberhard, D. Coomans and O. de Vel, ""THE CLASSIFICATION PERFORMANCE OF RDA"" Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of Mathematics and Statistics, James Cook University of North Queensland. (Also submitted to Journal of Chemometrics).
Inspiration
This data set is great for drawing comparisons between algorithms and testing out classifications models when learning new techniques"
Wiki Words,Word Count from 1 Million Wikipedia Pages,dataist,2,"Version 6,2017-03-08|Version 5,2017-03-08|Version 4,2017-03-08|Version 3,2017-03-08|Version 2,2017-03-08|Version 1,2017-02-17",,CSV,927 KB,Other,"1,497 views",89 downloads,2 kernels,,https://www.kaggle.com/dataistic/wiki-words,"Context
Human communication abilities have greatly evolved with time. Speech/Text/Images/Videos are the channels we often use to communicate, store/share information.""Text"" is one of the primary modes in formal communication and might continue to be so for quite some time.
I wonder, how many words, a person would Type in his lifetime, when he sends an email/text message or prepare some documents. The count might run into millions. We are accustomed to key-in words, without worrying much about the 'Effort' involved in typing the word. We don't bother much about the origin of the word or the correlation between the meaning and the textual representation. 'Big' is actually smaller than 'Small' just going by the words' length.
I had some questions, which, I thought, could be best answered by analyzing the BIG data we are surrounded with today. Since the data volume growing at such high rates, can we bring about some kind of optimization or restructuring in the word usage, so that, we are benefited in terms of Data storage, transmission, processing. Can scanning more documents, would provide better automated suggestions in email / chats, based on what word usually follows a particular word, and assist in quicker sentence completion.
What set of words, in the available text content globally, if we can identify and condense, would reduce the overall storage space required.
What set of words in the regular usage, email/text/documents, if we condense, would reduce the total effort involved in typing (keying-in the text) and reduce the overall size of the text content, which eventually might lead to lesser transmission time, occupy less storage space, lesser processing time for applications which feed on these data for analysis/decision making.
To answer these, we may have to parse the entire web and almost every email/message/blog post/tweet/machine generated content that is in or will be generated in every Phone/Laptop/Computer/Servers, Data generated by every person/bot. Considering tones of text lying around in databases across the world Webpages/Wikipedia/text archives/Digital libraries, and the multiple versions/copies of these content. Parsing all, would be a humongous task. Fresh data is continually generated from various sources. The plate is never empty, if the data is cooked at a rate than the available processing capability.
Here is an attempt to analyze a tiny chunk of data, to see, if the outcome is significant enough to take a note of, if the finding is generalized and extrapolated to larger databases.
Content
Looking out for a reliable source, I could not think of anything better than the Wikipedia database of Webpages. Wiki articles are available for download as html dumps, for any offline processing. https://dumps.wikimedia.org/other/static_html_dumps/, the dump which I downloaded is a ~40 GB compressed file (that turned in ~208 GB folder containing ~15 million files, upon extraction).
With my newly acquired R skills, I tried to parse the html pages, extract the distinct words with their total count in the page paragraphs.I could consolidate the output from the ""first million"" of html files out of available 15 million. Attached dataset ""WikiWords_FirstMillion.csv"" is a Comma Separated file with the list of words and their count. There are two columns - word and count. ""word"" column contains distinct words as extracted from the paragraphs in the wiki pages and ""count"" column has the count of occurrence in one million wiki pages. Non-Alphanumeric characters have been removed at the time of text extraction.
Any array of characters separated by space are included in the list of words and the count has been presented as is without any filters. To get better estimates, it should be OK to make suitable assumptions, like considering root words, ignoring words if they appear more specific to Wikipedia pages (Welcome, Wikipedia, Articles, Pages, Edit, Contribution.. ).
Acknowledgements
Wikimedia, for providing the offline dumps R Community, for the Software/Packages/Blog Posts/Articles/Suggestions and Solution on the Q & A sites
Inspiration
In case, the entire English Language community across the world decides to designate every alphabet as a word [Apart from 'A' and 'I' all other alphabets seem to be potential candidates to be a word, a one-lettered word],
(a) Which of the 24 words from the data set are most eligible to get upgraded as a one letter word. Assuming, it is decided to replace the existing words with the newly designated one-lettered word, to achieve storage efficiency.
(b) Assuming, the word count in the data set is a fair estimate of the composition of the words available in the global text content, (Say we do a ""Find"" and ""Replace"" on global text content). If the current big data size is 3 Exabytes (10 ^ 18), and say 30% of it is text content, how much space reduction can be achieved with (a), assuming 1 character requires 1 byte of storage space.
(c) Suppose, it is decided to accommodate 10 short cut keys for 10 different words,one short cut key for each word. Which 10 words would increase the speed of text documentation assuming, (1) same amount of time is taken to type any word (2) time required to type a word increases with its length
Curious to know the findings!!"
Chicago Crime Data,Public data from 2001 - 2016,Mike Pastore,2,"Version 1,2017-03-09",,CSV,15 MB,Other,"1,616 views",137 downloads,,,https://www.kaggle.com/mpastore/chicago-crime-data,"Context
Publicly available data from the City of Chicago. See more here.
Content
Data is from Jan 1, 2001 - Dec. 31, 2016. The original dataset has over 6,000,000 entries (1.4GB)...this is just a 1% random sample to make computation time quicker and because of Kaggle's 500MB limit.
Please visit this dataset's official page for a more complete description and list of caveats."
British Queen's Oversea Visits,OD datasets for British Queen's oversea visits,LiLi,2,"Version 1,2017-03-13",international relations,CSV,44 KB,ODbL,565 views,49 downloads,,0 topics,https://www.kaggle.com/lorcha/queenvisits,"Context
This dataset is for showing how to visualize OD datasets
Content
This dataset contains all the cities where the british queen has visited in her lifetime.
Acknowledgements
The dataset is obtained from the internet.
Past Research
No
Inspiration
Showing OD dataset is very fun."
Human Instructions - Chinese (wikiHow),82.558 formalised step-by-step instructions in Chinese from wikiHow,paolo,2,"Version 1,2017-03-18",,Other,1 GB,CC4,433 views,37 downloads,,0 topics,https://www.kaggle.com/paolop/human-instructions-chinese-wikihow,"82.558 Human Instructions in Chinese Extracted from wikiHow
Step-by-step instructions in Chinese extracted from wikiHow and decomposed into a formal graph representation in RDF.
This is one of multiple dataset repositories for different languages. For more information, additional resources, and other versions of these instructions in other languages see the main Kaggle dataset page:
https://www.kaggle.com/paolop/human-instructions-multilingual-wikihow
To cite this dataset use: Paolo Pareti, Benoit Testu, Ryutaro Ichise, Ewan Klein and Adam Barker. Integrating Know-How into the Linked Data Cloud. Knowledge Engineering and Knowledge Management, volume 8876 of Lecture Notes in Computer Science, pages 385-396. Springer International Publishing (2014) (PDF) (bibtex)
This dataset is based on original instructions from wikiHow accessed on the 3rd of March 2017.
For any queries and requests contact: Paolo Pareti"
Vectorized Handwritten Digits,Redefining MNIST. A new approach to an old problem.,Donfuzius,2,"Version 1,2017-03-06",,CSV,103 KB,CC0,842 views,37 downloads,8 kernels,,https://www.kaggle.com/donfuzius/vectordigits,"Context
I recently implemented an iPhone App for my younger daughter to learn multiplication. It probably was as much of a learning exercise for me as it was for her. But now I want to tackle the next challenge and add some more smarts with a module to recognise her handwritten digits. Instead of taking a pixel based approach, I took the opportunity of the touch-based input device to record the training data as vectorized paths.
Content
The data contains 400 paths for digits along with their matching labels. Each path is normalized to 20 vectors (2d) and the total length of the vectors is normalized to ~100. Find an example record of the JSON-structure below:
{ ""Items"": [ {""y"":1, ""id"": 1488036701781, ""p"": [[0,0],[1,-2],[7,-9],[3,-5],[6,-9],[2,-3],[3,-4],[1,-1],[1,-1],[0,0],[0,2],[0,10],[0,8],[0,17],[0,6],[0,10],[0,3],[0,3],[0,0],[-1,1]] }] }
Acknowledgements
Thanks to my daughter for the training data!
Inspiration
I hope you have fun with learning and I very much welcome hints on how to better capture the data."
MNIST data,digit images neuralnetworksanddeeplearning.com,BrianOn99,2,"Version 1,2017-03-11",,Other,16 MB,CC0,"1,373 views",178 downloads,24 kernels,0 topics,https://www.kaggle.com/brianon99/mnist-data,MNIST data from http://neuralnetworksanddeeplearning.com
Brazil Gdp & Electricity Consumption,From 1979 to 2015 - sources : World Bank and Bacen.,Especuloide,2,"Version 1,2017-03-16",,CSV,1 KB,CC0,826 views,64 downloads,3 kernels,0 topics,https://www.kaggle.com/robervalt/brazil-gdp-electricity-consumption,"Context
Condensation of data from World Bank (Gdp) and Bacen - Banco Central do Brasil (Electricity consumption). Gdp is expressed in Usd Trillion and Electricity in Terawatt hour year.
Content
Three columns : ""Year"", ""Tw/h"" and ""GDP""
Acknowledgements
World bank and Bacen."
Renewable Energy Generated in the UK,Cleaned data from data.gov.uk,louis,2,"Version 1,2017-03-12",,CSV,14 KB,Other,907 views,59 downloads,4 kernels,,https://www.kaggle.com/louissg/renewable-energy-statistics,"Context
This will act as the base data for the investigation into the possible solutions for the UK energy requirements
Content
A cleaned version of the UK statistics on renewable energy generation.
Acknowledgements
https://www.gov.uk/government/statistics/regional-renewable-statistics7
All content is available under the Open Government Licence v3.0,"
"São Paulo, Brazil - Railroad stations Map",Map of All são paulo railroads stations and train lines,Murilo Viviani,2,"Version 4,2017-07-07|Version 3,2017-06-19|Version 2,2017-06-19|Version 1,2017-06-19",,CSV,9 KB,CC0,171 views,18 downloads,,0 topics,https://www.kaggle.com/mviviani/sp-brrailroadmap,"Context
In progress...
Content
In progress...
Acknowledgements
In progress...
Inspiration
In progress..."
Indie Map,A complete social graph and crawl of the top 2300 IndieWeb sites.,Ryan,2,"Version 1,2017-07-01",,{}JSON,92 MB,CC0,350 views,7 downloads,,,https://www.kaggle.com/snarfed/indiemap,"The IndieWeb is a people-focused alternative to the ""corporate"" web. Participants use their own personal web sites to post, reply, share, organize events and RSVP, and interact in online social networking in ways that have otherwise been limited to centralized silos like Facebook and Twitter.
The Indie Map dataset is a social network of the 2300 most active IndieWeb sites, including all connections between sites and number of links in each direction, broken down by type. It includes:
5.8M web pages, including raw HTML, parsed microformats2, and extracted links with metadata.
631M links and 706K ""friend"" relationships between sites.
380GB of HTML and HTTP requests in WARC format.
The zip file here contains a JSON file for each site, which includes metadata, a list of other sites linked to and from, and the number of links of each type.
The complete dataset of 5.8M HTML pages is available in a publicly accessible Google BigQuery dataset. The raw pages can also be downloaded as WARC files. They're hosted on Google Cloud Storage.
More details in the full documentation.
Indie Map is free, open source, and placed into the public domain via CC0. Crawled content remains the property of each site's owner and author, and subject to their existing copyrights."
Car brands (1970 to 2016),"Car brands, model and versions from 1970 to 2016",ismail turkmen,2,"Version 1,2017-07-02",,CSV,668 KB,Other,475 views,52 downloads,,0 topics,https://www.kaggle.com/ihturkmen/car-versions,"You can find the brand, model and version information of almost all automobile manufacturers between 1970 and 2016 in this database."
Food preference,Analyse the food data to prefer,Ranjithkumar M,2,"Version 1,2017-07-18",,Other,5 MB,ODbL,"1,154 views",227 downloads,,0 topics,https://www.kaggle.com/rkmunusamy/food-preference,"Context
Classification calories and food types
Content
it's useful to analyse the food related queries by finding calories level, preferences, veg type etc.,
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Want to know the food preference quickly ?"
Presidential Cabinet Nominations,Senate confirmation vote records for cabinet nominees since 1976,US Senate,2,"Version 1,2017-02-08",politics,CSV,23 KB,CC0,824 views,60 downloads,,0 topics,https://www.kaggle.com/senate/confirmation-votes,"Context
The United States Constitution provides that the president ""shall nominate, and by and with the Advice and Consent of the Senate, shall appoint Ambassadors, other public Ministers and Consuls, Judges of the Supreme Court, and all other Officers of the United States, whose Appointments are not herein otherwise provided for..."" (Article II, section 2). This provision, like many others in the Constitution, was born of compromise, and, over the more than two centuries since its adoption, has inspired widely varying interpretations.
The president nominates all federal judges in the judicial branch and specified officers in cabinet-level departments, independent agencies, the military services, the Foreign Service and uniformed civilian services, as well as U.S. attorneys and U.S. marshals. The importance of the position, the qualifications of the nominee, and the prevailing political climate influence the character of the Senate's response to each nomination. Views of the Senate's role range from a narrow construction that the Senate is obligated to confirm unless the nominee is manifestly lacking in character and competence, to a broad interpretation that accords the Senate power to reject for any reason a majority of its members deems appropriate. Just as the president is not required to explain why he selected a particular nominee, neither is the Senate obligated to give reasons for rejecting a nominee.
Acknowledgements
The confirmation vote records were recorded, compiled, and published by the Office of the Secretary of the Senate."
QBI Image Enhancement,Data and Notebooks for the Image Enhancement Lecture,Kevin Mader,2,"Version 1,2017-03-01",,Other,6 MB,CC0,879 views,50 downloads,10 kernels,0 topics,https://www.kaggle.com/kmader/qbi-image-enhancement,"Image Enhancement
The data accompanying the lecture about Image Enhancement from Anders Kaestner as part of the Quantitative Big Imaging Course.
The slides for the lecture are here"
US Presidents heights: How low can u go?,Data set including the heights of the presidents of the USA [not the band],Zurda,2,"Version 1,2017-02-21",,CSV,1008 B,Other,747 views,71 downloads,3 kernels,0 topics,https://www.kaggle.com/hakabuk/us-presidents-heights-how-low-can-u-go,"Context
This is a small data set of US presidents heights."
Tsunamis History,DataSet of tsunamis during the history.,Rosanaider,2,"Version 1,2017-02-26",,Other,528 KB,Other,873 views,72 downloads,,,https://www.kaggle.com/rosado/tsunamishistory,"Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
Open Data 500 Companies,"A slightly modified version, to be used for personal research.",Jason Schenck,2,"Version 1,2017-05-09",,CSV,81 KB,Other,751 views,52 downloads,,,https://www.kaggle.com/jsche4/open-data-500-companies,"Content
This is a slightly modified version of the openly available data set 'Open Data 500 Companies - Full List' provided by the OD500 Global Network ('http://www.opendata500.com/').
I am using this dataset for a kernel project series which will be investigating the value and worth of a company's choice of logo design. Therefore, have removed columns such as ""description"" and ""short description"", as well as a few others. If you'd like the entire original dataset please download from the original source here --> 'http://www.opendata500.com/us/download/us_companies.csv'
Acknowledgements
I take no credit for the collection, production, or presentation of this data set. I am simply using it for a person research study. The creators are: http://www.opendata500.com/us/list/"
Crashes 2014 csv,Dataset gives details of crashes of 2014,QuinnCarver,2,"Version 1,2017-02-24",,CSV,129 MB,Other,565 views,59 downloads,,0 topics,https://www.kaggle.com/qcarver/crashes-2014-csv,"Context
Automotive crash data for 2014
Content
Data on weather conditions, people involved, locations etc where traffic accidents have occured.
Acknowledgements
This dataset is simply a port to csv from an xslx dataset uploaded by Aditi
Inspiration
Interested to know what are the factors involved that make auto accidents fatal"
Site clicks (hits) database,Scv file with codded names of parameters,IrinaAchkasova,2,"Version 1,2017-03-03",,CSV,149 MB,Other,504 views,24 downloads,,0 topics,https://www.kaggle.com/irinaachkasova/site-clicks-hits-database,This codded dataset is from marketing agency and reflact internet users activity at the site page.
Ice core DML94C07_38,"Dronning Maud Land, Antarctica",maarten,2,"Version 1,2017-02-22",,Other,29 KB,CC4,462 views,13 downloads,,0 topics,https://www.kaggle.com/maartenko/ice-core-dml94c07-38,"Context
Calcium, dust and nitrate concentrations in monthly resolution in ice core DML94C07_38 (B38). doi:10.1594/PANGAEA.837875
Content
Acknowledgements
Schmidt, Kerstin; Wegner, Anna; Weller, Rolf; Leuenberger, Daiana; Tijm-Reijmer, Carleen H; Fischer, Hubertus (2014): Calcium, dust and nitrate concentrations in monthly resolution in ice core DML94C07_38 (B38). doi:10.1594/PANGAEA.837875, In supplement to: Schmidt, Kerstin; Wegner, Anna; Weller, Rolf; Leuenberger, Daiana; Fischer, Hubertus (submitted): Variability of aerosol tracers in ice cores from coastal Dronning Maud Land, Antarctica. The Cryosphere
Inspiration
test data"
Sample Insurance Portfolio,"The sample insurance file contains 36,634 records in Florida for 2012",SachGupta,2,"Version 1,2017-05-24",finance,CSV,4 MB,Other,"2,622 views",405 downloads,4 kernels,,https://www.kaggle.com/sachgupta/sample-insurance-portfolio,"Context
Aggregate and visualize data how you want—from tabular to graphical and geographic forms—for more profitable underwriting.
Perform sophisticated, multi-dimensional analysis to supply any data combination and permutation.
Respond to claims quickly and improve customer satisfaction with real-time and historical access to catastrophe and hazard data
Content
The sample insurance file contains 36,634 records in Florida for 2012 from a sample company that implemented an agressive growth plan in 2012. There are total insured value (TIV) columns containing TIV from 2011 and 2012, so this dataset is great for testing out the comparison feature. This file has address information that you can choose to geocode, or you can use the existing latitude/longitude in the file."
Real Estate,New Hampshire and Vermont,samdeeplearning,2,"Version 1,2017-05-23",,Other,3 MB,Other,"1,581 views",178 downloads,,2 topics,https://www.kaggle.com/samdeeplearning/real-estate-vermont-newhampshire,"Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
Foursquare Tips,Sentiment Analysis of Portuguese Tips,thaisalmeida,2,"Version 1,2017-02-26",,CSV,18 MB,Other,"1,054 views",35 downloads,,,https://www.kaggle.com/thaisalmeida/tips-foursquare,"This was the dataset used in the article 'Sentiment Analysis of Portuguese Comments from Foursquare' (doi:10.1145/2976796.2988180).
The dataset is composed of tips referring to localities of the city of São Paulo/Brazil. To the data collection, we use the Foursquare API . The tips belong to the foursquare's categories: Food, Shop & Service and Nightlife Spot. The database has a total of 179,181 tips.
@inproceedings{Almeida:2016:SAP:2976796.2988180, author = {Almeida, Thais G. and Souza, Bruno A. and Menezes, Alice A.F. and Figueiredo, Carlos M.S. and Nakamura, Eduardo F.}, title = {Sentiment Analysis of Portuguese Comments from Foursquare}, booktitle = {Proceedings of the 22Nd Brazilian Symposium on Multimedia and the Web}, series = {Webmedia '16}, year = {2016}, isbn = {978-1-4503-4512-5}, location = {Teresina, Piau\&#237; State, Brazil}, pages = {355--358}, numpages = {4}, url = {http://doi.acm.org/10.1145/2976796.2988180}, doi = {10.1145/2976796.2988180}, acmid = {2988180}, publisher = {ACM}, address = {New York, NY, USA}, keywords = {foursquare, sentiment analysis, supervised learning}, }"
Word in French,"Various statistics on French words (Pronunciation, frequency, lemma...)",StephRouen,2,"Version 1,2017-02-27",,CSV,25 MB,CC4,610 views,37 downloads,,0 topics,https://www.kaggle.com/stephrouen/word-in-french,"Context
Lexique v3.81 on www.lexique.org/
Content
Words of the French language with pronunciation, grouping and statistics.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
USD Vs INR in past 10 year,have last 10 year average doller rate against year,jinesh John,2,"Version 1,2017-05-26",,CSV,293 B,Other,289 views,23 downloads,,0 topics,https://www.kaggle.com/jinesh777/usd-vs-inr-in-past-10-year,"Here i have data for last 10 year average Indian rupee against usd. it will try to predict the value of a year.
i am using leaner regression for this
i was new to machine learning. i was thinking of how usd values made changes in india how that changes happen if we passed any year this program will try to predict"
Admob data set,Admob data set to get an idea about earnings,bader,2,"Version 1,2017-05-12",,CSV,20 KB,CC0,288 views,11 downloads,,0 topics,https://www.kaggle.com/sulemanbader/admob-data-set,"Context
Increase your earnings through analyzing the admob data set.
Content
it has more than 12 column including earnings, impressions, and other attributes
Acknowledgements
Suleman bader owns this data set compiled by google admob.
Inspiration
To increase your earnings"
Council Plan performance indicators,Coventry City Council,sichunlam,2,"Version 3,2018-01-30|Version 2,2017-07-31|Version 1,2017-05-03",politics,CSV,146 KB,CC0,613 views,19 downloads,,0 topics,https://www.kaggle.com/sichunlam/coventrycc-performance-indicators,"Council Plan performance indicators by Coventry City Council
The Council Plan sets out Coventry City Council's vision and priorities for the city.
Content
These are the performance indicators in the Council Plan performance report in an open data format.
Acknowledgements
Coventry City Council
Find out more at: www.coventry.gov.uk/performance/ Visualisation: https://smarturl.it/CovPerformanceData"
Scheduling in Cloud computing,Deadline sensitive lease scheduling in cloud computing,MACHINE LEARNING DATASETS,2,"Version 2,2017-06-12|Version 1,2017-05-04",,Other,51 KB,CC0,700 views,88 downloads,,,https://www.kaggle.com/pitasr/scheduling-in-cloud-computing,"The OpenNebula is an open source environment which provides cloud resources with the help of Haizea as a lease manager. The Haizea supports different types of leases from which deadline sensitive lease is one of them. In real time, most of the leases are deadline sensitive leases. These deadline sensitive leases are scheduled by using the backfilling based scheduling algorithms.
Please don't forget to cite:
Nayak, Suvendu Chandan, and Chitaranjan Tripathy. ""Deadline sensitive lease scheduling in cloud computing environment using AHP."" Journal of King Saud University-Computer and Information Sciences (2016)."
DolarToday & SIMADI Scrap,Venezuela's Daily Crazy Currency Exchange Scraper for R,Bernardo Lares,2,"Version 1,2017-05-25",,Other,2 KB,CC0,330 views,11 downloads,,0 topics,https://www.kaggle.com/bernardolares/dolartoday-simadi-scrap,"Import libraries
    suppressMessages(library(XLConnect))
    suppressMessages(library(xlsx))
    suppressMessages(library(rvest))
    suppressMessages(library(dplyr))
    suppressMessages(library(lubridate))
    suppressMessages(library(ggplot2))
    suppressMessages(library(plotly))

    setwd("".../R DolarToday/"")

    #Download & import data from Dolartoday.com | The ""url"" changes every 5 minutes aprox
    url <- 'https://dolartoday.com/indicadores/'
    url <- read_html(url) %>% html_nodes('a') %>% html_attrs()
    url <- as.character(url[grep('xlsx',url)])
    download.file(url,""dolartoday.xlsx"",mode=""wb"")
    dt <- read.xlsx(""dolartoday.xlsx"", sheetName=""DolarToday"", colIndex = 1:2)
    dt$Fecha <- as.Date(dt$Fecha, format = ""%m-%d-%Y"")
    dt$Año <- as.numeric(format(dt$Fecha,'%Y'))
    dt$DolarToday <- gsub("","",""."",dt$DolarToday)
    dt$DolarToday <- as.numeric(dt$DolarToday)

    #Add SIMADI data
    simadi <- 'http://cambiobolivar.com/sistema-marginal-de-divisas/'
    simadi <- read_html(simadi) %>% html_nodes('.wp-table-reloaded') %>% html_table()
    simadi <- rbind(as.data.frame(simadi[1]),
                    as.data.frame(simadi[2]),
                    as.data.frame(simadi[3]),
                    as.data.frame(simadi[4]))
    simadi$Fecha <- as.Date(simadi$Fecha,format=""%d/%m/%Y"")
    simadi$Tasa.SIMADI <- as.numeric(sub("","",""."",sub("","",""."",simadi$Tasa.SIMADI)))
    colnames(simadi) <- c(""Fecha"",""SIMADI"")

    #Merge data
    dt <- (merge(dt, simadi, all = TRUE))
    dt <- mutate(dt,Relación=DolarToday/SIMADI)

    # First plot
    plot <- dt[dt$Año>=2016,]
    plot_ly(plot, x = ~Fecha, y = ~DolarToday, name = 'DolarToday',type='scatter',connectgaps=TRUE,mode='lines') %>%
      add_trace(y = ~SIMADI, name='SIMADI',mode='lines', connectgaps=TRUE) %>%
      layout(title=""Dólar Paralelo en Venezuela"",
             xaxis = list(title = ""Fecha""),
             yaxis = list(title = ""Bolívares por USD"")
    )

# Second plot
    dt$Mes <- month(dt$Fecha)
    dt$Año_mes <- paste(dt$Año,dt$mes,sep=""-"")
    ggplot(filter(dt,
                  Año >= 2016), 
           aes(x = as.factor(Mes), y = DolarToday)) +
      geom_jitter(alpha = 0.7) +
      geom_boxplot(fill = 'red', alpha = 0.7) +
      ggtitle(""Boxplot: DolarToday por mes"") +
      theme_bw() +
      scale_y_continuous(name = ""DolarToday"") +
      scale_x_discrete(name = ""Meses"") +
      facet_grid(Año ~ .)
    summary(filter(dt,Año==2017)%>%select(DolarToday))"
Jewish Baby Names,"Jewish names of boys and girls, include the meaning of the names",NetanelMalka,2,"Version 3,2017-05-08|Version 2,2017-05-08|Version 1,2017-05-08",,CSV,11 KB,Other,263 views,22 downloads,,0 topics,https://www.kaggle.com/netanel246/jewish-baby-names,"This describe names of girls and boys, include the meaning of there names.
The cells are: Gender, Name, Meaning, Origin
This data can help to enrich other's data sets."
Mines vs Rocks,For some reason I am required to include a subtitle,Matthew Carter,2,"Version 1,2017-05-27",,CSV,86 KB,Other,756 views,110 downloads,5 kernels,0 topics,https://www.kaggle.com/mattcarter865/mines-vs-rocks,"Connectionist Bench (Sonar, Mines vs. Rocks) Data Set
Content
http://archive.ics.uci.edu/ml/datasets/connectionist+bench+(sonar,+mines+vs.+rocks)"
Money Supply M2 BRIC economies,time series 1997-01-01 to 2015-12-01,Luigi,2,"Version 1,2017-05-20","finance
economics",CSV,23 KB,Other,271 views,21 downloads,,,https://www.kaggle.com/luigimersico/moneysupplym2bric,"Content
Money Supply M2 (Broader Money) BRIC economies in $ 1997-01-01 to 2015-12-01"
Mercedes Benz car sales data,USA market in units 2008-2016,Luigi,2,"Version 1,2017-05-23","business
economics
marketing",CSV,580 B,CC0,"1,323 views",151 downloads,,0 topics,https://www.kaggle.com/luigimersico/mercedes-benz-car-sales-data,frequency quarterly
The UN Refugee Agency Speeches,Speeches made by the High Commissioner of the UN Refugee Agency,Ben Rudolph,2,"Version 2,2017-05-24|Version 1,2017-05-20","politics
international relations
linguistics",CSV,21 MB,CC0,"1,270 views",74 downloads,3 kernels,,https://www.kaggle.com/benrudolph/unhcr-speeches,"Context
This is speeches scraped from unhcr.org website. It includes all speeches made by the High Commissioner up until June 2014.
Content
JSON Array of speeches with the following keys: ""author"", ""by"", ""content"", ""id"", ""title"".
Acknowledgements
www.unhcr.org
Inspiration
What words are most used? Which countries are mentioned? What do the high commissioners think about innovation? Gay rights?"
"Precipitation in Syracuse, NY",Summer droughts in CNY,Sam Edelstein,2,"Version 1,2016-08-18",,CSV,13 KB,CC0,647 views,34 downloads,2 kernels,0 topics,https://www.kaggle.com/samedelstein/precipitation-in-syracuse-ny,"This is filtered weather data that includes the date, precipitation, and average precipitation in Syracuse, NY from June-September 2011-2016."
US Flight Delay,Flight Delays for year 2016,NiranjanDeshpande,2,"Version 1,2017-05-19",,CSV,393 MB,Other,"1,307 views",163 downloads,,0 topics,https://www.kaggle.com/niranjan0272/us-flight-delay,"US Domestic Flights Delay: flight delays in the month of January,August, November and December of 2016"
The Best Recommender Engine : MovieLens,"20000263 ratings and 465564 tag applications, 27278 movies, 138,000 users",Prajit Datta,2,"Version 1,2017-01-05",,Other,58 MB,CC0,"4,541 views",177 downloads,,0 topics,https://www.kaggle.com/prajitdatta/the-best-recommender-engine-movielens,"Summary
This dataset (ml-20m) describes 5-star rating and free-text tagging activity from MovieLens, a movie recommendation service. It contains 20000263 ratings and 465564 tag applications across 27278 movies. These data were created by 138493 users between January 09, 1995 and March 31, 2015. This dataset was generated on October 17, 2016.
Users were selected at random for inclusion. All selected users had rated at least 20 movies. No demographic information is included. Each user is represented by an id, and no other information is provided.
The data are contained in six files, genome-scores.csv, genome-tags.csv, links.csv, movies.csv, ratings.csv and tags.csv. More details about the contents and use of all these files follows.
Usage License
Neither the University of Minnesota nor any of the researchers involved can guarantee the correctness of the data, its suitability for any particular purpose, or the validity of results based on the use of the data set. The data set may be used for any research purposes under the following conditions:
The user may not state or imply any endorsement from the University of Minnesota or the GroupLens Research Group. - The user must acknowledge the use of the data set in publications resulting from the use of the data set (see below for citation information).
The user may not redistribute the data without separate permission.
The user may not use this information for any commercial or revenue-bearing purposes without first obtaining permission from a faculty member of the GroupLens Research Project at the University of Minnesota.
The executable software scripts are provided ""as is"" without warranty of any kind, either expressed or implied, including, but not limited to, the implied warranties of merchantability and fitness for a particular purpose. The entire risk as to the quality and performance of them is with you. Should the program prove defective, you assume the cost of all necessary servicing, repair or correction.
In no event shall the University of Minnesota, its affiliates or employees be liable to you for any damages arising out of the use or inability to use these programs (including but not limited to loss of data or data being rendered inaccurate).
Citation
To acknowledge use of the dataset in publications, please cite the following paper:
F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages. DOI=http://dx.doi.org/10.1145/2827872
Content and Use of Files
Verifying the Dataset Contents
We provide a MD5 checksum with the same name as the downloadable .zip file, but with a .md5 file extension. To verify the dataset:
on linux
md5sum ml-20m.zip; cat ml-20m.zip.md5
on OSX
md5 ml-20m.zip; cat ml-20m.zip.md5
windows users can download a tool from Microsoft (or elsewhere) that verifies MD5 checksums
Check that the two lines of output contain the same hash value.
Formatting and Encoding
The dataset files are written as comma-separated values files with a single header row. Columns that contain commas (,) are escaped using double-quotes (""). These files are encoded as UTF-8. If accented characters in movie titles or tag values (e.g. MisÃ©rables, Les (1995)) display incorrectly, make sure that any program reading the data, such as a text editor, terminal, or script, is configured for UTF-8.
User Ids
MovieLens users were selected at random for inclusion. Their ids have been anonymized. User ids are consistent between ratings.csv and tags.csv (i.e., the same id refers to the same user across the two files).
Movie Ids
Only movies with at least one rating or tag are included in the dataset. These movie ids are consistent with those used on the MovieLens web site (e.g., id 1 corresponds to the URL https://movielens.org/movies/1). Movie ids are consistent between ratings.csv, tags.csv, movies.csv, and links.csv (i.e., the same id refers to the same movie across these four data files).
Ratings Data File Structure (ratings.csv)
All ratings are contained in the file ratings.csv. Each line of this file after the header row represents one rating of one movie by one user, and has the following format: userId,movieId,rating,timestamp
The lines within this file are ordered first by userId, then, within user, by movieId.
Ratings are made on a 5-star scale, with half-star increments (0.5 stars - 5.0 stars).
Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.
Tags Data File Structure (tags.csv)
All tags are contained in the file tags.csv. Each line of this file after the header row represents one tag applied to one movie by one user, and has the following format:
userId,movieId,tag,timestamp The lines within this file are ordered first by userId, then, within user, by movieId.
Tags are user-generated metadata about movies. Each tag is typically a single word or short phrase. The meaning, value, and purpose of a particular tag is determined by each user.
Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.
Movies Data File Structure (movies.csv)
Movie information is contained in the file movies.csv. Each line of this file after the header row represents one movie, and has the following format:
movieId,title,genres Movie titles are entered manually or imported from https://www.themoviedb.org/, and include the year of release in parentheses. Errors and inconsistencies may exist in these titles.
Genres are a pipe-separated list, and are selected from the following:
Action
Adventure
Animation
Children's
Comedy
Crime
Documentary
Drama
Fantasy
Film-Noir
Horror
Musical
Mystery
Romance
Sci-Fi
Thriller
War
Western
(no genres listed)
Links Data File Structure (links.csv)
Identifiers that can be used to link to other sources of movie data are contained in the file links.csv. Each line of this file after the header row represents one movie, and has the following format:
movieId,imdbId,tmdbId movieId is an identifier for movies used by https://movielens.org. E.g., the movie Toy Story has the link https://movielens.org/movies/1.
imdbId is an identifier for movies used by http://www.imdb.com. E.g., the movie Toy Story has the link http://www.imdb.com/title/tt0114709/.
tmdbId is an identifier for movies used by https://www.themoviedb.org. E.g., the movie Toy Story has the link https://www.themoviedb.org/movie/862.
Use of the resources listed above is subject to the terms of each provider.
Tag Genome (genome-scores.csv and genome-tags.csv)
This data set includes a current copy of the Tag Genome.
The tag genome is a data structure that contains tag relevance scores for movies. The structure is a dense matrix: each movie in the genome has a value for every tag in the genome.
As described in this article, the tag genome encodes how strongly movies exhibit particular properties represented by tags (atmospheric, thought-provoking, realistic, etc.). The tag genome was computed using a machine learning algorithm on user-contributed content including tags, ratings, and textual reviews.
The genome is split into two files. The file genome-scores.csv contains movie-tag relevance data in the following format:
movieId,tagId,relevance The second file, genome-tags.csv, provides the tag descriptions for the tag IDs in the genome file, in the following format:
tagId,tag The tagId values are generated when the data set is exported, so they may vary from version to version of the MovieLens data sets.
Cross-Validation
Prior versions of the MovieLens dataset included either pre-computed cross-folds or scripts to perform this computation. We no longer bundle either of these features with the dataset, since most modern toolkits provide this as a built-in feature. If you wish to learn about standard approaches to cross-fold computation in the context of recommender systems evaluation, see LensKit for tools, documentation, and open-source code examples."
"Donald J. Trump For President, Inc",Itemized expenditures totaling over 6M USD,Megan Risdal,2,"Version 1,2017-04-17","presidents
finance
politics",CSV,85 KB,Other,"1,344 views",76 downloads,6 kernels,,https://www.kaggle.com/mrisdal/donald-j-trump-for-president-inc,"Did you know that Donald J. Trump For President, Inc paid for a subscription to The New York Times on November 30th, 2016? Curious to see where else over six million USD was spent and for what purposes? This dataset was downloaded from Pro Publica so you can find out.
Content
Here's what you get:
Line number
Entity type
Payee name
Payee state
Date
Amount
Purpose
Contributions
Want to contribute to this dataset? Download contributions to Donald J. Trump, Inc here and share on the dataset's discussion forum.
Acknowledgements
This data was downloaded from Pro Publica. You can read about their data terms of use here."
mnist for tf,mnist for explore tensorflow,JerryWang,2,"Version 3,2017-05-16|Version 2,2017-05-16|Version 1,2017-05-16",,Other,15 MB,ODbL,506 views,32 downloads,19 kernels,0 topics,https://www.kaggle.com/miningjerry/mnist-for-tf,"Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
Open Postcode Geo,UK postcodes with additional fields optimised for geospace applications.,Dan Winchester,2,"Version 1,2016-11-16",,CSV,269 MB,Other,592 views,33 downloads,,0 topics,https://www.kaggle.com/danwinchester/open-postcode-geo,"UK postcodes with the below additional geospace fields:
Latitude
Longitude
Easting
Northing
Positional quality indicator
Postcode area
Postcode district
Postcode sector
Outcode
Incode
Country
Usecases
Geocode any dataset containing UK postcodes using either latitude and longitude or northing and easting.
Geocode older data using terminated postcodes.
Recognise postcodes in a variety of different formats, for example ""RG2 6AA"" and ""RG26AA"".
Group records into geographical hierarchies using postcode area, postcode district and postcode sector (see below graphic).
Format of a UK Postcode
Documentation
Full documentation is available on the Open Postcode Geo homepage:
https://www.getthedata.com/open-postcode-geo
Version
August 2016.
Updated quarterly, for the most up to date version please see:
https://www.getthedata.com/open-postcode-geo
API
Open Postcode Geo is also available as an API.
Licence
UK Open Government Licence
Attribution required (see ONS Licences for more info):
Contains OS data © Crown copyright and database right (2016)
Contains Royal Mail data © Royal Mail copyright and Database right (2016)
Contains National Statistics data © Crown copyright and database right (2016)
Derived from the ONS Postcode Directory.
Acknowledgements
Open Postcode Geo was created and is maintained by GetTheData, an open data portal organising UK open data geographically and signposting the source."
Tesla Stock Price,06/29/2017 to 03/17/2017,Rolando P. Aguirre,2,"Version 1,2017-03-19",,CSV,107 KB,ODbL,"1,082 views",105 downloads,,0 topics,https://www.kaggle.com/rpaguirre/tesla-stock-price,"Context
The subject matter of this dataset explores Tesla's stock price from its initial public offering (IPO) to yesterday.
Content
Within the dataset one will encounter the following:
The date - ""Date""
The opening price of the stock - ""Open""
The high price of that day - ""High""
The low price of that day - ""Low""
The closed price of that day - ""Close""
The amount of stocks traded during that day - ""Volume""
The stock's closing price that has been amended to include any distributions/corporate actions that occurs before next days open - ""Adj[usted] Close""
Acknowledgements
Through Python programming and checking Sentdex out, I acquired the data from Yahoo Finance. The time period represented starts from 06/29/2010 to 03/17/2017.
Inspiration
What happens when the volume of this stock trading increases/decreases in a short and long period of time? What happens when there is a discrepancy between the adjusted close and the next day's opening price?"
Trump's Taiwan Call,What are people tweeting regarding trump's taiwan call ?,Ujjwal,2,"Version 1,2016-12-03",,Other,126 KB,ODbL,747 views,25 downloads,,0 topics,https://www.kaggle.com/ujjwalsaxena/trumps-taiwan-call-tweets,"This data set contains recent tweets regarding The trump's call to Taiwan. I mined twitter for recent 2000 tweets. it's 3rd Dec, 2016, 6:19 Indian Standard Time. Can be used for sentiment analysis and future predictions. Thanx to me and Twitter Did You like it ??"
Crime Classification dataset,San Fransisco Police Department Crime Classification,Abida Aslam,2,"Version 1,2016-12-03",,CSV,398 KB,Other,"1,683 views",139 downloads,2 kernels,0 topics,https://www.kaggle.com/abidaaslam/crime,"Context
Crime Classification
Content
Crime Classification
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
Osu! Standard Rankings,Analysis of osu! standard rankings on 03/26/2017,Svidon,2,"Version 1,2017-03-30",,CSV,10 KB,Other,615 views,29 downloads,3 kernels,0 topics,https://www.kaggle.com/svidon/osurankings,"Context
osu! is a music rythm game that has 4 modes (check for more infos). In this dataset you can examine the rankings of the standard mode, taken on 26/03/2017 around 12PM. Ranking is based on pp (performance points) awarded after every play, which are influenced by play accuracy and score; pps are then summed with weights: your top play will award you the whole pp points of the map, then the percentage is decreased (this can maintain balance between strong players and players who play too much). You can find here many other statistics.
Content
The dataset contains some columns (see below) reporting statistics for every player in the top100 of the game in the standard mode. The ranking are ordered by pp. Some players seem to have the same points, but there are decimals which are not shown in the ranking chart on the site
Acknowledgements
I created this dataset on my own, so if you find something wrong please report. The data is public and accessible on this link ranking.
Inspiration
I uploaded this just for newcomers in the world (like me) who want an easy stuff to work with, but that can give a lot of results. Check out my Notebook that will soon follow"
Contributions to Presidential Campaigns (real),by Residents of North Carolina only! This is real-time data.,Gautam Doshi,2,"Version 1,2016-12-24",,CSV,22 MB,Other,"1,041 views",28 downloads,2 kernels,0 topics,https://www.kaggle.com/gndoshi/presidentialcampaignsncdata,"Context
Financial Contributions to Presidential Campaigns by the residents of North Carolina
Content
I acquired the data for free on the website: http://fec.gov/disclosurep/pnational.do. Out of all the states, I chose NC since I currently live in Raleigh, NC. The data set mostly consists of Categorical data such as the 'candidate name', 'contributor name', etc.
Acknowledgements
Udacity - Exploratory Data Analysis online class through which I was taken to the above website and I completed a project analyzing this data.
Inspiration
I have analyzed the data and made new variables too. I'm trying to think how linear models can be applied to such a data set and to my knowledge, it is really a waste of time. If you want to see my project, I will be posting it here and also would love to hear your opinion on what further analyses could be done/how it could be done!
Thank you!"
2015 Canadian General Election results,Results across all polling stations and candidates including political party,cedrikfd,2,"Version 1,2017-01-02",,CSV,40 MB,Other,"1,259 views",34 downloads,,3 topics,https://www.kaggle.com/cedrikfd/2015canadianelections,"Context
Looking at elections on TV or online is something, but that implies a ton of data. I was curious to see just how much. This is only a tiny bit of what analysts use.
Content
This represents all votes for all polling stations in the 42nd General Election, per cadidate.
Acknowledgements
I won't take much credit; data comes straight from the Elections Canada website. I only rearranged the data."
Terrorism Attack in the World (1970-2015),an interactive geographic visualization,Vladimir Alencar,2,"Version 1,2017-04-25",,CSV,5 KB,CC0,463 views,40 downloads,6 kernels,0 topics,https://www.kaggle.com/valencar/terrorism-attack-in-the-world,"Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
NBA player info,Only includes data from 2000-2005,DavidWesley-James,2,"Version 1,2017-03-20",,CSV,596 KB,Other,584 views,54 downloads,,0 topics,https://www.kaggle.com/davewj03/baskeballplayers,"Acknowledgments
This dataset was downloaded from the Open Source Sports website. It did not come with an explicit license, but based on other datasets from Open Source Sports, we treat it as follows:
This database is copyright 1996-2015 by Sean Lahman.
This work is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License. For details see: http://creativecommons.org/licenses/by-sa/3.0/
This is modified to only include players info from 2000-2005"
VR games list,List of VR capable games,Sridhar,2,"Version 1,2017-03-20",,CSV,20 KB,CC0,823 views,41 downloads,4 kernels,,https://www.kaggle.com/sridar1803/vr-games-mar17,"Context
I was recently tasked to put together a list of VR games.
Content
This is a single column list with titles of VR capable games.
Acknowledgements
This list was put together from info available on Steam website.
Inspiration
Wanted to put together a word cloud and see what words stand out."
Air Passengers,Number of air passengers per month,RakanNimer,2,"Version 1,2017-03-29",,CSV,2 KB,ODbL,"3,007 views",410 downloads,4 kernels,0 topics,https://www.kaggle.com/rakannimer/air-passengers,"Context
Air Passengers per month. Workshop dataset"
A Visual and Intuitive Train-Test Pattern,How good is your algorithm? A visual inspection of machine learning,Oscar Takeshita,2,"Version 1,2017-04-19","data visualization
multiclass classification",Other,992 KB,CC0,"1,003 views",35 downloads,8 kernels,,https://www.kaggle.com/pliptor/a-visual-and-intuitive-traintest-pattern,"Context
While studying neural networks in machine learning, I found an ingenious 2-D scatter pattern at the cn231 course by Andrej Karpathy. Decision boundaries for the three classes of points cannot be straight lines. He uses it to compare the behavior of a linear classifier and a neural network classifier. Often, we are content with the percentage of accuracy of our prediction or classification algorithm but a visualization tool helps our intuition about the trends and behavior of the classifier. It was definitely useful to catch something strange in the last example of this description. How does your preferred method or algorithm do with this input?
Content
The 2D scatter train pattern is a collection of 300 points with (X,Y) coordinates where -1<X,Y<1.
The 300 points are divided into 3 classes of 100 points each whose labels are 0, 1 and 2. Each class is an arm in the spiral.
The test file is simply a grid covering a square patch within the (X,Y) coordinate boundaries. The idea is to predict and assign labels to each point and visualize the decision boundaries and class areas.
The train pattern is provided in two formats: libsvm and csv formats. The test pattern is also provided in the above formats.
If you want to reproduce the data yourself or modify parameters, either run the python script in cn231 or you may check PicoNN out for a C++ version.
Here's the output I get with the neural network classifier PicoNN
An SVM classifier with radial basis kernel yields the following output. It looks better than the above neural network one.
An SVM classifier with linear kernel yields the following output. One can see it won't handle curved decision regions.
An SVM classifier with polynomial kernel yields the following output. Interestingly it decided to bundle the center of the spiral.
An SVM classifier with sigmoid kernel yields the following output. There is definitely something going on. A bug in my code? I could had wrongfully discarded this kernel for bad accuracy. Visualization was definitely useful for this case.
Acknowledgements
I thank Andrej Karpathy for making his excellent cn231 machine learning course freely available.
Inspiration
I was happy with the result of the 100-node neural network. I then tested with libsvm obtaining similar results. How do other algorithms do with it? Would you draw yourself a different decision boundary?"
NSE India stocks (companies),Intraday datasets (2015-2016),ramamet,2,"Version 1,2017-05-17",,CSV,2 GB,CC0,875 views,153 downloads,,0 topics,https://www.kaggle.com/ramamet4/nse-company-stocks,"Context
In this study, large number of National Stock Exchange(NSE), India stocks under different sectors are mined from various financial websites and data analytic steps are followed. Primary goal of this work is to explore the hidden context patterns between diverse group of stocks and discover the predictive analytic knowledge using machine learning algorithms. The transaction dataset are captured for NSE stocks using statistical computing software R. The price of the stock is determined by the market forces. Buyers and sellers quote the preferred price, so there is a dynamic data day by day. Though it is difficult to identify when to buy and sell the stock, technical indicators may support us to forecast the future price
Content
A data frame with 8 variables: index, date, time, open, high, low, close and id. For each year from 2013 to 2016, the number of trading data of each minute of given each date. The currency of the price is Indian Rupee (INR).
Code : market id
Date : numerical value (Ex. 20151203- to be converted as 2015/12/03)
Time : factor (Ex. 09:16)
Open : numeric (opening price)
High : numeric (high price)
Low : numeric (low price)
Close : numeric (closing price)
Volume : numeric (total volume traded)
Acknowledgements
References [1] Brett Lantz, Machine Learning with R . Packt Publishing Ltd., Birmingham, UK , 2015. [2] The R Project https://www.r-project.org/ [3] https://finance.yahoo.com/ [4] https://www.google.com/finance [5] https://www.nseindia.com/
Inspiration
machine learning (NSE stocks)"
Philadelphia Real Estate,Sample dataset of Philadelphia real estate for analysis,Harry,2,"Version 2,2017-05-13|Version 1,2017-05-13",,CSV,216 KB,Other,882 views,106 downloads,,,https://www.kaggle.com/harry007/philly-real-estate-data-set-sample,"Context
Real estate data set of Philly.
Content
Data set included Addresses, sales price, crime rate and rank by zipcode, school ratings and rank by zipcode, walkscore and rank by zip code, approximate rehab cost,
Acknowledgements
Data from phila.gov and other sites
Inspiration
Find out how data could impact house price."
Output for 20 kernels porto seguro,,areeves87,2,"Version 2,2017-11-07|Version 1,2017-11-05",,CSV,367 MB,CC0,111 views,734 downloads,,0 topics,https://www.kaggle.com/areeves87/kernel-census,This dataset does not have a description yet.
College Football/Basketball/Baseball Rankings,Weekly ordinal ranking of teams from various computer systems and major polls.,Masseycre,2,"Version 3,2017-12-09|Version 2,2016-09-02|Version 1,2016-09-02",,Other,58 MB,Other,"3,000 views",173 downloads,3 kernels,,https://www.kaggle.com/masseyratings/rankings,"I have compiled weekly rankings of college teams, and am making the archived data public.
FBS football (since 1997): http://www.masseyratings.com/cf/compare.htm
FCS football (since 2000): http://www.masseyratings.com/cf/compare1aa.htm
basketball (since 2001): http://www.masseyratings.com/cb/compare.htm
baseball (since 2010): http://www.masseyratings.com/cbase/compare.htm
Each line of the .csv files have the following fields:
sportyear
unique team id #
team name
ranking system's 2/3 letter code
ranking system's name
date of weekly ranking (yyyymmdd)
ordinal ranking
Note: in most cases, the rankings were collected on Monday or Tuesday of each week, and are based on games played through Sunday.
Please explore, test hypotheses, generate various metrics, time series, correlations, etc and please reference masseyratings.com in your research."
Student Intervension,Some data for whether a student passed the final exam.,James,2,"Version 1,2016-09-26",,CSV,39 KB,Other,"2,236 views",248 downloads,2 kernels,,https://www.kaggle.com/jameszhou92/student-intervension,This dataset is not novel. It is a copy of the Student Performance dataset available at UCI Machine Learning repository
Dilma impeachment Twitter Raw Data,Dilma impeachment Twitter Raw Data,Caio Moreno,2,"Version 1,2016-08-22",,CSV,6 MB,ODbL,"1,073 views",35 downloads,3 kernels,,https://www.kaggle.com/caiomsouza/dilma-impeachment-twitter-raw-data,"DITRD - Dilma impeachment Twitter Raw Data
Abstract:
Dilma impeachment Twitter Raw Data from ""Dilma impeachment Process period Dec 2015 to Jan 2016 and March 2016"".
Date Donated: 10 Feb 2016, 17 Mar 2016.
Source:
Authors: Caio Moreno
Data Set Information:
This data was collected using the The Streaming APIs.
https://dev.twitter.com/streaming/overview
Dilma impeachment Twitter Raw Data is a free and open source software. It is available under the terms of the Apache License Version 2.
Citation:
If you publish material based on databases obtained from this repository, then, in your acknowledgements, please note the assistance you received by using this repository. This will help others to obtain the same data sets and replicate your experiments. We suggest the following pseudo-APA reference format for referring to this repository:
Moreno, C. (2016). DITRD-v1.0.0 - Dilma impeachment Twitter Raw Data [https://github.com/caiomsouza/TwitterRawData]. Madrid, Spain: U-TAD, Certificate program in Data Science.
Link do download DITRD-v1.0.0.
https://github.com/caiomsouza/TwitterRawData/releases/download/DITRD-v1.0.0/twitter_raw_data.zip
Link do download DITRD-v1.0.1.
https://github.com/caiomsouza/TwitterRawData/releases/download/DITRD-v1.0.1/DITRD-v1.0.1.zip
CSV Files extracted using R
https://github.com/caiomsouza/TwitterRawData/releases/download/DilmaTwitterRawData-Extract-R-CSV-v1.0.0/TwittersData-Extract-R-CSV.zip
All releases: https://github.com/caiomsouza/TwitterRawData/releases"
Cousin Marriage Data,A large portion of the world is married to first or second cousins.,mepotts,2,"Version 1,2016-09-21",,CSV,933 B,Other,"1,441 views",77 downloads,,0 topics,https://www.kaggle.com/mepotts/cousin-marriage-data,"Cousin Marriage Data
There was a story on FiveThirtyEight.com about the prevalence of marriage to cousins in the United States. This is called Consanguinity and is defined as marriages between individuals who are second cousins or closer. The article included data put together in 2001 for a number of countries. The data source and the article are listed below.
The raw data behind the story Dear Mona: How Many Americans Are Married To Their Cousins? on FiveThirtyEight.com.
Link to FiveThirtyEight's public github repository.
Header | Definition
country | Country names
percent | Percent of marriages that are consanguineous
Source: cosang.net
Data flaws: While the data does compile older sources and some self-reported data, it does match the trends of more recent data based on global genomic data."
Brazil's House of Deputy Refunds,Refunds Spendings from 2009 to 2017,epattaro,2,"Version 1,2017-11-04",politics,CSV,318 MB,CC0,160 views,21 downloads,,,https://www.kaggle.com/epattaro/brazils-house-of-deputy-refunds-2009-2017,"Context:
In an attempt to provide transparency to the population of Brazil, the dadosaberto initiative publishes data on different topics related to the House of Deputies. Among those informations, is the amount of refunds requested by each deputy, on expenses related to their activities.
This dataset compiles those expenses from 2009 to 2017, translate the categories to english, fix some dates (missing, invalid) for analysis purpouses, and discards some information thats not useful.
There are some issues with the dataset provided by the initiative dadosabertos;
the dates are related to the receipt, not to the actual refund request.
parties/states relations to the candidate sometimes are missing
social security numbers sometimes are missing.
social security should always have length == 11 for a person or == 14 for a company. that doesnt happen.
among other issues.
Ive converted dates that were missing or corrupted with only the year. Ive also created a OHE column to help identify/filter those dates.
Content:
bugged_date: (binary) identify wether date had issues receipt_date: (datetime) receipt date // (int year) for when bugged_date == 1 deputy_id: (deputy_id) id number. (didnt check if it changed across year/legislation period for deputies) political_party: (string) deputy political party state_code: (string) Brazil's state that elected the deputy deputy_name: (string)
receipt_social_security_number: might be a persons SS number (11 digits long) or a business id number (14 digits long). Many cases with issues. receipt_description: (str / classes) class of spending under which the receipt fits establishment_name: (string) receipt_value: (int) $BR, 3BR$ ~ 1USD
Acknowledgments:
This data is from dadosabertos.camara.leg.br
** Inspiration: **
I have only slightly fiddled with the dataset, but there are some funny patterns to say the least. Brazil faces a huge problem with corruption and neglect with public funds, this dataset helps to show that.
I invite fellow kagglers to toy with the dataset, try to identify suspicious spendings as well as odd patterns.
PS: This is a more complete version of a dataset i posted a year ago, which did not include translations or other years spendings."
Data Scientists vs Size of Datasets,Hardware & Brain battle between data set size and data scientists,Laurae,2,"Version 1,2016-10-18",,CSV,6 KB,Other,"1,786 views",37 downloads,2 kernels,0 topics,https://www.kaggle.com/laurae2/data-scientists-vs-size-of-datasets,"This research study was conducted to analyze the (potential) relationship between hardware and data set sizes. 100 data scientists from France between Jan-2016 and Aug-2016 were interviewed in order to have exploitable data. Therefore, this sample might not be representative of the true population.
What can you do with the data?
Look up whether Kagglers has ""stronger"" hardware than non-Kagglers
Whether there is a correlation between a preferred data set size and hardware
Is proficiency a predictor of specific preferences?
Are data scientists more Intel or AMD?
How spread is GPU computing, and is there any relationship with Kaggling?
Are you able to predict the amount of euros a data scientist might invest, provided their current workstation details?
I did not find any past research on a similar scale. You are free to play with this data set. For re-usage of this data set out of Kaggle, please contact the author directly on Kaggle (use ""Contact User""). Please mention:
Your intended usage (research? business use? blogging?...)
Your first/last name
Arbitrarily, we chose characteristics to describe Data Scientists and data set sizes.
Data set size:
Small: under 1 million values
Medium: between 1 million and 1 billion values
Large: over 1 billion values
For the data, it uses the following fields (DS = Data Scientist, W = Workstation):
DS_1 = Are you working with ""large"" data sets at work? (large = over 1 billion values) => Yes or No
DS_2 = Do you enjoy working with large data sets? => Yes or No
DS_3 = Would you rather have small, medium, or large data sets for work? => Small, Medium, or Large
DS_4 = Do you have any presence at Kaggle or any other Data Science platforms? => Yes or No
DS_5 = Do you view yourself proficient at working in Data Science? => Yes, A bit, or No
W_1 = What is your CPU brand? => Intel or AMD
W_2 = Do you have access to a remote server to perform large workloads? => Yes or No
W_3 = How much Euros would you invest in Data Science brand new hardware? => numeric output, rounded by 100s
W_4 = How many cores do you have to work with data sets? => numeric output
W_5 = How much RAM (in GB) do you have to work with data sets? => numeric output
W_6 = Do you do GPU computing? => Yes or No
W_7 = What programming languages do you use for Data Science? => R or Python (any other answer accepted)
W_8 = What programming languages do you use for pure statistical analysis? => R or Python (any other answer accepted)
W_9 = What programming languages do you use for training models? => R or Python (any other answer accepted)
You should expect potential noise in the data set. It might not be ""free"" of internal contradictions, as with all researches."
DVLA Driving Licence Dataset,Driving licence data March 2016,mariakatosvich,2,"Version 1,2016-10-12",,Other,2 MB,ODbL,"2,728 views",148 downloads,,,https://www.kaggle.com/qwikfix/dvla-driving-licence-dataset,"These data sets contain data on current driving licences issued by the Driver and Vehicle Licensing Agency (DVLA). The DVLA is responsible for issuing driving licences in Great Britain (GB). Driving licences issued in Northern Ireland are the responsibility of the Northern Ireland Driver & Vehicle Agency and are outside the scope of this release.
DVLA’s drivers database changes constantly as the Agency receives driving licence applications and other information that updates the records of individual drivers. Therefore, it is only possible only to provide a snapshot of the state of the record at a particular time.
Contact DVLA for Further information about driving licensing which can be found at: https://www.gov.uk/browse/driving/driving-licences"
House Sales in Ontario,Draw an enhanced heatmap of House Prices,Mahdy Nabaee,2,"Version 1,2016-10-07",,CSV,2 MB,CC0,"2,736 views",222 downloads,7 kernels,,https://www.kaggle.com/mnabaee/ontarioproperties,"This dataset includes the listing prices for the sale of properties (mostly houses) in Ontario. They are obtained for a short period of time in July 2016 and include the following fields: - Price in dollars - Address of the property - Latitude and Longitude of the address obtained by using Google Geocoding service - Area Name of the property obtained by using Google Geocoding service
This dataset will provide a good starting point for analyzing the inflated housing market in Canada although it does not include time related information. Initially, it is intended to draw an enhanced interactive heatmap of the house prices for different neighborhoods (areas)
However, if there is enough interest, there will be more information added as newer versions to this dataset. Some of those information will include more details on the property as well as time related information on the price (changes).
This is a somehow related articles about the real estate prices in Ontario: http://www.canadianbusiness.com/blogs-and-comment/check-out-this-heat-map-of-toronto-real-estate-prices/
I am also inspired by this dataset which was provided for King County https://www.kaggle.com/harlfoxem/housesalesprediction"
Number of trains on the sections of the network,how many trains are using the tracks between the stations during a year,ChristianTrachsel,2,"Version 1,2016-08-29",,CSV,2 MB,Other,"1,206 views",43 downloads,,,https://www.kaggle.com/ctrachsel/number-of-trains-on-the-sections-of-the-network,"This data set only includes the track sections that belong to the standard-gauge track network of the SBB Group (SBB Infrastructure, Sensetalbahn, Thurbo).Track sections that are managed by other infrastructure operators, for example BLS Netz AG, Deutsche Bahn AG (DB) or Rhätische Bahn AG (RhB), are not included.
The data on the number of trains includes passenger and freight trains operated by all railway undertakings that have travelled on the track section in question, i.e. also trains run by BLS AG, Schweizerische Südostbahn AG, Crossrail AG and Railcare AG, for example.
https://data.sbb.ch/page/licence"
Breathing Data from a Chest Belt,Features from chest/diaphragm breathing signal to ventilation in l/min,SagarSen,2,"Version 1,2016-09-23",,CSV,55 KB,CC0,"1,294 views",63 downloads,,,https://www.kaggle.com/sagarsen/breathing-data-from-a-chest-belt,"The breathing signal is the expansion and contraction of the chest measured using a chest belt. Simultaneously, we obtained ventilation from the Douglas Bag (DB) method which is the gold standard. Given the breathing signal, we extract the average height (a) between adjacent local minima and maxima and the fundamental period (p). These are our features. We want to find a function f(x) that maps (a,b) into the flow of air calculated (based on the DB method) in the time window. The average height seems to have a quadratic/cubic relationship with ventilation while the period seems to be having an inverse relationship with the flow of air. ."
water pump,datasets for water pump challenge,Alexander Raboin,2,"Version 2,2016-11-02|Version 1,2016-11-02",,CSV,25 MB,CC0,"2,276 views",74 downloads,5 kernels,,https://www.kaggle.com/successf2fe/water-pump,water pump
Nürburgring Top 100,Top 100 lap times for the Nürburgring,Chris Scott,2,"Version 1,2016-11-13",,CSV,4 KB,CC0,"1,820 views",38 downloads,2 kernels,0 topics,https://www.kaggle.com/scottdchris/nurburgring100,"Context: This csv contains the top 100 lap times on the famous Nürburgring track in Germany.
Content: - Position on the list from 1-100 - Model year of the car - Car Make - Car Model - Lap Time
Acknowledgements: Sourced from https://nurburgringlaptimes.com/lap-times-top-100/
Inspiration: New to the data science world and saw found no data sets related to cars or racing so figured this might be a place to start. Hoping to contribute larger, more interesting datasets to the community in the future."
Data for public services on Brazil,Services and Institutes,AjaxFB,2,"Version 1,2016-11-11",,CSV,4 MB,Other,913 views,42 downloads,,,https://www.kaggle.com/ajaxfb/prodemg-services,Campus Party BH MG Brazil 2016 http://campuse.ro/challenges/hackathon-servicos-para-os-cidadaos/
finance study,,Tushar Mahendra Patil,2,"Version 1,2017-11-05",,Other,3 MB,CC4,580 views,48 downloads,,0 topics,https://www.kaggle.com/tusharpatil15/finance-study,This dataset does not have a description yet.
Bitcoin historical price,,Ronny Kimathi kaimenyi,2,"Version 1,2017-11-06",,CSV,28 KB,CC0,335 views,33 downloads,,0 topics,https://www.kaggle.com/ronnykym/bitcoinprice,"Context
I'm always concerned with the rise of bitcoin price. Investing in these coins require data driven knowledge and without the knowledge one is bound to make grave mistakes. It is just the other day bitcoin price hit 1600 USD. What is the future of this coin?
Content
The csv file contains daily closing price of a bitcoin from 28-Apr-13 to 3-Oct-17. Soon I will give the updated data set.
Date column: The dates of observation
Close column: The daily closing price in USD of a bitcoin
Acknowledgements
Many thanks to Block chain info and my friends.
Inspiration
Now that bitcoin has hit higher prices we would like to predict the direction of bitcoin price
Predict actual bitcoin price for a given day. will a neural network model work for my case?"
FoodClassification,,Christopher Lambert,2,"Version 1,2017-11-05",,{}JSON,57 KB,CC0,234 views,17 downloads,,0 topics,https://www.kaggle.com/theriley106/foodclassification,"Context
I needed a data set for a Hackathon project involving food classification. I gathered this data by scraping various online stores that only sold specific food items (ie Only Vegan food or only Halal food). I then compared those items to Walmart's Electrobit-backed API that happened to return ingredient information."
State wise tree cover India,refrenced from data.gov.in,lamda-dev,2,"Version 1,2016-11-11",,CSV,1 KB,ODbL,"1,277 views",78 downloads,5 kernels,,https://www.kaggle.com/lamdadev/state-wise-tree-cover-india,"state wise tree cover can be used to predict area useful for agriculture ,find density of forest cover,Number of tree approx. ,Environment statistics"
Natural Rate of Unemployment (Long-Term),Unemployment arising from all sources (except aggregate demand fluctuations),US Bureau of Labor Statistics,2,"Version 1,2016-11-07",,CSV,4 KB,Other,989 views,47 downloads,,0 topics,https://www.kaggle.com/bls/natural-rate-of-unemployment-longterm,"Context
This dataset lists the natural rate of unemployment (NAIRU) in the U.S., which is the rate of unemployment arising from all sources except fluctuations in aggregate demand. Estimates of potential GDP are based on the long-term natural rate. The short-term natural rate incorporates structural factors that are temporarily boosting the natural rate beginning in 2008. The short-term natural rate is used to gauge the amount of current and projected slack in labor markets, which is a key input into CBO's projections of inflation.
Content
Data includes the date of the quarterly collection and the natural rate of unemployment from January 1, 1949 through October 1, 2016.
Inspiration
What is the general trend of unemployment?
Can you compare this unemployment data with other factors found in any of the BLS databases, such as manufacturing employment rates and GDP?
Acknowledgement
This dataset is part of the US Department of Labor Bureau of Labor Statistics Datasets (the Federal Reserve Economic Data database), and the original source can be found here."
NYCHA Staten Island Asbestos Siebel Data,FOIL data from NYCHA's Siebel database,Progress Queens,2,"Version 2,2016-11-10|Version 1,2016-11-10",architecture,CSV,88 KB,CC0,"1,574 views",35 downloads,,,https://www.kaggle.com/progressqueens/nycha-staten-island-asbestos-siebel-data,"This is a subset of only Asbestos-related maintenance requests for the Borough of Staten Island received by Progress Queens from the New York City Housing Authority in response to a request filed under the State's Freedom of Information Law.
This subset was derived from the concatenation of the Siebel extracts, which were included in the subject FOIL response. Because of the poor condition of the data, the concatenation of the Siebel extracts was processed with some possible data loss. A general description of the quality of the data NYCHA produced was reported in an article published by Progress Queens.
The publisher of Progress Queens formed this dataset to study how does NYCHA treat maintenance requests for Asbestos, to determine how NYCHA escalates complaints made by tenants about Asbestos to ordering testing for Asbestos and to abatement, if necessary."
Employee Attrition,,Parmanand Sahu,2,"Version 1,2017-10-26",,CSV,1 MB,CC0,429 views,66 downloads,,0 topics,https://www.kaggle.com/analystanand/employee-attrition,This dataset does not have a description yet.
Data from OBD (On Board Diagnostics),On Board Diagnostics,SeaGoat,2,"Version 1,2017-11-03",,CSV,228 KB,CC0,221 views,32 downloads,,,https://www.kaggle.com/vbandaru/data-from-obd-on-board-diagnostics,This dataset does not have a description yet.
Mann Ki Baat Speech corpus,Indian PM Narendra Modi's speech addressing the nation every month,Shankar,2,"Version 3,2017-11-09|Version 2,2017-11-09|Version 1,2017-11-08","india
politicians
politics",CSV,753 KB,CC0,246 views,16 downloads,,0 topics,https://www.kaggle.com/shankarpandala/mann-ki-baat-speech-corpus,"Context
Narendra Modi is a great public speaker. This data set is an opportunity to understand what his speech's contain.
Content
This is an unstructured text data of every month. Starting from October 2014 to September 2017 each speech is provided as a text file.
Acknowledgements
Mann Ki Baat
Inspiration
I would like to see what makes his speech's great and attract crores of people."
Framingham Heart study dataset,,Aman Ajmera,2,"Version 1,2017-11-07",,CSV,187 KB,CC0,312 views,68 downloads,,0 topics,https://www.kaggle.com/amanajmera1/framingham-heart-study-dataset,This dataset does not have a description yet.
Spotify Artists,81323 Artists on Spotify,ElenaCall,2,"Version 1,2017-11-09","music
musicians",CSV,4 MB,CC4,485 views,52 downloads,,0 topics,https://www.kaggle.com/ehcall/spotify-artists,This dataset does not have a description yet.
creditcard,how to manage the unbalance dataset,jingli,2,"Version 1,2017-11-03",,CSV,144 MB,CC0,244 views,71 downloads,,0 topics,https://www.kaggle.com/jacklizhi/creditcard,"Context
This data download from kaggle .It's about the fraud in a bank. Use some method prevent fraud accident happen is very import for bank.
Content
The fraud id is the positive and the total number is less than the negative sample. So ,please pay more attention to the positive recall and precise.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
How to improve recall in a unbalance is very common status in true business sense. down_sampling? or up_sampling?"
Car sales,,GaganBhatia,2,"Version 1,2017-10-26",,CSV,16 KB,CC0,494 views,95 downloads,3 kernels,0 topics,https://www.kaggle.com/gagandeep16/car-sales,"This is the Car sales data set which include information about different cars . This data set is being taken from the Analytixlabs for the purpose of prediction In this we have to see two things
First we have see which feature has more impact on car sales and carry out result of this
Secondly we have to train the classifier and to predict car sales and check the accuracy of the prediction."
Grocery,,Bhamin Patel,2,"Version 1,2017-10-26",,CSV,161 MB,Other,365 views,33 downloads,,0 topics,https://www.kaggle.com/bhamin/grocery,This dataset does not have a description yet.
Investment growth forcast,growth of investment after some years?,Saqib Mujtaba,2,"Version 1,2017-11-06","finance
information technology",CSV,645 B,CC0,356 views,42 downloads,,0 topics,https://www.kaggle.com/saqibmujtaba/investment-growth-forcast,"Description
This is a data set for forecasting growth of investment after a period of some years say after 10 years if data growth pattern remains same.
Data Fields
Deposit date - gives the date of investment. Month - counter on total months PFT_Perc - percentage of profit earned PFT - calculated total profit on investment Investment - Total Investment for month
What we already know!
Their are some deposits made to grow the investment and based on which PFT earned is growing.
What we need to know.
Idea here is to know the growth of investment after some years. For instance after 40 months, which will be my investment ?"
psychology,,RishiBarath,2,"Version 1,2017-11-09",,CSV,6 KB,CC0,310 views,16 downloads,,0 topics,https://www.kaggle.com/rb1181/psychology,This dataset does not have a description yet.
kc_house,,Pramod Kumar,2,"Version 1,2017-11-03",,CSV,2 MB,Other,228 views,17 downloads,,0 topics,https://www.kaggle.com/pramodkumar8/kc-house,This dataset does not have a description yet.
Wisconsin Breast Cancer Database,breastCancer.csv dataset,Brian Rouse,2,"Version 1,2017-10-31",,CSV,20 KB,CC0,631 views,134 downloads,,0 topics,https://www.kaggle.com/roustekbio/breast-cancer-csv,"Context
This breast cancer databases was obtained from the University of Wisconsin Hospitals, Madison from Dr. William H. Wolberg.
Content
Past Usage:
Attributes 2 through 10 have been used to represent instances. Each instance has one of 2 possible classes: benign or malignant.
Wolberg,~W.~H., \& Mangasarian,~O.~L. (1990). Multisurface method of pattern separation for medical diagnosis applied to breast cytology. In {\it Proceedings of the National Academy of Sciences}, {\it 87}, 9193--9196. -- Size of data set: only 369 instances (at that point in time) -- Collected classification results: 1 trial only -- Two pairs of parallel hyperplanes were found to be consistent with 50% of the data -- Accuracy on remaining 50% of dataset: 93.5% -- Three pairs of parallel hyperplanes were found to be consistent with 67% of data -- Accuracy on remaining 33% of dataset: 95.9%
Zhang,~J. (1992). Selecting typical instances in instance-based learning. In {\it Proceedings of the Ninth International Machine Learning Conference} (pp. 470--479). Aberdeen, Scotland: Morgan Kaufmann. -- Size of data set: only 369 instances (at that point in time) -- Applied 4 instance-based learning algorithms -- Collected classification results averaged over 10 trials -- Best accuracy result: -- 1-nearest neighbor: 93.7% -- trained on 200 instances, tested on the other 169 -- Also of interest: -- Using only typical instances: 92.2% (storing only 23.1 instances) -- trained on 200 instances, tested on the other 169
Relevant Information:
Samples arrive periodically as Dr. Wolberg reports his clinical cases. The database therefore reflects this chronological grouping of the data. This grouping information appears immediately below, having been removed from the data itself:
Group 1: 367 instances (January 1989) Group 2: 70 instances (October 1989) Group 3: 31 instances (February 1990) Group 4: 17 instances (April 1990) Group 5: 48 instances (August 1990) Group 6: 49 instances (Updated January 1991) Group 7: 31 instances (June 1991) Group 8: 86 instances (November 1991)
Total: 699 points (as of the donated datbase on 15 July 1992)
Note that the results summarized above in Past Usage refer to a dataset of size 369, while Group 1 has only 367 instances. This is because it originally contained 369 instances; 2 were removed. The following statements summarizes changes to the original Group 1's set of data:
Group 1 : 367 points: 200B 167M (January 1989) Revised Jan 10, 1991: Replaced zero bare nuclei in 1080185 & 1187805 Revised Nov 22,1991: Removed 765878,4,5,9,7,10,10,10,3,8,1 no record : Removed 484201,2,7,8,8,4,3,10,3,4,1 zero epithelial : Changed 0 to 1 in field 6 of sample 1219406 : Changed 0 to 1 in field 8 of following sample: : 1182404,2,3,1,1,1,2,0,1,1,1
Number of Instances: 699 (as of 15 July 1992)
Number of Attributes: 10 plus the class attribute
Attribute Information: (class attribute has been moved to last column)
Attribute Domain
Sample code number id number
Clump Thickness 1 - 10
Uniformity of Cell Size 1 - 10
Uniformity of Cell Shape 1 - 10
Marginal Adhesion 1 - 10
Single Epithelial Cell Size 1 - 10
Bare Nuclei 1 - 10
Bland Chromatin 1 - 10
Normal Nucleoli 1 - 10
Mitoses 1 - 10
Class: (2 for benign, 4 for malignant)
Missing attribute values: 16
There are 16 instances in Groups 1 to 6 that contain a single missing (i.e., unavailable) attribute value, now denoted by ""?"".
Class distribution:
Benign: 458 (65.5%) Malignant: 241 (34.5%)
Acknowledgements
O. L. Mangasarian and W. H. Wolberg: ""Cancer diagnosis via linear programming"", SIAM News, Volume 23, Number 5, September 1990, pp 1 & 18.
William H. Wolberg and O.L. Mangasarian: ""Multisurface method of pattern separation for medical diagnosis applied to breast cytology"", Proceedings of the National Academy of Sciences, U.S.A., Volume 87, December 1990, pp 9193-9196.
O. L. Mangasarian, R. Setiono, and W.H. Wolberg: ""Pattern recognition via linear programming: Theory and application to medical diagnosis"", in: ""Large-scale numerical optimization"", Thomas F. Coleman and Yuying Li, editors, SIAM Publications, Philadelphia 1990, pp 22-30.
K. P. Bennett & O. L. Mangasarian: ""Robust linear programming discrimination of two linearly inseparable sets"", Optimization Methods and Software 1, 1992, 23-34 (Gordon & Breach Science Publishers).
Inspiration
Rouse Tek Bio informatics Cytogenomics Project is an attempt to bring the human genome to the understanding of how cancers develop. All of our bodies are composed of cells. The human body has about 100 trillion cells within it. And usually those cells behave in a certain fashion. They observe certain rules, they divide when they’re told to divide, they’re quiescent when they’re told to remain dormant, they stay within a particular position within their tissue and they don’t move out of that.
Occassionally however, a single cell, of those 100 trillion cells, behave in a different way. That cell keeps dividing when all its signals around it tell it to stop dividing. That cell ignores its counterparts around it and pushes them out of the way. That cell stops observing the rules of the tissue within which it is located and begins to move out of its normal position, invading into the tissues around it and sometimes entering the bloodstream and becoming a metastasis, depositing in another tissue of the body..
The reason the cell has gone rogue is because it has acquired within its genome, within its DNA, a number of abnormalities that cause it to behave as a cancer cell.
All 100 trillion cells in the human body have got a copy of the human genome, they have 2 copies, 1 maternal, 1 paternal. Throughout Life all those copies of the genome in those 100 trillion cells, are acquiring abnormal changes or somatic mutations. These mutations are present in the cell and are not transmitted from parents to offspring. They are constrained to that individual cell. Those mutations occur in every cell of the body, normal and abnormal, for a number of different reasons. They occur because every time a cell divides possibly one letter of code out of 3 billion is replicated incorrectly. And that’s 1 source of somatic mutations.
Another source is that our 100 trillion cells are being exposed to a number of different onslaughts like radiation, self generated chemicals from inhalation of things like tobacco smoke or even an unhealthy diet over time. Occasionally mechanisms in a particular cell make breakdown and the DNA of that cell begins to acquire somatic mutations rather more commonly than other cells.
So in summary, every cell in the body acquires mutations throughout a lifetime, and as we get older we acquire more and more somatic mutations in which occasionally a particular type of gene is mutated where the protein that it makes is abnormal and drives the cell to behave in a rogue fashion that we call cancer."
Viewing Solar Flares,A database of how a range of solar flares are viewed by different satellites.,MichaelKirk,2,"Version 1,2017-10-30","time series
astronomy
physics
space",CSV,2 MB,CC4,"1,097 views",41 downloads,,0 topics,https://www.kaggle.com/heliodata/instruments-solarflares,"Context
The subject of this dataset is multi-instrument observations of solar flares. There are a number of space-based instruments that are able to observe solar flares on the Sun; some instruments observe the entire Sun all the time, and some only observe part of the Sun some of the time. We know roughly where flares occur on the Sun but we don't know when they will occur. In this respect solar flares resemble earthquakes on Earth. This dataset is a catalog of which solar flares have been observed by currently operational space-based solar observatories.
Content
It includes that start time and end time of each solar flare from 1 May 2010 to 9 October 2017 and which instrument(s) they were observed by. It was collected by doing a retrospective analysis of the known pointing of seven different instruments with the location and times of 12,455 solar flares.
Acknowledgements
The dataset was compiled by Dr. Ryan Milligan based on publicly available data and are freely distributed. The citation of relevance is https://arxiv.org/abs/1703.04412.
Inspiration
This dataset represents the first attempted evaluation of how well space-based instrumentation co-ordinate when it comes to observing solar flares. We are particularly interested in understanding how often combinations of instruments co-observe the same flare. The ultimate purpose is to try to find strategies that optimize the scientific return on solar flare data given the limited space-based instrument resources available. More often than not, our greatest understanding of these explosive events come through simultaneous observations made by multiple instruments."
restaurant and consumer data,,Bharath NR,2,"Version 1,2017-10-27",,CSV,221 KB,CC0,197 views,37 downloads,,0 topics,https://www.kaggle.com/bharathnr/restaurant-and-consumer-data,This dataset does not have a description yet.
Titanic,,MHouellemont,2,"Version 1,2017-11-02",,CSV,91 KB,CC0,119 views,19 downloads,,0 topics,https://www.kaggle.com/mhouellemont/titanic,This dataset does not have a description yet.
Top How Tos on Google 2004 to 2017,"The Top ""How to"" searches on Google from 2004 to 2017",Google News Lab,2,"Version 1,2017-11-01",,CSV,3 KB,CC4,80 views,7 downloads,,0 topics,https://www.kaggle.com/GoogleNewsLab/top-how-tos-on-google-2004-to-2017,"The top ""how to"" related searches on Google from 2004 to 2017 worldwide. Top searches are searches with the highest search interest based on volume."
Health Care Searches By Metro Area in the US,Metropolitan area data on search interest in healthcare,Google News Lab,2,"Version 1,2017-11-01","journalism
healthcare
public health
+ 2 more...",CSV,11 KB,CC4,257 views,17 downloads,,0 topics,https://www.kaggle.com/GoogleNewsLab/health-care-searches-by-metro-area-in-the-us,"Indexed search interest in 'health care' from May 2 to May 4, 2017. 100 is the max value, and every number is relative to that."
Frightgeist 2017: Rankings for costumes,Rankings for Halloween costumes in October 2017 in the US,Google News Lab,2,"Version 1,2017-11-02","journalism
united states
internet",CSV,9 KB,CC4,76 views,4 downloads,,0 topics,https://www.kaggle.com/GoogleNewsLab/frightgeist-2017-rankings-for-costumes,Rankings for Halloween costumes in October 2017 in the US
ExpressionNet,,INFINITYLABS,2,"Version 3,2017-11-04|Version 2,2017-11-03|Version 1,2017-11-03",,Other,362 KB,CC0,113 views,3 downloads,,,https://www.kaggle.com/INFINITYLABS/expressionnet,"Context
Expressionist art works is a very excellent knowledge repository to gather insights about the human aesthetics and perceptions.
Content
This data set is the collection of expressionist art works across the world for the machine learning experiments.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
Pro and College Sports Lines,Game Summaries and Lines for Sports Predictions,Scottfree Analytics LLC,2,"Version 1,2017-11-08","american football
basketball
sports
time series",CSV,270 KB,CC0,451 views,49 downloads,,0 topics,https://www.kaggle.com/scottfree/sports-lines,"Context
After participating in many Kaggle competitions, we decided to open-source our Python AutoML framework AlphaPy. You can configure models using YAML, and it's a quick way to develop a classification or regression model with any number of scikit-learn algorithms. More importantly, we needed the ability to easily write custom transformations to engineer features that could be imported by other tools such as H2O.ai and DataRobot, which are commercial AutoML tools.
We were very interested in predicting the stock market and sporting events, so we developed some tools for collecting end-of-day stock market data and then transforming that time series data for machine learning. For the sports data, we painstakingly entered game results on a daily basis, and this dataset (albeit limited) is the result. Sports APIs such as Sportradar are becoming more prevalent, but you still need a pipeline to merge all that data to shape it for machine learning.
Content
We collected game results, line, and over/under data for the following sports and seasons:
NFL 2014, 2015 (2 Seasons)
NBA 2015-16 (1 Season)
NCAA Football 2015-16 (1 Season)
NCAA Basketball 2015-16 (1 Season)
Using AlphaPy, we developed a sports pipeline to analyze trend data in the game results. You can find documentation and examples here.
Inspiration
Every speculator's dream is to gain an edge, whether it be betting on sports or speculating on stocks. This dataset is just a small step for applying machine learning to the world of sports data."
Email Campaign Management for SME,Tracking Email Status in a Multi Channel Marketing Platform,Gokagglers,2,"Version 1,2017-10-10","business
product
marketing
internet",CSV,5 MB,Other,729 views,87 downloads,,,https://www.kaggle.com/loveall/email-campaign-management-for-sme,"Context
Most of the small to medium business owners are making effective use of Gmail based Email marketing Strategies for offline targeting of converting their prospective customers into leads so that they stay with them in Business
Content
we have different aspects of emails to characterize the mail and track the mail is ignored; read; acknowledged by the reader
Acknowledgements
corefactors.in
Inspiration
amount of advertising dollars spent on a product determines the amount of its sales, we could use regression analysis to quantify the precise nature of the relationship between advertising and sales. here we want everyone to experiment with this fun data , what value we can derive from email as a tool for compaign marketing in a multi channel marketing strategy of a Small to Medium Businesses"
1.2 Million Used Car Listings,"1.2 Million listings scraped from TrueCar.com - Price, Mileage, Make, Model",Evan Payne,2,"Version 2,2017-10-23|Version 1,2017-09-27",,CSV,140 MB,CC0,912 views,94 downloads,,,https://www.kaggle.com/jpayne/852k-used-car-listings,"Content
This data was acquired by scraping TrueCar.com for used car listings on 9/24/2017. Each row represents one used car listing."
"English Premier League Penalty Dataset, 2016/17",All you want to know about the penalties taken in the 2016/17 season.,ShubhamMaurya,2,"Version 1,2017-10-03","association football
sports",CSV,10 KB,CC0,544 views,69 downloads,2 kernels,2 topics,https://www.kaggle.com/mauryashubham/english-premier-league-penalty-dataset-201617,"Context
Penalty kicks in football are the easiest, and perhaps most elegant way of modelling a game theory situation in a real-world scenario. Given the limited number of options for both kickers and keepers, it makes for wonderful real-life data, which can be used in a professional context. However, official match records do not record the interesting aspects of the play - the direction of the kicker, the direction the keeper moves, where the ball lands, and so on. I watched all the penalties of the 2016/17 season of the EPL (thanks, Youtube!) and tagged the direction each player moves in. I believe this dataset will be extremely valuable to those who wish to experiment at the intersection of sports, game theory and data science.
Content
The dataset contains information of all 106 penalty kicks taken during the 2016/17 season of the English Premier League, with the following details - teams involved, player who took the kick, his foot, the direction the ball went, and the direction the keeper dove in [IMPORTANT: Direction is with reference to the kicker!], and what time the penalty was awarded. The saved column indicates whether the keeper saved it, or the kicker kicked it beyond the goal post.
There is missing data for 3 kicks - for one, the penalty was nullified due to a double kick, and for the other two, I simply couldn't find any video evidence of them. (If you do find them, please let me know). Also, please let me know if you find any other errors with the data.
Acknowledgements
This entire project was inspired by the brilliant work of Ignacio Palacios Huerta, whose story is wonderful, and whose papers are an absolute joy to read. My current project is basically emulating what Huerta has done with his (very vast) dataset.
I will be expanding this dataset to previous seasons penalties too, as and when I get time."
Weekly Gold Close Price 2015-2017,Weekly gold close price from 2015 to 2017 (2017-09-24),Nick Wong,2,"Version 2,2017-10-02|Version 1,2017-09-30",,CSV,8 KB,CC0,284 views,28 downloads,,,https://www.kaggle.com/nickwong64/gold2015-2017,"Context
As I am trying to learn and build an LSTM prediction model for equity prices, I have chosen gold price to begin.
Content
The file composed of simply 2 columns. One is the date (weekend) and the other is gold close price. The period is from 2015-01-04 to 2017-09-24.
Acknowledgements
Thanks to Jason of his tutorial about LSTM forecast: https://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/
Inspiration
William Gann: Time is the most important factor in determining market movements and by studying past price records you will be able to prove to yourself history does repeat and by knowing the past you can tell the future. There is a definite relation between price and time."
Trump Administration Financial Disclosures,Financial data submitted by Trump federal appointees,Aleksey Bilogur,2,"Version 1,2017-09-30","united states
government
money
politics",CSV,3 MB,CC0,839 views,42 downloads,,0 topics,https://www.kaggle.com/residentmario/trump-financial-disclosures,"Context
Before joining the federal executive administration, new government appointees must submit, amongst other things, detailed information regarding their finances and previous job history. Such disclosure rules are in place in order to prevent conflicts of interest, and are a fundamental part of the work done by government ethics commissions. This dataset is a condensed collection of information recovered from these forms for a selection of top Trump administration appointees.
Content
This dataset is split into five separate CSV files:
names_and_job_titles.csv -- The names and job titles of appointees. Not all appointees are included in this dataset.
jobs_before_joining_admin.csv -- Positions held by administration members immediately prior to their joining the federal government.
clients_before_joining_admin.csv -- Former appointee clients before joining the federal government.
income_sources_and_assets.csv -- All acknowledged and disclosed income sources and assets. The most important file.
debts.csv -- Known appointee debt obligations. This record is incomplete.
employee_agreements.csv -- Agreements that the appointee made as part of the conditions of their entering employment with the federal government.
Acknowledgements
ProPublica, The New York Times, the Associated Press, and others pooled their resources to collect and condense disclosure forms for many prominent members of the Trump administration. These were in turn collected into a public spreadsheet. This dataset is a further condensation of this work.
Inspiration
What can you discover about the finances and potential conflicts of interest of members of the Trump administration by looking at the raw government record?"
Moneyball,MLB Statistics 1962-2012,WesDuckett,2,"Version 1,2017-10-20","baseball
regression analysis",CSV,66 KB,CC0,"1,075 views",154 downloads,4 kernels,,https://www.kaggle.com/wduckett/moneyball-mlb-stats-19622012,"Context
In the early 2000s, Billy Beane and Paul DePodesta worked for the Oakland Athletics. While there, they literally changed the game of baseball. They didn't do it using a bat or glove, and they certainly didn't do it by throwing money at the issue; in fact, money was the issue. They didn't have enough of it, but they were still expected to keep up with teams that had much deeper pockets. This is where Statistics came riding down the hillside on a white horse to save the day. This data set contains some of the information that was available to Beane and DePodesta in the early 2000s, and it can be used to better understand their methods.
Content
This data set contains a set of variables that Beane and DePodesta focused heavily on. They determined that stats like on-base percentage (OBP) and slugging percentage (SLG) were very important when it came to scoring runs, however they were largely undervalued by most scouts at the time. This translated to a gold mine for Beane and DePodesta. Since these players weren't being looked at by other teams, they could recruit these players on a small budget. The variables are as follows:
Team
League
Year
Runs Scored (RS)
Runs Allowed (RA)
Wins (W)
On-Base Percentage (OBP)
Slugging Percentage (SLG)
Batting Average (BA)
Playoffs (binary)
RankSeason
RankPlayoffs
Games Played (G)
Opponent On-Base Percentage (OOBP)
Opponent Slugging Percentage (OSLG)
Acknowledgements
This data set is referenced in The Analytics Edge course on EdX during the lecture regarding the story of Moneyball. The data itself is gathered from baseball-reference.com. Sports-reference.com is one of the most comprehensive sports statistics resource available, and I highly recommend checking it out.
Inspiration
It is such an important skill in today's world to be able to see the ""truth"" in a data set. That is what DePodesta was able to do with this data, and it unsettled the entire system of baseball recruitment. Beane and DePodesta defined their season goal as making it to playoffs. With that in mind, consider these questions:
How does a team make the playoffs?
How does a team win more games?
How does a team score more runs?
They are all simple questions with simple answers, but now it is time to use the data to find the ""truth"" hidden in the numbers."
Human Activity Recognition,,prvns,2,"Version 1,2017-10-26",,CSV,12 MB,CC4,175 views,18 downloads,,0 topics,https://www.kaggle.com/singhpraveen/weight%20lifting%20exercises%20dataset,"Context
Human Activity Recognition - HAR - has emerged as a key research area in the last years and is gaining increasing attention by the pervasive computing research community (see picture below, that illustrates the increasing number of publications in HAR with wearable accelerometers), especially for the development of context-aware systems. There are many potential applications for HAR, like: elderly monitoring, life log systems for monitoring energy expenditure and for supporting weight-loss programs, and digital assistants for weight lifting exercises.
Content
This human activity recognition research has traditionally focused on discriminating between different activities, i.e. to predict ""which"" activity was performed at a specific point in time (like with the Daily Living Activities dataset above). The approach we propose for the Weight Lifting Exercises dataset is to investigate ""how (well)"" an activity was performed by the wearer. The ""how (well)"" investigation has only received little attention so far, even though it potentially provides useful information for a large variety of applications,such as sports training.
ix young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).
Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes. Participants were supervised by an experienced weight lifter to make sure the execution complied to the manner they were supposed to simulate. The exercises were performed by six male participants aged between 20-28 years, with little weight lifting experience. We made sure that all participants could easily simulate the mistakes in a safe and controlled manner by using a relatively light dumbbell (1.25kg).
Acknowledgements
This dataset is licensed under the Creative Commons license (CC BY-SA) - Wallace Ugulino (wugulino at inf dot puc-rio dot br) - Eduardo Velloso - Hugo Fuks"
prostate.csv,,tvscitechtalk,2,"Version 1,2017-10-14",,CSV,9 KB,Other,202 views,34 downloads,,0 topics,https://www.kaggle.com/tvscitechtalk/prostatecsv,This dataset does not have a description yet.
FEM simulations,Data to build a regression surrogate model of a complex numerical model,David,2,"Version 2,2017-10-09|Version 1,2017-09-29",regression analysis,CSV,610 KB,CC0,"1,134 views",58 downloads,,,https://www.kaggle.com/daalgi/fem-simulations,"Context
Engineers use numerical models to analyze the behavior of the systems they are studying. By means of numerical models you can prove whether a design is safe or not, instead of making a prototype and testing it. This gives you great flexibility to modify parameters and to find a cheaper design that satisfies all the safety requirements.
But when the models are too complex, the numerical simulations can easily last from a few hours to a few days. In addition, during the optimization process you might need a few tens of trials. So in order to simplify the process we can build a simple 'surrogate' model that yields similar results to the original one. Here's when Machine Learning comes in!
Content
The dataset contains the data of about 6000 numerical simulations (finite element models, FEM). It must be pointed out that there is no noise in the data, that is, if we run again the simulations we'd get the same results. There are 9 input parameters and 4 output results.
Inputs (continuous and positive values): (1) load parameters: ecc, N, gammaG. (2) material parameters: Esoil, Econc. (3) geometry parameters: Dbot, H1, H2, H3.
Outputs (continuous values): (1) stress related results: Mr_t, Mt_t, Mr_c, Mt_c.
Acknowledgements
The parametric numerical model was self-made.
Inspiration
The data comes from deterministic numerical simulations. Under this circumstance, is there any way we can find a regression model that gives accurate results? Let's say something like 5% error (True_value / Predicted_value within the range of [0.95, 1.05]).
What would be the most appropriate regression algorithms? What accuracy can we expect?"
Amazon Reviews,Reviews for Video Games in Amazon,Shantanu Acharya,2,"Version 2,2017-10-22|Version 1,2017-10-10",,{}JSON,148 MB,ODbL,589 views,87 downloads,,0 topics,https://www.kaggle.com/shanwizard/amazon-reviews,This dataset does not have a description yet.
crime against women in India,,ChNaveen,2,"Version 1,2017-10-30",,Other,244 KB,CC0,180 views,41 downloads,,0 topics,https://www.kaggle.com/navinch/crime-against-women-in-india,This dataset does not have a description yet.
ASX Australia Equity Prices - 1997 to 2017,Historical Stock prices for all companies listed in Australia,Rohk,2,"Version 2,2018-01-28|Version 1,2017-10-12","historiography
finance",CSV,79 MB,Other,257 views,46 downloads,,0 topics,https://www.kaggle.com/therohk/asx-equity-prices,"Context
20 years of Daily Stock Prices of all companies listed on ASX (Australian Securities Exchange).
Date Range: 1997-01-02 to 2017-12-31
Exchange Website: https://www.asx.com.au/
Content
Total Records: 6,475,470
Total Companies: 2,228
asx-equity-price.csv Fields : Ticker, Date, Open, High, Low, Close, Volume
asx-tickers.csv Fields: ticker, company, industry
Acknowledgements
The data posted here is merely a concatenation of all files on the below website.
All Credits To: https://www.asxhistoricaldata.com/
Inspiration
This has been added to help correlate stock prices with my Australian news headlines dataset.
Telecom giant Telstra and the Mining gaint BHP Billiton are highly referenced (1000+ headlines) in the news and would be interesting case studies."
Do Conference Livetweets Get More Traffic?,Engagement metrics for 313 tweets (239 from a conference),Rachael Tatman,2,"Version 1,2017-10-27","social groups
marketing
twitter",CSV,15 KB,CC0,378 views,46 downloads,,0 topics,https://www.kaggle.com/rtatman/do-tweets-from-conferences-get-more-traffic,"Context
I like to livetweet conferences when I attend them, for my own reference later on and to help other people who aren't attending the conference keep up-to-date. After the last conference I attended, I was curious: do livetweets get more or less engagement than other types of tweets?
Content
This dataset contains information on 314 tweets sent from my personal Twitter account between September 29, 2017 and October 26, 2017. For each tweet, the following information is recorded:
at_conference?: Whether the tweet was sent during the conference
day: The day the tweet was sent
impressions: How many times the tweet was seen
engagements: How many times the tweet was engaged with (sum of the following columns)
retweets: How many times the tweet was retweeted
likes: How many times the tweet was liked
user profile clicks: How many times someone clicked on my profile from the tweet
url clicks: How many times someone clicked a URL in the tweet (not all tweets have URL's)
hashtag clicks: How many times someone clicked on a hashtag in the tweet (not all tweets have hashtags)
detail expands: How many times someone expanded the tweet
follows: How many times someone followed me from the tweet
media views: How many times someone viewed media embedded in the tweet (not all tweets have media)
media engagements: How many times someone clicked on media embedded in the tweet (not all tweets have media)
Inspiration
Do conference tweets get more engagement?
Does my account get more engagement during a conference?
Do the types of engagement differ depending on whether I'm at a conference?"
Github stared repos with photos,github most starred repos,Jaime Valero,2,"Version 1,2017-10-27",,CSV,228 KB,CC0,83 views,5 downloads,,0 topics,https://www.kaggle.com/jaimevalero/github-stared-repos-with-photos,This dataset does not have a description yet.
Customer Churn,Predicting the customers who will churn,Aadrika Singh,2,"Version 1,2017-10-17",,Other,25 MB,Other,"1,146 views",204 downloads,,0 topics,https://www.kaggle.com/aadrika/customer-churn,"Context
In the modern-world scenario of telecommunication industries, the customers have a range of options to choose from, as far as service providers are concerned. Factors such as perceived frequent service disruptions, poor customer service experiences, and better offers from other competing carriers may cause a customer to churn (likely to leave).
Customer churn includes customers stopping the use of a service, switching to a competitor service, switching to a lower-tier experience in the service or reducing engagement with the service."
NYC Borough Boundaries,(Clipped to shoreline),Liza Bolton,2,"Version 1,2017-10-02","cities
geography",Other,1 MB,CC0,442 views,52 downloads,2 kernels,0 topics,https://www.kaggle.com/dataembassy/nyc-borough-boundaries,"Content
Necessary shapefiles to create maps of New York City and its boroughs.
Acknowledgements
These files have been made available by the New York City Department of City Planning and were retrieved from http://www1.nyc.gov/site/planning/data-maps/open-data/districts-download-metadata.page on 27 September, 2017.
Inspiration
These shapefiles might pair nicely with the New York building and elevator data also on here, as well as the NYC Tree Census I use it for."
URL Database,With some features I extracted and responses for whether it is phishing or not?,Baris Simsek,2,"Version 3,2017-10-29|Version 2,2017-10-29|Version 1,2017-10-29","web sites
computer security
internet",CSV,250 KB,ODbL,488 views,58 downloads,,0 topics,https://www.kaggle.com/simsek/openphishcom-phishing-urls-on-oct-2-2017,"Context
I studied on this data set to predict phishing web sites by using:
1- Just URL string 2- The content broadcasts from the url
Content
This data set just has URL list.
Acknowledgements
Data gathered from openphish.com.
Inspiration
Does a URL itself or content of the URL show us whether it is phishing or not?"
MLB 2017,Major League Baseball American League East Hitting Data,Nick Torsky,2,"Version 1,2017-10-10",,Other,382 KB,CC3,341 views,59 downloads,,0 topics,https://www.kaggle.com/ntorsky/mlb-2017,"Context
Data of hitters in MLB's AL East
Content
Basic fundamental data on AL East hitters sorted descending by Plate Appearances
Acknowledgements
Inspiration
I love baseball and stats."
Titanic,,Joao Januario,2,"Version 1,2017-10-14",,CSV,91 KB,CC0,95 views,12 downloads,,0 topics,https://www.kaggle.com/joaojanuario/titanic3,This dataset does not have a description yet.
HIPAA Breaches from 2009-2017,Complete archives of reported HIPAA breaches,Archangell,2,"Version 1,2017-10-29",,CSV,1 MB,CC0,133 views,13 downloads,,0 topics,https://www.kaggle.com/archangell/hipaa-breaches-from-20092017,"Context
The official archive data set from the DHHS on all reported PHI data breaches from medical and dental centers (Military and Civilian).
Content
The name of the practice, the amount of people effected and the breach description.
Acknowledgements
I did not create this data set, it is from the archives of the Department of Health and Human Services.
Inspiration
This can be used to cross reference with OSHA violations and to determine the most common breach types"
GTZAN music/speech collection,,lnicalo,2,"Version 1,2017-10-24",,Other,162 MB,CC0,326 views,27 downloads,,,https://www.kaggle.com/lnicalo/gtzan-musicspeech-collection,"Context
The need for music-speech classification is evident in many audio processing tasks which relate to real-life materials such as archives of field recordings, broadcasts and any other contexts which are likely to involve speech and music, concurrent or alternating. Segregating the signal into speech and music segments is an obvious first step before applying speech-specific or music-specific algorithms. Indeed, speech-music classification has received considerable attention from the research community (for a partial list, see references below) but many of the published algorithms are dataset-specific and are not directly comparable due to non-standardised evaluation.
Content
Dataset collected for the purposes of music/speech discrimination. The dataset consists of 120 tracks, each 30 seconds long. Each class (music/speech) has 60 examples. The tracks are all 22050Hz Mono 16-bit audio files in .wav format."
iris_data,Hello World of Machine Learning and Deep Learning,kamran,2,"Version 2,2017-11-30","food and drink
internet",CSV,5 KB,CC0,186 views,25 downloads,2 kernels,0 topics,https://www.kaggle.com/kamrankausar/iris-data,This dataset does not have a description yet.
College Football Statistics,"Every Player, Team, and Play from 2005 - 2013",Matt Hixon,2,"Version 1,2017-11-28","universities and colleges
american football
sports",Other,32 MB,CC0,355 views,43 downloads,,0 topics,https://www.kaggle.com/mhixon/college-football-statistics,"Content
This dataset contains information about players and teams and their statistics from 2005-2013. It also includes every play of every drive for every game played between 2005-2013.
Acknowledgements
Thanks to cbfstats.com and J. Albert Bowden II for the dataset."
Brazil Elections 2014,Election results and candidate data.,Eliezer Bourchardt,2,"Version 1,2017-11-10","brazil
government
politics",CSV,29 MB,CC0,101 views,9 downloads,,0 topics,https://www.kaggle.com/eliezerfb/brazil-elections-2014,"Context
Includes data for all candidates from all units of the federation and yours list of property declarations.
Content
Data of candidates brazilian national elections of 2014. Source: http://www.tse.jus.br/eleitor-e-eleicoes/estatisticas/repositorio-de-dados-eleitorais-1/repositorio-de-dados-eleitorais"
Fantasy Premier League,Fantasy premier league player historic data,Adithya Ganesh,2,"Version 1,2017-11-22",,CSV,399 KB,ODbL,251 views,14 downloads,,,https://www.kaggle.com/adithyarganesh/fantasy-premier-league,"Context
I love football and wanted to gather a data-set of a list of football players along with their each game performance from various different sources.
Content
The csv file has the fantasy premier league data of all players who played in 3 seasons and a detailed spreadsheet of each player is provided.
Acknowledgements
Thanks to TURD from tableau for some of the data.
Inspiration
We all wondered if it is possible to predict the future! well with the player data against each team and conditions we get to check if the future prediction is truly possible!"
IITM-HeTra,Dataset for Vehicle Detection in Heterogeneous Traffic Scenarios,DeepakMittal,2,"Version 1,2017-11-24",,Other,276 MB,GPL,162 views,7 downloads,,,https://www.kaggle.com/deepak242424/iitmhetra,"We generated our own dataset (IITM-HeTra) from cameras monitoring road traffic in Chennai, India. To ensure that data are temporally uncorrelated, we sample a frame every two seconds from multiple video streams. We extracted 2400 frames in total.
We manually labeled 2400 frames under different vehicle categories. The number of available frames reduced to 1417 after careful scrutiny and elimination of unclear images. We initially defined eight different vehicle classes commonly seen in Indian traffic. Few of these classes were similar while two classes had less number of labeled instances; these were merged into similar looking classes. For example, in our dataset, we had different categories for small car, SUV, and sedan which were merged under the light motor vehicle (LMV) category.
A total of 6319 labeled vehicles are available in the collected dataset. This includes 3294 two-wheelers, 279 heavy motor vehicles (HMV), 2148 cars, and 598 auto-rickshaws. A second dataset was created by merging cars and auto-rickshaws together into light motor vehicle (LMV) class. Approximately 25.2\% of vehicles were occluded."
Jupyter Notebook,,Dayana Moncada,2,"Version 1,2017-11-10",,Other,227 KB,CC0,223 views,5 downloads,,0 topics,https://www.kaggle.com/dmonca63/jupyter-notebook,This dataset does not have a description yet.
Cifar-10,,Quan Nguyen,2,"Version 1,2017-11-22",,Other,162 MB,CC0,207 views,35 downloads,3 kernels,0 topics,https://www.kaggle.com/quanbk/cifar10,This dataset does not have a description yet.
Predict Happiness,predict the happinessfor customers,Elizabeth Sam,2,"Version 1,2017-11-28",,CSV,32 MB,Other,135 views,12 downloads,,0 topics,https://www.kaggle.com/elizabethsam/happiness,This dataset does not have a description yet.
Interactive Hand Gesture,,Destin,2,"Version 4,2018-01-25|Version 3,2018-01-25|Version 2,2018-01-25|Version 1,2017-10-17","human-computer interaction
classification",Other,2 GB,Other,289 views,49 downloads,,0 topics,https://www.kaggle.com/destin369y/interactivehandgesture1,"Context
There's a story behind every dataset and here's your opportunity to share yours.
Interactive Hand Gesture. Here are color images and as well depth images of hand gestures grouped by their classes.
Copyright: Author: Chengyin Liu; Email: destin369y at gmail.com; Year: 2015.
Please acknowledge my name if you use this dataset, thank you.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
RGB-D hand gesture images taken by depth camera. Grouped by classes. Please refer to ""class.txt"" Used for hand gesture recognition evaluation.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
This dataset is used for my hand gesture recognition research at National Taiwan University, in the Intelligent Robot Lab under the lead of Prof. Li-Chen Fu. More details about our lab, please visit http://robotlab.csie.ntu.edu.tw/
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
Elementary Python Functions 7,Program Synthesis and Induction,Brandon Trabuco,2,"Version 2,2017-10-21|Version 1,2017-10-21","mathematics
programming languages",CSV,56 MB,Other,159 views,4 downloads,,0 topics,https://www.kaggle.com/btrabucco/epf-7,"Context
Automated composition of computer programs has been a standing challenge since the early days of artificial intelligence, and has no clear solutions with modern-day research in deep learning. Indeed, modeling latent context representations in language proves to be a difficult task singularly, and combined with applying structured procedural knowledge in a generative fashion quickly becomes intractable for complex domain specific languages. There is a clear need for research in devising efficiently learned combinations of these independent problems. We present this basic dataset of elementary mathematical functions encoded in the python programming language to encourage future research in this field, and to benchmark our own deep learning
Content
This dataset contains a total of 335922 elementary mathematical functions with examples for inputs with corresponding outputs. The first line in the dataset csv file contains a header describing the contents of each column. The first column is labeled function_name, and the following rows contains a function name with a unique integer index. The next twenty columns contain function_input_X and function_output_X for all integers between 0 and 9 inclusively, where the following rows contains string encoded python floating point numbers after executing the corresponding function. The final column is labeled function_code, and contains a single-line lambda statement elementary mathematical function.
Acknowledgements
We thank the guidance from the student instructors of the Machine Learning @ Berkeley research group, and the discussions had with members of the Redwood Institute of Theoretical Neuroscience.
Inspiration
We hope to see a neural architecture that is capable of programming new computer code without human supervision, in any programming language, solving even scientific computing tasks.
MIT License
Copyright (c) 2017 Brandon Trabucco
Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:
The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
Running Times Data for High School Students,Mile Times Being Compared to different Formulas,Semin,2,"Version 1,2017-10-17",running,CSV,7 KB,CC0,411 views,48 downloads,,0 topics,https://www.kaggle.com/eliotyoon/running-times-data-for-high-school-students,"My research is taking data from high schooler's 2 mile times and converting it using my formula against other running formulas. I'm trying to find the fatigue decrease by using their time.
The Actual Times are the 1 mile and 2 mile times, The age is the age grading formula, The VO2 Max is the formula used for hypothetical running tmes, And the cameron and Riegel formulas are for prediction times. Taking the average of them all, I incorporated my formula and compared how close my prediction was to the average of all the formulas."
flights,,Fredrik Jonsson,2,"Version 1,2017-11-27",,CSV,204 MB,Other,408 views,90 downloads,,0 topics,https://www.kaggle.com/freddejn/flights,This dataset does not have a description yet.
Economic Indicators,A collection of economic indicators from Jan '07 to Sep '17 by month.,Robert Nolan,2,"Version 2,2017-11-29|Version 1,2017-11-24",,CSV,14 KB,Other,163 views,12 downloads,,0 topics,https://www.kaggle.com/robertnolan/economic-indicators,This dataset does not have a description yet.
Titanic,,Sai C,2,"Version 1,2017-11-14",,CSV,91 KB,CC0,937 views,208 downloads,2 kernels,0 topics,https://www.kaggle.com/nsaikn/titanic,This dataset does not have a description yet.
Online Retail,,puneet,2,"Version 1,2017-11-10",,Other,23 MB,Other,990 views,259 downloads,2 kernels,,https://www.kaggle.com/puneetbhaya/online-retail,This dataset does not have a description yet.
Pill Count detection,detecting number of pills by audio.,Vonage Garage,2,"Version 2,2017-11-13|Version 1,2017-11-13",medicine,Other,38 MB,CC0,173 views,10 downloads,,0 topics,https://www.kaggle.com/vonagegarage/pill-count-detection,"This dataset is a experimentation of how to detect number of pill inside a pill bottle, through audio alone. There are plenty of games that exist that try to guess the number of items inside a container. Its usually a game for fun, and just a game of chance. But can we game the system?
For this experiment, we decided to use child resistant medicine pill bottles. and filled these bottles with generic Acetaminophen gel caps https://www.costco.com/.product.100017017.html
The following data contains 10, 10 seconds audio recordings of a generic pill bottle being shaken by one individual with a different number of pills. In our example, we recorded a user shaking a pill bottle with 1 pill, 10 pills, 25 pills and 50 pills. Each folder contains 10, 10 second samples of each"
All UK Active Company Names,"3,838,469 company details",Brian J,2,"Version 1,2017-11-15",,CSV,43 MB,CC0,110 views,8 downloads,,0 topics,https://www.kaggle.com/dalreada/all-uk-active-company-names,"I work with UK company information on a daily basis, and I thought it would be useful to publish a list of all active companies, in a way that could be used for machine learning.
There are 3,838,469 rows in the dataset, one for each active company. Each row, has the company name, date of incorporation and the Standard Industrial Classification Code.
The company list is from the publicly available 1st November 2017 Companies House snapshot.
The SIC code descriptions are from the gov.uk website.
In the file AllCompanies.csv each row is formatted as follows:
CompanyName - Alpha numberic company name
IncorporationDate - in British date format, dd/mm/yyyy
SIC - 5 digits or if not known, None - see separate file for description of each code.
Inspiration
Possible uses for this data is to use ML to suggest a new unique but suitable name for a company based on what other companies of the same SIC are called.
Perhaps analyse how company names have evolved over time.
Using ML, perhaps determine what a typical company name looks like, maybe analyse if company names have got longer or more complicated over time.
I am sure there are many more possible uses for this data in ways, that I cannot imagine.
This is my second go (the first was published a few hours ago) at publishing a dataset on any medium, so any useful tips and hints would be extremely welcome.
Links to the raw data sources are here:
Companies House http://download.companieshouse.gov.uk/en_output.html
SIC Codes https://www.gov.uk/government/publications/standard-industrial-classification-of-economic-activities-sic"
California Facilities Pollutant Emissions Data,Data for geocoded facilities in all 58 counties,Florin Langer,2,"Version 2,2017-11-21|Version 1,2017-11-21",pollution,Other,1 MB,CC0,201 views,14 downloads,,0 topics,https://www.kaggle.com/florinlanger/cal-facilities,"Context
Created for use in the Renewable and Appropriate Energy Lab at UC Berkeley and Lawrence Berkeley National Laboratory.
Content
Geography: All 58 Counties of the American State of California
Time period: 2015
Unit of analysis: Tons per year
Variables:
CO: County ID as numbered in the County dropdown menu on the California Air Resources Board Facility Search Tool
AB
FACID
DIS
FNAME
FSTREET
FCITY
FZIP
FSIC: Facility Standard Industrial Classification Code specified by the US Department of Labor
COID
DISN
CHAPIS
CERR_CODE
TOGT: Total organic gases consist of all hydrocarbons, i.e. compounds containing hydrogen and carbon with or without other chemical elements.
ROGT: Reactive organic gases include all the organic gases exclude methane, ethane, acetone, methyl acetate, methylated siloxanes, and number of low molecular weight halogenated organics that have a low rate of reactivity.
COT: The emissions of CO are for the single species, carbon monoxide.
NOXT: The emissions of NOx gases (mostly nitric oxide and nitrogen dioxide) are reported as equivalent amounts of NO2.
SOXT: The emissions of SOx gases (sulfur dioxide and sulfur trioxide) are reported as equivalent amounts of SO2.
PMT: Particulate matter refers to small solid and liquid particles such as dust, sand, salt spray, metallic and mineral particles, pollen, smoke, mist and acid fumes.
PM10T: PM10 refers to the fraction of particulate matter with an aerodynamic diameter of 10 micrometer and smaller. These particles are small enough to penetrate the lower respiratory tract.
PM2.5T: PM2.5 refers to the fraction of particulate matter with an aerodynamic diameter of 2.5 micrometer and smaller. These particles are small enough to penetrate the lower respiratory tract.
lat: Facility latitude geocoded by inputting FSTREET, FCITY, California FZIP into Bing’s geocoding service.
lon: Facility longitude geocoded in the same way.
Sources: All columns except for lat and lon were scraped from the California Air Resources Board Facility Search Tool using the Request module from Python’s Urllib library. The script used is included below in scripts in case you would like to get additional columns.
The lat and lon columns were geocoded using the Geocoder library for Python with the Bing provider.
Scripts
download.py
import pandas as pd out_dir = 'ARB/' file_ext = '.csv' for i in range(1, 59): facilities = pd.read_csv(""https://www.arb.ca.gov/app/emsinv/facinfo/faccrit_output.csv?&dbyr=2015&ab_=&dis_=&co_="" + str(i) + ""&fname_=&city_=&sort=FacilityNameA&fzip_=&fsic_=&facid_=&all_fac=C&chapis_only=&CERR=&dd="") for index, row in facilities.iterrows(): curr_facility = pd.read_csv(""https://www.arb.ca.gov/app/emsinv/facinfo/facdet_output.csv?&dbyr=2015&ab_="" + str(row['AB']) + ""&dis_="" + str(row['DIS']) + ""&co_="" + str(row['CO']) + ""&fname_=&city_=&sort=C&fzip_=&fsic_=&facid_="" + str(row['FACID']) + ""&all_fac=&chapis_only=&CERR=&dd="") facilities.set_value(index, 'PM2.5T', curr_facility.loc[curr_facility['POLLUTANT NAME'] == 'PM2.5'].iloc[0]['EMISSIONS_TONS_YR']) facilities.to_csv(out_dir + str(i) + file_ext)
geocode.py
import geocoder import csv directory = 'ARB/' outdirectory = 'ARB_OUT/' for i in range(1, 59): with open(directory + str(i) + "".csv"", 'rb') as csvfile, open(outdirectory + str(i) + '.csv', 'a') as csvout: reader = csv.DictReader(csvfile) fieldnames = reader.fieldnames + ['lat'] + ['lon'] # Add new columns writer = csv.DictWriter(csvout, fieldnames) writer.writeheader() for row in reader: address = row['FSTREET'] + ', ' + row['FCITY'] + ', California ' + row['FZIP'] g = geocoder.bing(address, key='API_KEY') newrow = dict(row) if g.latlng: newrow['lat'] = g.json['lat'] newrow['lon'] = g.json['lng'] writer.writerow(newrow) # Only write row if successfully geocoded"
Inflow Level of Wastewater Treatment,The records saved from a Wastewater infrastructure in a Greek Island,panos,2,"Version 1,2017-11-23",bodies of water,CSV,237 KB,ODbL,111 views,5 downloads,,,https://www.kaggle.com/panosa/inflow-level,This dataset does not have a description yet.
UK 2016 Road Safety Data,Data from the UK Department for Transport,ThomasLuby,2,"Version 1,2017-11-11","safety
government
cycling
road transport",CSV,63 MB,CC0,386 views,78 downloads,,0 topics,https://www.kaggle.com/bluehorseshoe/uk-2016-road-safety-data,"Context
UK police forces collect data on every vehicle collision in the UK on a form called Stats19. Data from this form ends up at the DfT and is published at https://data.gov.uk/dataset/road-accidents-safety-data
Content
There are 4 CSVs and an Excel file in this set. Accidents is the primary table and has references by Accident_Index to the other tables.
Acknowledgements
Department for Transport and the UK's wonderful Open Gov initiative https://data.gov.uk/
Inspiration
Are there patterns for accidents involving different road users?
Can we predict the safest / most dangerous times to travel
Can this data help route cyclists around accident hotspots taking into account the time of day, weather, route etc
Are certain cars more accident prone than others?"
Commuter train timetable,Commuter train service in Stockholm 2012,Abderrahman (Abdou) Ait Ali,2,"Version 1,2018-01-15","transport
public transport
rail transport",CSV,156 MB,Other,125 views,8 downloads,,2 topics,https://www.kaggle.com/abdeaitali/commuter-train-timetable,"Context
Train commuter service in Stockholm in 2012.
Content
Description of the train commuter service during one week-day. The data includes information about: - train timetable. - passengers flow
Acknowledgements
The data is generated with the help of Samtrafiken - Trafiklab API (see. www.trafiklab.se).
Inspiration
Crowdedness: How crowd the trains are? Which lines are crowded? How frequent should the train run?"
"Anime Data (Score, Staff, Synopsis, and Genre)",Consisting 4029 anime data,Canggih P Wibowo,2,"Version 2,2018-02-22|Version 1,2018-01-16","popular culture
film
animation
entertainment",CSV,2 MB,CC4,168 views,7 downloads,5 kernels,0 topics,https://www.kaggle.com/canggih/anime-data-score-staff-synopsis-and-genre,"Context
Japanese animation, which is known as anime, has become internationally widespread nowadays. This dataset provides data on anime taken from Anime News Network.
Content
This dataset consists of 4029 anime data in 5 files. All of the csv files use '|' delimiter.
- Anime Title (datatitle-all-share-new.csv)
- Anime Synopsis (datasynopsis-all-share-new.csv)
- Anime Genre (datagenre-all-share-new.csv)
- Anime Staff (datastaff-all-share-new.csv)
- Anime Scores (datascorehist-all-share-new.csv)
Anime ID and Staff were taken as what they seen on Anime News Network system. While the scores are taken based on the histogram of scores on each anime page and normalized.
Acknowledgements
The dataset was collected from http://www.animenewsnetwork.com on 10 May 2016. If you use this dataset in publications, please cite:
Wibowo, C. P. (2016). A Minimum Spanning Tree Representation of Anime Similarities. arXiv preprint arXiv:1606.03048
Inspiration
This dataset can be used to build recommendation systems, predict a score, visualize anime similarity, etc."
Badminton Game Data (BWF Super Series 2015-2017),Badminton scores from 11872 games,Canggih P Wibowo,2,"Version 2,2018-02-22|Version 1,2018-01-16",sports,CSV,324 KB,CC4,109 views,6 downloads,,0 topics,https://www.kaggle.com/canggih/badminton-game-data-bwf-super-series-20152017,"Context
This dataset mainly features the score changes during badminton games in the rally-point system.
Content
The dataset contains 11872 games from 5131 matches in BWF Super Series Tournaments. There are 6 fields:
- Year: I collected 3 years data: 2015-2017.
- Tournament: For each year, there are 12 Super Series Tournaments.
- Round: 1 - Round 1; 2 - Round 2; Q - Quarter Finals; S - Semi-Finals; F - Finals
- Match: Information about the countries of the players.
- Type: MS - Men's Single; WS - Women's Single; MD - Men's Double; WD - Women's Double; XD - Mixed Double
- Scores: Score changes during the games.
Acknowledgements
The dataset was collected from bwfbadminton.com. I wrote codes to scrap the information.
Inspiration
Performance of the players is reflected on how the score changes during the games. Exploring this information may help us to predict or learn something related to badminton games."
Rum Data,Analyze Responsibly (ages 21 and up only please),mrpantherson,2,"Version 1,2017-11-11","food and drink
alcohol",CSV,534 KB,CC0,235 views,20 downloads,,0 topics,https://www.kaggle.com/mrpantherson/rum-data,"Context
Walking around a Total Wine one day I wondered if there was any data I could find that would help me figure out what new rums to try, I later was able to find some information on rumratings.com. I'm now the hit of every party thanks to this data and a few box plots.
Content
The data was scraped from rumratings, unfortunately a good portion of the data is sparse, I go into more detail about that in the kernel. There is slightly more information that could be grabbed but I didn't want to overburden the site, and well that would take a bit more time to write.
Acknowledgements
Mainly I would just like to thank the site for existing as a resource, and of course to any contributors on this data.
Inspiration
I'd like to see some more EDA done, i'll be doing my work in Python so I like seeing the R kernels, interesting to see another approach."
Car Insurance,,Sanket Kumar,2,"Version 1,2017-11-17",,CSV,30 MB,CC0,519 views,25 downloads,,,https://www.kaggle.com/sk9000/car-insurance,This dataset does not have a description yet.
MNIST dataset,,LunarLlama,2,"Version 1,2018-01-06",,CSV,15 MB,Other,64 views,21 downloads,,0 topics,https://www.kaggle.com/lunarllama/mnist-dataset,This dataset does not have a description yet.
U.S. Educational Attainment [1995-2015],An easy-to-use compilation of census data for U.S. education levels,Roy Garrard,2,"Version 1,2017-11-16","united states
education",Other,1 MB,CC0,162 views,29 downloads,,0 topics,https://www.kaggle.com/noriuk/us-educational-attainment-19952015,"Context
The United States Census Bureau conducts regular surveys to assess education levels in the U.S. These surveys sample participants' highest levels of education (i.e. high school diploma, bachelor's degree, etc.) The attached CSV file aggregates data for the years 1995, 2005, and 2015.
Content
Data is organized into columns representing the survey year, age range, sex of participants, and education level. For example, [1995, 18_24, Male, ...] represents the 1995 survey for men ages 18-24.
It's worth noting that the surveys varied somewhat in granularity. The 2015 survey divided categories more finely (18-24, 25-29, 29-34...) while the 2005 and 1995 surveys were coarser (18-24, 25-34, ...). This could create some distortion depending on the analysis used.
Sources
Main
https://www.census.gov/topics/education/educational-attainment/data/tables.All.html
2015
Table 1. Educational Attainment of the Population 18 Years and Over, by Age, Sex, Race, and Hispanic Origin: 2015
https://www.census.gov/data/tables/2015/demo/education-attainment/p20-578.html
2005
Table 6. Educational Attainment of Employed Civilians 18 to 64 Years, by Occupation, Age, Sex, Race, and Hispanic Origin: 2005
https://www.census.gov/data/tables/2005/demo/educational-attainment/cps-detailed-tables.html
1995
Educational Attainment in the United States: March 1995
https://www.census.gov/data/tables/1995/demo/educational-attainment/p20-489.html"
tweets,tweets related to dental care affordability and dental + opioid use,Izzy,2,"Version 3,2017-11-27|Version 2,2017-11-27|Version 1,2017-11-17","healthcare
dentistry
twitter
internet",CSV,24 MB,CC0,340 views,30 downloads,,0 topics,https://www.kaggle.com/izzykayu/tweets,"Context
Many many search queries bypassing the limits of Twitter API. these files can easily be read using read_delim in R and using ; as the delimiter. The search query is in the name of the csv. The time runs from this year until ~2012, but each set varies.
Dental pain, alternatives, opioid prescription, patient visits, dental advertisements, misinformation, and more are found within these datasets.
This tweet set is great for text analytics, machine learning, and etc.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
I believe healthcare should be a right for all, and that includes dental care? What alternatives do people turn to when they cannot afford the care they need? I hear many of my friends, in their 20s like myself, not receiving coverage or waiting to go to the dentist. Although age is not included in these csv. user profiles if public can be grabbed using the twitter API."
Fortnite: Battle Royale - Weapon Attributes,Stats for all weapons in patch 2.1.0,jruots,2,"Version 1,2018-01-13",video games,CSV,3 KB,Other,205 views,20 downloads,,0 topics,https://www.kaggle.com/jruots/fortnite-battle-royale-weapon-attributes,"This dataset has attributes for all available weapons in Fortnite: Battle Royale as of patch 2.1.0 or as of January 10th 2018. All credit for this data goes to SoumyDev at http://www.fortnitechests.info/
The dataset features the following columns:
Name: Name of the weapon
DPS: The damage per second of the weapon
Damage: The damage done by the weapon
Critical %: The critical hit chance of the weapon
Crit. Damage: The critical hit damage of the weapon
Fire Rate: The fire rate of the weapon
Mag. Size: The size of the magazine of the weapon
Range: The range of the weapon
Durability: The durability of the weapon
Reload Time: The reload time of the weapon
Ammo Cost: The cost in ammunition to fire a single projectile
Impact: The impact of the weapon i.e. the damage it does to buildings
Rarity: The rarity of the weapon
Type: What type of weapon is in question
In total there are 14 columns with 43 rows or weapons
As mentioned previously, all credit for this data goes to SoumyDev at http://www.fortnitechests.info/
Potential uses for the dataset:
Find out is some weapon in the game underutilized
What is the best weapon for different situations?
Are there different ways to classify weapons than by their type and rarity?"
rmedanew,,Rajeev,2,"Version 56,2018-01-13|Version 55,2018-01-13|Version 54,2018-01-13|Version 53,2018-01-12|Version 52,2018-01-12|Version 51,2018-01-12|Version 50,2018-01-12|Version 49,2018-01-12|Version 48,2018-01-12|Version 47,2018-01-12|Version 46,2018-01-12|Version 45,2018-01-12|Version 44,2018-01-12|Version 43,2018-01-12|Version 42,2018-01-12|Version 41,2018-01-12|Version 40,2018-01-12|Version 39,2018-01-12|Version 38,2018-01-12|Version 37,2018-01-12|Version 36,2018-01-12|Version 35,2018-01-11|Version 34,2018-01-11|Version 33,2018-01-11|Version 32,2018-01-11|Version 31,2018-01-11|Version 30,2018-01-11|Version 29,2018-01-11|Version 28,2018-01-11|Version 27,2018-01-11|Version 26,2018-01-11|Version 25,2018-01-11|Version 24,2018-01-10|Version 23,2018-01-10|Version 22,2018-01-10|Version 21,2018-01-10|Version 20,2018-01-10|Version 19,2018-01-10|Version 18,2018-01-10|Version 17,2018-01-10|Version 16,2018-01-10|Version 15,2018-01-10|Version 14,2018-01-10|Version 13,2018-01-10|Version 12,2018-01-10|Version 11,2018-01-10|Version 10,2018-01-09|Version 9,2018-01-09|Version 8,2018-01-09|Version 7,2018-01-09|Version 6,2018-01-09|Version 5,2018-01-09|Version 4,2018-01-09|Version 3,2018-01-09|Version 2,2018-01-09|Version 1,2018-01-09",,CSV,4 MB,Other,216 views,8 downloads,,0 topics,https://www.kaggle.com/rajeevmeda/rmedanew,This dataset does not have a description yet.
University Statistics,Statistics surrounding 311 US Universities,Christopher Lambert,2,"Version 1,2018-01-22",universities and colleges,{}JSON,347 KB,CC0,850 views,78 downloads,,,https://www.kaggle.com/theriley106/university-statistics,"Context
Data was grabbed from US-News: https://www.usnews.com
The following data points are included in this data set:
Ranking
Acceptance-Rate
Act-Avg
Sat-Avg
Photo
Cost after Financial Aid
City
Sortname
Zip
Percent Receiving Aid
State
Average High School GPA
Tied Ranking
Public/Private University
Business Reputation Score
Tuition
Engineering Reputation Score
Enrollment Size
Region
It includes statistics surrounding the following 311 US Universities.
Princeton University
Harvard University
University of Chicago
Yale University
Columbia University
Massachusetts Institute of Technology
Stanford University
University of Pennsylvania
Duke University
California Institute of Technology
Dartmouth College
Johns Hopkins University
Northwestern University
Brown University
Cornell University
Rice University
Vanderbilt University
University of Notre Dame
Washington University in St. Louis
Georgetown University
Emory University
University of California--Berkeley
University of California--Los Angeles
University of Southern California
Carnegie Mellon University
University of Virginia
Wake Forest University
University of Michigan--Ann Arbor
Tufts University
New York University
University of North Carolina--Chapel Hill
Boston College
College of William and Mary
Brandeis University
Georgia Institute of Technology
University of Rochester
Boston University
Case Western Reserve University
University of California--Santa Barbara
Northeastern University
Tulane University
Rensselaer Polytechnic Institute
University of California--Irvine
University of California--San Diego
University of Florida
Lehigh University
Pepperdine University
University of California--Davis
University of Miami
University of Wisconsin--Madison
Villanova University
Pennsylvania State University--University Park
University of Illinois--Urbana-Champaign
Ohio State University--Columbus
University of Georgia
George Washington University
Purdue University--West Lafayette
University of Connecticut
University of Texas--Austin
University of Washington
Brigham Young University--Provo
Fordham University
Southern Methodist University
Syracuse University
University of Maryland--College Park
Worcester Polytechnic Institute
Clemson University
University of Pittsburgh
American University
Rutgers University--New Brunswick
Stevens Institute of Technology
Texas A&M University--College Station
University of Minnesota--Twin Cities
Virginia Tech
Baylor University
Colorado School of Mines
University of Massachusetts--Amherst
Miami University--Oxford
Texas Christian University
University of Iowa
Clark University
Florida State University
Michigan State University
North Carolina State University--Raleigh
University of California--Santa Cruz
University of Delaware
Binghamton University--SUNY
University of Denver
University of Tulsa
Indiana University--Bloomington
Marquette University
University of Colorado--Boulder
University of San Diego
Drexel University
Saint Louis University
Yeshiva University
Rochester Institute of Technology
Stony Brook University--SUNY
SUNY College of Environmental Science and Forestry
University at Buffalo--SUNY
University of Oklahoma
University of Vermont
Auburn University
Illinois Institute of Technology
Loyola University Chicago
University of New Hampshire
University of Oregon
University of South Carolina
University of Tennessee
Howard University
University of Alabama
University of San Francisco
University of the Pacific
University of Utah
Arizona State University--Tempe
Iowa State University
Temple University
University of Kansas
University of St. Thomas
The Catholic University of America
DePaul University
Duquesne University
University of Missouri
Clarkson University
Colorado State University
Michigan Technological University
Seton Hall University
University of Arizona
University of California--Riverside
University of Dayton
University of Nebraska--Lincoln
Hofstra University
Louisiana State University--Baton Rouge
Mercer University
The New School
Rutgers University--Newark
University of Arkansas
University of Cincinnati
University of Kentucky
George Mason University
New Jersey Institute of Technology
San Diego State University
University of South Florida
Washington State University
Kansas State University
Oregon State University
St. John Fisher College
University of Illinois--Chicago
University of Mississippi
University of Texas--Dallas
Adelphi University
Florida Institute of Technology
Ohio University
Seattle Pacific University
University at Albany--SUNY
Oklahoma State University
University of Massachusetts--Lowell
University of Rhode Island
Biola University
Illinois State University
University of Alabama--Birmingham
University of Hawaii--Manoa
University of La Verne
University of Maryland--Baltimore County
Immaculata University
Maryville University of St. Louis
Missouri University of Science & Technology
St. John's University
University of California--Merced
University of Louisville
Mississippi State University
Rowan University
University of Central Florida
University of Idaho
Virginia Commonwealth University
Kent State University
Robert Morris University
Texas Tech University
Union University
University of Hartford
Edgewood College
Lesley University
Lipscomb University
Suffolk University
University of Maine
University of Wyoming
Azusa Pacific University
Ball State University
Montclair State University
Pace University
West Virginia University
Andrews University
Indiana University-Purdue University--Indianapolis
University of Houston
University of New Mexico
University of North Dakota
Widener University
New Mexico State University
North Dakota State University
Nova Southeastern University
University of North Carolina--Charlotte
Bowling Green State University
California State University--Fullerton
Dallas Baptist University
University of Massachusetts--Boston
University of Nevada--Reno
Central Michigan University
East Carolina University
Florida A&M University
Montana State University
University of Alaska--Fairbanks
University of Colorado--Denver
University of Massachusetts--Dartmouth
University of Montana
Western Michigan University
Florida International University
Louisiana Tech University
South Dakota State University
Southern Illinois University--Carbondale
University of Alabama--Huntsville
University of Missouri--Kansas City
Utah State University
Ashland University
Benedictine University
California State University--Fresno
Gardner-Webb University
Georgia State University
Shenandoah University
University of South Dakota
Wayne State University
American International College
Augusta University
Barry University
Boise State University
Cardinal Stritch University
Clark Atlanta University
Cleveland State University
Eastern Michigan University
East Tennessee State University
Florida Atlantic University
Georgia Southern University
Grand Canyon University
Indiana State University
Indiana University of Pennsylvania
Jackson State University
Kennesaw State University
Lamar University
Liberty University
Lindenwood University
Middle Tennessee State University
Morgan State University
National Louis University
North Carolina A&T State University
Northern Arizona University
Northern Illinois University
Oakland University
Old Dominion University
Portland State University
Prairie View A&M University
Regent University
Sam Houston State University
San Francisco State University
Spalding University
Tennessee State University
Tennessee Technological University
Texas A&M University--Commerce
Texas A&M University--Corpus Christi
Texas A&M University--Kingsville
Texas Southern University
Texas State University
Texas Woman's University
Trevecca Nazarene University
Trinity International University
University of Akron
University of Arkansas--Little Rock
University of Louisiana--Lafayette
University of Louisiana--Monroe
University of Maryland--Eastern Shore
University of Memphis
University of Missouri--St. Louis
University of Nebraska--Omaha
University of Nevada--Las Vegas
University of New Orleans
University of North Carolina--Greensboro
University of Northern Colorado
University of North Texas
University of South Alabama
University of Southern Mississippi
University of Texas--Arlington
University of Texas--El Paso
University of Texas--Rio Grande Valley
University of Texas--San Antonio
University of the Cumberlands
University of Toledo
University of West Florida
University of West Georgia
University of Wisconsin--Milwaukee
Valdosta State University
Wichita State University
Wright State University
Alliant International University
Argosy University
California Institute of Integral Studies
Capella University
Idaho State University
Northcentral University
Trident University International
Union Institute and University
University of Phoenix
Walden University
Wilmington University"
Nazi Tweets,"1000 nazi scalps---er, Twitter accounts, scraped for 100k tweets",Sarai Rosenberg,2,"Version 1,2017-11-18",internet,{}JSON,57 MB,CC4,132 views,15 downloads,,0 topics,https://www.kaggle.com/saraislet/nazi-tweets,"Context
I used a bag of words to find 900 public Nazi/altright Twitter accounts, and this is (up to) 200 tweets scraped from each account, in JSON. (November 14, 2017)
Content
These are Twitter status objects. They contain a variety of Twitter user information, hashtags, and other attributes.
Inspiration
I'm working on refining a Nazi-detection engine---first by refining the bag of words to help improve the dataset and data collection, then by using the improved dataset to train an ML model: https://github.com/Saraislet/sturm/"
Songs Emotion,,Sandy HE,2,"Version 1,2018-01-15",emotion,CSV,57 MB,Other,259 views,15 downloads,,0 topics,https://www.kaggle.com/sandyhe/songs-emotion,"Context
It is about recognizing songs emotion. Generally, for songs, audio features and lyrics could be used. For ground truth data, it tends to use online tags if dataset scale is large. Otherwise, music experts or trained candidates could be organized to label songs emotion.
Content
Here songs data is from The Million Song Dataset (MSD). It contains almost 1 million songs data. And in the stage1, only tempo, loudness and mode are extracted for analysis as audio feature. For checking convenience, artist and title are added.
For ground truth data, tags from lastfm is adopted.
For two sets of data mentioned above, some preprocessing work is done. If you have more interests, you can find all original data here. https://labrosa.ee.columbia.edu/millionsong/pages/getting-dataset
songs data and tags data could be connected by track_id.
In the further stage, timbre and pitch could be used in an appropriate way, especially related to time-series analysis. Adding genre, similarity or other features to assist songs emotion prediction.
Acknowledgements
The Million Song Dataset was created under a grant from the National Science Foundation, project IIS-0713334. The original data was contributed by The Echo Nest, as part of an NSF-sponsored GOALI collaboration. Subsequent donations from SecondHandSongs.com, musiXmatch.com, and last.fm, as well as further donations from The Echo Nest, are gratefully acknowledged.
Inspiration
Use this dataset to do some exploratory data analysis, predict emotion for songs if you like."
Disk Space Data,Disk space data collected over several months,WildGrok,2,"Version 1,2018-01-11",,CSV,834 KB,CC0,55 views,,,0 topics,https://www.kaggle.com/wildgrok/disk-space-data,"Context
Disk space captured for several months for a set of Windows servers
Content
Contents are the server name, disk drive, total disk space, free disk space and percentage of free space
Acknowledgements
Thanks to Kaggle for providing this development environment
Inspiration
My initial goal is to add a column with the moving average of free disk space (for 7 days), to be used for forcasting"
Top 50 startups in India 2017 by Economic Times,List of 50 most promising startups for 2017 in India according to Economic Times,Abhishek,2,"Version 2,2018-01-24|Version 1,2018-01-23","business
product",CSV,6 KB,CC0,141 views,12 downloads,,0 topics,https://www.kaggle.com/abhishekrathi/top-50-startups-in-india-2017-by-economic-times,"Context
I was searching for the most sought after startups in India and i came to know about these startups. The list contains the overview of the top 50 promising startups in India.
Content
The excel sheet contains the startup and the idea they are working on. Data was acquired by Economic Times newspaper. I have put that data inside this excel sheet to upload on Kaggle. This list was published on December 2016. It was meant to represent the startups which are most expected to do well in 2017.
The data was collected in 2016. Columns Contain 1) Name of startup 2) Industry 3) Location 4) Founding Team 5) Business Idea
Acknowledgements
The data was collected by Economic Times India. I have just put that data into an excel sheet. Original Data : 50 Hot Startups in 2017
Inspiration
This dataset can be updated to include even more startups. We can have a list of startups, their ideas and whether they succeeded or not. Thus this kind of data can be used to predict the possibility of success or failure of a business idea."
Jokes: Questions and Answers,The Most Usable Joke Dataset,Brendan Finan,2,"Version 9,2018-02-10|Version 8,2018-02-10|Version 7,2018-02-09|Version 6,2018-01-30|Version 5,2018-01-20|Version 4,2018-01-19|Version 3,2018-01-14|Version 2,2018-01-13|Version 1,2018-01-12","linguistics
nlp
text data",CSV,6 MB,GPL,206 views,17 downloads,,0 topics,https://www.kaggle.com/bfinan/jokes-question-and-answer,"Context
My goal with this dataset is to create the largest and most organized dataset of jokes.
Tools for this dataset are on my Github
Content
Jokes reduced to only the Question and the Answer.
Duplicates NOT removed
Offensive jokes NOT removed
Acknowledgements
Question-Answer Jokes by Jiri Roznovjak
Short Jokes by Abhinav Moudgil
Inspiration
Humor is one of the most difficult domains of natural language processing."
TA restaurants data 31 euro cities,Restaurants information from TA for 31 european cities,Damien BENESCHI,2,"Version 4,2018-01-15|Version 3,2018-01-14|Version 2,2018-01-12|Version 1,2018-01-10","food and drink
europe",CSV,7 MB,Other,153 views,15 downloads,,0 topics,https://www.kaggle.com/damienbeneschi/krakow-ta-restaurans-data-raw,"Context
This dataset has been obtained by scraping TA (the famous tourism website) for information about restaurants for a given city. The scraper goes through the restaurants listing pages and fulfills a raw dataset. The raw datasets for the main cities in Europe have been then curated for futher analysis purposes, and aggregated to obtain this dataset.
The scraper is a Python script, available on the GitHub repository here.
It uses principally pandas and BeautifulSoup libraries.
IMPORTANT: the restaurants list contains the restaurants that are registrered in the TA database only. All the restaurants of a city may not be resgistered in this database.
Content
The dataset contain restaurants information for 31 cities in Europe: Amsterdam (NL), Athens (GR) , Barcelona (ES) , Berlin (DE), Bratislava (SK), Bruxelles (BE), Budapest (HU), Copenhagen (DK), Dublin (IE), Edinburgh (UK), Geneva (CH), Helsinki (FI), Hamburg (DE), Krakow (PL), Lisbon (PT), Ljubljana (SI), London (UK), Luxembourg (LU), Madrid (ES), Lyon (FR), Milan (IT), Munich (DE), Oporto (PT), Oslo (NO), Paris (FR), Prague (CZ), Rome (IT), Stockholm (SE), Vienna (AT), Warsaw (PL), Zurich (CH).
The data is a .csv file comma-separated that contains 125 433 entries (restaurants). It is structured as follow: - Name: name of the restaurant
City: city location of the restaurant
Cuisine Style: cuisine style(s) of the restaurant, in a Python list object (94 046 non-null)
Ranking: rank of the restaurant among the total number of restaurants in the city as a float object (115 645 non-null)
Rating: rate of the restaurant on a scale from 1 to 5, as a float object (115 658 non-null)
Price Range: price range of the restaurant among 3 categories , as a categorical type (77 555 non-null)
Number of Reviews: number of reviews that customers have let to the restaurant, as a float object (108 020 non-null)
Reviews: 2 reviews that are displayed on the restaurants scrolling page of the city, as a list of list object where the first list contains the 2 reviews, and the second le dates when these reviews were written (115 673 non-null)
URL_TA: part of the URL of the detailed restaurant page that comes after 'www.tripadvisor.com' as a string object (124 995 non-null)
ID_TA: identification of the restaurant in the TA database constructed a one letter and a number (124 995 non-null)
Missing information for restaurants (for example unrated or unreviewed restaurants) are in the dataset as NaN (numpy.nan).
Acknowledgements
This work has been done as a personal interest but also as a training of the skills I got from the DataCamp data science bootcamp I have followed.
I hope you will find this dataset inspiring and will make great stories out of it that I will be pleased to read :)"
Telco Churn,Combination of Customer and call data detail,PangKW,2,"Version 3,2018-01-19|Version 2,2018-01-18|Version 1,2018-01-17",,Other,627 KB,CC0,159 views,40 downloads,,0 topics,https://www.kaggle.com/pangkw/telco-churn,This dataset does not have a description yet.
2 Class Classification,,Mission San Jose AI Club,2,"Version 2,2017-11-27|Version 1,2017-11-17",,Other,12 KB,CC0,144 views,13 downloads,,0 topics,https://www.kaggle.com/msjaiclub/2classclassification,This dataset does not have a description yet.
Retail Transaction Data,Retail transaction and promotion response data,Regi,2,"Version 1,2018-01-23",business,CSV,862 KB,CC0,559 views,47 downloads,,,https://www.kaggle.com/regivm/retailtransactiondata,"Context
The data provides customer and date level transactions for few years. It can be used for demonstration of any analysis that require transaction information like RFM. The data also provide response information of customers to a promotion campaign.
Content
Transaction data provides customer_id, transaction date and Amount of purchase. Response data provides the response information of each of the customers. It is a binary variable indicating whether the customer responded to a campaign or not.
Acknowledgements
Extremely thankful numerous kernel and data publishers of Kaggle and Github. Learnt a lot from these communities.
Inspiration
More innovative approaches for handling RFM Analysis."
FastText,embeddings with sub-word information,vsmolyakov,2,"Version 1,2018-01-01",linguistics,Other,107 MB,CC0,385 views,92 downloads,2 kernels,0 topics,https://www.kaggle.com/vsmolyakov/fasttext,"Context
FastText word embeddings trained on English wikipedia
FastText embeddings are enriched with sub-word information useful in dealing with misspelled and out-of-vocabulary words.
Content
Each line contains a word followed by 300-dimensional embedding
Acknowledgements
P. Bojanowski, E. Grave, A. Joulin, T. Mikolov, ""Enriching Word Vectors with Subword Information"", arXiv 2016
FastText Embeddings: https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md
Inspiration
Q1: How does FastText compare with Glove and word2vec embeddings?
Q2: What are the different approaches for learning embeddings with sub-word information?
Q3: How does FastText compare with character-level n-gram representation of words?"
Bitcoin & Altcoins in 2017,Price transition of bitcoin and altcoins in 2017,inaba,2,"Version 1,2017-12-31","finance
economics",CSV,808 KB,CC4,363 views,53 downloads,,0 topics,https://www.kaggle.com/minaba/bitcoin-altcoins-in-2017,"Context
I made this dataset for Coursera assignment (Applied Plotting, Charting & Data Representation in Python).
Content
Price transition of crypto-currencies in 2017. These data were downloaded via Poloniex API."
Predict Angina,Prediction of Angina from healthcare,KagglerSN,2,"Version 1,2018-01-21",,Other,12 KB,Other,95 views,10 downloads,,0 topics,https://www.kaggle.com/snehal1409/predict-angina,"Many women who are initially thought to have angina turn out to have normal coronary angiograms, that is they are found not to have angina after all. A study was carried out to assess the feasibility of a preliminary screening test. For a large number of patients who were thought to have angina, information on a number of possible risk factors was collected and then their subsequent angina status was recorded. The data is available as an R data frame entitled angina and contains the following information:
status: whether woman turns out to have angina (yes/no) age: age of a woman smoke: smoking status (1=current-, 2=ex-, 3=non-smoker) cig: current average number of cigarettes per day hyper: hypertension (1=absent, 2=mild, 3=moderate) angfam: family history of angina (yes/no) myofam: family history of myocardial infarction (yes/no) strokefam: family history of stroke (yes/no) diabetes: does woman have diabetes? (yes/no) Missing values are coded as NA.
The main aim of this study was to try to find out which, if any, of the health variables, are associated with angina and whether some subset of them could be used to help predict the dependent variable angina status. The accompanying document on the `Model selection through backward elimination’ is going to be useful for that purpose. More specifically, it would be helpful to be able to estimate the risk/probability that a woman with a particular combination of these health variables truly has angina. If such a scheme of estimating risks can be constructed, is it likely to be useful? i.e. is it good at predicting whether a woman has angina or not (since the treatment of angina is expensive)? In addition, it would be of interest to estimate the individual effects of important variables. For example, if smoking seems to be a risk factor, then what is the odds of a smoker having angina relative to a non-smoker? What about ex-smokers and light smokers?"
International Mathematical Olympiad (IMO) Scores,One of the most prestigious math contests in the world,Bai Li,2,"Version 1,2018-01-03","mathematics
mathematics education
education",CSV,809 KB,CC0,237 views,9 downloads,,0 topics,https://www.kaggle.com/luckyt/imo-scores,"First held in 1959, the International Mathematical Olympiad is an annual math competition for top high school students around the world. It consists of six problems, divided between two days: on each day, contestants are given 4.5 hours to solve three problems.
This dataset contains scores of the IMO from 1984 to 2017. The data for years 1959-1984 do not always record the scores for each individual problem, so it is omitted from this dataset.
Source: scraped from imo-official.org."
Output of running second order model (lb 0.177),,tomcwalker,2,"Version 1,2018-01-21",,CSV,235 KB,CC0,40 views,7 downloads,,0 topics,https://www.kaggle.com/tomcwalker/output-of-running-second-order-model-lb-0177,This dataset does not have a description yet.
Drug Induced Deaths,,qizheng,2,"Version 1,2018-01-02","united states
death
illegal drugs
demographics",CSV,37 KB,CC0,290 views,52 downloads,,0 topics,https://www.kaggle.com/yuqizheng/drug-induced-deaths,This dataset does not have a description yet.
BIG MART SALES PREDICTION,,Debashish Dalal,2,"Version 1,2018-01-04",,CSV,2 MB,ODbL,312 views,49 downloads,,0 topics,https://www.kaggle.com/devashish0507/big-mart-sales-prediction,This dataset does not have a description yet.
My Sleep Log,My sleep log for 3+ monthes,Elnur Jabarov,2,"Version 2,2018-02-03|Version 1,2018-02-03","sleep
healthcare
personal life",CSV,6 KB,CC0,130 views,17 downloads,,0 topics,https://www.kaggle.com/jelnur/my-sleep-log,"Context
I've always had problems with sleep. I felt bad either undersleeping or oversleeping. So last October I promised a friend of mine to get up at 5 AM for one month and started logging sleep data to a Google Spreadsheet. Sometimes I broke my promise, sometimes I was sick or on vacation and overslept but mainly kept my promise. Now with the help of Kaggle community I want to analyze this data and know how much sleep I need. Need to say that I'm fan of Biphasic and polyphasic sleep.
Content
I put a reminder on todo list to enter data everyday. I've decided not to capture many features besides times, only how easy felt asleep, how easy got up and how felt afterwards. There're usually more than one row per day. I usually get up by 5-6 AM and if feel very sleepy go back to sleep again. Sometimes I had a nap in the afternoon or in weekends.
Inspiration
I would like to know minimum amount of sleep I need to feel either ""Very good"" (4.5) or ""Super good"" (5). I guess this will be an average time for the last N days. By the way after how many days body resets it's sleep clock? It would be nice to know optimal time to go to bed too."
Recruit Restaurant Visitor Forecasting Data,Including Weather Data,Anvesh Tummala,2,"Version 5,2017-12-30|Version 4,2017-12-30|Version 3,2017-12-30|Version 2,2017-12-30|Version 1,2017-12-28",,CSV,28 MB,CC3,119 views,48 downloads,,0 topics,https://www.kaggle.com/anvesh525/recruit-restaurant-visitor-forecasting-data,This dataset does not have a description yet.
College Common Data Sets,Dataset Containing 173 College Common Data Sets,Christopher Lambert,2,"Version 2,2018-01-21|Version 1,2018-01-21",universities and colleges,Other,77 MB,CC0,865 views,37 downloads,,0 topics,https://www.kaggle.com/theriley106/college-common-data-sets,"Dataset Containing 173 College Common Data Sets
Contains Common Data Sets for the Following Schools:
Alabama State University
Angelo State University
Arapahoe Community College
Arkansas Tech University
Aurora University
Baldwin Wallace University
Beloit College
Bemidji State University
Berea College
Binghamton University
Boston University
Bucknell University
Cabrini University
California Baptist University
California State University, Bakersfield
California State University, Long Beach
California State University, Los Angeles
California State University, Sacramento
Carnegie Mellon University
Case Western Reserve University
Christopher Newport University
Clark University
Colby College
College of Charleston
Collin College
Colorado College
Colorado School of Mines
Colorado State University-Pueblo
Columbia College
Concordia University Texas
Cornell University
Davidson College
Delaware Technical Community College
DeSales University
Dickinson College
Drake University
Drew University
Duquesne University
East Central University
Eastern Washington University
Embry Riddle Aeronautical University-Daytona Beach
Fairfield University
Florida Gulf Coast
Florida International University
Fort Hays State University
Georgia Institute Of Technology
Gettysburg College
Hamilton College
Hollins University
Humboldt State University
Iowa State University
Jackson State University
John Jay College of Criminal Justice
Kennesaw State University
Lafayette College
Lane College
Lee University
Le Moyne College
Lenoir Rhyne University
Life University
Loyola University Maryland
Lubbock Christian University
Lycoming College
Lynn University Common Data Set
Malone University
Marlboro College
Maryville University
Massachusetts Maritime Academy
Metropolitan State University of Denver
Michigan Technological University
Middlebury College
Millersville University
Mississippi State University
Mott Community College
Neumann University
Northeastern State University
Northern Arizona University
Northern Kentucky University
Nyack College
Oklahoma Christian University
Oklahoma State University
Old Dominion University
Oral Roberts University
Pepperdine University
Pomona College
Prescott College
Providence College
Reed College
Regis University
Rensselaer Polytechnic Institute
Rice University
Rochester College
Rutgers University
Saint Vincent College
San Francisco State University
Santa Clara University
Scripps College
Seton Hill University
Sewanee
Shippensburg University
Simpson University
Slippery Rock University
Smith College
Sonoma State University
Southeastern Community College
Southeastern Oklahoma State University
Southwestern Oklahoma State University
Springfield College
St. Ambrose University
Stanford University
Stephen F. Austin State University
SUNY Oneonta
SUNY Potsdam
Sweet Briar College
Taylor University
Temple University
Tennessee Wesleyan University
Texas A&M University - Kingsville
Texas A&M University
Texas Wesleyan University
The College at Brockport
The College of New Jersey
The University of Scranton
The University of Southern Mississippi
The University of Tennessee
Trinity University
Tufts University
Tulane University
Tulsa Community College
University at Buffalo
University Enrollment
University of California - Davis
University of California, Riverside
University of Colorado Boulder, 2015
University of Delaware
University of Kentucky
University of Louisville
University of Maine
University of Missouri
University of Montana
University of Mount Olive: 2016
University of Nebraska Kearney
University of Nebraska-Lincoln
University of Nevada, Reno
University of New Hampshire
University of New Mexico
University of North Alabama
University of North Carolina at Charlotte
University of Pennsylvania
University of Pikeville
University of Puget Sound
University of Science and Arts
University of Texas Rio
University of the Sciences in Philadelphia
University of Wisconsin
University Wide Common Data Set 2015
Villanova University
Virginia Commonwealth University
Washburn University
Washington and Lee University
Washington College
Weber State University
Wellesley College
Wesleyan University
Westfield State University
Westminster College
Wheaton College
Whitman College
Widener University
Worcester Polytechnic Institute
Xavier University of Louisiana
Xavier University
Yale University"
Walmart_Store_Database,Database of all US Walmart/Sam's Club Stores,Christopher Lambert,2,"Version 1,2018-01-21",,CSV,388 KB,CC0,125 views,15 downloads,,0 topics,https://www.kaggle.com/theriley106/database-of-all-walmart-stores-in-the-us,"Context
Database of all Walmart and Sam's Club stores in the United States
Python wrapper surrounding the dataset"
Handwriting Verification,Handwriting Verification,Anupam Wadhwa,2,"Version 2,2018-01-28|Version 1,2018-01-28",classification,CSV,1 MB,CC0,113 views,8 downloads,,,https://www.kaggle.com/anupamwadhwa/handwriting-verification,We are building a system to detect whether a group of words is written by one writer or more than one writes are involved. We need help in improving the efficiency of the classification. Help us in improving the classifier for given files.
Large Movie Review Dataset,"A set of 50,000 highly-polarized reviews from the Internet Movie Database.",Kris,2,"Version 3,2018-01-20|Version 2,2018-01-20|Version 1,2018-01-20",,Other,125 MB,Other,202 views,9 downloads,2 kernels,0 topics,https://www.kaggle.com/pankrzysiu/keras-imdb,"Context
This is a huge dataset and takes around 400 seconds to load into kernel. If you need quickly IMDB data in Keras kernel use the following dataset instead:
https://www.kaggle.com/pankrzysiu/keras-imdb-reviews
Content
A set of 50,000 highly-polarized reviews from the Internet Movie Database.
Usage Instructions
aclImdb_v1.zip file
This file is to be used directly in your code. The .zip file will be automatically uncompressed by Kaggle.
imdb* files
from os import listdir, makedirs
from os.path import join, exists, expanduser

cache_dir = expanduser(join('~', '.keras'))
if not exists(cache_dir):
    makedirs(cache_dir)
datasets_dir = join(cache_dir, 'datasets')
if not exists(datasets_dir):
    makedirs(datasets_dir)

# If you have multiple input files, change the below cp commands accordingly, typically:
# !cp ../input/keras-imdb/imdb* ~/.keras/datasets/
!cp ../input/imdb* ~/.keras/datasets/
Acknowledgements
The files are on the net in these locations:
https://s3.amazonaws.com/text-datasets/imdb.npz
https://s3.amazonaws.com/text-datasets/imdb_word_index.json
They are used by keras imdb.py:
https://github.com/keras-team/keras/blob/master/keras/datasets/imdb.py
Inspiration
""Python Deep Learning"" Book example is using this:
https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/6.1-using-word-embeddings.ipynb"
LinkedIn Profile Data,,Andrew Truman,2,"Version 1,2018-01-02",,CSV,5 MB,CC0,478 views,65 downloads,,0 topics,https://www.kaggle.com/killbot/linkedin,"Context
Anonymized data from profiles scraped on LinkedIn. Contains data from about 15000 profiles. Profiles came from people predominantly located in Australia. Includes all their work history as well as analysis of their photo and name.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
College life Missuri Institute,,WorldValueSurvey,2,"Version 1,2018-01-03",,Other,11 KB,ODbL,49 views,2 downloads,,0 topics,https://www.kaggle.com/worldvaluesurvey/college-life-missuri-institute,This dataset does not have a description yet.
Ted Talks Transcript,trancripts in multiple languages,Wei T,2,"Version 2,2018-01-28|Version 1,2018-01-27","text mining
text data",CSV,546 MB,CC0,200 views,20 downloads,,0 topics,https://www.kaggle.com/goweiting/ted-talks-transcript,"An extension to Kaggle's TED dataset
Using transcripts provided by TED.com, a dataset that combines combines YouTube metadata (at the time of scrapping) and metadata from Kaggle's TED dataset.
This extension provides not just additional metadata from YouTube, but also additional transcripts of the same video, but in different languages (e.g. Portugese, French, Arabic, Chinese, Japnese, Korean, Turkish, Dutch...). In total, 111 different languages are available (most videos do not have transcript for all languages).
Content
For each of the 111 languages in tedDirector.zip, each language file is a CSV file with the following headers:
videoID - YouTube IDs
lang - Language code
title - Title of the TED Talk
transcript - Transcript of the TED Talk in lang
Acknowledgements
This dataset was developed as part of a larger dataset used for an information retrieval assignment. In that assignment, my team and I used TED talks to evaluate different configuration of search engine algorithms. We also used different languages for the search and retrieve task, to test for reliability of our search engine. More information can be found in our Github repository. Dataset is downloaded from TedTalksDirector using YouTube-dl.
Code for downloading can be found from the IR project.
More about language codes here from w3schools.com.
Inspiration
Some of the problem experienced while preparing this dataset: 1. How can we improve the matching of YouTube dataset to the data scrapped from TED talk"
social_network_in_schools,,hisadlkhd,2,"Version 2,2018-01-29|Version 1,2018-01-22",,CSV,197 KB,CC0,75 views,4 downloads,,0 topics,https://www.kaggle.com/hikarilu/socail-network-in-schools,"Context：
This data set contains the communication of students with the smartphones in the schools in a week. We set a evaluation to describe the quality of the user the through the first 3 pieces of information.
Content：
Useful information : it means the pieces of useful information between communication. Connection frequency: it means the times the user connects to other users. Effective connection: it means the successful times of communication with others. Evaluation: it is set to judge the popularity of the users.
Acknowledgements：
It's collected by an online survey in several schools.
Inspiration：
Can we use the data to find popular student in the school? It's useful for teachers to choose students that good at making friends."
Dog Parks of NYC,Where can a dog be a dog in NYC? The city's official off-leash areas,City of New York,2,"Version 3,2018-01-24|Version 2,2018-01-24|Version 1,2018-01-23","cities
animals",CSV,14 KB,CC0,111 views,10 downloads,,0 topics,https://www.kaggle.com/new-york-city/dog-parks-of-nyc,"Context
This is the City of New York's list of official dog parks.
Some ideas for what could be interesting: mapping the most dog-friendly neighborhoods in the city, perhaps to help dog owners find an apartment in an area with access to bigger or multiple dog runs. If one combined this dataset with others like NYC's dog licensing dataset, one could explore the ratio of the area of dog run space to the number of dogs, perhaps by neighborhood. This could show dogs per square foot of dog park space, painting a picture of which neighborhoods are satisfying the needs of canine citizens and which ones have some work to do.
Content
The data were last updated by the City of New York April 11, 2017.
These are the fields in the dataset:
Prop_ID: A unique identifier for the property. The first character is a abbreviation of the borough, followed by a 3 digit number. Anything after the first 4 characters represents a subproperty. Boroughs:
X - Bronx, B - Brooklyn, M - Manhattan, Q - Queens, R - Staten Island
To find more data on each Prop_ID (exact address, etc.), these parks datasets may be used: http://www.nycgovparks.org/bigapps/DPR_Parks_001.xml
http://www.nycgovparks.org/bigapps/DPR_Parks_001.json
Name: Name of the property
Address: Approximate location of the dog run or off-leash area (note, this is very approximate - to be more precise, cross-reference to the bigger parks dataset above)
DogRuns_Type: Dog Run or Off-Leash Area
Accessible: (Y)es or (N)o - wheelchair accessible
Notes: Additional notes
Acknowledgements
The dataset comes from the City of New York's Open Data project."
Course Material: Walmart Challenge,A processed version of an old recruitment challenge,Bletchley Bootcamp,2,"Version 1,2018-02-02",,CSV,14 MB,Other,361 views,56 downloads,16 kernels,0 topics,https://www.kaggle.com/bletchley/course-material-walmart-challenge,"Context
This dataset originally stems from a Walmart recruiting challenge. It is used here for educational purposes only.
Content
The dataset contains anonymized sales by department for 45 Walmart stores as well as supporting features.
Acknowledgements
The dataset belongs to Walmart and is used here only for educational purposes
Inspiration
Try to predict weekly sales."
Python Utility Code for Deep Learning Exercises,Used in the course kaggle.com/deep-learning,DanB,2,"Version 8,2018-01-11|Version 7,2018-01-11|Version 6,2018-01-05|Version 5,2018-01-05|Version 4,2017-12-20|Version 3,2017-12-20|Version 2,2017-12-20|Version 1,2017-12-20",,Other,3 KB,CC0,465 views,205 downloads,42 kernels,0 topics,https://www.kaggle.com/dansbecker/python-utility-code-for-deep-learning-exercises,This dataset does not have a description yet.
Stock Market Index ETF Daily Datasets,"Includes Index ETF SPY, Inverse Index ETFs, and the Volatility Index",Joe Young,2,"Version 5,2018-02-25|Version 4,2018-02-19|Version 3,2018-02-08|Version 2,2018-01-11|Version 1,2017-12-18","time series
finance",CSV,1 MB,CC0,197 views,14 downloads,,0 topics,https://www.kaggle.com/jsphyg/spy-etf-stock-data,"Context
Some of the most challenging problems in machine learning for me have been in predicting stock market results. So I can use Kaggle's kernels on this data, I'm adding the data to Kaggle.
Content
Acknowledgements
This data was gathered using the tidyquant package."
Economy Rankings,Explaining the ease of doing business rankings,Pradeep.narayanan,2,"Version 1,2017-12-21","finance
demographics",Other,16 KB,ODbL,166 views,38 downloads,,0 topics,https://www.kaggle.com/pradeepp/economy-rankings,"Context
About ease of doing business around the world.
Content
Column includes : Ease of Doing Business | Starting a Business | Dealing with Construction Permits | Getting Electricity | Registering Property | Getting Credit | Protecting Minority Investors | Paying Taxes | Trading across Borders | Enforcing Contracts | Resolving Insolvency
Acknowledgements
World Bank | http://www.doingbusiness.org/rankings
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
Cross-position activity recognition,,Jindong Wang,2,"Version 1,2017-12-21",time series,Other,80 MB,CC0,161 views,2 downloads,2 kernels,0 topics,https://www.kaggle.com/jindongwang92/crossposition-activity-recognition,"This directory contains the cross-position activity recognition datasets used in the following paper. Please consider citing this article if you want to use the datasets.
Jindong Wang, Yiqiang Chen, Lisha Hu, Xiaohui Peng, and Philip S. Yu. Stratified Transfer Learning for Cross-domain Activity Recognition. 2018 IEEE International Conference on Pervasive Computing and Communications (PerCom).
These datasets are secondly constructed based on three public datasets: OPPORTUNITY (opp) [1], PAMAP2 (pamap2) [2], and UCI DSADS (dsads) [3].
Here are some useful information about this directory. Please feel free to contact jindongwang@outlook.com for more information.
This is NOT the raw data, since I have performed feature extraction and normalized the features into [-1,1]. The code for feature extraction can be found in here: https://github.com/jindongwang/activityrecognition/tree/master/code. Currently, there are 27 features for a single sensor. There are 81 features for a body part. More information can be found in above PerCom-18 paper.
There are 4 .mat files corresponding to each dataset: dsads.mat for UCI DSADS, opp_hl.mat and opp_ll.mat for OPPORTUNITY, and pamap.mat for PAMAP2. Note that opp_hl and opp_loco denotes 'high-level' and 'locomotion' activities, respectively. (1) dsads.mat: 9120 * 408. Columns 1~405 are features, listed in the order of 'Torso', 'Right Arm', 'Left Arm', 'Right Leg', and 'Left Leg'. Each position contains 81 columns of features. Columns 406~408 are labels. Column 406 is the activity sequence indicating the executing of activities (usually not used in experiments). Column 407 is the activity label (1~19). Column 408 denotes the person (1~8). (2) opp_hl.mat and opp_loco.mat: Same as dsads.mat. But they contain more body parts: 'Back', 'Right Upper Arm', 'Right Lower Arm', 'Left Upper Arm', 'Left Lower Arm', 'Right Shoe (Foot)', and 'Left Shoe (Foot)'. Of course we did not use the data of both shoes in our paper. Column 460 is the activity label (please refer to OPPORTUNITY dataset to see the meaning of those activities). Column 461 is the activity drill (also check the dataset information). Column 462 denotes the person (1~4). (3) pamap.mat: 7312 * 245. Columns 1~243 are features, listed in the order of 'Wrist', 'Chest', and 'Ankle'. Column 244 is the activity label. Column 245 denotes the person (1~9).
There are another 3 datasets with the prefix 'cross_', containing only 4 common classes of each dataset. This is for experimenting the cross-dataset activity recognition (see our PerCom-18 paper). The 4 common classes are lying, standing, walking, and sitting. (1) cross_dsads.mat: 1920*406. Columns 1~405 are features. Column 406 is labels. (2) cross_opp.mat: 5022*460. Columns 1~459 are features. Column 460 is labels. (3) cross_pamap.mat: 3063 * 244. Columns 1~243 are features. Column 244 is labels.
-------- Original references for the 3 datasets:
[1] R. Chavarriaga, H. Sagha, A. Calatroni, S. T. Digumarti, G. Troster, ¨ J. d. R. Millan, and D. Roggen, “The opportunity challenge: A bench- ´ mark database for on-body sensor-based activity recognition,” Pattern Recognition Letters, vol. 34, no. 15, pp. 2033–2042, 2013.
[2] A. Reiss and D. Stricker, “Introducing a new benchmarked dataset for activity monitoring,” in Wearable Computers (ISWC), 2012 16th International Symposium on. IEEE, 2012, pp. 108–109.
[3] B. Barshan and M. C. Yuksek, “Recognizing daily and sports activities ¨ in two open source machine learning environments using body-worn sensor units,” The Computer Journal, vol. 57, no. 11, pp. 1649–1667, 2014."
Bitcoin_PriceMovement,,Prem Patrick,2,"Version 1,2017-12-17",,CSV,39 KB,CC0,207 views,24 downloads,,0 topics,https://www.kaggle.com/prempatrick007/bitcoin-pricemovement,This dataset does not have a description yet.
ConversationAIDataset,,eoveson,2,"Version 1,2017-12-30",,Other,80 MB,CC0,149 views,386 downloads,,0 topics,https://www.kaggle.com/eoveson/conversationaidataset,This dataset does not have a description yet.
Fraud Detection Societe Generale,,Arihant Jain,2,"Version 1,2017-12-15",,CSV,32 MB,CC0,193 views,9 downloads,,0 topics,https://www.kaggle.com/arihant456/fraud-detection-societe-generale,This dataset does not have a description yet.
Market Segmentation,Segment the market to understand market trends.,Seetharam Indurti,2,"Version 1,2017-12-27",,CSV,255 KB,Other,424 views,35 downloads,,0 topics,https://www.kaggle.com/seetzz/market-segmentation,"Context
Market Segmentation: A company must have different strategies for product promotion for different individuals. Not every individual has the same requirement and demand. So the company has to segment, target and position the product according to the tastes of various individuals.
Segmentation of products based on the revenue generated in different regions is done to understand the market trends.
The segmentation will help the company to devise marketing strategies and promotional schemes to position the right product according to the preference of the consumers in any given segment.
Content
The store dataset contains 10,000 observations or we can call it transactions of 5 variables. Each row represents the transaction made by the reps. Each column contains the attributes of this dataset include: reps - Representative who are involved in the promotion and sale of the products in their respective region. products - There are 12 brands of products promoted by the company. qty - Quantity sold in units revenue - Revenue generated for each transaction region - There are 4 regions - East, North, South and West India.
It would also be possible to predict the revenue and make it a regression task.
Inspiration
I am hosting this dataset in order to give it wider exposure, to give the community an opportunity to experiment with different algorithms / models."
UCI Communities and Crime Unnormalized Data Set,,KK16,2,"Version 2,2018-02-21|Version 1,2017-12-28",,{}JSON,681 KB,CC4,126 views,16 downloads,,0 topics,https://www.kaggle.com/kkanda/communities%20and%20crime%20unnormalized%20data%20set,"Context
Introduction: The dataset used for this experiment is real and authentic. The dataset is acquired from UCI machine learning repository website [13]. The title of the dataset is ‘Crime and Communities’. It is prepared using real data from socio-economic data from 1990 US Census, law enforcement data from the 1990 US LEMAS survey, and crimedata from the 1995 FBI UCR [13]. This dataset contains a total number of 147 attributes and 2216 instances.
The per capita crimes variables were calculated using population values included in the 1995 FBI data (which differ from the 1990 Census values).
Content
The variables included in the dataset involve the community, such as the percent of the population considered urban, and the median family income, and involving law enforcement, such as per capita number of police officers, and percent of officers assigned to drug units. The crime attributes (N=18) that could be predicted are the 8 crimes considered 'Index Crimes' by the FBI)(Murders, Rape, Robbery, .... ), per capita (actually per 100,000 population) versions of each, and Per Capita Violent Crimes and Per Capita Nonviolent Crimes)
predictive variables : 125 non-predictive variables : 4 potential goal/response variables : 18
Acknowledgements
http://archive.ics.uci.edu/ml/datasets/Communities%20and%20Crime%20Unnormalized
U. S. Department of Commerce, Bureau of the Census, Census Of Population And Housing 1990 United States: Summary Tape File 1a & 3a (Computer Files),
U.S. Department Of Commerce, Bureau Of The Census Producer, Washington, DC and Inter-university Consortium for Political and Social Research Ann Arbor, Michigan. (1992)
U.S. Department of Justice, Bureau of Justice Statistics, Law Enforcement Management And Administrative Statistics (Computer File) U.S. Department Of Commerce, Bureau Of The Census Producer, Washington, DC and Inter-university Consortium for Political and Social Research Ann Arbor, Michigan. (1992)
U.S. Department of Justice, Federal Bureau of Investigation, Crime in the United States (Computer File) (1995)
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?
Data available in the dataset may not act as a complete source of information for identifying factors that contribute to more violent and non-violent crimes as many relevant factors may still be missing.
However, I would like to try and answer the following questions answered.
Analyze if number of vacant and occupied houses and the period of time the houses were vacant had contributed to any significant change in violent and non-violent crime rates in communities
How has unemployment changed crime rate(violent and non-violent) in the communities?
Were people from a particular age group more vulnerable to crime?
Does ethnicity play a role in crime rate?
Has education played a role in bringing down the crime rate?"
Weather DataSet for RV Challenge,Weather information about 9 prefectures in Japan for RV Challenge,Kâzım Anıl Eren,2,"Version 3,2017-12-19|Version 2,2017-12-19|Version 1,2017-12-19",,CSV,178 KB,Other,96 views,11 downloads,,0 topics,https://www.kaggle.com/kazimanil/rv-weather,"Disclaimer
I have prepared the data for RV_Forecasting EDA and I do have no claim on data's usage rights. All data belongs to JMA. Please use it at your own risk. I hope you could find this dataset useful as well and share your experiences with me.
Description
Since JMA does not do all the possible calculations in many weather stations, I have focused on prefecture-centres which nearly all calculations are made in the expense of accuracy in the geographic tagging. It is assumed that all the districts within a prefecture should have similar weather conditions in a given day.
Content
date calendar date in classic Y-M-D format.
air_district1 could be harvested from air_area_name column via str_split_fixed(air_area_name, "" "", n = 3)[,1] code utilizing stringr package in R
avg_temp Average Temperature Data
rain_amount Amount of Precipitation in centimetres
sunshine_hours Daylight in hours
wind_speed Wind_Speed in kilometres-per-hour
snowfall Total snowfall during day in centimetres"
Indian Census Data with Geospatial indexing,Calculating population weighted mean of geospatial data of each Indian state,Sumit Kumar,2,"Version 1,2017-12-21","india
geography",CSV,91 KB,CC0,130 views,13 downloads,,0 topics,https://www.kaggle.com/sirpunch/indian-census-data-with-geospatial-indexing,"Dataset Description:
This dataset has population data of each Indian district from 2001 and 2011 censuses.
The special thing about this data is that it has centroids for each district and state.
Centroids for a district are calculated by mapping border of each district as a polygon of latitude/longitude points in a 2D plane and then calculating their mean center.
Centroids for a state are calculated by calculating the weighted mean center of all districts that constitutes a state. The population count is the weight assigned to each district.
Example Analysis:
The complete code for calculating the centroids and web scraping for the data is shared on GitHub.
The purpose of this project was to map population density center for each state.
You can also read about the complete project here: https://medium.com/@sumit.arora/plotting-weighted-mean-population-centroids-on-a-country-map-22da408c1397
Output Screenshots: Indian districts mapped as polygons
Mapping centroids for each district
Mean centers of population by state, 2001 vs. 2011
National center of population"
Stanford MSA + US Mass Shootings,Merging two datasets,Jesse Montgomery,2,"Version 3,2017-12-17|Version 2,2017-12-16|Version 1,2017-12-16",,CSV,3 MB,CC0,69 views,3 downloads,,0 topics,https://www.kaggle.com/jlmontie/stanford-msa-2017,"Context
I appreciate the thoroughness of Stanford Mass Shootings in America (MSA) and the continuous efforts to update US Mass Shootings Last 50 Years (1966-2017). This dataset merges the two, adding 14 records to Stanford MSA.
Content
See original datasets for more information.
Acknowledgements
I acknowledge the contributors of the two original datasets, Zeeshan-ul-hassan Usmani (US Mass Shootings) and Carlos Paradis (Stanford MSA)."
ENEM - ENADE,ENADE and ENEM scores combined,Augusto Pertence,2,"Version 2,2017-12-27|Version 1,2017-12-19","brazil
education",CSV,8 MB,CC0,107 views,8 downloads,,0 topics,https://www.kaggle.com/pertence/enem-enade,"Context
National Exam of (Higher Education) Student Performance [ENADE]
National Exam of Upper Secondary Education [ENEM]
National Institute for Educational Studies and Research [INEP]
Content
Each row is a student that finished higher education education and has his ENADE and ENEM scores and more data.
For more metadata: see the xlsx files (portuguese).
References
http://inep.gov.br/microdados
http://portal.inep.gov.br/web/guest/about-inep"
MNIST Data for Digit Recognition,Digit Recognition by MNIST contains training and testing data,Sylvia Mittal,2,"Version 1,2017-12-22","weight training
digital media
machine learning
+ 2 more...",Other,11 MB,CC0,138 views,11 downloads,,0 topics,https://www.kaggle.com/sylvia23/mnist-data-for-digit-recognation,"This dataset contains training and testing data for digit recognition which includes hand written images of digits.
It contains four zip files which you can easily include in your neural network. So, download all four of them by clicking ""Download all"" button.
This is the MNIST dataset used world-wide to check the performance of neural networks based upon digit recognition.
It also contains training and testing labels."
Cats and Dogs,,Siddartha,2,"Version 1,2017-12-27",,Other,830 MB,CC0,350 views,101 downloads,,0 topics,https://www.kaggle.com/siddarthareddyt/cats-and-dogs,This dataset does not have a description yet.
Mall_Customers,,shwetabh123,2,"Version 1,2017-12-23",,CSV,4 KB,CC0,153 views,23 downloads,,0 topics,https://www.kaggle.com/shwetabh123/mall-customers,This dataset does not have a description yet.
Hepatitis B Virus Levels of Patients (Re-upload),,Lucas Erring,2,"Version 1,2017-12-23",healthcare,Other,7 KB,Other,181 views,25 downloads,,,https://www.kaggle.com/drlucaserring/hepatitis-b-virus-levels-of-patients,"Context
Hepatitis B levels of some random patients that I've collected over the course of 8 years, from random hospitals I've worked at. Every patient also had one of the 3 broad categories of diseases. At the time, technical limitations kept me from accurately collecting the exact disease/disorder the patient had, so this has to do.
(Had to re-upload this dataset because Kaggle decided to delete it)"
Tashkeela: Arabic diacritization corpus,a corpus for Arabic diacritizer development,Taha Zerrouki,2,"Version 1,2017-12-23",,Other,122 MB,GPL,351 views,19 downloads,,0 topics,https://www.kaggle.com/linuxscout/tashkeela,"Context
Arabic diacritics are often missed in Arabic scripts. This feature is a handicap for new learner to read َArabic, text to speech conversion systems, reading and semantic analysis of Arabic texts.
The automatic diacritization systems are the best solution to handle this issue. But such automation needs resources as diactritized texts to train and evaluate such systems.
Content
Data is a collection of Arabic vocalized texts, which covers modern and classical Arabic language. The Data contains over 75 million of fully vocalized words obtained from 97 books, structured in text files.
The corpus is collected mostly from Islamic classical books [14], and using semi-automatic web crawling process. The Modern Standard Arabic texts crawled from the Internet represent 1.15% of the corpus, about 867,913 words, while the most part is collected from Shamela Library, which represent 98.85%, with 74,762,008 words contained in 97 books
Acknowledgements
We acknowledge the efforts made by Shamela library volunteers to write, diacritize and make free texts.
Inspiration
Can Machine vocalize an arabic text with précision? What semantic data can be extracted from vocalized texts"
SqueezeNet 1.1,SqueezeNet 1.1 Pre-trained Model for PyTorch,PyTorch,2,"Version 1,2017-12-15","machine learning
pre-trained model",Other,4 MB,CC0,330 views,5 downloads,,0 topics,https://www.kaggle.com/pytorch/squeezenet11,"SqueezeNet 1.0
SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size
Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet).
Authors: Forrest N. Iandola, Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, Kurt Keutzer
https://arxiv.org/abs/1602.07360
SqueezeNet Architectures
What is a Pre-trained Model?
A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.
Why use a Pre-trained Model?
Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it."
VGG-11 with batch normalization,VGG-11 Pre-trained model with batch normalization for PyTorch,PyTorch,2,"Version 1,2017-12-16","machine learning
pre-trained model",Other,471 MB,CC0,165 views,,,0 topics,https://www.kaggle.com/pytorch/vgg11bn,"VGG-11
Very Deep Convolutional Networks for Large-Scale Image Recognition
In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.
Authors: Karen Simonyan, Andrew Zisserman
https://arxiv.org/abs/1409.1556
VGG Architectures
What is a Pre-trained Model?
A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.
Why use a Pre-trained Model?
Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it."
Meneame.net front page news,News from the front page of meneame.net,Mrverde,2,"Version 1,2017-12-10","news agencies
reddit",CSV,42 MB,CC0,224 views,9 downloads,,,https://www.kaggle.com/mrverde/meneamenet-front-page-news,"Context
This database includes the information about the news which appears in the front page of meneame.net, a Spanish website similar to reddit. Data from 2005/12 to 2017/12/04. There are about 177.000 observations with 17 variables: news, users, post time, positive votes, negative votes, description and so on. The most part of the news are in Spanish.
Esta base de datos incluye la información de las noticias que aparecen en la portada de la web meneame.net, un sitio web similar a reddit. Los datos van desde Diciembre de 2005 hasta el 4 de diciembre de 2017. Hay 177.000 observaciones con 17 variables: noticias, usuarios, hora de envío, votos negativos, votos positivos, descripción, etc. La mayor parte de las noticias están en español.
Content
Index - News index
  noticia - Content of the news
  link_noticia - Link of the news
  web - Original website
  usuario - User
  fecha_envio - Date when the post was sent
  fecha_publicacion - Date when the post was acces to the front page
  meneos - Positive + Anonymous votes
  clicks - Clicks on the news
  comentarios - Comments
  votos_positivos - Positive votes
  votos_anonimos - Anonymous votes
  votos_negativos - Negative votes
  karma - Web points earned
  sub - Subsection
  extracto - Summary of the news
Acknowledgements
Best regards to meneame.net admins and users."
BachelorsDegreeWomenUSA,,SureshSrinivas,2,"Version 1,2017-12-05",,CSV,6 KB,CC0,72 views,5 downloads,,0 topics,https://www.kaggle.com/sureshsrinivas/bachelorsdegreewomenusa,This dataset does not have a description yet.
Densnet121+fine tuning,,Aydin Ayanzadeh,2,"Version 1,2017-12-11",,Other,29 MB,CC0,51 views,2 downloads,,0 topics,https://www.kaggle.com/ayanzadeh93/densnet121fine-tuning,This dataset does not have a description yet.
Madison Lakes Ice Cover,Lake Monona and Lake Mendota lake ice close and open dates,Greg,2,"Version 1,2017-12-10","lakes
climate",CSV,6 KB,CC0,69 views,3 downloads,,0 topics,https://www.kaggle.com/gregnetols/madison-lakes-ice-cover,"Content
Both datasets MononaIce and MendotaIce are structured the same.
WINTER - The ending year for the winter. For example the the winter of 2016-2017 would be represented by 2017. CLOSED - The date the lake froze over. The lake is considered closed if greater than 50% of the lake's surface is frozen. OPENED - The date the lake ice melted. The lake is considered opened if less than 50% of the lake's surface is frozen. DAYS - Total days in the season that the lake was closed.
Acknowledgements
Thank you to the Wisconsin State Climatology Office"
CUSTOMER CHURN,,rayen,2,"Version 1,2017-12-05",,CSV,955 KB,CC0,276 views,43 downloads,,,https://www.kaggle.com/rayenkhayat/customer-churn,This dataset does not have a description yet.
Indoor Car Track,for autonomous vehicles,Aman Agarwal,2,"Version 1,2017-12-08",,CSV,5 MB,Other,149 views,12 downloads,,0 topics,https://www.kaggle.com/firstofhisname/indoor-car-track,"Context
This data was collected to train an autonomous car prototype as our college project.
Content
The data consists of 14x32 grayscale pixel values along with proper labels as in which direction to maneuver the car. The labels are represented as follows: L- Left, R- Right, F- Front, S- Stop
Inspiration
Udacity Autonomous driving nano degree program, Waymo, Tesla self-driving car"
Restaurant-reviews,,Harshit Joshi,2,"Version 1,2017-12-10",,Other,60 KB,Other,135 views,26 downloads,2 kernels,0 topics,https://www.kaggle.com/hj5992/restaurantreviews,This dataset does not have a description yet.
Bank Churn Modelling,,Harshit Joshi,2,"Version 1,2017-12-10",,CSV,669 KB,Other,199 views,27 downloads,,0 topics,https://www.kaggle.com/hj5992/bank-churn-modelling,This dataset does not have a description yet.
ELO for EPL 15 matchday,,Vardan,2,"Version 1,2017-12-09",,CSV,793 B,Other,33 views,0 downloads,,0 topics,https://www.kaggle.com/vardan95ghazaryan/elo-for-epl-15-matchday,This dataset does not have a description yet.
Poetry,Poems and Lyrics as TXT files,paultimothymooney,2,"Version 14,2018-02-09|Version 13,2018-02-04|Version 12,2018-02-04|Version 11,2018-02-04|Version 10,2018-02-04|Version 9,2018-02-04|Version 8,2017-12-30|Version 7,2017-12-30|Version 6,2017-12-30|Version 5,2017-12-13|Version 4,2017-12-13|Version 3,2017-12-13|Version 2,2017-12-11|Version 1,2017-12-11","poetry
music
text data",Other,6 MB,CC0,259 views,25 downloads,,0 topics,https://www.kaggle.com/paultimothymooney/poetry,"Context
TXT files for Poetry Generation with Python
Content
TXT files of lyrics and poems
Acknowledgements
Free lyric hosting websites
Inspiration
TXT files for Poetry Generation with Python"
PC_Games,,Hakob Sukiasyan,2,"Version 1,2017-12-05",,CSV,476 KB,CC0,235 views,27 downloads,,,https://www.kaggle.com/sukiasyan/pc-games,This dataset does not have a description yet.
Lower Back Pain Symptoms Dataset(labelled),Collection of physical spine data,Ali Hussain,2,"Version 1,2017-12-05","orthopedic surgery
medicine",CSV,41 KB,Other,194 views,19 downloads,,0 topics,https://www.kaggle.com/alihussain1993/lower-back-pain-symptoms-datasetlabelled,"310 Observations, 13 Attributes (12 Numeric Predictors, 1 Binary Class Attribute - No Demographics)
Lower back pain can be caused by a variety of problems with any parts of the complex, interconnected network of spinal muscles, nerves, bones, discs or tendons in the lumbar spine. Typical sources of low back pain include:
The large nerve roots in the low back that go to the legs may be irritated The smaller nerves that supply the low back may be irritated The large paired lower back muscles (erector spinae) may be strained The bones, ligaments or joints may be damaged An intervertebral disc may be degenerating An irritation or problem with any of these structures can cause lower back pain and/or pain that radiates or is referred to other parts of the body. Many lower back problems also cause back muscle spasms, which don't sound like much but can cause severe pain and disability.
While lower back pain is extremely common, the symptoms and severity of lower back pain vary greatly. A simple lower back muscle strain might be excruciating enough to necessitate an emergency room visit, while a degenerating disc might cause only mild, intermittent discomfort.
This data set is about to identify a person is abnormal or normal using collected physical spine details/data."
Dogs vs Cats,,Onofrio_BIScience,2,"Version 1,2017-12-05",,Other,815 MB,ODbL,561 views,148 downloads,,0 topics,https://www.kaggle.com/biaiscience/dogs-vs-cats,This dataset does not have a description yet.
Protein Contact Prediction,For developing novel (and more accurate) methods for protein contact prediction,Badri Adhikari,2,"Version 2,2017-12-06|Version 1,2017-12-06",healthcare,Other,721 MB,GPL,167 views,12 downloads,,0 topics,https://www.kaggle.com/badriadhikari/protein-contact-prediction,This dataset does not have a description yet.
Safe Driver Prediction,,Nelson,2,"Version 2,2017-12-07|Version 1,2017-12-07",,Other,2 MB,CC0,282 views,76 downloads,,0 topics,https://www.kaggle.com/mu202199/safe-driver-prediction,"Context
Nothing ruins the thrill of buying a brand new car more quickly than seeing your new insurance bill. The sting’s even more painful when you know you’re a good driver. It doesn’t seem fair that you have to pay so much if you’ve been cautious on the road for years.
Try to build a model that predicts the probability that a driver will initiate an auto insurance claim in the next year! Good Luck
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"
Fake News detection,,jruvika,2,"Version 1,2017-12-08",,CSV,5 MB,ODbL,279 views,44 downloads,,0 topics,https://www.kaggle.com/jruvika/fake-news-detection,This dataset does not have a description yet.
data sensors,,Jihane HAMMOUT,2,"Version 1,2017-12-12",,{}JSON,3 MB,CC0,220 views,24 downloads,,0 topics,https://www.kaggle.com/jhammout/data-sensors,This dataset does not have a description yet.
Bitcoin Twitter Feed,,John Doe,2,"Version 1,2017-12-13",,CSV,1 MB,Other,149 views,39 downloads,,0 topics,https://www.kaggle.com/gwhittington/bitcointwitter,This dataset does not have a description yet.
bitcoin twitter,,John Doe,2,"Version 1,2017-12-13",,CSV,1 MB,Other,337 views,60 downloads,,0 topics,https://www.kaggle.com/gwhittington/twitter2,This dataset does not have a description yet.
VGG-11,VGG-11 Pre-trained Model for PyTorch,PyTorch,2,"Version 2,2017-12-15|Version 1,2017-12-13","machine learning
pre-trained model",Other,471 MB,CC0,412 views,3 downloads,,0 topics,https://www.kaggle.com/pytorch/vgg11,"VGG-11
Very Deep Convolutional Networks for Large-Scale Image Recognition
In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.
Authors: Karen Simonyan, Andrew Zisserman
https://arxiv.org/abs/1409.1556
VGG Architectures
What is a Pre-trained Model?
A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.
Why use a Pre-trained Model?
Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it."
Pakistan Education Performance Dataset,Pakistan Education Performance Dataset - Primary Schooling,Mesum Raza Hemani,2,"Version 1,2017-12-01",,CSV,248 KB,Other,115 views,12 downloads,,0 topics,https://www.kaggle.com/mesumraza/pakistan-education-performance-dataset,"Context
This Dataset is consolidated from the surveys of Alif Ailaan & Aser Pakistan on Primary Schooling Performance.
Content
Dataset contains different dimensions and measures across the board that were used to measure the performance. I have compiled individual file of each year into one for your facilitation.
Acknowledgements
This Dataset is compiled by me and i am very thankful to ASER Pakistan & Alif Ailaan for providing PDFs from which i extracted.
Inspiration
Explore this Visualization to interpret performance province wise, city wise and identify different needs and room for actionable improvements.
For starting see this: https://public.tableau.com/views/PakistanEducationalPerformance-Dashboard/PakistanEducationPerformanceDashboard?%3Aembed=y&%3AshowVizHome=no&%3Adisplay_count=y&%3Adisplay_static_image=y&%3AbootstrapWhenNotified=true"
Weather in Australia,Predict whether or not it will rain tomorrow in Australia,Joe Young,2,"Version 1,2017-12-04","weather
binary classification",CSV,4 MB,CC0,201 views,32 downloads,,0 topics,https://www.kaggle.com/jsphyg/weather-dataset-rattle-package,"Context
This is an easy-to-use binary classification dataset for predicting whether or not it will rain tomorrow. Use for binary classification modeling.
Content
The content is daily weather observations from multiple Australian weather stations. The target variable is RainTomorrow, which means: Did it rain the next day? Yes or No. So the dataset is already set up for prediction -- no need to change the Target with a lead function.
Acknowledgements
This dataset comes from the Rattle package. The Rattle package is currently not available to Kaggle's kernels."
Electronic Music Features Dataset,Electronic music audio features for automatic genre classification,Caparrini,2,"Version 1,2017-12-04",music,CSV,1 MB,GPL,301 views,20 downloads,,,https://www.kaggle.com/caparrini/beatsdataset,"Context
What makes us, humans, able to tell apart two songs of different genres? Maybe you have ever been in the diffcult situation to explain show it sounds the music style that you like to someone. Then, could an automatic genre classifcation be possible?
Content
Each row is an electronic music song. The dataset contains 100 song for each genre among 23 electronic music genres, they were the top (100) songs of their genres on November 2016. The 71 columns are audio features extracted of a two random minutes sample of the file audio. These features have been extracted using pyAudioAnalysis (https://github.com/tyiannak/pyAudioAnalysis)."
Basket Ball Computer Vision,Object recognition with basketball image,Chris Evi-Parker,2,"Version 1,2017-12-02",basketball,Other,8 MB,CC0,209 views,7 downloads,,0 topics,https://www.kaggle.com/chrisparker126/basket-ball-computer-vision,"Context
I'm interested in object recognition, and trying to learn how to build CNN models able to recognize specific objects.
Content
So I created this data set by taking pictures around my flat with a basketball in each image. The data is made up of several pairs of image and annotation files (*.annote). The annotation file subdivides the image into 200x200 pixel windows and says whether basketball is present in it or not.
Acknowledgements
The initial analysis has used the mnist deep CNN model given in the tensor flow example adapted for multiple channels and image resolution
Inspiration
I'd like to some feedback both on my model and alternative way of building OR CNN models. Have fun!"
Rossmann Store Extra,,Dromosys,2,"Version 1,2017-12-02",,CSV,468 KB,Other,123 views,19 downloads,,0 topics,https://www.kaggle.com/dromosys/rossmann-store-extra,Extra data for Rossman store from http://files.fast.ai/part2/lesson14/rossmann.tgz
Fortnite: Battle Royale Chest Location Coordinates,Latitude and longitude coordinates from http://www.fortnitechests.info/,jruots,2,"Version 1,2017-12-02",video games,CSV,4 KB,Other,369 views,16 downloads,,0 topics,https://www.kaggle.com/jruots/fortnite-battle-royale-chest-location-coordinates,"Context
Fortnite: Battle Royale has over 20 million unique players, but there are no datasets on Kaggle yet! This one is small, but better than nothing! It contains the location coordinates of chests in Fortnite: Battle Royale as of the 1st of December 2017.
Content
There are three columns in the dataset. The first one has an identifier which is a running number starting from 1. The second column has latitudinal coordinates for chests. The third column has longitudinal coordinates for chests.
Acknowledgements
The data originates from: http://www.fortnitechests.info/ which is updated by SoumyDev (soumyydev@gmail.com)
Neither SoumyDev or the uploader of this dataset are affiliated with Epic Games or any of his partners. All copyrights reserved to their respective owners.
Inspiration
Based on chest coordinates, what are the best clusters to land on at the beginning of the match?"
Titanic: Passenger Nationalities,Passenger Nationalities on the Titanic,Warren Elder,2,"Version 1,2017-12-05",,CSV,29 KB,Other,75 views,10 downloads,,0 topics,https://www.kaggle.com/warrenelder/titanic-passenger-nationalities,"Context
This dataset provides the nationalities of passengers on the Titanic. It is meant to be merged with the Titanic Dataset to allow analysis of how a passengers nationality effected survival chances aboard the Titanic.
The passengers nationality was predicted using the open API from NamePrism http://www.name-prism.com/api using the passenger Name variable from the Titanic dataset.
Details from site; NamePrism is a non-commercial nationality/ethnicity classification tool that aims to support academic research, e.g. sociology and demographic studies. In this project, we learn name embeddings for name parts (first/last names) and classify names to 39 leaf nationalities and 6 U.S. ethnicities.
Research covered at; Nationality Classification using Name Embeddings. Junting Ye, Shuchu Han, Yifan Hu, Baris Coskun, Meizhu Liu, Hong Qin and Steven Skiena. CIKM, Singapore, Nov. 2017.
Content
The data consists of two variables; PassengerId and Nationality in two files that constitute training and test data for the Titanic Dataset.
Acknowledgements
-NamePrism
Inspiration
Did a passenger nationality on-board the Titanic affect their survival chances?"
Stemmed and Lementized English words,Contains 2 json files here precessed text is key,kamesh s,2,"Version 1,2017-12-07",,{}JSON,856 KB,ODbL,70 views,11 downloads,,0 topics,https://www.kaggle.com/kameshsoft/stemmed-and-lementized-english-words,"lem.json
This file contains lementized english words as key holding their original words as value
Eaxmple content
json {""abbreviation"": [""abbreviations"", ""abbreviation""]}
stem.json
This file contains stemmed english words as key holding their original words as value
Eaxmple content
json {""abandon"": [""abandoned"", ""abandonment"", ""abandoning"", ""abandon"", ""abandonable"", ""abandoner""]}"
Los Angeles Weather During 2014,Weather of LA throughout the year,Adnan Rasheed,2,"Version 1,2017-12-04",,CSV,3 KB,CC0,78 views,5 downloads,,0 topics,https://www.kaggle.com/adnanr94/los-angeles-weather-during-2014,"Context
This data set contains weather data for Los Angeles (L.A.) during 2014.
Content
Day : Number of day of an year, Type of Weather : this column tells type of weather in a day
Inspiration
The goal is to count how many times each type of weather occurred over the course of the year. During this , how to manipulate the data with lists, and make good progress towards that goal."
Births in U.S 1994 to 2003,Birth Dates In The United States,Adnan Rasheed,2,"Version 1,2017-12-05",,CSV,63 KB,CC0,142 views,12 downloads,,0 topics,https://www.kaggle.com/adnanr94/births-in-us-1994-to-2003,"Context
Births in U.S during 1994 to 2003.
Content
The data set has the following structure:
year - Year
month - Month
date_of_month - Day number of the month
day_of_week - Day of week, where 1 is Monday and 7 is Sunday
births - Number of births
Acknowledgements
Data set from the Centers for Disease Control and Prevention's National National Center for Health Statistics
Inspiration
Make a dictionary that shows total number of births on each day of week?"
