{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "# Improving deep neural networks\n",
    "\n",
    "</font>\n",
    "\n",
    "- Bias and Variance  \n",
    "- Regularization: Weight decay \n",
    "- Regularition: dropout\n",
    "- Getting more data\n",
    "- Early stopping\n",
    "- Vanishing / Exploding gradients\n",
    "- Normalization of input \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Bias and Variance\n",
    "\n",
    "</font>\n",
    "\n",
    "**High bias**  (underfitting): train classification error is high and dev classification error is high.\n",
    "<br>**High variance** (overfitting): train classification error is low but dev classification error is high.\n",
    "<br>**Optimal** : train classification error is low and dev classification error is low.\n",
    "<br><br>Note: need to consider **human classification error** e.g. \n",
    "<br>train classification error = 20% , dev classification error = 22% is **high bias** when human classification error is close to zero,<br> but it is close to **optimal classification** when human classification error is close to 20%.\n",
    "\n",
    "\n",
    "Main machine learning appoach: Resolve **high bias first**, then resolve **high variance**.\n",
    "\n",
    "The following can resolve **high bias**: \n",
    "- build larger network (more layers/more units) \n",
    "- train longer \n",
    "- apply advanced optimization algorithm\n",
    "- reduce learning rate \n",
    "- reduce regularization \n",
    "- normalize input \n",
    "- use advanced params initialization\n",
    "- use other network architecture \n",
    "- use more/other features\n",
    "\n",
    "The following can resolve **high variance**: \n",
    "- get more data (consider augmentated data)\n",
    "- increase regularization \n",
    "- use other network architecture \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Regularization: Weight decay (frobenius norm/ L2 norm regularization )\n",
    "\n",
    "\n",
    "</font>\n",
    "\n",
    "$$ \\mathcal{L} = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))\\quad\\rightarrow \\quad\n",
    "\\mathcal{L}_{F} = \\mathcal{L} +  \\frac{\\lambda}{2m} \\sum\\limits_{l = 1}^{L}  \\sum\\limits_{i = 1}^{n^{[l]}}  \\sum\\limits_{j = 1}^{n^{[l-1]}} (w^{[l]}_{ij}) ^2\n",
    "\\\\\n",
    "\\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}} \\,@\\, A^{[l-1] T} \\quad\\rightarrow \\quad \n",
    "\\frac{\\partial \\mathcal{L}_{F} }{\\partial W^{[l]}} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}}+ \\frac{\\lambda}{m} W^{[l]}\\\\\n",
    "\\quad \\\\\n",
    "W^{[l]}  = W^{[l]} - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}} \\quad=\\quad W^{[l]} - \\alpha \\cdot (\\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}}+ \\frac{\\lambda}{m} W^{[l]}) \\quad = \\quad W^{[l]} \\cdot (1 -\\frac{\\alpha \\lambda}{m}) - \\alpha \\cdot \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}}\n",
    "\\\\\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "### Implementation\n",
    "\n",
    "</font>\n",
    "\n",
    "- Develop cost with regularization\n",
    "- Correct the backward propagation \n",
    "- Correct the training model to consider lambda and new cost with regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Regularization: dropout\n",
    "\n",
    "\n",
    "</font>\n",
    "\n",
    "<img src = \"data/19_dropout2.png\" align = 'left' height = '500' width = '500'>\n",
    "\n",
    "<div style=\"clear:left;\"></div>\n",
    "\n",
    "Note: Dropout is only on training model stage but not at predicting \n",
    "\n",
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "### Implementation\n",
    "\n",
    "</font>\n",
    "\n",
    "Define `keep_prob` e.g. `0.8` that means to shutdown approximately `20%` of units. In forward propagation correct computing post-activation: \n",
    "\n",
    "- Generate random matrix of shape $A$:<br> \n",
    "    $\\quad $ `D = np.random.rand(A.shape[0], A.shape[1])` \n",
    "    \n",
    "- convert entries of D to 0 or 1 (using keep_prob as the threshold):<br>\n",
    "    $\\quad $ `D = D <keep_prob` \n",
    "    \n",
    "- shut down some neurons of A:<br>\n",
    "    $\\quad $`A = A * D`\n",
    "    \n",
    "- scale the value of neurons that haven't been shut down (**Inverted dropout**):<br>\n",
    "    $\\quad$ `A = A / keep_prob` \n",
    "\n",
    "Note: \n",
    "- You may apply different thresshold (`keep_prob`) values to defferent layers  \n",
    "- Don't apply dropout to the last layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Data Augmentation to reduce the variance \n",
    "\n",
    "</font>\n",
    "\n",
    "<img src = \"data/19_cat.png\" align = 'left' height = '300' width = '300'>\n",
    "\n",
    "<img src = \"data/19_cat_2.png\" align = 'left' height = '300' width = '300'>\n",
    "<img src = \"data/19_cat_3.png\" align = 'left' height = '300' width = '300'>\n",
    "<img src = \"data/19_cat_4.png\" align = 'left' height = '900' width = '900'>\n",
    "<div style=\"clear:left;\"></div>\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Early stopping\n",
    "\n",
    "</font>\n",
    "\n",
    "<img src = \"data/19_early_stop.png\" align = 'left' height = '500' width = '500'>\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Vanishing / Exploding gradients\n",
    "\n",
    "</font>\n",
    "\n",
    "<img src = \"data/19_van.png\" align = 'left' height = '500' width = '500'>\n",
    "<div style=\"clear:left;\"></div>\n",
    "\n",
    "Simplified case: all $b^{[l]}=0$ and all $g^{[l]}(z) = z$\n",
    "<br>Then $\\hat{y} = W^{[1]} \\, @ \\, W^{[1]} \\, @ \\, W^{[2]} \\, @ \\, ... \\, @ \\, W^{[L]} \\, @ \\, X$\n",
    "\n",
    "<br>Let's assume the init $W^{[l]} = \\begin{bmatrix} 0.9 & 0 \\\\ 0 & 0.9 \\end{bmatrix}$  Then $\\hat{y} =  \\begin{bmatrix} 0.9 & 0 \\\\ 0 & 0.9 \\end{bmatrix}^{L-1} @ W^{[L]}  @  X $\n",
    "<br>If $L$ is large then the gradients of first layers will be very small and the gradient steps is very little productive, thus the convergence is very slow.\n",
    "\n",
    "<br>\n",
    "In practice there is also case of exploding gradient. \n",
    "<br>If the size of layer $l$ is large then the $Z^{[l]} = W^{[l]} @ A^{[l-1]}+ b^{[l]}$ is very large. e.g. for single unit:  $z= w_{1} x_{1} + w_{2} x_{2} + ...+  w_{n} x_{n}$, to avoid output of layer very large, need to initialize the parameters being based on $n$: the larger $n$ the smaller $W$  \n",
    "<br> One of approach is to use Xavier initialization: $$W^{[l]} = random \\cdot (\\frac{1}{\\sqrt{(n^{[l-1]}}}) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "    \n",
    "## Normalizing inputs\n",
    "\n",
    "</font>\n",
    "\n",
    "Normalizing inputs allows to avoid exploding gradients and speeds up the training process\n",
    "\n",
    "<img src = \"data/19_norm_1.png\" align = 'left' height = '400' width = '400'>\n",
    "<img src = \"data/19_norm_3.png\" align = 'left' height = '400' width = '400'>\n",
    "<img src = \"data/19_norm_2.png\" align = 'left' height = '300' width = '300'>\n",
    "\n",
    "<img src = \"data/19_norm_4.png\" align = 'left' height = '350' width = '350'>\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
